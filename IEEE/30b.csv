"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"A Multi-Level Synergistic Image Decomposition Algorithm for Remote Sensing Image Fusion","X. Zou; W. Feng; Y. Quan; Q. Li; G. Dauphin; M. Xing","Department of remote sensing science and technology, School of Electronic Engineering, Xidian University, Xi'an, P. R. China; Research Institute of Advanced Remote Sensing Technology, Xidian University, Xi'an, P. R. China; Research Institute of Advanced Remote Sensing Technology, Xidian University, Xi'an, P. R. China; School of Physical Science and Technology, Northwestern Polytechnical University, Xi'an, P. R. China; Laboratory of Information Processing and Transmission, L2TI, Institut Galilée, University Paris XIII, France; Academy of Advanced Interdisciplinary Research, Xidian University, Xi'an, P. R. China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","3754","3757","Image fusion is a technique for improving the image quality, and image decomposition is one of a common method of image processing. In this paper, a novel multi-level synergistic image decomposition algorithm is proposed for the fusion of remote sensing images. The fusion framework decomposes different input images at different level to extract the salient and low-rank parts, respectively. The salient parts are fused using a designed weighted fusion method based on nuclearnorm, and the low-rank parts are fused using weighted average strategy. The proposed method shows a superior fusion performance in the compared experiments with PCA and GS methods.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884942","National Natural Science Foundation of China(grant numbers:61772397,12005159); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884942","image fusion;multi-level image decom-position;remote sensing;SAR image;Multispectral image","Image quality;Image decomposition;Sensors;Matrix decomposition;Remote sensing;Image fusion;Image reconstruction","geophysical image processing;image enhancement;image fusion;remote sensing","remote sensing image fusion;image quality;image processing;fusion framework;low-rank parts;salient parts;weighted fusion method;multilevel synergistic image decomposition algorithm;GS method;PCA method","","","","8","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Multi-source high resolution remote sensing image fusion based on intelligent decision","W. Zhang; S. Li; Z. Hao; S. Yang","Key Laboratory of Space Utilization, Technology and Engineering Center for Space Utilization Chinese Academy of Sciences, Beijing, China; Key Laboratory of Space Utilization, Technology and Engineering Center for Space Utilization Chinese Academy of Sciences, Beijing, China; Key Laboratory of Space Utilization, Technology and Engineering Center for Space Utilization Chinese Academy of Sciences, Beijing, China; Key Laboratory of Space Utilization, Technology and Engineering Center for Space Utilization Chinese Academy of Sciences, Beijing, China","2016 5th International Conference on Computer Science and Network Technology (ICCSNT)","19 Oct 2017","2016","","","370","375","Application fields of multisource high resolution remote sensing image fusion are expending ceaselessly. Especially, commercial remote sensing satellite data and image fusion algorithms are constantly emerging. This paper proposed an intelligent decision strategy that can be used to query various remote sensing data for recommending the optimal image fusion data combination and appropriate fusion algorithm. The system architecture was defined as four layers including interaction layer, service layer, component layer, and data layer. In order to extend more plug-ins, we introduced the Zero-C distributed service framework to invoke different types of service interface. Besides, a main interface of intelligent inference system is developed for users to submit image fusion tasks.","","978-1-5090-2129-1","10.1109/ICCSNT.2016.8070183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070183","Intelligent Decision;Image Fusion;High Resolution Remote Sensing Image;Expert Rules","Image fusion;Remote sensing;Inference algorithms;Transforms;Spatial resolution;Principal component analysis","geophysical image processing;image fusion;remote sensing;user interfaces","commercial remote sensing satellite data;intelligent decision strategy;optimal image fusion data combination;interaction layer;service layer;component layer;data layer;intelligent inference system;image fusion tasks;application fields;multisource high resolution remote sensing image fusion;remote sensing data querying;Zero-C distributed service framework;service interface;intelligent inference system interface","","","","27","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"An adaptive image fusion rule for remote sensing images based on the particle swarm optimization","R. Gharbia; A. H. El Baz; A. E. Hassanien","Nuclear Materials Authority, Egypt; Faculty of Science, Damietta University, Egypt; Faculty of Computers and Information, Cairo University, Cairo, Egypt","2016 International Conference on Computing, Communication and Automation (ICCCA)","16 Jan 2017","2016","","","1080","1085","This paper proposes an adaptive remote sensing image fusion technique based on the particle swarm optimization (PSO) to get the optimum fused image. Firstly, the principal component analysis (PCA) is applied such as feature extraction. The PCA is applied to the multi-spectral (MS) images to concentrate the spatial resolution. Secondly, the discrete cosine transform (DCT) transforms the images into the frequency domain. Thirdly, the particle swarm optimization (PSO) is used to obtain an adaptive weight of the fusion rule. Then, the adaptive fusion rule is applied to the DCT coefficients. Finally, the fused image is obtained through the inverse of principal component analysis (IPCA) and the inverse of the discrete cosine transform (IDCT). The different satellite sensors have different characteristics in reflecting spectral and spatial information of the same scene. Therefore, the proposed technique was implemented on many satellite images such as MODIS, ETM+, SPOT, ASTER and MSS satellite. The experimental results demonstrated that the adaptive remote sensing image fusion technique based on the PSO preserves the spectral resolution and improves the spatial information.","","978-1-5090-1666-2","10.1109/CCAA.2016.7813903","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813903","Remote sensing;Multi-sensor image fusion;Adaptive image fusion rule;principal component analysis (PCA);discrete cosine transform (DCT);the particle swarm optimization (PSO);panchromatic (Pan) and multi-spectral(MS) image;image fusion quality metrics","Image fusion;Discrete cosine transforms;Principal component analysis;Remote sensing;Satellites;Particle swarm optimization","discrete cosine transforms;feature extraction;geophysical image processing;image fusion;image resolution;inverse transforms;particle swarm optimisation;principal component analysis;remote sensing;spectral analysis","adaptive image fusion rule;particle swarm optimization;adaptive remote sensing image fusion;PSO;feature extraction;multispectral images;MS images;spatial resolution;discrete cosine transform transforms;DCT transforms;inverse principal component analysis;IPCA;inverse discrete cosine transform;IDCT;satellite sensors;satellite images;spatial information","","4","","20","IEEE","16 Jan 2017","","","IEEE","IEEE Conferences"
"Remote Sensing Image Fusion Technology Based on DSP","Y. Song; W. Feng; Y. Quan; Y. Liu; Q. Li; G. Dauphin; Y. Wang; M. Xing","Research Institute of Advanced Remote Sensing Technology, Xidian University, Xian, P. R. China; Research Institute of Advanced Remote Sensing Technology, Xidian University, Xian, P. R. China; Research Institute of Advanced Remote Sensing Technology, Xidian University, Xian, P. R. China; Research Institute of Advanced Remote Sensing Technology, Xidian University, Xian, P. R. China; School of Physical Science and Technology, Northwestern Polytechnical University, Xian, P. R. China; Laboratory of Information Processing and Transmission, L2TI, Institut Galilé, University Paris XIII, France; Research Institute of Advanced Remote Sensing Technology, Xidian University, Xian, P. R. China; Academy of Advanced Interdisciplinary Research, Xidian University, Xi'an, P. R. China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","3359","3362","In this paper, the fusion method of the weighted median filter Gram-Schmidt transform transplants to the digital signal processor (DSP). Image fusion technology has always been a key technology in the field of remote sensing image processing, but the algorithm is rarely implemented on mobile devices, so the scope of use has great limitations. The algorithm in the paper blends multispectral images and panchromatic images in the same location. The multispectral image is filtered by using a weighted median filter, and then the processed image and the panchromatic image are fused through the Gram-Schmidt transform. The filtering process reduces noise interference in the image, and the fused image combines the advantages of both images with high resolution and high color information. Due to the portability of DSP chips, the algorithm can be mounted on many mobile devices. Reduce the process of data transfer and make the image processing process more convenient.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884802","National Natural Science Foundation of China(grant numbers:61772397,12005159); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884802","Digital Signal Processor;Image Fusion;Weighted Median Filter;Gram-Schmidt Transform;Multispectral Image;Panchromatic Image","Numerical analysis;Image color analysis;Signal processing algorithms;Transforms;Linear algebra;Filtering algorithms;Information filters","digital signal processing chips;geophysical image processing;image colour analysis;image fusion;image processing;median filters;remote sensing;sensor fusion","sensing image fusion technology;fusion method;weighted median filter Gram-Schmidt transform transplants;digital signal processor;remote sensing image processing;mobile devices;paper blends multispectral images;panchromatic image;multispectral image;filtering process;fused image;DSP chips;image processing process","","","","9","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"A comparative analysis on GF-2 remote sensing image fusion effects","J. Chu; J. Fan; Y. Chen; F. Zhang","National Marine Environmental Monitoring Center, Dalian, China; National Marine Environmental Monitoring Center, Dalian, China; National Marine Environmental Monitoring Center, Dalian, China; National Marine Environmental Monitoring Center, Dalian, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","3770","3773","GF-2 is a civil remote sensing satellite developed by China in 2014. In addition to the highest spatial resolution, it is also featured with high radiometric precision and high positioning accuracy, etc.. However, regarding current supervision of sea resources, perfect data processing work flow has not been formed yet. In this paper, four methods of Brovey, Gram-Spectral pan sharpening (G-S), PC Spectral SharPening (PC) and Pansharp (PSH) are adopted to conduct comparative experiments for image fusion and evaluate images that have been fused both subjectively and objectively. Objective evaluation indexes including the average value, the variance, the entropy, the average gradient and the correlation coefficient are selected to compute and analyze fusion effects. According to the research results, it could be indicated that such four fusion methods of GF-2 Satellite are able to be used to significantly improve both the spatial resolution and the utilization ratio of images. And the method of PSH should be employed in case that GF-2 images are applied into visual interpretation and thematic charting.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729977","GF-2;image fusion;marine management;quality assessment","Satellites;Remote sensing;Spatial resolution;Satellite broadcasting;Image fusion;Entropy;Visualization","image fusion;image resolution;oceanographic techniques;remote sensing","GF-2 remote sensing image fusion effects;civil remote sensing satellite;China;AD 2014;radiometric precision;sea resources;data processing work flow;Brovey method;gram-spectral pan sharpening method;PC spectral sharpening method;PC spectral pansharp method;objective evaluation indexes;fusion methods;spatial image resolution;image utilization ratio;visual interpretation;thematic charting","","4","","8","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"An Algorithm Based on PCGP Image Fusion for Multi-Source Remote Sensing Images","Z. Ji; L. Xu; H. Wang; Y. Zhang","Key Laboratory of Marine Environmental Monitoring and Information Processing, Ministry of Industry; School of electronic and information engineering, Harbin Institute of Technology, Harbin, China; School of electronic and information engineering, Harbin Institute of Technology, Harbin, China; Key Laboratory of Marine Environmental Monitoring and Information Processing, Ministry of Industry","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2860","2863","Heterogeneous images imaged by different types of sensors have different imaging mechanisms, reflecting the characteristics of different sides of the target scene; while multi-source images formed by different working platforms or at different times have different imaging perspectives, and provide different target scene information. The use of multi-source heterogeneous images for fusion to obtain target and scene information more accurately and comprehensively has potential important applications in many fields such as military, medicine, and meteorology, and has become an important branch of image processing research. To this end, a PCGP algorithm is proposed in this paper to realize the fusion of optical images from different sources and SAR images. It first applies PCA transformation to the multi-source data images to obtain the principal component variables, then performs histogram matching on the first principal components of the transformed data sources, and finally uses the gradient pyramid decomposition algorithm to fuse the matched images to obtain a fused image. Then, the proposed fusion algorithm is tested in the fusion task of remote sensing images from different sources of GF2, GF6 and GF3. The experimental results show that the proposed fusion algorithm has better results.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884334","National Natural Science Foundation of China(grant numbers:61201304,61201308); Ministry of Industry and Information Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884334","Multi-sensor heterogeneous remote sensing image;image fusion;PCGP algorithm","Biomedical optical imaging;Transforms;Optical imaging;Optical sensors;Optical reflection;Task analysis;Remote sensing","feature extraction;geophysical image processing;gradient methods;image fusion;image matching;image processing;optical images;principal component analysis;radar imaging;remote sensing;sensor fusion;synthetic aperture radar","PCGP image fusion;multisource remote sensing;different imaging mechanisms;multisource images;different working platforms;different imaging perspectives;different target scene information;multisource heterogeneous images;potential important applications;image processing research;PCGP algorithm;optical images;SAR images;multisource data images;transformed data sources;gradient pyramid decomposition algorithm;matched images;fused image;fusion algorithm;fusion task;remote sensing images","","","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Remote sensing image fusion based on structured dictionary learning and joint sparse representation","X. Li; J. Wang; N. Ai; Y. Zhang; J. Peng","Faculty of Information Science and Technology, Northwest University, Shaanxi Province, Xi'an, China; Faculty of Information Science and Technology, Northwest University, Shaanxi Province, Xi'an, China; Faculty of Information Science and Technology, Northwest University, Shaanxi Province, Xi'an, China; Faculty of Information Science and Technology, Northwest University, Shaanxi Province, Xi'an, China; Faculty of Information Science and Technology, Northwest University, Shaanxi Province, Xi'an, China","2017 12th IEEE Conference on Industrial Electronics and Applications (ICIEA)","8 Feb 2018","2017","","","1200","1205","We describe a novel image fusion method for remote sensing based on structured dictionary learning and joint sparse representation which improves both quality and execution time. First, structured dictionaries are separately trained from the panchromatic (PAN) image and multispectral (MS) image by the double-sparsity model. Then, we use the joint sparse model to obtain the innovation components of the PAN image that are absent from the MS bands. Finally, the fused image is constructed by injecting the innovation components of PAN image into the low-resolution multispectral image by injection model. Experimental results show that our method has better performance than other state-of-the-art methods in terms of the spectral information, spatial resolution and computational efficiency.","2158-2297","978-1-5090-6161-7","10.1109/ICIEA.2017.8283022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8283022","Image fusion;structured dictionary learning;joint sparse representation;panchromatic image;multispectral image","Technological innovation;Dictionaries;Machine learning;Spatial resolution;Sparse matrices;Remote sensing;Image fusion","compressed sensing;image fusion;image representation;image resolution;remote sensing","PAN image;fused image;low-resolution multispectral image;image fusion method;remote sensing;panchromatic image;joint sparse model;image fusion;dictionary learning;sparse representation","","1","","17","IEEE","8 Feb 2018","","","IEEE","IEEE Conferences"
"Remote sensing image fusion using Statistical Univariate Finite Mixture Model in Shearlet Domain","B. Biswas; A. Dey; K. N. Dey","Department of Computer Science and Engineering, University of Calcutta; Department of Computer Science and Engineering, University of Calcutta; Department of Computer Science and Engineering, University of Calcutta","2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","28 Sep 2015","2015","","","2186","2191","Remote sensing image fusion is a process that integrates the spatial detail of panchromatic (PAN) image and the spectral information of a low-resolution multispectral (MS) image and produces a fused image that contain both high spatial and spectral details. In this paper, a new remote sensing image fusion method is proposed based on Statistical Univariate Finite Mixture Model (UFMM) in Shearlet Domain. Foremost, the Shearlet sub-bands for PAN and MS image are achieved by Shearlet Transform (ST). Latter, a novel fusion strategy is designed for both low and high pass sub-bands. Finally, the fused image is achieved by Inverse Shearlet Transform (IST). By comparing with the well-known methods in terms of several quality evaluation indexes, the experimental results on QuickBird and IKONOS images show the superiority of our method.","","978-1-4799-8792-4","10.1109/ICACCI.2015.7275940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275940","Image fusion;Univariate finite mixture model;Shearlet transform;Profile likelihood;Singular value decomposition","Transforms;Image fusion;Remote sensing;Gaussian distribution;Principal component analysis;Indexes;Mixture models","geophysical image processing;image fusion;image resolution;inverse transforms;mixture models;remote sensing;spectral analysis","statistical univariate finite mixture model;shearlet domain;remote sensing image fusion;panchromatic image;PAN image;spectral information;low-resolution multispectral image;MS image;high spatial details;spectral details;UFMM;shearlet subbands;low pass sub-bands;high pass sub-bands;inverse shearlet transform;IST;quality evaluation indexes;QuickBird images;IKONOS images","","1","","20","IEEE","28 Sep 2015","","","IEEE","IEEE Conferences"
"Splitting the hyperspectral-multispectral image fusion problem autonomously into weighted pan-sharpening tasks — The spectral grouping concept","C. Grohnfeldt; X. X. Zhu; R. Bamler","Remote Sensing Technology Institute, DLR German Aerospace Center, Wessling, Germany; Chair of Remote Sensing Technology, Technische Universitat Munchen (TUM), Germany; Chair of Remote Sensing Technology, Technische Universitat Munchen (TUM), Germany","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","23 Oct 2017","2015","","","1","4","Due to the trade-off between spatial and spectral resolution in optical imaging systems, hyperspectral instruments provide data of comparatively low spatial resolution. In order to develop higher resolution hyperspectral data via post-processing, image fusion algorithms can be applied to combine this data with imagery of lower spectral but higher spatial resolution. The similar but simpler pan-sharpening problem, which refers to the fusion of low resolution multispectral with higher resolution panchromatic imagery, has been extensively studied and solved in the past two decades. This paper aims at making the large number of sophisticated pan-sharpening methods usable to solving the more challenging hyperspectral-multispectral image fusion problem. We propose a generic algorithm - called Spectral Grouping - to devide the hyperspectral-multispectral image fusion problem autonomously into weighted pan-sharpening problems using the relative spectral responses of both instruments.","2158-6276","978-1-4673-9015-6","10.1109/WHISPERS.2015.8075412","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075412","image fusion;hyperspectral resolution enhancement;generalization of pan-sharpening methods;spectral grouping","Spatial resolution;Image fusion;Niobium;Instruments;Hyperspectral imaging","geophysical image processing;geophysical signal processing;geophysical techniques;image fusion;image resolution;optical images;remote sensing","hyperspectral instruments;comparatively low spatial resolution;higher resolution hyperspectral data;image fusion algorithms;higher spatial resolution;similar but simpler pan-sharpening problem;higher resolution panchromatic imagery;sophisticated pan-sharpening methods;hyperspectral-multispectral image fusion problem;weighted pan-sharpening problems;weighted pan-sharpening tasks;spectral grouping concept;spectral resolution;optical imaging systems","","","","13","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"A novel high resolution remote sensing image change detection algorithm based on image fusion and fuzzy clustering models","M. Liu; Y. Liu","Shandong University, SDU, Jinan, China; Beijing University of Posts and Telecommunication, BUPT, Beijing, China","2016 International Conference on Inventive Computation Technologies (ICICT)","26 Jan 2017","2016","3","","1","6","In this paper, we propose a novel high resolution remote sensing image change detection algorithm based on image fusion and fuzzy clustering models. The original images are fused based on the Wavelet to get the comprehensive differencing map which reserves enough variation characteristics as well as reducing the noise. The idea of modifying traditional clustering algorithms will enhance the final result of the change map. This method is able to achieve the higher detection accuracy based on the revised fuzzy clustering algorithm and could obtain better differencing map with integration of image fusion. Experiments on real images point out the effectiveness and feasibility of the proposed methodology compared with other state-of-the-art ones visually and numerically.","","978-1-5090-1285-5","10.1109/INVENTIVE.2016.7830104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7830104","Remote Sensing;Change Detection;Image Fusion;Fuzzy Clustering;High Resolution","Remote sensing;Image fusion;Spatial resolution;Change detection algorithms;Algorithm design and analysis;Clustering algorithms","image fusion;image resolution;pattern clustering","remote sensing image change detection algorithm;image fusion;fuzzy clustering models;wavelet;clustering algorithms;fuzzy clustering algorithm;differencing map","","2","","23","IEEE","26 Jan 2017","","","IEEE","IEEE Conferences"
"High resolution remote sensing image fusion method based on curvelet and HCS","S. Yang; S. Li; C. Chen; H. Zheng","Technology and Engineering Center for Space Utilization, University of Chinese Academy of Sciences, Beijing, China; Technology and Engineering Center for Space Utilization, University of Chinese Academy of Sciences, Beijing, China; Technology and Engineering Center for Space Utilization, University of Chinese Academy of Sciences, Beijing, China; Technology and Engineering Center for Space Utilization, University of Chinese Academy of Sciences, Beijing, China","2016 8th IEEE International Conference on Communication Software and Networks (ICCSN)","10 Oct 2016","2016","","","677","680","To obtain high spatial and spectral resolution image, we propose a novel method of high resolution remote sensing image fusion based on the second generation curvelet transform and hyperspherical color space transform which can fuse n-band multispectral and panchromatic images. The GF-1 satellite images are used as experimental data, and the fused image are quantitatively analyzed according to the mean, the standard deviation, the correlation coefficient, the information entropy and the average gradient. The results show that the proposed method has better performance than other fusion methods such as Principal Component Analysis, Gramm-Schmidt, and Hyperspherical Color Sharpening.","","978-1-5090-1781-2","10.1109/ICCSN.2016.7586609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7586609","high resolution;remote sensing;image fusion;curvelet transform;hyperspherical color sharpening","Transforms;Image fusion;Spatial resolution;Principal component analysis;Remote sensing;Image color analysis","curvelet transforms;geophysical image processing;image fusion;image resolution;principal component analysis","high resolution remote sensing image fusion method;spectral resolution image;spatial resolution image;second generation curvelet transform;hyperspherical color space transform;panchromatic images;multispectral images;GF-1 satellite images;mean deviation;standard deviation;correlation coefficient;information entropy;average gradient;principal component analysis;Gramm-Schmidt;hyperspherical color sharpening","","","","12","IEEE","10 Oct 2016","","","IEEE","IEEE Conferences"
"Research of Image Fusion Method about ZY-3 Panchromatic Image and Multispectral Image","C. Qiu; J. Wei; Q. Dong","College of Geomatics, Xi’an University of Science and Technology, Xi’an, China; College of Geomatics, Xi’an University of Science and Technology, Xi’an, China; Shaanxi Institute of Surveying and Mapping of Land, Xi’an, China","2018 Fifth International Workshop on Earth Observation and Remote Sensing Applications (EORSA)","3 Jan 2019","2018","","","1","5","At present, there are few researches on the fusion method of ZY-3 satellite. The paper takes the ZY-3 image of Xi'an as an example, the image fusion experiments were mainly performed using the Brovey transform and Gram-schmidt transform method. Then qualitative analysis from the following three aspects: sharpness, texture features, and hue. Quantitative evaluation from four aspects including standard deviation, information entropy, mean value and correlation coefficient. Combined with the results of qualitative and quantitative analysis above, it shows that Gram-schmidt transform is the most suitable fusion method for panchromatic image and multispectral image of ZY-3 satellite in the existing fusion method.","","978-1-5386-6642-5","10.1109/EORSA.2018.8598582","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598582","Remote Sensing Image Fusion;ZY-3 satellite image;Brovey transform;Gram-schmidt transform;Accuracy evaluation","Transforms;Remote sensing;Image fusion;Image resolution;Histograms;Satellites;Standards","geophysical image processing;image fusion;image texture;land cover;remote sensing","image fusion method;ZY-3 Panchromatic Image;multispectral image;ZY-3 satellite;image fusion experiments;correlation coefficient;Brovey transform;Gram-schmidt transform;texture features;hue;information entropy;Xi'an","","","","10","IEEE","3 Jan 2019","","","IEEE","IEEE Conferences"
"Heterogeneous Remote Sensing Image Fusion Based on Homogeneous Transformation and Target Enhancement","Q. Zhang; X. Wang; Z. Wang; G. Li","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","2022 IEEE International Conference on Unmanned Systems (ICUS)","29 Dec 2022","2022","","","688","693","Heterogeneous remote sensing (HRS) image fusion effectively improves image interpretability to better achieve the subsequent target detection tasks. Current HRS image fusion methods own limited target improvement performance leading to degraded target detection performance because they mainly focus on enhancing the geometric structure and texture details. In this paper, a novel homogeneous transformation and target enhancement (HTTE) module-based HRS image fusion method is designed. In HTTE, deep homogeneous feature fusion is used to transform HRS images into homogeneous images with similar image styles and then the proposal-copula-based target enhancement strategy is utilized to fuse homogeneous images, enhance target information, and relatively suppress background clutter therein. Experiments using measured high-resolution spaceborne and airborne inshore HRS data show that our proposed HTTE-based method improves the image fusion quality in comparison with current commonly used methods.","2771-7372","978-1-6654-8456-5","10.1109/ICUS55513.2022.9987218","National Key R&D Program of China(grant numbers:2021YFA0715201); Autonomous Research Project of Department of Electronic Engineering at Tsinghua University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9987218","heterogeneous remote sensing images;image fusion;homogeneous transformation;target enhancement","Integrated optics;Fuses;Object detection;Transforms;Optical imaging;Optical sensors;Clutter","geophysical image processing;image fusion;image texture;object detection;remote sensing","airborne inshore HRS data;current HRS image fusion methods own limited target improvement performance;deep homogeneous feature fusion;geometric structure;heterogeneous remote sensing image fusion;homogeneous images;homogeneous transformation;HRS images;HTTE-based method;image fusion quality;image interpretability;proposal-copula-based target enhancement strategy;similar image styles;subsequent target detection tasks;target detection performance;target enhancement module-based HRS image fusion method;target information;texture details","","","","22","IEEE","29 Dec 2022","","","IEEE","IEEE Conferences"
"Multispectral image fusion based on the common vector approach","K. Özkan; Ş. Işık; G. G. Dordinejad","Department of Computer Engineering, Eskisehir Osmangazi University, Eskisehir, Turkey; Department of Computer Engineering, Eskisehir Osmangazi University, Eskisehir, Turkey; Department of Electrical and Electronics Engineering, Eskisehir Osmangazi University, Eskisehir, Turkey","2016 International Symposium on INnovations in Intelligent SysTems and Applications (INISTA)","22 Sep 2016","2016","","","1","6","Multispectral image fusion has attracted much attention in the area of computer vision based image processing for remote sensing, industrial automation, surveillance, medical and defense applications. The process carried out in image fusion is combining useful information stated on different channels related to the same scene. Since the proposed image fusion technique greatly improve the performance of image classification, segmentation and edge detection, a new solution is required to combine multispectral images in order to get more informative and good visualized one as well as preserving the important details behind them. By considering this fact, we have introduced a new image fusion approach based on the Common Vector Approach (CVA) concept. By examining the visual results, one can observe that CVA method presents good results as compared with Principal Component Analysis (PCA), Independent Component Analysis (ICA) and Singular Value Decomposition (SVD).","","978-1-4673-9910-4","10.1109/INISTA.2016.7571825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7571825","multispectral image fusion;common vector approach;remote sensing","Image fusion;Principal component analysis;Databases;Remote sensing;Visualization;Image reconstruction;Measurement","computer vision;image classification;image fusion;image segmentation;independent component analysis;principal component analysis;remote sensing;singular value decomposition;vectors","multispectral image fusion;common vector approach method;CVA method;computer vision based image processing;remote sensing;industrial automation;surveillance;medical applications;defense applications;image classification;image segmentation;edge detection;principal component analysis;PCA;independent component analysis;ICA;singular value decomposition;SVD","","1","","11","IEEE","22 Sep 2016","","","IEEE","IEEE Conferences"
"A Distributed Parallel Optimization of Remote Sensing Image Fusion Algorithm Based on Nonlocal Tensor CP Decomposition","C. Qin; Z. Wu; Y. Zhang; J. Sun; Y. Xu; P. Zheng; Z. Wei","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2370","2373","Combining tensor decomposition and image nonlocal information for remote sensing image fusion method (NCTCP) can effectively preserve the spatial structure of the image, and can obtain good image fusion results, accordingly. However, the NCTCP that is a serial algorithm cannot handle massive remote sensing images due to the computing resources limitation of a single computer. To address this issue, we propose a distributed parallel nonlocal tensor CP decomposition optimization algorithm (DP_NCTCP) based on the Spark platform. The alternating direction method of multipliers(ADMM) in NCTCP is divided into two distributed computing subtasks that can be executed on Spark in parallel to improve the efficiency. Compared with NCTCP, the DP_NCTCP achieves high speedups without the degradation of fusion quality measures accuracy by fusing the real hyperspectral images.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884571","National Natural Science Foundation of China(grant numbers:61772274,61701238,61671243); Jiangsu Provincial Natural Science Foundation of China(grant numbers:BK20180018,BK20170858); Fundamental Research Funds for the Central Universities(grant numbers:30917015104,30919011103,30919011402); China Postdoctoral Science Foundation(grant numbers:2017M611814); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884571","Spark;CP Decomposition;Hyperspectral images;Multispectral images;Distributed parallel computation","Degradation;Tensors;Distributed databases;Convex functions;Sparks;Image fusion;Optimization","geophysical image processing;image fusion;optimisation;remote sensing;tensors","tensor decomposition;remote sensing image fusion;spatial structure;serial algorithm;massive remote sensing images;computing resource limitation;single computer;DP_NCTCP;alternating direction method;distributed computing subtasks;hyperspectral images;distributed parallel nonlocal tensor CP decomposition optimization;fusion quality measure accuracy","","","","7","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Evaluation on BJ-2 Image Fusion Algorithms for Satellite Images of Coastal Aquaculture Sea Areas","J. Chu; Y. Chen; J. Zhao; F. Wang","National Marine Environmental Monitoring Center, Dalian, China; National Marine Environmental Monitoring Center, Dalian, China; National Marine Environmental Monitoring Center, Dalian, China; National Marine Environmental Monitoring Center, Dalian, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","2826","2829","Coastal aquaculture sea areas is of great significance to monitor these aquaculture sea areas through the means of a remote sensing technology. The images after fusion are provided with both the high-resolution feature of panchromatic images and the spectral characteristics of multi-spectral images, thereby strengthening the image information and facilitating its application. According to the characteristics of the aquaculture sea area in concern, five fusion algorithms, namely, Brovey Transform, Gram-Schmidt Transform, Principal Component Analysis, Pan Sharp, and Nearest Neighbor Diffusion PanSharpening were adopted to fuse the BJ-2 multi-spectral and panchromatic image data of the areas in the shallow sea raft culture, cage culture, and reclamation culture acquired in Zhangzhou coastal aquaculture sea area of the Fujian Province. Then the fusion images are evaluated subjectively and objectively to provide the best fusion scheme for monitoring aquaculture by remote sensing. The results showed that: (1) the five fusion algorithms are able to significantly improve both the spatial resolution and the utilization ratio of the BJ-2 satellite images; (2) the fusion effects of PSH method can provide the most optimum solution in terms of spectral retentivity and detail expression, and are the best in all the three fusion experiments on aquaculture sea information; (3) the acquired bright-color and high-contrast images make the fusion effects of NNDiffuse the best in terms of image contrast and information enhancement; (4) PSH algorithm is appropriate for use when BJ-2 is used for visual interpretation and thematic charting of shallow sea raft culture, cage culture and reclamation culture and other information; on the other hand, it is recommended to use NNDiffuse for automatic classification and identification.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898091","BJ-2;Image fusion;Aquaculture sea area;Quality evaluation.","Aquaculture;Remote sensing;Sea measurements;Satellites;Image fusion;Principal component analysis;Transforms","aquaculture;geophysical image processing;geophysical techniques;image fusion;image resolution;principal component analysis;remote sensing","remote sensing technology;panchromatic images;spectral characteristics;multispectral images;image information;panchromatic image data;shallow sea raft culture;cage culture;reclamation culture;Zhangzhou coastal aquaculture sea area;fusion images;fusion scheme;monitoring aquaculture;BJ-2 satellite images;fusion effects;fusion experiments;aquaculture sea information;high-contrast images;BJ-2 Image Fusion Algorithms;coastal aquaculture sea areas;Brovey Transform;Gram-Schmidt Transform;Principal Component Analysis;Nearest Neighbor Diffusion PanSharpening","","2","","11","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"A Proposed Methodology for the Fusion of Microwave and Optical Remote Sensing Data","K. S. Ulabhaje; M. S. Arya","Computer Science & Engineering Department, GHRCE, Nagpur; HOD Department of Computer Science, GHRCE, Nagpur","2019 3rd International Conference on Computing Methodologies and Communication (ICCMC)","29 Aug 2019","2019","","","731","736","The merging of images obtained by satellites through remote sensing has evolved into an established protocol. In popular parlance such blending is known as image fusion. This is done chiefly because it gives myriad advantages. Image fusion comes in extremely useful in the observation, study and analysis of diverse fields, including environment, agriculture and other related areas. In essence, what happens in image fusion is that the needed data or information is gleaned from numerous images. These images then are coalesced to form fewer pictures. The ideal, of course, is the blending of them into a lone picture. This is highly sought-after because the image thus intermingled is said to contain all relevant data and, moreover, is more suitable and error-free than an image secured from one single source. Needless to say, it also incorporates all the information that is needed. Besides this, there are other benefits. For one, it curtails the volume of data. For another, it produces images that are pertinent and apt. This paper's chief objective is to proffer a suggested methodology on the fusion between the capabilities of optical and microwave satellite images and to improve the visible quality of Landsat image.","","978-1-5386-7808-4","10.1109/ICCMC.2019.8819731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8819731","Image Fusion;Remote Sensing;Satellite Images;Wavelet Transforms;Image Processing;Landsat;Sentinel;PSNR;Estimated Parameters;Quality","Remote sensing;Sensor fusion;Radar imaging;Data integration;Image fusion","data visualisation;geophysical image processing;image fusion;remote sensing;sensor fusion","optical remote sensing data;image fusion;optical satellite images;microwave satellite images;Landsat image","","","","36","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"FY-3D/MERSI-II Meteorological Satellite Image Fusion Method and its Application","Z. Yunyu; C. Yingying; W. Ming; H. Mingqiong; T. Jing","School of Resource and Environmental Science, Wuhan University, Wuhan, China; Hubei Meteorological Service Center, Wuhan, China; Institute of Heavy Rain, CMA, Wuhan, China; Hubei Meteorological Service Center, Wuhan, China; Hubei Meteorological Service Center, Wuhan, China","2020 IEEE 3rd International Conference on Information Systems and Computer Aided Education (ICISCAE)","2 Nov 2020","2020","","","237","240","In order to solve the problem that the spatial resolution of some channels of the FY-3D satellite medium-resolution spectral imager (MERSI-II) is not high enough, 19 channel images with a spatial resolution of 1000 meters are merged and enhanced. Based on the geometric correction of the original data, the Gram-Schimidt Transform remote sensing fusion algorithm is selected, and the panchromatic image extracted from the MERSI-II 250-meter spatial resolution image is used. Then selected the WMO “natural color” synthesis scheme is to display RGB three-color synthesis on the fused image. The result of FY-3D satellite image data fusion shows that the fused image has clear colors, which not only retains the multi-spectral characteristics of 1000-meter channel data, but also has the high-resolution advantage of 250-meter channel data. The comparison experiment found that compared with the original image, the fused image has greatly improved the ability to recognize the topography of rivers, lakes, land and sea boundaries, and mountain range trends. And the ability to recognize snow, vegetation cover, and structural features of cloud has also been significantly enhanced. The fusion algorithm can greatly improve the remote sensing fine analysis capability of FY-3D / MERSI - II images in the fields of disaster prevention and mitigation and ecological civilization construction.","","978-1-7281-8304-6","10.1109/ICISCAE51034.2020.9236890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9236890","FY-3D;Image fusion;spatial resolution;remote sensing","Spatial resolution;Remote sensing;Satellites;Monitoring;Image fusion;Snow;Image color analysis","ecology;geophysical image processing;geophysical techniques;image colour analysis;image fusion;image resolution;remote sensing;sensor fusion","FY-3D satellite medium-resolution spectral imager;channel images;Gram-Schimidt Transform remote sensing fusion algorithm;panchromatic image;spatial resolution image;WMO natural color synthesis scheme;FY-3D satellite image data fusion;channel data;high-resolution advantage;MERSI-II meteorological satellite image fusion method;RGB three-color synthesis;topography;rivers;lakes;sea boundaries;land boundaries;snow;mountain range trends;vegetation cover;structural features","","","","24","IEEE","2 Nov 2020","","","IEEE","IEEE Conferences"
"A unified framework for spatio-temporal-spectral fusion of remote sensing images","X. Meng; H. Shen; L. Zhang; Q. Yuan; H. Li","Wuhan University, Wuhan, Hubei, CN; Wuhan University, Wuhan, Hubei, CN; Wuhan University, Wuhan, Hubei, CN; Wuhan University, Wuhan, Hubei, CN; Wuhan University, Wuhan, Hubei, CN","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","2584","2587","In this paper, a unified framework for the spatio-temporal-spectral fusion of remote sensing images is proposed. The relationships between the observed images and the desired image are first established based on general image observation models. Maximum a posteriori (MAP) theory is then employed to formulate the unified fusion framework. The proposed method is able to fuse images from an arbitrary number of optical sensors with different spatial, temporal, and spectral resolutions. The experimental results verify the effectiveness of the proposed method.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326340","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326340","unified framework;image fusion;spatio-temporal-spectral resolutions;maximum a posteriori (MAP);remote sensing","Remote sensing;Spatial resolution;MODIS;Image fusion;Sensors;Indexes","geophysical image processing;geophysical techniques;image fusion;image resolution;optical sensors;remote sensing","spatio-temporal-spectral fusion;remote sensing images;general image observation models;maximum a posteriori theory;unified fusion framework;optical sensors;spatial resolution;temporal resolution;spectral resolution","","8","","13","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"A compressed-sensing-based approach for remote sensing image fusion","M. Khateri; H. Ghassemian","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2016 24th Iranian Conference on Electrical Engineering (ICEE)","10 Oct 2016","2016","","","1809","1814","Remote sensing image pan-sharpening is an image fusion process which fuses a low-resolution multi-spectral (LRMS) image with its corresponding high-resolution panchromatic (HRP) image to create a high-resolution multi-spectral (HRMS) image. In this paper, pan-sharpening methods based on compressed sensing (CS) theory are proposed. In the proposed methods, the HRP and LRMS dictionaries are learned from the input images (HRP, LRMS). Moreover, this paper proposes a new algorithm to reconstruct the unknown HRMS image by considering remote sensing physics. The proposed algorithm extracts non-overlapping patches from input images and provides an initial estimation of HRMS dictionary. Then, the initial HRMS dictionary and LRMS image are used to reconstruct unknown HRMS image. The algorithm neither needs to extract overlapping patches, nor training dataset. So, it makes the proposed methods fast and practical. Furthermore a high-pass filter is used to preserve more details in the fusion process. The proposed methods are tested on WorldView-2 and QuickBird satellite images and these results are compared with several popular and state-of-the-art methods quantitatively and visually.","","978-1-4673-8789-7","10.1109/IranianCEE.2016.7585815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7585815","Remote sensing;pan-sharpening;image fusion;compressed sensing (CS);dictionary learning","","compressed sensing;geophysical image processing;image fusion;image reconstruction;image resolution;remote sensing","remote sensing image fusion;compressed-sensing-based approach;low-resolution multispectral image;high-resolution panchromatic image;high-resolution multispectral image;compressed sensing theory;unknown HRMS image reconstruction;remote sensing image pan-sharpening;remote sensing physics;nonoverlapping patches;HRMS dictionary;LRMS dictionary;QuickBird satellite images;WorldView-2 images","","1","","23","IEEE","10 Oct 2016","","","IEEE","IEEE Conferences"
"Remote Sensing Image Fusion Method Based on Adaptive Fractional Differential","X. Li; X. Nie; Z. Ding; H. Huang; Y. Zhang; L. Feng","College of Electronics and Information Engineering, Chongqing Three Gorges University, Chongqing, China; College of Electronics and Information Engineering, Chongqing Three Gorges University, Chongqing, China; Radar Research Laboratory, Beijing Institute of Technology, Beijing, China; College of Electronics and Information Engineering, Chongqing Three Gorges University, Chongqing, China; College of Electronics and Information Engineering, Chongqing Three Gorges University, Chongqing, China; College of Electronics and Information Engineering, Chongqing Three Gorges University, Chongqing, China","2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)","21 Aug 2020","2019","","","1","5","In order to resolve the issues of spectral distortion and lack of details in image fusion, a new remote sensing image fusion method based on adaptive fractional differential is proposed, which can dynamically enhance the remote sensing image according to different image features. Firstly, up-sampling operation is used in multispectral image, and the multispectral image is transformed by IHS transformation to obtain luminance component. Secondly, panchromatic image and luminance component are enhanced by adaptive fractional differential. Next, the enhanced luminance component and panchromatic image are integrated to obtained a new image, and then the new image is dealt with by histogram matching. Finally, the fused image is reconstructed by adopting the inverse IHS transformation. Experimental results indicate that the proposed method can enhance the details and textures while preserving spectral information and smooth areas in image fusion. The proposed method outperforms other traditional fusion algorithms both in visual effect and objective evaluation indexes.","","978-1-7281-2345-5","10.1109/ICSIDP47821.2019.9173428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173428","image processing;remote sensing image fusion;adaptive fractional differential;intensity-hue-saturation transformation;histogram matching","","brightness;geophysical image processing;image fusion;image texture;remote sensing","remote sensing image fusion method;spectral distortion;image features;multispectral image;panchromatic image;enhanced luminance component;fused image;IHS transformation;histogram matching;objective evaluation indexes;adaptive fractional differential;up-sampling operation;image textures","","","","12","IEEE","21 Aug 2020","","","IEEE","IEEE Conferences"
"Urban Landscape Remote Sensing Image Fusion Based on 3D Box Dimension Algorithm","Z. Shen; Z. Li","Heilongjiang University, Harbin, China; Heilongjiang University, Harbin, China","2022 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)","23 May 2022","2022","","","1279","1282","Remote sensing of urban landscape is a comprehensive detection method formed by using human visual characteristics to observe natural laws and man-made things and extract rich information from images. It features visibility, dynamics, and rapid analysis capabilities. We can improve the utilization efficiency of urban land resources and achieve sustainable development through the classification and planning of landscapes. This paper mainly uses the experimental method and the comparative method to analyze the urban landscape remote sensing image fusion in detail. The experimental data shows that the 3D box dimension value of different urban landscapes is around 2.9. Urban landscape remote sensing image fusion is a complex multi-source nonlinear, data point and regional coordinated development problem. The three-dimensional box dimension value measured in this paper is generally high, and it is necessary to optimize the accurate basic data and the calculation methods and algorithms built into the software.","","978-1-6654-0902-5","10.1109/IPEC54454.2022.9777539","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9777539","3D box dimension value;urban landscape;remote sensing image;fusion algorithm","Visualization;Three-dimensional displays;Software algorithms;Spatial databases;Software;Software measurement;Sustainable development","feature extraction;geophysical image processing;image classification;image fusion;remote sensing;stereo image processing;sustainable development;town and country planning","three-dimensional box dimension value;urban landscape remote sensing image fusion;3D box dimension algorithm;comprehensive detection method;human visual characteristics;natural laws;man-made things;urban land resources;classification;sustainable development;multisource nonlinear;regional coordinated development;data point","","","","12","IEEE","23 May 2022","","","IEEE","IEEE Conferences"
"Unsupervised Change Detection in Remote sensing Image Based on Image Fusion in Nonsubsampled Shearlet Transform Domain and fuzzy k-means clustering","D. Lv; F. Li; Q. Guo; X. Wang; T. Chen","State Grid Xinjiang Electric Power Co., Power Science Research Institute, Urumqi, China; State Grid Xinjiang Electric Power Co., Power Science Research Institute, Urumqi, China; State Grid Xinjiang Electric Power Co., Power Science Research Institute, Urumqi, China; State Grid Xinjiang Electric Power Co., Power Science Research Institute, Urumqi, China; State Grid Xinjiang Electric Power Co., Power Science Research Institute, Urumqi, China","2018 IEEE 3rd Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","16 Dec 2018","2018","","","1568","1573","In order to improve the detection precision and shorten the detection time, a novel unsupervised change detection method based on image fusion in nonsubsampled shearlet transform(NSST)domain and fuzzy k-means clustering is proposed in this paper. Frost filter is used to reduce the noise of the experimental images. The proposed neighborhood ratio operator and the common log-ratio operator are used to obtain difference images. In order to utilize fully the complementary information of the neighborhood ratio and the ratio images to obtain a better difference image, a novel fusion strategy in NSST domain is proposed. Since there are still noise in the difference images, the image denoising method with adaptive Bayes threshold in the NSST domain is applied to the high frequency coefficients of the difference images to reduce the noise. And then the proposed fusion strategy is applied to the low frequency bands and the denoised high frequency bands for getting the fused difference image. The change detection map is obtained by clustering the fused difference images utilizing k-means algorithm into two disjoint classes: changed and unchanged. The experimental results clearly show that the proposed detection operator has better detection performance and shorter running time, compared with the other reported algorithms.","2381-0947","978-1-5386-4509-3","10.1109/IAEAC.2018.8577920","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8577920","Image fusion in nonsubsampled shearlet domain;Change detection;Image denoising method with adaptive Bayes threshold;Fuzzy k-means clustering","Clustering algorithms;Change detection algorithms;Transforms;Image edge detection;Image fusion;Fuses","Bayes methods;image denoising;image filtering;image fusion;pattern clustering;remote sensing;unsupervised learning;wavelet transforms","remote sensing image;image fusion;nonsubsampled shearlet transform domain;neighborhood ratio operator;common log-ratio operator;ratio images;fusion strategy;NSST domain;image denoising method;change detection map;detection operator;adaptive Bayes threshold;Frost filter;fuzzy k-means clustering;unsupervised change detection method","","1","","23","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Hyperspectral and multispectral image fusion based on constrained CNMF unmixing","Y. Zhang; Y. Gao; Y. Liu; M. He","School of Electronics and Information, Northwestern Polytechnical University, Xi'an, P. R. China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, P. R. China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, P. R. China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, P. R. China","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","23 Oct 2017","2015","","","1","4","Hyperspectral (HS) remote sensing image with finer spectral information has great advantages in feature identification and classification. However, the spatial resolution of HS image is usually low due to physical limitation and data transfer requirement. In this paper, the low-spatial-resolution HS image is fused with the high-spatial-resolution multispectral (MS) image of the same observation scene to improve its spatial resolution. A novel spectral unmixing based HS and MS image fusion approach (VSC-CNMF based fusion approach) is proposed, in which CNMF with minimum endmember simplex volume and abundance sparsity constraints is employed for coupled unmixing of HS and MS images. The fused image is built by the product of endmember signature matrix derived from HS image unmixing and fractional abundance matrix derived from MS image unmixing. Simulative experiments illustrate that compared with the CNMF based fusion technique, the newly proposed VSC-CNMF based fusion algorithm is capable of producing fused images closer to the reference image with better spectral fidelity.","2158-6276","978-1-4673-9015-6","10.1109/WHISPERS.2015.8075463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075463","Hyperspectral;image fusion;resolution enhancement;unmixing","Image fusion;Spatial resolution;Matrix decomposition;Convergence;Cost function;Hyperspectral sensors","image fusion;remote sensing","multispectral image fusion;hyperspectral remote sensing image;data transfer requirement;low-spatial-resolution HS image;high-spatial-resolution multispectral image;minimum endmember simplex volume;HS image unmixing;MS image unmixing;spectral unmixing;hyperspectral image fusion;CNMF unmixing;VSC-CNMF based fusion algorithm;endmember signature matrix","","3","","6","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Fusion of multispectral and SAR images using sparse representation","H. Zhang; H. Shen; L. Zhang","School of Resource and Environmental Sciences, Wuhan University, P. R. China; School of Resource and Environmental Sciences, Wuhan University, P. R. China; Mapping and Remote Sensing, Wuhan University, P. R. China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","7200","7203","Complementary information from multi-sensor can be integrated to effectively solve many problems in remote sensing application. Synthetic Aperture Radar (SAR) imaging can be a feasible alternative to traditional optical remote sensing techniques because it is independent of solar illumination and weather conditions. This paper proposes a novel fusion framework combining IHS transform with sparse representation theory to fuse multispectral and SAR images. In addition, the simultaneous orthogonal matching pursuit (SOMP) technique is introduced to guarantee the efficiency. Experiments on various datasets have verified the effectiveness of proposed method.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730878","Image fusion;Synthetic Aperture Radar;sparse representation;simultaneous orthogonal matching pursuit","Synthetic aperture radar;Remote sensing;Image fusion;Optical imaging;Optical sensors;Transforms;Adaptive optics","geophysical image processing;image fusion;remote sensing by radar;synthetic aperture radar","multispectral image fusion;SAR image fusion;multisensor complementary information;remote sensing application;synthetic aperture radar;traditional optical remote sensing techniques;weather conditions;solar illumination;novel fusion framework;IHS transform;simultaneous orthogonal matching pursuit;SOMP technique","","4","","11","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"A novel remote sensing image fusion algorithm using ICA bases","C. S. Manu; C. V. Jiji","Department of Electronics & Communication, College of Engineering Trivandrum, Thiruvananthapuram, Kerala, IN; Department of Electronics & Communication, College of Engineering Trivandrum, Thiruvananthapuram, Kerala, IN","2015 Eighth International Conference on Advances in Pattern Recognition (ICAPR)","2 Mar 2015","2015","","","1","6","Image fusion is the method of gathering information from images obtained from sensors of different modalities in order to create an image that is more detailed than the input images. This paper presents a novel method for fusion of low-resolution multispectral (MS) images with a high-resolution panchromatic (PAN) image using Stationary Wavelet Transform (SWT) and Independent Component Analysis (ICA). Here, the MS image is modified using the PAN image at the wavelet decomposition level, followed by performing fast ICA algorithm on the individual bands, and thereby providing better resolution to the output image. In this paper, we also use parameters like correlation coefficient (CC), peak signal-to-noise ratio (PSNR), mean structural similarity measure (MSSIM) and edge stability mean square error (ESMSE) to measure the quality of the fused image. From experimental results, we observed that SWT method efficiently preserves spectral information, while ICA efficiently preserves spatial information. The proposed method is found to show better performance compared to other standard fusion methods like IHS fusion, PCA fusion, DWT fusion, SWT fusion and ICA fusion.","","978-1-4799-7458-0","10.1109/ICAPR.2015.7050690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7050690","Remote Sensing Image fusion;Mutlimodal Image fusion;Stationary Wavelet Transform;Independent Component Analysis;Wavelet analysis","Discrete wavelet transforms;PSNR;Principal component analysis;Spatial resolution","image fusion;independent component analysis;least mean squares methods;principal component analysis;remote sensing;wavelet transforms","remote sensing image fusion algorithm;ICA bases;low-resolution multispectral images;MS images;high-resolution panchromatic image;PAN image;stationary wavelet transform;independent component analysis;wavelet decomposition level;correlation coefficient;CC;peak signal-to-noise ratio;PSNR;mean structural similarity measure;MSSIM;edge stability mean square error;ESMSE;SWT method;IHS fusion;PCA fusion;DWT fusion;SWT fusion;ICA fusion","","3","","19","IEEE","2 Mar 2015","","","IEEE","IEEE Conferences"
"Evaluation of Image Fusion Methods Applied to the ZY1-04 Data of Chengdu Area","F. Zhong; W. Yang","Key Laboratory of Earth Exploration and Information Techniques of Ministry of Education, (Chengdu University of Technology), School of Geoscience and Technology, (Southwest Petroleum University), Chengdu, China; Key Laboratory of Earth Exploration and Information Techniques of Ministry of Education, (Chengdu University of Technology), Chengdu, China","2021 IEEE International Conference on Information Communication and Software Engineering (ICICSE)","23 Apr 2021","2021","","","140","143","Different remote sensing datasets have considerably heterogeneous natures. To utilize multisource data, image fusion has been widely researched as an effective and efficient processing approach in remote sensing applications. One of the most challenging problems is how to obtain images with higher spatial and spectral resolution through pan-sharpening. It is also an unsolved problem in the application of domestically-produced satellites which have been developed rapidly in recent years. To find an appropriate fusion method for the ZY1-04 data of Chengdu area, we analyzed four representative algorithms: GramSchmidt, Pansharp, NNDiffuse and SFIM. Image preprocessing was carefully done and the fundamental steps were analyzed. Performance assessment of the fusion algorithms was given both qualitatively and quantitatively. The result suggests that GramSchmidt algorithm generally has the best overall performance for the ZY1-04 data, while other methods are still distinctive in certain aspects.","","978-1-6654-1919-2","10.1109/ICICSE52190.2021.9404082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9404082","image fusion;ZY1-04 data;pixel-level;multispectral;panchromatic","Satellites;Conferences;Software algorithms;Spatial resolution;Remote sensing;Image fusion;Software engineering","geophysical image processing;image fusion;image resolution;remote sensing;sensor fusion","image fusion methods applied;ZY1-04 Data;Chengdu area;different remote sensing datasets;considerably heterogeneous natures;multisource data;effective processing approach;efficient processing approach;remote sensing applications;higher spatial resolution;spectral resolution;appropriate fusion method;ZY1-04 data;image preprocessing;fusion algorithms","","","","21","IEEE","23 Apr 2021","","","IEEE","IEEE Conferences"
"UFN-GAN: An unsupervised generative adversarial network for remote sensing image fusion","H. Dai; X. Liu; Y. Qiao; K. Zheng; X. Xiao; Z. Cai","School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Computer Science, China University of Geosciences, Whuhan, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","1803","1808","Different sensors acquire different images in the same area, such as multi-spectral (MS) images and panchromatic (PAN) images. Normally, the MS images possess high spectral resolution but low spatial resolution, while PAN images are opposite in the distribution of spectral and spatial information. Image fusion is a common method to obtain the information of PAN and MS images simultaneously. To generate clearer fusion image with abundant information, we design an unsupervised fusion net based on generative adversarial network (GAN), named UFNGAN for remote sensing image fusion. In our proposed UFNGAN, an adversarial net is designed between our generator and two discriminators to adequately retain the spectral and spatial information of original images without supervision. MS images and PAN images are fused by our generator, which consists of an encoder and a decoder. Our encoder is used to extract deeper feature maps of the original images, and the decoder is applied to rebuild images. Furthermore, the Spatial-Information-Enhancement (SIE) model is utilized to obtain spatial information of MS images for enhancing PAN image, and the Edge-Detection-Registration (EDR) method is applied to register the original images to avoid fused images distortion. At last, experiments are performed on QuickBird and GaoFen-2 datasets.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727490","Unsupervised learning;Image fusion;Generative adversarial network;Deep learning;Remote sensing images","Image edge detection;Generative adversarial networks;Feature extraction;Distortion;Generators;Decoding;Sensors","geophysical image processing;image fusion;image resolution;neural nets;remote sensing;unsupervised learning","remote sensing image fusion;multispectral images;panchromatic images;MS images;high spectral resolution;PAN images;spatial information;fusion image;unsupervised fusion net;spectral information;original images;fused images distortion;unsupervised generative adversarial network;spatial-information-enhancement model;UFNGAN;QuickBird;GaoFen-2 datasets;encoder","","","","10","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Sparse Representation-Based Image Fusion for Multi-Source NDVI Change Detection","M. Zhang; Y. Chen; S. Li; X. Tian","Electronic Information School, Wuhan University, China; Electronic Information School, Wuhan University, China; Electronic Information School, Wuhan University, China; Electronic Information School, Wuhan University, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","692","695","The normalized differential vegetation index (NDVI) is a useful index for change detection in remote sensing vegetation analysis. Multi-source NDVI change detection, which utilizes the NDVI information at different time from multiple satellites, can solve the problem of long-revisiting periods for a single source (satellite). However, the spatial resolution of NDVI calculated from the multispectral images of different satellites is different. A sparse representation-based image fusion method is proposed to improve the spatial resolution of NDVI. First, a high spatial-resolution vegetation index (HRVI) is utilized. The proposed method is based on the assumption that NDVI and HRVI with different resolutions will have the same sparse coefficients under some specific dictionaries. In the experiment, the proposed method is compared with several state-of-the-art methods to demonstrate its efficiency. Furthermore, its application in multi-source NDVI change detection verified by datasets from GF-1 and GF-2 satellites.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324353","National Natural Science Foundation of China(grant numbers:61971315); Natural Science Foundation of Hubei Province(grant numbers:2018CFB435); Fundamental Research Funds for the Central Universities(grant numbers:2042018kf1009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324353","Image fusion;sparse representation;multi -source;change detection","Spatial resolution;Satellites;Vegetation mapping;Pansharpening;Image fusion;Indexes;Remote sensing","image fusion;vegetation mapping","multisource NDVI change detection;normalized differential vegetation index;remote sensing vegetation analysis;NDVI information;single source;sparse representation-based image fusion method;spatial-resolution vegetation index;multispectral images","","2","","21","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Remote sensing image fusion using intensity-hue-saturation transform and steerable pyramid transform","S. Xu; Z. Li","Jinan Preschool Education College, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China","2017 Chinese Automation Congress (CAC)","1 Jan 2018","2017","","","4959","4963","Remote sensing image fusion (also known as Pan-sharpening) can be used to get a high spatial resolution multispectral image by fusing a low spatial resolution multispectral image with a high-spatial resolution panchromatic image of the same scene. We developed a pan-sharpening algorithm by applying the IHS transform and SPT (steerable pyramid transform). SPT transformation is used to decompose the panchromatic image (histogram matched with the multispectral images) into SPT coefficients. The composite SPT coefficients are obtained by substituting the subband of the SPT coefficients in the lowest frequency domain with the multispectral image's IHS intensity proportion in the colour space. Inverse SPT on the composite SPT coefficients is performed to produce a new intensity component. Finally, through the IHS inverse colour space transform, we can get the pan-sharpened multispectral image. Pan-sharpening results using real data show the pan-sharpening performance of the developed algorithm is better than the performance of the traditional methods, such as the algorithms based on IHS colour transform, DWT muliti-resolution wavelet transform or DWFT multi-resolution wavelet frame transform.","","978-1-5386-3524-7","10.1109/CAC.2017.8243658","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8243658","pan-sharpening;remote sensing;IHS colour transform;steerable pyramid transform","Image color analysis;Remote sensing;Spatial resolution;Discrete wavelet transforms","discrete wavelet transforms;geophysical image processing;image colour analysis;image fusion;image resolution;remote sensing","intensity component;IHS inverse colour space;pan-sharpening performance;IHS colour;multiresolution wavelet frame;intensity-hue-saturation;steerable pyramid;pan-sharpening algorithm;SPT transformation;composite SPT coefficients;multispectral image;remote sensing image fusion;inverse SPT coefficients","","1","","10","IEEE","1 Jan 2018","","","IEEE","IEEE Conferences"
"Integration of Landsat-8 Multispectral Band and Cartosat-1 Digital Elevation Model using Image Fusion Techniques","L. Bopche; P. P. Rege","Electronics and Telecommunication College of Engineering, Pune; Electronics and Telecommunication College of Engineering, Pune","2021 8th International Conference on Signal Processing and Integrated Networks (SPIN)","19 Oct 2021","2021","","","15","19","Significant technical restrictions, such as limited data storage on the satellite platform in space and limited bandwidth for communication with the ground station, restrict satellite sensors from simultaneously recording images with higher spatial and spectral resolutions. Image fusion and pan-sharpening have proved to be effective methods in remote sensing studies to handle these restrictions. Image fusion combines Panchromatic and Multispectral band images to produce a composite or fused image with better spatial and spectral resolutions. In this paper, five different pan-sharpening approaches viz., the Intensity-Hue-Saturation transformation, the Brovey Transformation, the ESRI pan-sharpening transformation, the Gram-Schmidt method, and Simple Mean transformation are tested on the Landsat-8 MS band and Cartosat-1 Digital Elevation Model (DEM) PAN band. Results are analyzed visually and quantitatively to verify the spectral characteristics of the fused images in order to assess the efficiency of various image fusion techniques. A statistical index such as Root Mean Square Error (RMSE) is used to evaluate the fused image.","2688-769X","978-1-6654-3564-2","10.1109/SPIN52536.2021.9566131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9566131","Digital Elevation Model;Image fusion;Noise suppression filters;Remote sensing","Earth;Satellites;Vegetation mapping;Signal processing algorithms;Sensors;Spatial resolution;Image fusion","digital elevation models;geophysical image processing;image fusion;image resolution;remote sensing","Landsat-8 Multispectral band;image fusion techniques;data storage;satellite platform;limited bandwidth;ground station;satellite sensors;spatial resolutions;spectral resolutions;remote sensing studies;composite fused image;pan-sharpening approach;Intensity-Hue-Saturation transformation;Brovey Transformation;ESRI pan-sharpening transformation;Gram-Schmidt method;Simple Mean transformation;Landsat-8 MS band;Cartosat-1 Digital Elevation Model PAN band","","","","9","IEEE","19 Oct 2021","","","IEEE","IEEE Conferences"
"Deepsen3: Deep Multi-Scale Learning Model For Spatial-Spectral Fusion Of Sentinel-2 And Sentinel-3 Remote Sensing Images","A. Alboody; M. Puigt; G. Roussel; V. Vantrepotte; C. Jamet; T. -K. Tran","LISIC – UR 4491, Univ. Littoral Côte d’Opale, Longuenesse, France; LISIC – UR 4491, Univ. Littoral Côte d’Opale, Longuenesse, France; LISIC – UR 4491, Univ. Littoral Côte d’Opale, Longuenesse, France; CNRS, LOG – UMR 8187, Univ. Littoral Côte d’Opale, Wimereux, France; CNRS, LOG – UMR 8187, Univ. Littoral Côte d’Opale, Wimereux, France; CNRS, LOG – UMR 8187, Univ. Littoral Côte d’Opale, Wimereux, France","2022 12th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","22 Nov 2022","2022","","","1","5","Recently, deep learning methods that integrate image features gradually became a hot development trend in fusion of multispectral and hyperspectral remote sensing images, aka multi-sharpening. Fusion of a low spatial resolution hyperspectral image (LR-HSI datacube) with its corresponding high spatial resolution multispectral image (HR-MSI datacube) to reconstruct a high spatial resolution hyperspectral image (HR-HSI) has been a significant subject in recent years. Nevertheless, it is still difficult to achieve a high quality of spatial and spectral information fusion. In this paper, we propose a Deep Multi-Scale Learning Model (called DeepSen3) of spatial-spectral information fusion based on multi-scale inception residual convolutional neural network (CNN) for more efficient hyperspectral and multispectral image fusion from ESA remote sensing satellite missions (Sentinel-2 and Sentinel-3 images). The proposed DeepSen3 fusion network was applied to Sentinel-2 MSI (13 spectral bands with a spatial resolution ranging from 10, 20 to 60 m) and Sentinel-3 OLCI (21 spectral bands with a spatial resolution of 300 m) images. Extensive experiments demonstrate that the proposed DeepSen3 network achieves the best performance (both qualitatively and quantitatively) compared with recent state-of-the-art deep learning approaches.","2158-6276","978-1-6654-7069-8","10.1109/WHISPERS56178.2022.9955139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955139","Deep Learning;Residual Convolutional Neural Network (ResNet-CNN);Multi-Scale Inception;Feature Extraction;Spatial-Spectral Image Fusion;Sentinel-2 and Sentinel-3 Remote Sensing Images;HyperSpectral Images (HSI);Multi-Spectral Images (MSI)","Deep learning;Satellites;Signal processing;Market research;Distance measurement;Convolutional neural networks;Spatial resolution","artificial satellites;convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;image fusion;remote sensing","deep multiscale learning model;DeepSen3 fusion network;ESA remote sensing satellite missions;high spatial resolution hyperspectral image;low spatial resolution hyperspectral image fusion;multiscale inception residual convolutional neural network;multisharpening;multispectral remote sensing image fusion;Sentinel-2 MSI images;Sentinel-3 OLCI images;Sentinel-3 remote sensing images;spatial-spectral information fusion","","","","20","IEEE","22 Nov 2022","","","IEEE","IEEE Conferences"
"Multi-Supervised Recursive-CNN for Hyperspectral and Multispectral Image Fusion","Y. Lu; J. Yang; L. Xiao","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2440","2443","Deep learning has been widely used in remote sensing images fusion in recent years. However, many deep learning based methods use interpolation or a deconvolutional layer to upsample the low-resolution hyperspectral(LRHS) image, then the upsampled image is concated with the high-resolution multispectral(HRMS) image to fed the network, which lead to the spatial-spectral information loss. In this study, we propose a multi-supervised recursive convolutional neural network(MRCNN) for HS/MS images fusion. Specifically, we use the upsampling recursive sub-net(URSN) to upsample the LRHS image, which can effectively avoid the spatial-spectral information loss. In addition, to make our deep network much lighter, we introduce recursive learning to the network by using a residual block as the recursive unit for several recursions. Finally, a multi-supervised learning strategy is adopted for enhancing the gradient propagation and avoiding vanishing and exploding gradients. Simulated experiments on Cave and Moffett Field datasets show that the proposed network outperforms many state-of-the-art ones.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553598","National Natural Science Foundation of China(grant numbers:61871226,61571230,62001226); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553598","hyperspectral;image fusion;recursive learning;multi-supervision","Deep learning;Interpolation;Convolutional neural networks;Image fusion;Hyperspectral imaging","convolutional neural nets;geophysical image processing;gradient methods;image fusion;image resolution;learning (artificial intelligence);remote sensing","recursive unit;multisupervised learning strategy;gradient propagation;multisupervised recursive-CNN;hyperspectral image fusion;multispectral image fusion;remote sensing images fusion;deep learning based methods;deconvolutional layer;upsampled image;spatial-spectral information loss;LRHS image;deep network;recursive learning;exploding gradients;upsampling recursive sub-net;URSN;multisupervised recursive convolutional neural network;MRCNN;HS/MS images fusion;low-resolution hyperspectral","","1","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Improvement of Classification Accuracy Using Image Fusion Techniques","R. Singh; R. Gupta","Department of Civil Engineering, Birla Institute of Technology & Sciences, Pilani, Rajasthan, India; Department of Civil Engineering, Birla Institute of Technology & Sciences, Pilani, Rajasthan, India","2016 International Conference on Computational Intelligence and Applications (ICCIA)","20 Oct 2016","2016","","","36","40","Remote sensing techniques have been widely used for identification of land use and land cover features. Land information can be easily collected by classification of satellite images in the context of their use. In this paper study area has been classified into three classes i.e. settlement, trees and agricultural by classification of an image which has been enhanced using fusion of two images. The spatial and spectral resolutions of different satellite images provide better information with the aid of initial processing of image and fusion of both images. The satellite images fused together are multispectral IRS-P6 also called Resourcesat-1 satellite, on board LISS-III sensor provide image with spatial resolution of 23.5 m and an IRS-P5 also called Cartosat-1 satellite provides single band panchromatic image with spatial resolution of 2.5 m. Erdas Imagine 9.1 software has been used for image processing, fusion and supervised classification of the images. The Brovery, Multiplicative and Principal Component Analysis (PCA) method have been used for image fusion. The resultant images have been classified using the supervised classification with maximum likelihood parametric rule for information extraction and comparison between them in terms of their accuracy.","","978-1-5090-4174-9","10.1109/ICCIA.2016.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7600311","Image fusion techniques;image classification;accuracy assessment","Image fusion;Spatial resolution;Satellites;Principal component analysis;Remote sensing;Image classification","geophysical image processing;image classification;image enhancement;image fusion;land cover;land use;maximum likelihood estimation;principal component analysis;remote sensing","classification accuracy;image fusion techniques;remote sensing techniques;land use identification;land cover features;land information collection;satellite image classification;settlement class;trees;agricultural class;image enhancement;spatial resolutions;spectral resolutions;satellite images;multispectral IRS-P6;Resourcesat-1 satellite;LISS-III sensor;IRS-P5;Cartosat-1 satellite;single band panchromatic image;Erdas Imagine 9.1 software;image processing;image supervised classification;Brovery method;multiplicative method;principal component analysis;PCA method;maximum likelihood parametric rule;information extraction","","7","","14","IEEE","20 Oct 2016","","","IEEE","IEEE Conferences"
"The research of multispectral remote sensing image classification based on unmixing for the Loess Plateau","X. -b. Kong; Li Li; Weiying Sun; Guanju Wei","Key Laboratory of the Loess Plateau Soil Erosion and Water Loss Process and Control, Yellow River Institute of Hydraulic Research, Zhengzhou Henan, China; Key Laboratory of the Loess Plateau Soil Erosion and Water Loss Process and Control, Yellow River Institute of Hydraulic Research, Zhengzhou Henan, China; Key Laboratory of the Loess Plateau Soil Erosion and Water Loss Process and Control, Yellow River Institute of Hydraulic Research, Zhengzhou Henan, China; College of Water Resources and Electric Power, Qinghai University, Xining Qinghai, China","2016 4th International Workshop on Earth Observation and Remote Sensing Applications (EORSA)","29 Aug 2016","2016","","","461","464","The ecological environment of the Loess Plateau in China is the most vulnerable area, and also one of most serious soil erosion areas. Medium-resolution and multi-spectral remote sensing image, such as Landsat Imagery, plays an important role in the study of the ecological environment monitoring and soil erosion, especially for medium and large scale or watershed level. However, how to improve the accuracy of remote sensing image classification, and to extracte more accurate covering feature information, has become a key problem preventing remote sensing applications in the study of soil erosion, ecological environment, and other related disciplines.","","978-1-5090-1479-8","10.1109/EORSA.2016.7552851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552851","Multispectral remote sensing;Spectral Umixing;Image Criteria classification","Remote sensing;Soil;Agriculture;Earth;Conferences;Image classification;Water resources","calibration;environmental monitoring (geophysics);erosion;feature extraction;image classification;image fusion;radiometry;remote sensing","image classification;Loess plateau;China;soil erosion areas;ecological environment monitoring;feature information;medium-resolution image;multispectral remote sensing image;spatial resolution;Chabagou watershed;landsat8 OLI image;radiometric calibration;atmospheric correction;image fusion;linear mixed pixel decomposition;sub-pixel level distribution;surface features","","","","9","IEEE","29 Aug 2016","","","IEEE","IEEE Conferences"
"Algorithm of remote sensing image matching based on corner-point","W. Changjie; N. Hua","Beijing Institute of Space Mechanics & Electricity, Beijing, China; Beijing Institute of Space Mechanics & Electricity, Beijing, China","2017 International Workshop on Remote Sensing with Intelligent Processing (RSIP)","26 Jun 2017","2017","","","1","4","Feature extraction is an important method to obtain remote sensing image information. It has significant influence on recognition, analysis, matching, fusion, segmentation of remote sensing image. Image registration is usually classified into two categories: the intensity-based method and the feature-based method. The corner-point is one of the basic features of the images, which has many information and can easily be detected. In the area of remote sensing image application, two or more images are usually mosaiced as one image. According to remote sensing image matching, a method of image matching based on Harris corner-point combined with SURF algorithm is proposed in this paper. First of all, feature points are detected using Harris algorithm, that has the ability of noise immunity and stability. Then, calculating by SURF algorithm, the main directions of the feature points are determined and the feature descriptors are generated. Ratio method is used to get initial matching, and RANSAC algorithm is used to eliminate errors and achieve accurate matching. The result demonstrates that the Harris corner-point image registration described is stable and efficient. The method can be well applied in the remote sensing image processing and geometric positioning accuracy evaluation.","","978-1-5386-1990-2","10.1109/RSIP.2017.7958803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7958803","Harris corner;remote sensing image matching;scale-invariant;SURF;feature extraction","Feature extraction;Remote sensing;Image matching;Robustness;Algorithm design and analysis;Image registration","feature extraction;geophysical image processing;image classification;image fusion;image matching;image registration;image segmentation;remote sensing","remote sensing image matching;feature extraction;remote sensing image information;image recognition;image fusion;image segmentation;Image registration;image classification;intensity-based method;feature based method;image mosaicing;Harris corner-point imaging algorithm;SURF algorithm;feature point detection;stability;feature descriptor;RANSAC algorithm;geometric positioning accuracy evaluation;remote sensing image processing","","6","","","IEEE","26 Jun 2017","","","IEEE","IEEE Conferences"
"Remote sensing image fusion using Hausdorff fractal dimension in shearlet domain","B. Biswas; A. Dey; K. N. Dey","Department of Computer Science and Engineering, University of Calcutta; Department of Computer Science and Engineering, University of Calcutta; Department of Computer Science and Engineering, University of Calcutta","2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","28 Sep 2015","2015","","","2180","2185","Preservation of spectral information and enhancement of spatial resolution is the most important issue in remote sensing image fusion. In this paper, a new remote sensing satellite image fusion method using shearlet transform (ST) with Hausdorff fractal dimension(HFD) estimation method is proposed. Firstly, ST is used in each high-spatial-resolution panchromatic (PAN) image and multi-spectral image (MS). Then, the low frequency sub-band coefficients from different images are combined according to the HFD method which estimates and selects the modified low-pass band automatically. The composition of different high-pass sub-band coefficients achieved by the ST decomposition is discussed in detail. Finally, we achieve fusion results from the inverse transformation of ST. Experimental results show that the proposed method outperforms many state-of-the-art techniques in both subjective and objective evaluation measures.","","978-1-4799-8792-4","10.1109/ICACCI.2015.7275939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275939","Remote sensing image fusion;Shearlet Transform;Hausdorff fractal dimension;mutual information","Image fusion;Remote sensing;Fractals;Image edge detection;Satellites;Discrete wavelet transforms","image colour analysis;image enhancement;image fusion;image resolution;inverse transforms;remote sensing","Hausdorff fractal dimension estimation method;shearlet domain;spectral information preservation;spatial resolution enhancement;remote sensing image fusion;HFD estimation method;remote sensing satellite image fusion method;high-spatial resolution panchromatic image;high-spatial resolution PAN image;multispectral image;MS image;modified low-pass band automatic selection;high-pass subband coefficients;ST decomposition","","","","20","IEEE","28 Sep 2015","","","IEEE","IEEE Conferences"
"HNU-HMiF: A UAV-Borne Dataset for Hyperspectral and Multispectral Image Fusion","C. Li; X. Liu; X. Kang; S. Li","College of Electrical and Information Engineering, Hunan University, Changsha; College of Electrical and Information Engineering, Hunan University, Changsha; College of Electrical and Information Engineering, Hunan University, Changsha; College of Electrical and Information Engineering, Hunan University, Changsha","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4448","4451","Fusion of hyperspectral images (HSIs) and multispectral images (MSIs) with different resolutions is an active research topic in the field of remote sensing. However, HSI-MSI fusion assessments in existing researches are basically conducted on simulated data gone through spectral or spatial downsampling, which cannot reflect the actual performances of fusion methods in application scenarios. To conquer this problem, a new remote sensing dataset- Hunan UAV-borne HSIs and MSIs fusion(HNU-HMiF) dataset is provided in this paper. The proposed dataset contains fine registered hyperspectral and multispectral image pairs captured by unmanned aerial vehicle (UAV) covering different ground objects, and can be used to evaluate, select, and even develop fusion methods for users or researchers. Successful applications including method evaluation and comparison confirm the validity and reliability of the proposed dataset.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554180","National Natural Science Fund of China(grant numbers:61901166); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554180","Dataset;hyperspectral image;remote sensing;UAV-borne imagery;image fusion","Measurement;Pansharpening;Unmanned aerial vehicles;Sensors;Reliability;Task analysis;Spatial resolution","autonomous aerial vehicles;geophysical image processing;geophysical techniques;hyperspectral imaging;image fusion;image registration;object detection;remote sensing","remote sensing dataset;UAV-Borne dataset;hyperspectral image fusion;multispectral image fusion;HSI-MSI fusion assessments;spectral downsampling;spatial downsampling;hunan UAV-borne HSI;unmanned aerial vehicle;HNU-HMiF","","","","6","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A Survey on Image Fusion Techniques and Performance Metrics","C. C. Chaithra; N. L. Taranath; L. M. Darshan; C. K. Subbaraya","Dept. of CS&E, AIT, Karnataka, India; Dept. of CS&E, AIT, Karnataka, India; Dept. of CS&E, AIT, Karnataka, India; AIT, Kamataka, India","2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA)","30 Sep 2018","2018","","","995","999","Image fusion is the process of merging two or more relevant information into one image. The resulted image will have more explanatory than original images. Multispectral image (MS) is obtained from satellite, multispectral image having rich spectral information and low spatial resolution. MS have less information which is not suitable for remote sensor application. Panchromatic (PAN) image is one of the types of satellite images. PAN images have more spectral information but low spatial information. In remote sensing application more spatial and spectral information is required, so merging MS and PAN will result in rich spatial and spectral image. Many fusion algorithms are supported to fuse MS and PAN. Some techniques are principal component analysis, discrete wavelet transform, pixel-level image fusion and multisensor image fusion. Qualitative analysis determines the performance of fused image by comparison between original image and resulted fused image. Some qualitative metrics are evaluated using Root mean square error (RMSE), Relative global dimensional synthesis error (ERGAS), Quality factor (Q4), Cross correlation (CC) and Spectral angle mapper (SAM). This paper reviews about various fusion techniques in remote sensor and quality metrics.","","978-1-5386-0965-1","10.1109/ICECA.2018.8474818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8474818","Principal component analysis (PCA);Intensity-Hue-Saturation;wavelets transform and quality metrics","Image fusion;Principal component analysis;Remote sensing;Spatial resolution;Wavelet transforms;Conferences","discrete wavelet transforms;geophysical image processing;geophysical techniques;image fusion;image resolution;principal component analysis;remote sensing","spectral information;Relative global dimensional synthesis error;Quality factor;fused image;multisensor image fusion;pixel-level image fusion;fusion algorithms;spectral image;remote sensing application;low spatial information;PAN images;satellite images;panchromatic image;remote sensor application;low spatial resolution;MS;multispectral image;performance metrics;image fusion techniques","","2","","12","IEEE","30 Sep 2018","","","IEEE","IEEE Conferences"
"DNDT: Infrared and Visible Image Fusion Via DenseNet and Dual-Transformer","H. Zhao; R. Nie","School of Information Science and Engineering, Yunnan University, Kunming; School of Information Science and Engineering, Yunnan University, Kunming","2021 International Conference on Information Technology and Biomedical Engineering (ICITBE)","3 Feb 2022","2021","","","71","75","Image fusion plays an important role in real life, especially in remote sensing, image enhancement, and so on. Among all kinds of image fusion algorithms, Transformer has been proposed recently for image fusion with great potential, but it also limited localization abilities due to insufficient low-level details. To address this issue, we proposed a new fusion framework called DenseNet Dual-Transformer(DT) for infrared and visible image fusion. It extracts rich feature information through the encoder of DenseNet, On the other hand, utilized DT to pay attention to different aspects of information, and integrate all aspects of information. We argue that DT as a fusion strategy, local and remote information can be capture. A large number of experiments have proved that the performance of the fusion algorithm proposed in the paper is better than many existing algorithms.","","978-1-6654-0099-2","10.1109/ICITBE54178.2021.00025","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Yunnan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9694296","image fusion;transformer;DenseNet","Location awareness;Feature extraction;Transformers;Data mining;Information technology;Image fusion;Remote sensing","image enhancement;image fusion","remote sensing;image enhancement;image fusion algorithms;fusion framework;DenseNet Dual-Transformer;visible image fusion;rich feature information;fusion strategy;local information;remote information;fusion algorithm","","2","","21","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"An Improved IHS Image Fusion Algorithm using Medoid Intensity Match and Bilateral Filter","M. M. Tiwari; I. Misra; S. M. Moorthi; D. Dhar","Computer Science Department, Bennett University, Greater Noida, Uttar Pradesh, India; Signal and Image Processing Group, Space Applications Centre, ISRO, Ahmedabad, Gujarat, India; Signal and Image Processing Group, Space Applications Centre, ISRO, Ahmedabad, Gujarat, India; Signal and Image Processing Group, Space Applications Centre, ISRO, Ahmedabad, Gujarat, India","2021 IEEE International India Geoscience and Remote Sensing Symposium (InGARSS)","13 Jun 2022","2021","","","500","503","Image fusion is a productive way to combine multi-sensor images and extract maximum information from enhance remote sensing data. The paper describes a novel methodology to improve IHS image fusion algorithm by medoid intensity computation from multispectral images and suppress noise of panchromatic image using bilateral filter. Medoid provides an optimal representation of multispectral images and act as an intensity component in IHS color space. Bilateral filter preserves the edge information of high resolution data while reduce the noise in homogenous regions. Improved IHS image fusion algorithm by amalgamation of medoid intensity and bilateral filter found to be an effective technique to generate fused images. The technique developed is evaluated with Cartosat-1 panchromatic and Resourcesat-2 multispectral closest acquisition images. The datasets are geometrically transformed and co-registered to make it eligible for image fusion exercises. The visual assessment confirms improved IHS better retain the spatial details while preserving the spectral characteristics. Improved IHS is compared with state-of-the-art component substitution image fusion techniques and found to be perform better in almost all the standard metrics.","","978-1-6654-4249-7","10.1109/InGARSS51564.2021.9791967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9791967","Image Fusion;IHS;Medoid;Bilateral Filter;panchromatic;multispectral","Measurement;Matched filters;Visualization;Image resolution;Image edge detection;Filtering algorithms;Information filters","geophysical image processing;geophysical techniques;image colour analysis;image fusion;image resolution;remote sensing","improved IHS image fusion algorithm;medoid intensity match;bilateral filter;multisensor images;enhance remote sensing data;medoid intensity computation;multispectral images;panchromatic image;IHS color space;image fusion exercises;state-of-the-art component substitution image fusion techniques","","","","15","IEEE","13 Jun 2022","","","IEEE","IEEE Conferences"
"Psgan: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening","X. Liu; Y. Wang; Q. Liu","Beijing Advanced Innovation Center for Big Data and Brain Computing; Beijing Advanced Innovation Center for Big Data and Brain Computing; Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China","2018 25th IEEE International Conference on Image Processing (ICIP)","6 Sep 2018","2018","","","873","877","Remote sensing image fusion (also known as pan-sharpening) aims to generate a high resolution multi -spectral image from inputs of a high spatial resolution single band panchromatic (PAN) image and a low spatial resolution multi-spectral (MS) image. In this paper, we propose PSGAN, a generative adversarial network (GAN) for remote sensing image pansharpening. To the best of our knowledge, this is the first attempt at producing high quality pan-sharpened images with GANs. The PSGAN consists of two parts. Firstly, a two-stream fusion architecture is designed to generate the desired high resolution multi -spectral images, then a fully convolutional network serving as a discriminator is applied to distinct “real” or “pan-sharpened” MS images. Experiments on images acquired by Quickbird and GaoFen-1 satellites demonstrate that the proposed PSGAN can fuse PAN and MS images effectively and significantly improve the results over the state of the art traditional and CNN based pan-sharpening methods.","2381-8549","978-1-4799-7061-2","10.1109/ICIP.2018.8451049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451049","Image fusion;pan-sharpening;GAN;deep learning;remote sensing","Generators;Gallium nitride;Spatial resolution;Remote sensing;Task analysis;Training","geophysical image processing;image fusion;image resolution;remote sensing","two-stream fusion architecture;Quickbird and GaoFen-1 satellites;high resolution multi-spectral images;MS images;high quality pan-sharpened images;remote sensing image pansharpening;PSGAN;low spatial resolution multispectral;high spatial resolution single band panchromatic image;sensing image fusion;remote sensing image pan-sharpening;generative adversarial network;psgan","","61","","25","IEEE","6 Sep 2018","","","IEEE","IEEE Conferences"
"A new fusion method for remote sensing images based on salient region extraction","L. Zhang; J. Zhang","College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China","2017 IEEE International Conference on Image Processing (ICIP)","22 Feb 2018","2017","","","1960","1964","The goal of the remote sensing image fusion is to inject the detail information extracted from panchromatic (PAN) images to multispectral (MS) images with minimized spectral distortion. However, different regions in the image may practically have different demands on the spatial and spectral resolution. In this paper, a new fusion method for remote sensing images based on salient region extraction is proposed. By introducing the hybrid visual saliency analysis, information in the PAN and MS image are automatically partitioned into two categories: salient and non-salient regions. Then, a sub-region fusion strategy is applied to fuse the non-salient and salient regions respectively. For non-salient regions, such as farmland and mountains, the wavelet transform is used in the process of spatial infusion to suppress spectral distortion. As for salient regions like residential areas, the windowed IHS transform is carried out for its merits of effective integration of spatial and spectrum information. Experimental results demonstrate that our proposal achieves a better balance between spatial injection and spectral maintenance in different regions.","2381-8549","978-1-5090-2175-8","10.1109/ICIP.2017.8296624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8296624","Image fusion;remote sensing;saliency analysis;IHS transform;wavelet transform","Remote sensing;Feature extraction;Geoscience;Principal component analysis;Handheld computers;Indexes","feature extraction;image colour analysis;image fusion;image resolution;remote sensing;wavelet transforms","spatial spectrum information;spatial injection;remote sensing images;salient region extraction;remote sensing image fusion;panchromatic images;multispectral images;spatial resolution;spectral resolution;hybrid visual saliency analysis;sub-region fusion strategy;nonsalient regions;spectral distortion;wavelet transform;windowed IHS transform;intensity-hue-saturation transform","","5","","22","IEEE","22 Feb 2018","","","IEEE","IEEE Conferences"
"Hyperspectral and Multispectral Image Fusion Using Non-Convex Relaxation Low Rank and Total Variation Regularization","Y. Yuan; Q. Wang; X. Li","School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi'an, Shaanxi, P. R. China; School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi'an, Shaanxi, P. R. China; School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi'an, Shaanxi, P. R. China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2683","2686","Hyperspectral (HS) and multispectral (MS) image fusion is an important task to construct an HS image with high spatial and spectral resolutions. In this paper, we present a novel HS and MS fusion method using non-convex low rank tensor approximation and total variation regularization. In specific, the Laplace based low-rank model is formed to exploit spatial-spectral correlation and nonlocal similarity of the HS image, and the second-order total variation is used to describe the local smoothness structure in the spatial domain and adjacent bands. Also, an effective optimization algorithm is designed for the proposed model. In the experiments, we demonstrate the superiority of the proposed method compared to several state-of-the-art approaches.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323227","National Natural Science Foundation of China(grant numbers:U1864204,61773316,U1801262,61871470); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323227","Hyperspectral (HS) image;multispectral (MS) image;image fusion;low-rank approximation;total variation","Spatial resolution;Hyperspectral imaging;Image fusion;Minimization;Correlation;TV;Optimization","approximation theory;concave programming;geophysical image processing;hyperspectral imaging;image fusion;image resolution;remote sensing;tensors","hyperspectral image fusion;multispectral image fusion;nonconvex relaxation low rank;total variation regularization;MS image;HS image;high spatial resolutions;spectral resolutions;nonconvex low rank tensor approximation;Laplace based low-rank model;spatial-spectral correlation;second-order total variation;nonlocal similarity;local smoothness structure;adjacent bands;optimization algorithm","","1","","12","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Fine-scale impervious surface extraction by fusing high resolution remote sensing images and vehicle digital measurable images","Zhenfeng Shao; Dewen Wu; Changzeng Tang; Yang Song","State Key Laboratory of Information Engineering in Surveying, Wuhan University; Chinese Academy of Surveying &Mapping, Haidian District Beijing City; Geomatics Center of Guangxi, Nanning, China; Guangzhou Urban Planning & Design Institute, Guangzhou, China","2016 4th International Workshop on Earth Observation and Remote Sensing Applications (EORSA)","29 Aug 2016","2016","","","382","386","Impervious surface area (ISA) as the indicator of urbanization has a significance for urban ecological environment evaluation. Impervious surface extracting methods based on high-resolution remote sensing imagery can extract ISA in a fine-scale. However, a series of consequent problems cannot be ignored, such as shadows from tall building and canopies. In order to solve the shadow problem, high-resolution remote sensing image from Wuhan city is divided into three classes which are shadows, impervious surface and pervious. Due to the shadow and vegetation area along the street, the vehicle mounted digital measurable images are introduced to obtain fine-scale impervious surface information. D-S theory of evidence was employed to detect the shadow area according to spectral information. Then the impervious surface was extracted by fusing VHR remote sensing image and vehicle digital measurable images. The results showed that the proposed method was capable to provide the extraction of fine-scale impervious surface.","","978-1-5090-1479-8","10.1109/EORSA.2016.7552834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552834","Image fusion;high resolution image;vehicle mounted Digital Measurable Image;impervious surface;extract","Image segmentation;Image resolution","feature selection;image fusion;inference mechanisms;remote sensing;uncertainty handling;vegetation","high resolution remote sensing image fusion;surface extraction;vehicle digital measurable images;impervious surface area;urban ecological environment evaluation;impervious surface extracting methods;Wuhan city;shadow area;vegetation area;vehicle mounted digital measurable images;D-S theory;spectral information;fine-scale impervious surface","","","","7","IEEE","29 Aug 2016","","","IEEE","IEEE Conferences"
"FPGA implementation of satellite image fusion using wavelet substitution method","G. Mamatha; V. Sumalatha; M. V. Lakshmaiah","Dept. of ECE, JNTUACEA, Ananthapuramu, India; Dept. of ECE, JNTUACEA, Ananthapuramu, India; Dept. of ECE, JNTUACEA, Ananthapuramu, India","2015 Science and Information Conference (SAI)","3 Sep 2015","2015","","","1155","1159","Image fusion is a process of merging the relevant information from a set of images into a single image. The goal of image fusion in remote sensing is to create new images that contain both low spatial resolution multispectral data (color information) and high spatial resolution panchromatic data (details). The choice of appropriate wavelet filters for the design and implementation on FPGA, a detailed analysis has been carried out in MATLAB Simulink R2010b software using averaging, additive and substitutive fusion rules. In this paper substitutive rule using the Haar, Daubechies 3 (db3) and Cohen Daubechies Feauveau (CDF) 9/7 filters are used for image decomposition and reconstruction. CDF 9/7 is found to be the best filter and is chosen for FPGA implementation. Single level 2D-DWT based image fusion has been performed using substitutive method and then the hardware software co-simulation design has been synthesized in Xilinx ISE 13.1 and implemented on ML605 Virtex 6 FPGA kit. From the results, it is observed that the design consumes a total power of 4.349W and operates at a maximum frequency of 849.618MHz.","","978-1-4799-8547-0","10.1109/SAI.2015.7237290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7237290","Image Fusion;Image Resampling;Image Registration;DWT;FPGA","Image fusion;Field programmable gate arrays;Image resolution;Software;Hardware;Generators","artificial satellites;discrete wavelet transforms;field programmable gate arrays;geophysical image processing;hardware-software codesign;image colour analysis;image filtering;image fusion;image reconstruction;image resolution;remote sensing","FPGA implementation;satellite image fusion;wavelet substitution method;information merging;remote sensing;low-spatial resolution multispectral data;color information;high-spatial resolution panchromatic data;wavelet filters;MATLAB Simulink R2010b software;averaging fusion rule;additive fusion rule;substitutive fusion rule;Haar-Daubechies-3 filter;Cohen-Daubechies-Feauveau filter;CDF-9/7 filter;image decomposition;image reconstruction;single-level 2D-DWT based image fusion;substitutive method;hardware software co-simulation design;Xilinx ISE 13.1;ML605 Virtex 6 FPGA kit","","7","","10","IEEE","3 Sep 2015","","","IEEE","IEEE Conferences"
"Comparison Analysis of High-Resolution Remote Sensing Image Fusion Based on the Multiresolution Pyramid Algorithm","R. Su; L. Sun; P. Qi","School of Intelligent Construction, Wuchang University of Technology, Wuhan, China; Beijing North-Star Technology Development Co., Ltd, Beijing, China; School of Intelligent Construction, Wuchang University of Technology, Wuhan, China","2021 IEEE International Conference on Data Science and Computer Application (ICDSCA)","23 Dec 2021","2021","","","659","663","High resolution remote sensing image fusion can overcome the shortage of spectral and spatial information of panchromatic and multispectral images, and obtain the fusion images with high spatial-spectral resolution. In this paper, Laplacian, contrast, gradient and morphological pyramid algorithms are used to verify the fusion of WorldView-II panchromatic and multispectral images. The fusion effects of the four algorithms are compared and analyzed from the combination of qualitative and quantitative evaluation methods. The experimental results show that the contrast pyramid algorithm effectively improves the spectral fidelity of high resolution remote sensing image fusion while maintaining the spatial information; There is little difference between gradient and Laplacian pyramid algorithm; morphological pyramid algorithm is the worst.","","978-1-6654-4054-7","10.1109/ICDSCA53499.2021.9650243","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650243","image fusion;pyramid algorithm;high resolution image;spectral fidelity","Laplace equations;Image edge detection;Conferences;Nonlinear filters;Data science;Visual effects;High frequency","geophysical image processing;image fusion;image resolution;remote sensing;wavelet transforms","spectral fidelity;high resolution remote;spatial information;Laplacian pyramid algorithm;morphological pyramid algorithm;high-resolution remote sensing image fusion;multiresolution pyramid algorithm;multispectral images;fusion images;spatial-spectral resolution;fusion effects;contrast pyramid algorithm","","","","15","IEEE","23 Dec 2021","","","IEEE","IEEE Conferences"
"Image fusion of MODIS AOD (collection 6) in China based on uncertainty","Y. Xie; Y. Xue; J. Guang; L. Mei; C. Fan; Y. Che; L. She","University of Chinese Academy of Sciences, Beijing, China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","3365","3368","In order to improve the accuracy and spatial coverage of AOD datasets, we proposed a method to obtain a consistent dataset with higher spatial coverage and better accuracy from Deep Blue (DB) AOD and Dark Target (DT) AOD products. The fusion algorithm consists of three parts: the first part is to remove the system errors, the second part is to calculate the uncertainty and fusion of datasets using the maximum likelihood estimate method, and the third part is to mask outliers. The MBE, MAE, RMB and RMSE of DB AOD in 2015 are 0.04, 0.13, 1.10 and 0.20 respectively, the MBE, MAE, RMB and RMSE of DT AOD in 2015 are 0.07, 0.12, 1.18 and 0.17 respectively, the MBE, MAE, RMB and RMSE of combined AOD provided by MODIS in 2015 are 0.05, 0.11, 1.12 and 0.16 respectively, and the MBE, MAE, RMB and RMSE of fusion data after mask with a threshold of 0.20 in 2015 are 0.03, 0.10, 1.08 and 0.15 respectively. The accuracy of fusion data after mask is obviously superior to the original data and the combined data provided by MODIS. In addition, the spatial coverage of the data has also been significantly improved.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127719","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127719","Image fusion;AOD","Atmospheric measurements;Atmosphere;Uncertainty","aerosols;atmospheric optics;image fusion;maximum likelihood estimation;remote sensing","AOD datasets;consistent dataset;higher spatial coverage;Deep Blue AOD;Dark Target AOD products;fusion algorithm;maximum likelihood estimate method;DT AOD;combined AOD;fusion data;image fusion;MODIS AOD","","1","","7","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Research on Semantic Annotation Based Image Fusion Algorithm","J. Ni; X. Qian; Q. Li; X. Xu","School of Mechanical, Electronic & Information Engineering, China University of Mining & Technology, Beijing, China; School of Mechanical, Electronic & Information Engineering, China University of Mining & Technology, Beijing, China; Engineering Integrated Experimental Teaching, Beijing Union University, Beijing, China; Engineering Integrated Experimental Teaching, Beijing Union University, Beijing, China","2017 International Conference on Computer Systems, Electronics and Control (ICCSEC)","26 Aug 2018","2017","","","945","948","There exists semantic gap between the user demand and the image fusion process. In order to solve the problem, a semantic labeling model of images is proposed and an image fusion mechanism based on semantic annotation is presented. The fusion mechanism is based on the semantic similarity of concepts. Experimental results show that the image fusion mechanism can improve the efficiency of image fusion.","","978-1-5386-3573-5","10.1109/ICCSEC.2017.8446720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8446720","Semantic gap;Semantic annotation;Image fusion;Concept tree;Information entropy","Semantics;Image fusion;Libraries;Labeling;Mathematical model;Training;Remote sensing","image fusion;semantic networks","user demand;image fusion process;image fusion mechanism;semantic annotation;image fusion algorithm;semantic gap;semantic similarity","","1","","9","IEEE","26 Aug 2018","","","IEEE","IEEE Conferences"
"Single Sensor Image Fusion Using A Deep Convolutional Generative Adversarial Network","F. Palsson; J. R. Sveinsson; M. O. Ulfarsson","Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland","2018 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","27 Jun 2019","2018","","","1","5","Recently deployed multispectral sensors can acquire multispectral images where different bands have different spatial resolution depending on wavelength. An example is the Sentinel-2 constellation which can acquire multispectral bands of 10 m, 20 m, and 60 m resolution, covering the visible, near-infrared (NIR) and short-wave infrared (SWIR) parts of the electromagnetic spectrum. In this paper, a method to perform image fusion of the fine and coarse spatial resolution bands to increase the resolution of the coarser bands is proposed. The method is based on a so-called Generative Adversarial Network (GAN) and uses a deep convolutional design for both the generator and the discriminator. In experiments, it is demonstrated that the proposed method gives good results when compared to state-of-the-art single sensor image fusion methods using both simulated and real Sentinel-2 datasets.","2158-6276","978-1-7281-1581-8","10.1109/WHISPERS.2018.8747268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747268","Image fusion;generative adversarial network;convolutional network;Sentinel-2","Image resolution;Image fusion;Generators;Gallium nitride;Training;Generative adversarial networks;Signal resolution","geophysical image processing;geophysical techniques;image fusion;image resolution;image sensors;remote sensing","deep convolutional Generative Adversarial Network;multispectral images;Sentinel-2 constellation;multispectral bands;electromagnetic spectrum;fine resolution bands;coarse spatial resolution bands;deep convolutional design;Sentinel-2 datasets;multispectral sensors;visible band;short-wave infrared band;single sensor image fusion methods;near-infrared band;wavelength 10.0 m;wavelength 60.0 m;wavelength 20.0 m","","7","","16","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"Investigating GF-5 Hyperspectral and GF-1 Multispectral Data Fusion Methods for Multitemporal Change Analysis","W. Sun; K. Ren; G. Yang; X. Meng; Y. Liu","Department of Geography and Spatial Information Techniques, Ningbo University, Zhejiang Province, China; Department of Geography and Spatial Information Techniques, Ningbo University, Zhejiang Province, China; Department of Geography and Spatial Information Techniques, Ningbo University, Zhejiang Province, China; Electrical Engineering and Computer Science, Ningbo University, Zhejiang Province, China; Shanghai Institute of Technical Physics (SITP), Chinese Academy of Sciences","2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp)","14 Oct 2019","2019","","","1","4","Multitemporal change analysis is one of the essential purposes for discovering knowledge from various remote sensing terrestrial earth observation techniques. Particularly, the China Gaofen-5 (GF-5) hyperspectral imager provides a new data source for multitemporal change analysis. Its 330 bands, 60 km swath width and 5-10 nm spectrum resolutions make it captures subtle changes in spectrum responses of ground objects across different images. Unfortunately, its 30 spatial resolution still hinders its accurate geospatial location in some specific applications. Therefore, we explore state-of-the-art data fusion methods and seek an appropriate fusing method of GF-5 hyperspectral and GF-1 multispectral data to benefit multitemporal change analysis. We utilize four image fusion methods and implement six evaluation criteria to holistically evaluate the performance of different methods. Experimental results on three datasets of Taihu Lake and Poyang Lake in China show that the Modulation transfer functions-generalized Laplacian pyramid (MTF-GLP) has smaller spectral distortion, better spatial fidelity and requires moderate computational time than the other three methods. It accordingly can be a good choice for fusing GF-5 and GF-1 remote sensing data in both classification and multitemporal change analysis.","","978-1-7281-4615-7","10.1109/Multi-Temp.2019.8866908","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8866908","Hyperspectral imagery;image fusion;GF-5;GF-1;remote sensing;multitemporal analysis;classification","Hyperspectral imaging;Spatial resolution;Image fusion;Lakes;Satellites","crops;geophysical image processing;geophysical signal processing;hyperspectral imaging;image classification;image fusion;image resolution;image sensors;lakes;optical transfer function;remote sensing;sensor fusion","GF-5 hyperspectral;GF-1 multispectral data fusion methods;multitemporal change analysis;remote sensing terrestrial earth observation techniques;China Gaofen-5;state-of-the-art data fusion methods;image fusion methods;GF-1 remote sensing data","","1","","14","IEEE","14 Oct 2019","","","IEEE","IEEE Conferences"
"Multi-sensor image fusion based on empirical wavelet transform","K. J. A. Sundar; M. Jahnavi; K. Lakshmisaritha","Shanmugha Arts Science Technology and Research Academy, Thanjavur, Tamil Nadu, IN; School of Computing, SASTRA University, Thanjavur, India; School of Computing, SASTRA University, Thanjavur, India","2017 International Conference on Electrical, Electronics, Communication, Computer, and Optimization Techniques (ICEECCOT)","8 Feb 2018","2017","","","93","97","The advancement in technology had led to the requirement for a single image with high spatial and good spectral resolution in various applications such as remote sensing, surveillance and medical diagnosis. Most of the technology and equipment available has already reached the level of saturation in providing such convincing image or data. Image fusion techniques allow the integration of different information sources. From a pair of hyper-spectral image with low spatial and a pan image with high spatial resolutions, the aim is to synthesize a single image with highest spatial resolution and spectral content. However, apart from the standard image fusion techniques, in this paper we propose multi-sensor image fusion using empirical wavelet transform. In this image fusion scheme, the empirical wavelet transforms of the input images are appropriately combined, and the new image is obtained by taking the inverse empirical wavelet transform of the fused wavelet coefficients. With the necessity of multi-sensor data in many fields such as remote sensing, medical imaging, machine vision, military applications, sensor fusion has emerged as an upcoming and promising research area. The main objective of image fusion is to reconstruct new images that are more convincing for human visual perception, highly suitable for object detection and target recognition.","","978-1-5386-2361-9","10.1109/ICEECCOT.2017.8284646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8284646","Multi sensor image fusion;Empirical wavelet transform;Image reconstruction;Super reesolution","Wavelet transforms;Image reconstruction;Spatial resolution;Image fusion;Imaging","geophysical image processing;hyperspectral imaging;image fusion;image resolution;interpolation;object detection;remote sensing;sensor fusion;wavelet transforms","highest spatial resolution;standard image fusion techniques;multisensor image fusion;image fusion scheme;empirical wavelet transforms;fused wavelet coefficients;multisensor data;high spatial resolution;hyper-spectral image;high spatial resolutions;spectral resolution;human visual perception;object detection;target recognition","","6","","19","IEEE","8 Feb 2018","","","IEEE","IEEE Conferences"
"A Review on Multi-Model Medical Image Fusion","S. Polinati; R. Dhuli","School of Electronics Engineering (SENSE), VIT, Vellore, India; Department of Electronics and Communication Enginnering, VIT, AP, India","2019 International Conference on Communication and Signal Processing (ICCSP)","25 Apr 2019","2019","","","0554","0558","Nowadays, Image fusion seems to be the most promising area in image processing. It plays a pivotal role in different applications, namely medical diagnosis, object detection and recognition, navigation, military, civilian surveillance, robotics, satellite imaging for remote sensing. The process of image fusion aims to integrate two or more images into a single image, which consists of more useful information when compared with each of the source images without introducing any artefacts. In this review paper, three aspects are considered: image fusion methods on spatial domain and transform domain methods, Image fusion rules on transform domain method and image fusion metrics. This review includes different applications, including medical image fusion methodologies.","","978-1-5386-7595-3","10.1109/ICCSP.2019.8697906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8697906","Spatial Domain;Transform Domain;Image Fusion;Metrics;Fusion Rules","Image fusion;Biomedical imaging;Transforms;Feature extraction;Measurement;Magnetic resonance imaging;Distortion","image fusion;medical image processing;object detection;transforms","Image fusion rules;medical image fusion methodologies;multimodel medical Image fusion;image processing;navigation;satellite imaging;single image;source images;image fusion methods;medical diagnosis;object detection;object recognition;civilian surveillance;military surveillance;robotics;remote sensing","","5","","26","IEEE","25 Apr 2019","","","IEEE","IEEE Conferences"
"Two-dimensional stockwell transform based image fusion for combining multifocal images","E. M. Babu; S. D. Maniks; N. M. Nandhitha; N. Selvarasu; S. E. Roslin; R. Chakravarthi; M. S. Sangeetha","Dept. of ECE, Sathyabama University, Chennai; Dept. of ECE, Sathyabama University, Chennai; School of EEE, Sathyabama University, Chennai; Dept. of ECE, SVS Group of Institutions, Warangal, Telungana; School of EEE, Sathyabama University, Chennai; School of EEE, Sathyabama University, Chennai; School of EEE, Sathyabama University, Chennai","2017 International Conference on Intelligent Sustainable Systems (ICISS)","21 Jun 2018","2017","","","710","714","In real time image acquisition, especially in the areas of remote sensing and telemedicine, capturing the entire scenario/specimen/scene is not possible even with high resolution sensors. Hence repeated image analysis is done on a large set of images. It increases the computational complexity of the analysis system especially when a large set of images is to be analyzed. Hence it is necessary to identify a technique that reduces the computational time. One such technique is image fusion which allows a set of images to be combined. Image fusion is defined as the process of combining two images which not only retains complexity info but also strengths redundant information. Conventionally wavelet transforms are used or decomposing the images into low and high frequency components. Later choose maximum and choose average are used for combining the coefficients. Though wavelet transfer provides the spectral information, it could not provide details about the phase or phase difference. Also in case of bursts in images, wavelet transform could not provide the appropriate information. Hence it is necessary to identify a suitable transform for obtaining the burst details. Stockwell transform provides frequency, time and phase information. In this work, a set of multi focal images are acquired and Stockwell transform is applied on this image. Later appropriate fusion rules are used to combine this features and inverse Stockwell is used on the fused image to obtain the original image. Performance of the proposed technique is measured in terms of Root Mean Square Error and standard deviation. Performance of the proposed technique is compared with that of wavelets based image fusion and is found that Stockwell transform provides better results (both quantitatively and qualitatively) than wavelet transform.","","978-1-5386-1959-9","10.1109/ISS1.2017.8389265","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8389265","Stockwell Transform;Image Fusion;Multifocal Images;Root Mean Square Error;Standard Deviation","Image fusion;Wavelet transforms;Standards;Conferences;Real-time systems;Sensors","computational complexity;image fusion;image sensors;mean square error methods;remote sensing;telemedicine;wavelet transforms","image acquisition;image analysis;stockwell transform;remote sensing;telemedicine;sensor resolution;computational complexity;image decomposing;root mean square error;standard deviation;spectral information;wavelets based image fusion;wavelet transform;multifocal images","","1","","6","IEEE","21 Jun 2018","","","IEEE","IEEE Conferences"
"Satellite Image Fusion Based on Improved Fast Discrete Curvelet Transforms","K. Jemseera; P. Noufal","Dept. of Electron. & Commun., M.E.S Coll. of Eng., Kuttippuram, India; Dept. of Electron. & Commun., M.E.S Coll. of Eng., Kuttippuram, India","2015 Fifth International Conference on Advances in Computing and Communications (ICACC)","17 Mar 2016","2015","","","430","433","Image fusion is the process of merging two or more images into a more informative single image. Satellite image fusion uses high resolution panchromatic image and low resolution multispectral image. IHS, Brovery, PCA, Wavelet, Curvelet etc are the existing techniques available for image fusion. But simultaneous retention of spatial and spectral resolution is an important concern in remote sensing applications. In this paper we proposed an improved Satellite image fusion method based on Fast Discrete curvelet Transform (FDCT) via wrapping. The method uses an improved fusion rule, were the maximum FDCT coefficients from each cell of the Intensity component of the MS image and histogram matched PAN image are taken. The resulting image is then undergone a comparative analysis with the outcomes of existing methodologies. The comparative analysis proves that the proposed method retains more spatial and spectral details than other methods.","","978-1-4673-6994-7","10.1109/ICACC.2015.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7433897","Image Fusion;Fast Discrete Curvelet Transforms;Intensity-Hue-Transforms;wavelet transforms","Image fusion;Spatial resolution;Discrete wavelet transforms;Satellites","curvelet transforms;image fusion;remote sensing","satellite image fusion;fast discrete curvelet transforms;panchromatic image;low resolution multispectral image;remote sensing;maximum FDCT coefficients","","","","13","IEEE","17 Mar 2016","","","IEEE","IEEE Conferences"
"Comparison of Remote Sensing Image Fusion Rules Based on the Laplacian Pyramid Transformation","R. Su; L. Sun; J. Wang","School of Intelligent Construction, Wuchang University of Technology, Wuhan, China; Beijing North-Star Technology Development Co., Ltd, Beijing, China; School of Intelligent Construction, Wuchang University of Technology, Wuhan, China","2021 IEEE International Conference on Data Science and Computer Application (ICDSCA)","23 Dec 2021","2021","","","670","674","In order to select the fusion rules to improve the quality of remote sensing image fusion and data utilization, WorldView-II images are fused using 16 fusion rules. Better fusion rules can be identified using qualitative and quantitative evaluation methods in the spectral, spatial and comprehensive index. The experimental results show that low frequency using multispectral image has a good effect, can better maintain the spectral fidelity of fusion images. When using high frequency consistency check, the evaluation index is the highest, and the fusion image can highlight the edge and texture information of the image, easy for human eye observation. High frequency using the regional mean gradient, the fusion effect is slightly poor and the spectral fidelity is slightly lower. When using high frequency absolute value maximization, the fusion effect is general, and there is a certain confusion phenomenon. When using high frequency regional energy, the fusion effect is poor and may introduce too much spatial information.","","978-1-6654-4054-7","10.1109/ICDSCA53499.2021.9650282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650282","Laplacian pyramid;fusion rules;image fusion;high frequency;low frequency","Laplace equations;Image edge detection;Conferences;Computer applications;Data science;High frequency;Indexes","geophysical image processing;image fusion;image texture;remote sensing","high frequency absolute value maximization;fusion effect;high frequency regional energy;remote sensing image fusion rules;laplacian pyramid transformation;data utilization;WorldView-II images;fusion rules;qualitative evaluation methods;quantitative evaluation methods;spectral index;spatial index;comprehensive index;multispectral image;spectral fidelity;fusion image;high frequency consistency check","","","","15","IEEE","23 Dec 2021","","","IEEE","IEEE Conferences"
"A novel remote sensing image change detection algorithm based on image fusion and wavelet theory","X. Yan; H. Liu; C. Wang","Beijing Jiaotong University, China; Indiana University, Bloomington, USA; College of Resource and Environment, Chengdu University of Information Technology, China","2016 International Conference on Communication and Electronics Systems (ICCES)","30 Mar 2017","2016","","","1","6","Image change detection is based on the analysis in different time from the same area of two or more images, detect the feature in the region information changes over time. Remote sensing image change detection has been widely used in such as the dynamic monitoring of forest resources monitoring, the change of land cover and use, agricultural resources survey, urban planning layout, environmental monitoring and analysis, assessment of natural disasters, geographic data update and military reconnaissance in the strategic objectives (such as roads, Bridges, airports) of dynamic monitoring and many other fields. Multilayer perceptron learning and classification of certain prior knowledge is known as a condition, namely the network weights adjustment was conducted under the condition of supervision. In practical application, sometimes cannot provide the required prior knowledge, which requires network has the ability to self-learning. Kohonen self-organizing feature map is put forward by the the self-learning function of neural network through the self-organizing feature map neural network model and combining the change detection task to obtain higher detection accuracy and stronger adaptability.","","978-1-5090-1066-0","10.1109/CESYS.2016.7889896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889896","Image Retrieval;On line and offline systems;sparse coding;content bases image retrieval systems","Image fusion;Spatial resolution;Discrete wavelet transforms;Buildings","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);multilayer perceptrons;object detection;remote sensing;self-organising feature maps;wavelet transforms","Kohonen self-organizing feature map;self-learning function;self-organizing feature map neural network model;change detection task;supervision condition;network weights adjustment;knowledge classification;multilayer perceptron learning;region information;feature detection;wavelet theory;image fusion;remote sensing image change detection algorithm","","","","15","IEEE","30 Mar 2017","","","IEEE","IEEE Conferences"
"Hyperspectral and multispectral wasserstein barycenter for image fusion","J. Mifdal; B. Coll; N. Courty; J. Froment; B. Vedel","LMBA, Universite de Bretagne Sud, France; Universitat de Illes Balears, Palma de Mallorca, Spain; Uuiversité de Bretagne Sud, IRISA, France; LMBA, Universite de Bretagne Sud, France; LMBA, Universite de Bretagne Sud, France","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","3373","3376","The fusion of hyperspectral and multispectral images is a crucial task nowadays for it allows the extraction of relevant information from the fused image. Fusion consists of the combination of the spectral information of the hypespectral image (h) and the spatial information of the multispectral image (m). The fused image (f) has both good spatial and spectral information. In this paper we suggest a new hyperspectral and multispectral image (h-m) fusion approach based on Optimal Transport (OT) which highlights the idea of energy transfer from the starting images m and h to the resulting image f. The simulations show that the suggested method is effective and compares competitively with other state-of-the-art methods.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127721","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127721","Image Fusion;Optimal Transport;Wasserstein Barycenter","Hyperspectral imaging;Image fusion;Sensors;Spatial resolution;Measurement;Minimization","geophysical image processing;hyperspectral imaging;image fusion;image sensors;remote sensing","image fusion;spectral information;spatial information;multispectral Wasserstein barycenter;hyperspectral Wasserstein barycenter;optimal transport","","5","","16","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Study on panchromatic and multispectral image fusion based on SFIM and CA transform","Guiqing He; Zhuqiang Shao; Siyuan Xing; DanDan Dong; Xiaoyi Feng","School of Electronics and Information, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, Shaanxi, China","2016 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","24 Nov 2016","2016","","","1","6","With the successive launch and rapid development of the new satellite WorldView-2 and WorldView-3, panchromatic and multispectral image fusion become a hot research topic. To resolve the dilemma of the currently existing methods for panchromatic and multispectral image fusion, viz. unavoidable spectral distortion or the need to introduce cumbersome frequency analysis and reconstruction, a method has been proposed which is based on SFIM (Smoothing Filter-based Intensity Modulation) and CA (Correspondence Analysis). Firstly, the weighted gradient adaptive filtering SFIM model is introduced, whose simple calculation feature has been utilized to extract the spatial information of panchromatic images. Secondly, the statistical CA transform has been brought in and its multivariable analysis feature has been used to process the infusion of spatial information. As a result of the above two processes the novel fusion method has been proposed which is based on SFIM and CA transform. Theoretical and experimental studies show that the proposed method can not only significantly maintain spectral characteristics, in absence of frequency decomposition and reconstruction, but also effectively infuse detailed spatial information, along with the elegancy of simple calculation and real time. In the scenario of panchromatic and multi-spectral image fusion such as similar lighting conditions and physical properties, the proposed method is more suitable for the fusion systems which require fast interactive processing and real-time visualization, and is better than those which are based upon multi-scale analysis.","","978-1-5090-2708-8","10.1109/ICSPCC.2016.7753705","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7753705","Image fusion;SFIM;CA;Gradient weighted adaptive filtering","Transforms;Smoothing methods;Satellites;Spatial resolution;Information filters;Image fusion","adaptive filters;geophysical image processing;image filtering;image fusion;remote sensing;smoothing methods;transforms","multispectral image fusion;panchromatic image fusion;smoothing filter-based intensity modulation;correspondence analysis;weighted gradient adaptive filtering SFIM model;statistical CA transform;multivariable analysis feature;spatial information infusion;satellite remote sensing","","","","10","IEEE","24 Nov 2016","","","IEEE","IEEE Conferences"
"Hyperspectral and multispectral image fusion using CNMF with minimum endmember simplex volume and abundance sparsity constraints","Y. Zhang; Y. Wang; Y. Liu; C. Zhang; M. He; S. Mei","School of Electronics and Information Northwestern Polytechnical University, Xi'an, P. R. China; School of Electronics and Information Northwestern Polytechnical University, Xi'an, P. R. China; School of Electronics and Information Northwestern Polytechnical University, Xi'an, P. R. China; School of Electronics and Information Northwestern Polytechnical University, Xi'an, P. R. China; School of Electronics and Information Northwestern Polytechnical University, Xi'an, P. R. China; School of Electronics and Information Northwestern Polytechnical University, Xi'an, P. R. China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","1929","1932","Hyperspectral (HS) remote sensing image with finer spectral information has great advantages in feature identification and classification. However, the spatial resolution of HS image is usually low due to practical limitations. In this paper, the low-spatial-resolution HS image is fused with the high-spatial-resolution multispectral (MS) image of the same observation scene to improve its spatial resolution. A novel spectral unmixing based HS and MS image fusion approach (VSC-CNMF) is proposed, in which CNMF with minimum endmember simplex volume and abundance sparsity constraints is employed for coupled unmixing of HS and MS images. Simulative experiments are employed for verification and comparison. The experimental results illustrate that the newly proposed VSC-CNMF based HS and MS fusion algorithm outperforms several state-of-the-art unmixing based fusion approaches in cases with moderate number of endmembers.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326172","Hyperspectral;image fusion;spasity;simplex volume;unmixing","Image fusion;Spatial resolution;Yttrium;Image reconstruction;Hyperspectral imaging","atmospheric optics;remote sensing","multispectral image fusion;CNMF;endmember simplex volume;abundance sparsity constraints;remote sensing image;practical limitations;MS image fusion approach;state-of-the-art unmixing based fusion","","13","","4","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Spatio-Spectral Image Fusion Using Local Embeddings","P. Saxena; A. Jain","Department of Computer Science and Engineering, Maulana Azad National Institute of Technology, Bhopal, India; Department of Computer Science and Engineering, Maulana Azad National Institute of Technology, Bhopal, India","2020 IEEE International Students' Conference on Electrical,Electronics and Computer Science (SCEECS)","7 May 2020","2020","","","1","3","Image fusion is extensively used in remote sensing to combine multiple images into a more informative single image, which is more suitable for human and machine perception. The purpose of image fusion algorithms is to achieve a single image with high spatial resolution and high spectral resolution. The major challenges faced by existing algorithms are of output image quality in terms of colour distortion and blurring. This paper models the spatio-spectral image fusion as a problem of non-linear dimensionality reduction. The aim is to define each point of fused image as a linear combination of its neighbours in multi spectral image and the corresponding pixel values from down scaled panchromatic image. The assumption here is that the spectral behaviour of fused image is similar to that of multispectral image. The performance of the model is compared with existing state of the art methods for same-sensor dataset.","2688-0288","978-1-7281-4862-5","10.1109/SCEECS48394.2020.215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9087080","fusion;multisensor fusion;spatio-spectral fusion;pansharpening;non-linear dimensionality reduction;local embeddings.","Image quality;Image color analysis;Nonlinear distortion;Spatial resolution;Image fusion;Remote sensing;Signal to noise ratio","geophysical image processing;hyperspectral imaging;image colour analysis;image fusion;image resolution;image restoration;remote sensing","high spatial resolution;high spectral resolution;output image quality;image colour distortion;image blurring;remote sensing;down scaled panchromatic image;local embeddings;multispectral image;spatio-spectral image fusion","","","","10","IEEE","7 May 2020","","","IEEE","IEEE Conferences"
"Comparative Analysis of the Fusion Methods Based on GF-3 Radar and GF-1 Multispectral Data","Z. Fu; J. Qi; D. Zhang; J. Wang; W. Zhang; X. Han","China Aero Geophysical Survey and Remote Sensing Center for Land and Resources, Beijing, China; China Aero Geophysical Survey and Remote Sensing Center for Land and Resources, Beijing, China; China Aero Geophysical Survey and Remote Sensing Center for Land and Resources, Beijing, China; China Aero Geophysical Survey and Remote Sensing Center for Land and Resources, Beijing, China; China Aero Geophysical Survey and Remote Sensing Center for Land and Resources, Beijing, China; China Aero Geophysical Survey and Remote Sensing Center for Land and Resources, Beijing, China","2018 Fifth International Workshop on Earth Observation and Remote Sensing Applications (EORSA)","3 Jan 2019","2018","","","1","5","To make better use of the advantages of radar and promote the application of image fusion based on radar data, the author uses different fusion methods based on GF-3 SAR and GF-1 MSS, and evaluates the fusion results by analyzing mean, variance, information entropy, average gradient, spectral distortion and correlation coefficient. The results show that HSV and GS transforms have the best performances in overall. PC is recognized as the third, while it is still remarkable that it has the best ability of spectral retention. And the specialty in NIR band makes PC more conducive for extraction of vegetation. Brovey and Multiplicative transforms are not effective in comparison.","","978-1-5386-6642-5","10.1109/EORSA.2018.8598556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598556","GF-3;multispectral image;SAR;fusion;remote sensing","Radar imaging;Transforms;Distortion;Correlation coefficient;Information entropy;Remote sensing","entropy;geophysical image processing;image fusion;remote sensing;sensor fusion;synthetic aperture radar;transforms;vegetation mapping","comparative analysis;GF-3 Radar;GF-1 Multispectral Data;image fusion;radar data;different fusion methods;GF-3 SAR;GF-1 MSS;fusion results;information entropy;average gradient;spectral distortion;correlation coefficient;spectral retention","","","","9","IEEE","3 Jan 2019","","","IEEE","IEEE Conferences"
"Fusion of SAR and optical remote sensing data — Challenges and recent trends","M. Schmitt; F. Tupin; X. X. Zhu","Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; LTCI, Université Paris Saclay, Paris, France; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","5458","5461","In this paper, we summarize challenges, proposed solutions and recent trends in the field of SAR-optical remote sensing data fusion. Although being a pre-processing step before the actual fusion-by-estimation, it is shown that matching and coregistration is one of the core challenges in that regard, which is mainly due to the strongly different geometric and radiometric properties of the two observation types. We then review some of the published fusion methods and discuss the future trends of this topic.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128239","synthetic aperture radar (SAR);optical imagery;remote sensing;data fusion","Optical sensors;Remote sensing;Synthetic aperture radar;Optical imaging;Data integration;Estimation","image fusion;image matching;image registration;radar imaging;remote sensing;sensor fusion;synthetic aperture radar","geometric properties;published fusion methods;radiometric properties;fusion-by-estimation;SAR-optical remote sensing data fusion","","21","","31","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Nonlocal Low-Rank Regularization for Hyperspectral and High-Resolution Remote Sensing Image Fusion","M. Cao; W. Bao; K. Qu; X. Zhang; X. Ma","School of Computer Science and Engineering, North Minzu University, Yinchuan, China; School of Computer Science and Engineering, North Minzu University, Yinchuan, China; School of Computer Science and Engineering, North Minzu University, Yinchuan, China; School of Computer Science and Engineering, North Minzu University, Yinchuan, China; School of Computer Science and Engineering, North Minzu University, Yinchuan, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1444","1447","Fusion of high spatial resolution multispectral images (HR-MSI) and low spatial resolution hyperspectral images (LR-HSI) of the same scene can effectively combine spectral and spatial information to obtain high resolution hyperspectral images (HR-HSI), but it can also cause spectral distortion. To address this problem, we propose a new fusion algorithm (NLLR) based on a combination of low-rank prior and observation model in this paper. In the proposed NLLR method, we incorporate nonlocal spatial similarity and low-rank prior into the fusion problem to better simulate the spatial and spectral features of HR-HSI. By extracting tensor blocks from the hyperspectral and multispectral images, performing a chunking clustering operation on the hyperspectral and mul-tispectral data respectively, and constraining the fusion model using low-rank regularization to transform it into solving a convex optimization problem, followed by iterative optimization of the optimization problem using the alternating direction method of multiplier (ADMM), which can achieve an accurate reconstruction. Experimental results show that NLLR can provide better fusion performance compared to state-of-the-art fusion models.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883100","Natural Science Foundation of Ningxia Province(grant numbers:2020AAC02028,2021AAC03179); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883100","Hyperspectral image;fusion;nonlocal spatial similarity;low-rank","Tensors;Transforms;Feature extraction;Convex functions;Iterative methods;Spatial resolution;Optimization","geophysical image processing;image fusion;image resolution;remote sensing;tensors","nonlocal low-rank regularization;high-resolution remote sensing image;high spatial resolution multispectral images;low spatial resolution hyperspectral images;spectral information;spatial information;high resolution hyperspectral images;HR-HSI;spectral distortion;fusion algorithm;observation model;NLLR method;nonlocal spatial similarity;fusion problem;simulate the spatial features;spectral features;hyperspectral data;mul-tispectral data;fusion model;convex optimization problem;fusion performance;state-of-the-art fusion models","","","","8","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Bi-Temporal Remote Sensing Image Fusion Via Semi-Coupled Low-Rank Tensor Approximation","Y. Wang; W. Li; N. Liu; R. Tao","School of Information and Electronics, Beijing Institute of Technology; School of Information and Electronics, Beijing Institute of Technology; School of Information and Electronics, Beijing Institute of Technology; School of Information and Electronics, Beijing Institute of Technology","2022 12th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","22 Nov 2022","2022","","","1","4","Fusion of hyperspectral (HS) and multispectral (MS) images at single time has been well studied, but the problem of fusing these images acquired from different dates still remains to be solved. Up till now, current methods fail to establish an efficient temporal mapping extraction scheme while make use of the advantage of tensor-based model. To deal with this issue, a novel bi-temporal HS-MS fusion method called Semi-coupled Low-rank Tensor Approximation (S-LRTA) is proposed. The method firstly employs Tucker decomposition to make the fusion task a factor estimation problem. Then it captures the natural low-rank property of hyperspectral image (HSI) with a sparse constraint on the core tensor. Particularly, a Hadamard-product based temporal variability descriptor is blended into the Tucker model to extract the temporal relationship which is the crucial difficulty in bi-temporal fusion problem. Lastly, an efficient Block Coordinate Descent (BCD) based optimization scheme is developed to solve the objective function. Experimental results demonstrate the superiority of the proposed method compared with state-of-the-art methods.","2158-6276","978-1-6654-7069-8","10.1109/WHISPERS56178.2022.9955047","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955047","Hyperspectral Images;Bi-temporal Fusion;Low-rank Tensor;Tucker Decomposition","Tensors;Dictionaries;Estimation;Signal processing;Feature extraction;Linear programming;Task analysis","approximation theory;estimation theory;feature extraction;geophysical image processing;hyperspectral imaging;image fusion;optimisation;remote sensing;tensors","BCD based optimization scheme;bi-temporal HS-MS fusion method;Bi-Temporal Remote Sensing Image Fusion;core tensor;efficient block coordinate descent based optimization scheme;efficient temporal mapping extraction scheme;factor estimation problem;fusion task;Hadamard-product based temporal variability descriptor;hyperspectral image fusion;MS images;multispectral images;natural low-rank property;objective function;S-LRTA;semicoupled low-rank tensor approximation;sparse constraint;temporal relationship;tensor-based model;Tucker decomposition","","","","10","IEEE","22 Nov 2022","","","IEEE","IEEE Conferences"
"Accurate Classification of Remote Sensed Data for Landuse/Land Class of Mangaluru Coastal Region","S. C. Srikrishna; T. Ashok Kumar; K. Shivaprakash","VCET, Puttur, Karnataka, India; Department of ECE, PESITM, Shivamogga, Karnataka, India; VCET, Puttur, Karnataka, India","2017 International Conference on Current Trends in Computer, Electrical, Electronics and Communication (CTCEEC)","6 Sep 2018","2017","","","347","352","Classification of remotely sensed data for land use/land class is one of the key areas for researchers in remote sensing fields. Classification of remote sensed data is necessary for the better categorization of semi urban land features such as wetlands, forests, agriculture and other land and water types. For the accurate classification of remotely sensed data, a high quality data set is needed. In the present research study, to classify the data set, PAN sharpened(fused) images are considered. Image fusion provides better quality and more informative image data set as compared to original data. The performance of different fusion techniques are then evaluated to select the best possible technique which gives better result for image classification.","","978-1-5386-3243-7","10.1109/CTCEEC.2017.8455079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8455079","Image fusion;Remote sensing;Image classification;Multispectral;Panchromatic","Remote sensing;Spatial resolution;Standards;Image fusion;Satellites;Decision trees","geophysical image processing;image classification;image fusion;land use;remote sensing","land use-land class;Mangaluru Coastal Region;wetlands;forests;agriculture;PAN sharpened images;image fusion;image classification;India;more informative image data;water types;semiurban land features;remote sensed data;remote sensing fields","","","","20","IEEE","6 Sep 2018","","","IEEE","IEEE Conferences"
"A Robust Hyperspectral Image Fusion Method for Unregistered Data","Y. Chen; W. Pan; L. Wang; B. Wang; Z. An","Jiangsu Polytechnic College of Agriculture and Forestry, Jurong, China; Jiangsu Polytechnic College of Agriculture and Forestry, Jurong, China; Jiangsu Polytechnic College of Agriculture and Forestry, Jurong, China; Jiangsu Polytechnic College of Agriculture and Forestry, Jurong, China; Jurong Baisheng Strawberry Cooperative, Jurong, China","2022 7th International Conference on Image, Vision and Computing (ICIVC)","19 Sep 2022","2022","","","772","776","Fusion of hyperspectral image (HSI) and panchromatic image (PI) is an elementary yet crucial topics in remote sensing image processing. The fused data will be characterized by the high spatial and spectral information simultaneously. Although conventional pixel level based fusion methods have been proposed and accomplish the task, they ignore the condition for fusion that the input source images should be registered perfectly. To address the problem, we propose a novel fusion scheme, which aims at fusing HSI and PI for registered and unregistered cases. The proposed scheme first extracts spatial information from PI, and then embeds the spatial information into the HSI. In this way, the proposed method achieves competitive spatial detail and spectral information, which is also demonstrated in experiment results.","","978-1-6654-6734-6","10.1109/ICIVC55077.2022.9886025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9886025","hyperspectral image fusion;unregistered images;spatial detail;experiment assessment","Data mining;Task analysis;Image fusion;Hyperspectral imaging","geophysical image processing;hyperspectral imaging;image fusion;image registration;remote sensing","HSI;spatial information extraction;spectral information;hyperspectral image fusion method;panchromatic imaging;remote sensing image processing;data fusion;input source image fusion scheme;pixel level based fusion methods;PI","","","","25","IEEE","19 Sep 2022","","","IEEE","IEEE Conferences"
"High-reflectivity objects distributed optical satellite image fusion based on NDVI classification","Z. Zexing; X. Qizhi; W. Haibo; Y. Wenyong","Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China; Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China; China Centre for Resources satellite Data and Application, Beijing, China; China Centre for Resources satellite Data and Application, Beijing, China","2017 2nd International Conference on Frontiers of Sensors Technologies (ICFST)","18 Dec 2017","2017","","","231","235","Ratioing method is one type of the most famous fusion methods in remote sensing image fusion domain. Generally, the ratioing method synthesizes a low-resolution panchromatic (Pan) image by adaptive weighted summation of a multispectral (MS) image. Consequently, the accuracy of the weights for low-resolution Pan image synthesis is of great importance. However, in most cases, the optical satellite images contain lots of high-reflectivity objects, such as clouds covered regions, and high-reflectivity buildings. These objects are saturate due to their strong reflectance. The distortion of saturated objects results in the failure of weights calculation, so that causes the color distortion of fused images. To solve the problem, this paper proposes a high-reflectivity objects distributed optical satellite image fusion method based on NDVI classification. First, the NDVI index is employed to classify the pixels of a MS image into high-reflectivity group and normal group, then the pixels in normal group is used to calculate the weighted coefficients, finally the fused image is obtained by ratioing transform. Experimental results on a large number of test images show that the proposed method has good performance on reducing color distortion.","","978-1-5090-4860-1","10.1109/ICFST.2017.8210509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8210509","Image fusion;Pan-sharpening;Remote sensing image;NDVI index","Remote sensing;Optical distortion;Optical sensors;Image color analysis;Image resolution;Optical imaging;Distortion","geophysical image processing;image classification;image colour analysis;image fusion;image resolution;sensor fusion;vegetation;vegetation mapping","high-reflectivity objects;NDVI classification;ratioing method;famous fusion methods;remote sensing image fusion domain;low-resolution panchromatic image;adaptive weighted summation;multispectral image;low-resolution Pan image synthesis;optical satellite images;high-reflectivity buildings;strong reflectance;saturated objects results;weights calculation;color distortion;fused image;optical satellite image fusion method;MS image;high-reflectivity group;test images;ratioing transform;NDVI index","","","","20","IEEE","18 Dec 2017","","","IEEE","IEEE Conferences"
"Image fusion via dynamic gradient sparsity and anisotropic spectral-spatial total variation","C. -C. Zheng; T. -Z. Huang; L. -J. Deng; X. -L. Zhao; H. -X. Dou","School of Mathematical Sciences, University of Electronic Science and Technology of China, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, China","2017 IEEE International Conference on Image Processing (ICIP)","22 Feb 2018","2017","","","1452","1456","In this paper, we develop a sparsity based model for the fusion of a high spatial-resolution image and a multispectral image. The given model is based on the combination of a dynamic gradient sparsity (DGS) and an anisotropic spectral-spatial total variation (ASSTV). We design an alternating direction method of multipliers (ADMM) based algorithm to solve the proposed model. In contrast to existing approaches, the proposed method can generate more spatial details as well as preserve favorable spectral information. Experimental results demonstrate that the proposed approach outperforms several state-of-the-art image fusion methods both quantitatively and visually, in terms of both pansharpening application of remote sensing images and fusion application of natural color images.","2381-8549","978-1-5090-2175-8","10.1109/ICIP.2017.8296522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8296522","","Image fusion;Remote sensing;Color;Mathematical model;Image color analysis;Integrated circuit modeling;Multiresolution analysis","image colour analysis;image fusion;image resolution;remote sensing","image fusion methods;alternating direction method of multipliers;natural color images;fusion application;remote sensing images;multispectral image;spatial-resolution image;anisotropic spectral-spatial total variation;dynamic gradient sparsity","","2","","21","IEEE","22 Feb 2018","","","IEEE","IEEE Conferences"
"Multi-resolution spatial incorporation for MODIS and LANDSAT image fusion using CSSTARFM","Swathika R; T. S. Sharmila","Department of Information Technology, SSN College of Engineering, Kalavakkam, Chennai, India; Department of Information Technology, SSN College of Engineering, Kalavakkam, Chennai, India","2016 IEEE Region 10 Conference (TENCON)","9 Feb 2017","2016","","","691","696","The remote sensing applications require superior of spatial resolution because of the cloud interference and the low temporal resolution. To solve this issue, a novel image fusion system is been proposed in view of a multi-resolution and spatially synthesized. The principle goal of this study is to create Landsat-like engineered information with the fine spatial resolution of Landsat Enhanced Thematic Mapper plus (Landsat ETM+) information and the high fleeting determination of Moderate Resolution Imaging Spectra radiometer (MODIS) information. For the usage of the above approach, we utilized a strategy CSSTARFM (Clustering spectra spatial temporal and spatial reflectance fusion model) in which linear-regression technique was utilized to evaluate the various impacts in sensor frameworks. To alter the spatial variation in surface reflectance a weighted linear blended model was used. The proposed procedure yields a composite image with the spatial resolution of the higher resolution image while holding the spatial and temporal qualities of the medium spatial determination image.","2159-3450","978-1-5090-2597-8","10.1109/TENCON.2016.7848091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848091","Image fusion;Landsat;Moderate Resolution Imaging Spectra radiometer (MODIS);Spatial Synthesized;Multi-resolution Unmixing;Multi-source Unmixing","Satellites;Remote sensing;Earth;MODIS;Spatial resolution;Reflectivity","artificial satellites;geophysical image processing;image fusion;image resolution;reflectivity;regression analysis;remote sensing;spatiotemporal phenomena","multiresolution spatial incorporation;MODIS image fusion;Landsat image fusion;CSSTARFM;remote sensing;spatial resolution;cloud interference;temporal resolution;spatial synthesis;Landsat Enhanced Thematic Mapper-plus information;Landsat ETM+ information;Moderate Resolution Imaging Spectra radiometer information;CSSTARFM strategy;clustering spectra spatial temporal-and-spatial reflectance fusion model;linear-regression technique;sensor frameworks;surface reflectance;weighted linear blended model;composite image;spatial qualities;temporal qualities;medium spatial determination image","","","","22","IEEE","9 Feb 2017","","","IEEE","IEEE Conferences"
"Coastal wetland classification based on high resolution SAR and optical image fusion","J. Yang; G. Ren; Y. Ma; Y. Fan","School of Geosciences, China University of Petroleum Beijing, Changping-qu, Beijing, CN; The First Institute of Oceanography, SOA, Qingdao, China; The First Institute of Oceanography, SOA, Qingdao, China; School of Geosciences, China University of Petroleum Beijing, Changping-qu, Beijing, CN","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","886","889","In this paper, the data source are GF-1 WFV image and Radarsat-2 SAR image covering the Yellow River Estuary wetland eastern area. The paper first uses Gram-Schmidt algorithm for fusing GF-1 image and different polarimetric mode SAR images, and then uses the method of SVM for supervised classification. Finally, the accuracy of the classification results and the capacity of information extraction are compared. The experiment results show:(1) the classification accuracy of fusing the VV polarimetric mode of SAR image and GF-1 image is better than other fusion image, reaching 83.78%, closing to the classification accuracy of GF-1 image. The classification accuracy of tidal flat reed in VV polarimetric fusion image is better than that of GF-1.(2) Tidal flat, river and aquaculture pond have the highest classification accuracy in all the fusion images.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729224","GF-1;SAR;image fusion;SVM;coastal wetland","Synthetic aperture radar;Wetlands;Support vector machines;Remote sensing;Sea measurements;Rivers;Satellites","geophysical image processing;image classification;image fusion;image resolution;radar polarimetry;remote sensing by radar;synthetic aperture radar;terrain mapping;wetlands","coastal wetland classification;data source;GF-1 WFV image;Radarsat-2 SAR image;Yellow River Estuary wetland eastern area;Gram-Schmidt algorithm;GF-1 image fusion;polarimetric mode SAR images;supervised classification accuracy;VV polarimetric mode;tidal flat reed;VV polarimetric fusion image;aquaculture pond;high resolution SAR-optical image fusion;information extraction","","12","","13","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Deep Learning Model for Target Detection in Remote Sensing Images Fusing Multilevel Features","X. Wang; Y. Ban; H. Guo; L. Hong","School of Computer Science, Shaanxi Normal University, Shaanxi, China; School of Computer Science, Shaanxi Normal University, Shaanxi, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore; School of Computer Science, Shaanxi Normal University, Shaanxi, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","250","253","Target detection in remote sensing image has long been one of the research focuses in related areas. This paper proposed a deep learning model for target detection in remote sensing image fusing multilevel features and applied to detect aircrafts in remote sensing images. Because the model is small in size and applies fusion of multilevel features, the detection accuracy of aircraft targets with different scales and denser in remote sensing images has been improved, without compromising the detection speed. A packet fusion reject detection bounding boxes (PFR-DBB) algorithm was also proposed, which is able to better remove redundant detection boxes and further improve detection accuracy. With the experiment results of two remote sensing aircraft data sets detection based on the model, it is proved that small-scale deep networks can also achieve high performance for multi-scale aircraft target detection on small sample data sets.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898759","Feature Fusion;Convolutional Neural Network;Remote Sensing Image;Target Detection;Post Processing","Feature extraction;Remote sensing;Object detection;Aircraft;Atmospheric modeling;Training;Convolution","aircraft;feature extraction;image fusion;learning (artificial intelligence);object detection;remote sensing","remote sensing image;deep learning model;detection accuracy;aircraft targets;detection speed;detection bounding boxes algorithm;redundant detection boxes;remote sensing aircraft data sets detection;multiscale aircraft target detection;remote sensing images;multilevel features fusion","","9","","9","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Sparse representation and PCA method for image fusion in remote sensing","X. Zhang; D. Ni; Z. Gou; H. Ma","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","2016 2nd International Conference on Control, Automation and Robotics (ICCAR)","9 Jun 2016","2016","","","324","330","Image fusion in remote sensing is an issue to fuse the texture information of panchromatic (PAN) channel and the spectral information of multispectral (MS) channels with lower spatial resolution (LR). In this paper, a method named SPCA is proposed to deal with image fusion from the perspective of sparse representation and PCA, in which the correlations both within and between channels are effectively modeled. First, the sparse representation theory is applied to remote sensing images. Second, the dictionaries of PAN and MS images are joint-learned, and a thought of PCA is applied to construct dictionaries of MS images of high spatial resolution (HR). Then the fusion images can be calculated with constructed dictionaries and sharing coefficient. Finally, the residual produced by sparse representation is interpolated as compensation. Compared with four methods in four evaluation indexes, SPCA method gives competitive or even better results on LandSat8 and QuickBird.","","978-1-4673-9859-6","10.1109/ICCAR.2016.7486749","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486749","sparse representation;PCA;interpolation;image fusion;remote sensing","Dictionaries;Remote sensing;Principal component analysis;Image fusion;Satellites;Spatial resolution;Mathematical model","image fusion;image texture;remote sensing","image fusion;texture information;panchromatic channel;PAN channel;spectral information;multispectral channels;MS channels;lower spatial resolution;LR;sparse representation theory;remote sensing images;MS images;high spatial resolution;HR;fusion images;SPCA method;LandSat8;QuickBird","","2","","13","IEEE","9 Jun 2016","","","IEEE","IEEE Conferences"
"Comparative Analysis of Pixel Level Fusion Algorithms in High Resolution SAR and Optical Image Fusion","H. Liu; Y. Ye; J. Zhang; C. Yang; Y. Zhao","Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; The Second Topographic Surveying Brigade of Ministry of Natural Resources, Xian, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2829","2832","Fusion of Synthetic aperture radar (SAR) and optical images is a significant topic in the field of remote sensing. As a typical category of image fusion methods, pixel level image fusion algorithms have been widely used in SAR-optical image fusion to integrate their complementary information and facilitate the subsequent interpretation and application. The effectiveness of these methods has been demonstrated in different literatures based on the experiment carried on specific, individual datasets, which make a comprehensive comparison of these algorithms difficult to achieve. This paper builds a sub-meter SAR and optical image dataset covering different types of scenes, the performance of 11 pixel level image methods is then investigated based on qualitative and quantitative analysis. Result shows the gradient pyramid (GP) achieve a high quality fusion when dealing with Optical-SAR image fusion task of residents, the non subsampled contourlet transform (NSCT) performs best when fusing images containing farmland and mountains.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883331","SAR;optical image;image fusion","Optical distortion;Transforms;Optical imaging;Adaptive optics;Radar polarimetry;Optical sensors;Task analysis","image fusion;optical images;radar imaging;remote sensing;sensor fusion;synthetic aperture radar;transforms","pixel level image fusion algorithms;SAR-optical image fusion;subsequent interpretation;specific datasets;individual datasets;sub-meter SAR;optical image dataset;11 pixel level image methods;qualitative analysis;quantitative analysis;high quality fusion;fusing images;comparative analysis;pixel level fusion algorithms;high resolution SAR;optical images;significant topic;image fusion methods","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"A Decision-Level Fusion Method Based on Convolutional Neural Networks for Remote Sensing Scene Classification","B. Jiang; X. Li; T. Sun; S. Wang","Beijing Institute of Remote Sensing Information; Department of Electronic Engineering, Tsinghua University, Beijing, China; Beijing Institute of Remote Sensing Information; Department of Electronic Engineering, Tsinghua University, Beijing, China","2018 IEEE 3rd Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","16 Dec 2018","2018","","","128","132","Scene classification is a key issue in the field of remote sensing image analysis. In recent years, a lot of scene classification methods have emerged due to the convenient acquisition of high spatial resolution remote sensing images. In the existing literature, the feature-level fusion method is widely used to improve the classification performance. In this paper, we propose a decision-level fusion method based on convolutional neural networks (CNNs) for remote sensing scene classification. The proposed method accomplishes the classification task through two steps. In the first step, a CNN is fine-tuned using the training samples, and the soft-max layer of the CNN is used to obtain the top-N possible classes of each test sample. In the second step, multiple linear support vector machines (SVMs) are trained using the features extracted from the fully-connected layer of a pre-trained CNN. The final class of each test sample is determined by the SVMs from the top-N possible classes obtained in the first step. Experiments on the widely used UC Merced data set and AID data set demonstrate the effectiveness of the proposed method.","2381-0947","978-1-5386-4509-3","10.1109/IAEAC.2018.8577603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8577603","scene classification;remote sensing;convolutional neural network (CNN);fusion","Feature extraction;Image analysis;Remote sensing;Training;Spatial resolution;Convolutional neural networks;Fuses","feature extraction;geophysical image processing;image classification;image fusion;neural nets;remote sensing;support vector machines","decision-level fusion method;convolutional neural networks;remote sensing scene classification;remote sensing image analysis;scene classification methods;high spatial resolution remote sensing images;feature-level fusion method;classification performance;classification task;soft-max layer;feature extraction;multiple linear support vector machines;UC Merced dataset;AID dataset","","3","","19","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Multi-Source Remote Sensing Image Registration Based on Local Deep Learning Feature","Y. Zhang; Z. Zhang; G. Ma; J. Wu","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, P.R. China; Xining Center of Natural Resources Comprehensive Survey, China Geological Survey, Qinghai, P.R. China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, P.R. China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, P.R. China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","3412","3415","Due to huge differences in radiation characteristics and geometric characteristics of multi-source remote sensing images, presenting a big challenge for high-precision registration. In this paper, a new registration method based on deep learning is proposed. First, we use the convolutional neural network to extract deep learning features of the reference and sensed image after adaptive down-sampling, and extract 512-dimensional descriptor on the feature map to calculate the matching result, homography matrix and overlap area. Then, the circumscribed rectangle of the overlapping area is divided into blocks, and the same name point information extracted from all sub-blocks is combined to obtain matching result of source image pair, and then homography matrix of the source image pair is estimated. Finally, the registration result is obtained. Results show that the proposed algorithm has strong adaptability and robustness in the registration of multiple heterogeneous images in mountains, hills and plains.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553142","Image registration;Deep learning;Overlap area;Multi-source remote sensing image","Deep learning;Image sensors;Image registration;Feature extraction;Optical imaging;Radar polarimetry;Robustness","feature extraction;geophysical image processing;image fusion;image matching;image registration;learning (artificial intelligence);neural nets;remote sensing","deep learning features;512-dimensional descriptor;feature map;homography matrix;overlap area;overlapping area;source image pair;registration result;multiple heterogeneous images;multisource remote sensing image registration;local deep learning feature;huge differences;radiation characteristics;geometric characteristics;multisource remote sensing images;high-precision registration;registration method;convolutional neural network","","1","","5","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Change Cross-Detection Based on Label Improvements and Multi-Model Fusion for Multi-Temporal Remote Sensing Images","Z. Li; F. Lu; H. Zhang; G. Yang; L. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, P.R. China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, P.R. China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, P.R. China; School of Electronic Information, Wuhan University, Wuhan, P.R. China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, P.R. China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2054","2057","Change detection is a geospatial application for social good whose development is restricted by a slow-growing labeling technology and outdated classification labels for remotely sensed images. In this paper, a change cross-detection method based on label improvements and multi-model fusion is proposed for Multi-temporal Semantic change Detection (MSD) with weak, noisy, and low-resolution labels. For unmatched labels, a Siamese Skip_FCN network is proposed to generate preliminary labels at high-resolution. Subsequently, a multi-model fusion method is introduced to perform accurate and stable land cover classification. In addition, a cross-detection structure is used to generate high precision change maps and a post-processing step further improves the final results. In the track MSD of the 2021 Data Fusion Contest (DFC21-MSD), the proposed method achieved a mean intersection over union (mIoU) of 70.25% in phase 1 and 67.72% in phase 2, ranking first in both phases [1].","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553120","National Natural Science Foundation of China(grant numbers:42071322); Natural Science Foundation of Hubei Province(grant numbers:2020CFA053); Natural Science Foundation of Qinghai Province(grant numbers:2020-ZJ-927); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553120","Change detection;models fusion;Multi-resolution;pseudo labels","Semantics;Data integration;Geospatial analysis;Noise measurement;Labeling;Remote sensing","geophysical image processing;image classification;image fusion;image resolution;land cover;object detection;remote sensing;terrain mapping","multitemporal semantic change detection;Siamese Skip-FCN network;stable land cover classification;accurate land cover classification;multimodel fusion method;unmatched labels;low-resolution labels;change cross-detection method;remotely sensed images;outdated classification labels;labeling technology;geospatial application;multitemporal remote sensing;data fusion contest;high precision change maps;cross-detection structure","","3","","7","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Different Levels Multi-source Remote Sensing Image Fusion","K. Lin; W. Li; H. Liu; J. Wu","School of Geographic and Biologic Information, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Geographic and Biologic Information, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Telecommunications & Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Geographic and Biologic Information, Nanjing University of Posts and Telecommunications, Nanjing, China","2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)","21 Aug 2020","2019","","","1","5","In order to study the fusion methods of multi-source remote sensing image, this paper plans to fuse the optical images and synthetic aperture radar (SAR) images in pixel level, feature level and decision level, respectively. For the purpose of combining the merits of fusion at aforementioned levels, the fusion results are assigned different weights for multi-level fusion. Several qualitative and quantitative evaluations are also presented, and they show that the multi-level fusion does pretty-well performance in brightness, sharpness, contrast, amount of information, classification effect, etc. It concludes that the multi-level fusion can significantly extract much information while retaining more details of the original image.","","978-1-7281-2345-5","10.1109/ICSIDP47821.2019.9173281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173281","Multi-source remote sensing image;SAR;fusion;qualitative and quantitative evaluation","","geophysical image processing;image fusion;optical images;radar imaging;remote sensing;sensor fusion;synthetic aperture radar","multilevel fusion;different levels multisource remote sensing image;fusion methods;optical images;synthetic aperture radar images;pixel level;feature level;decision level;aforementioned levels;fusion results","","","","14","IEEE","21 Aug 2020","","","IEEE","IEEE Conferences"
"Comparison and Analysis of Image Fusion Algorithms for GF-2 Satellite","W. -Z. Zhou; J. -Z. Li","The School of Information Science& Engineering, Hebei University of Science and Technology, China; The School of Information Science& Engineering, Hebei University of Science and Technology, China","2018 10th International Conference on Modelling, Identification and Control (ICMIC)","11 Nov 2018","2018","","","1","6","Image fusion is an indispensable step in the technology of satellite image processing. After orthorectification and registration can image fusion, the image fusion can effectively increase the utilization rate of the image and improve the discrimination of objects in the image. Firstly, this article introduces the background and significance of image fusion technology nowadays, and then introduced the related algorithms of image fusion in detail. Finally, the algorithm is implemented through experiments. The experiment adopts “GF-2” satellite to acquire the image data. The Xiongan New Area divided by Hebei Province was chosen as the experimental area. Comparing and analyzing the Baiyangdian Wharf in Anxin County, which is rich in features, high contrast, and low cloudiness. Several fusion algorithms were compared and analyzed by visual observation and quantitative analysis. The experimental results in this paper can be used to provide a reference for the fusion and application of “GF-2” satellite image data.","","978-1-5386-5416-3","10.1109/ICMIC.2018.8529899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8529899","Image fusion;Compared;Satellite imagery","Image fusion;Satellites;Interpolation;Spatial resolution;Principal component analysis;Wavelet analysis","geophysical image processing;image fusion;image processing;remote sensing","satellite image data;image fusion technology;satellite image processing;GF-2 satellite;image fusion algorithms","","","","18","IEEE","11 Nov 2018","","","IEEE","IEEE Conferences"
"Research and implementation of high resolution remote sensing image registration method","W. -z. Zhou; Q. -x. Li","School of Information Science & Engineering, Hebei University of Science and Technology, China; School of Information Science & Engineering, Hebei University of Science and Technology, China","2017 9th International Conference on Modelling, Identification and Control (ICMIC)","22 Mar 2018","2017","","","699","704","Image registration is one of the most popular image processing technologies in recent years. It is an indispensable step in image fusion, change detection, image mosaic and so on. This paper first introduces the basic principle, remote sensing image registration procedures, and, based on regional characteristics, mixed model and physical model of the four main registration methods, advantages and disadvantages of several methods of image registration are briefly expounded. Secondly, this paper selects the registration method based on feature in Tangshan area of Hebei Province as the study area, the image data acquisition through the “GF-1” remote sensing satellite remote sensing image, respectively in 2015 and 2016 in Tangshan registration process, and finally through the artificial comparison and analysis. The changes of total nodes that surface of grassland and buildings in the Tangshan area and two years land use, to provide data support for the relevant departments.","","978-1-5090-6575-2","10.1109/ICMIC.2017.8321545","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8321545","Remote Sensing Image;Image Registration;Feature","Feature extraction;Image registration;Remote sensing;Dogs;Computational modeling;Deformable models;Analytical models","data acquisition;geophysical image processing;image fusion;image processing;image registration;image segmentation;remote sensing","GF-1 remote sensing satellite remote sensing image;Tangshan registration process;Tangshan area;high resolution remote;image registration method;popular image processing technologies;image fusion;image mosaic;remote sensing image registration procedures;main registration methods;image data acquisition","","","","15","IEEE","22 Mar 2018","","","IEEE","IEEE Conferences"
"Long Time Sequence Monitoring of Chaohu Algal Blooms Based on Multi-source Optical and Radar Remote Sensing","H. Shiyu; M. Xiaoshuang; W. Yanlan","School of Resources and Environmental Engineering, Anhui University Anhui University Collaborative Innovation Center for Mine Environmental Restoration and Wetland Ecological Security, Hefei, Anhui, China; School of Resources and Environmental Engineering, Anhui University Anhui University Collaborative Innovation Center for Mine Environmental Restoration and Wetland Ecological Security, Hefei, Anhui, China; School of Resources and Environmental Engineering, Anhui University line 3: Anhui University Collaborative Innovation Center for Mine Environmental Restoration and Wetland Ecological Security, Hefei, Anhui, China","2018 Fifth International Workshop on Earth Observation and Remote Sensing Applications (EORSA)","3 Jan 2019","2018","","","1","5","In this study, the multi-source optical remote sensing and radar remote sensing from 2008 to 2017 are used to perform long-term sequential monitoring of Chaohu blooms. A spatio-temporal fusion model is employed to fill the gap period to obtain temporal and spatial changes of Chaohu bloom in the past ten years. Through the principal component analysis technology, the mechanisms of the main climate-driven factors in the change of water area are explored. The experimental results show that the combination of multi-source optical and radar remote sensing and spatio-temporal fusion model can be well applied to long-term bloom monitoring; besides, it has also been found that the area of water bloom in Chaohu Lake increases with the increase of temperature, wind speed and sunshine duration.","","978-1-5386-6642-5","10.1109/EORSA.2018.8598609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598609","optical and radar remote sensing;Chaohu Lake;spatio-temporal fusion;long time sequence monitoring","Remote sensing;Radar remote sensing;Lakes;Optical sensors;Earth;Optical imaging;Artificial satellites","environmental monitoring (geophysics);geophysical image processing;image fusion;lakes;principal component analysis;remote sensing by radar;sunlight","long time sequence monitoring;Chaohu algal blooms;radar remote sensing;Chaohu bloom;spatio-temporal fusion model;temporal changes;spatial changes;long-term bloom monitoring;water bloom;principal component analysis technology","","1","","21","IEEE","3 Jan 2019","","","IEEE","IEEE Conferences"
"Multi-sensor image fusion of the lunar image data using DT-CWT and curvelet transform","A. A. Micheal; K. Vani","Anna University Chennai, Chennai, Tamil Nadu, IN; Department of Information Science, Technology Anna University, Chennai, Tamil Nadu, India","2017 4th International Conference on Electronics and Communication Systems (ICECS)","16 Oct 2017","2017","","","49","53","Multisensor image fusion is the process of combining relevant information from high spatial resolution image and high spectral resolution image. This paper proposes a new image fusion method based on Dual Tree-Complex Wavelet Transform (DTCWT), and Curvelet transform for remotely sensed lunar image data in order to extract features accurately. Different fusion techniques have been used in the past separately for spatial and spectral quality image enhancement. In this study, we use a new image fusion technique based on Dual Tree - Complex Wavelet Transform (DT-CWT) and Curvelet transforms. Results indicate that the fused lunar image shows good spatial fidelity and the spectral resolution of the fused product was preserved after image data fusion. It is seen that 95.98% of the spectral content is preserved by curvelet fusion. From the results of statistical evaluation parameters demonstrated for the two study sites, it is found that curvelet transform gives better results than the other techniques commonly used.","","978-1-5090-3355-3","10.1109/ECS.2017.8067835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8067835","Image Fusion;Multi Sensor;Lunar Image;DT-CWT;Curvelet Transform","Spatial resolution;Image fusion;Wavelet transforms;Moon;Correlation coefficient","curvelet transforms;geophysical image processing;image enhancement;image fusion;image resolution;remote sensing;trees (mathematics);wavelet transforms","multisensor image fusion;DT-CWT;high spatial resolution image;high spectral resolution image;image fusion method;Dual Tree-Complex Wavelet Transform;remotely sensed lunar image data;spatial quality image enhancement;spectral quality image enhancement;image data fusion;curvelet fusion","","1","","10","IEEE","16 Oct 2017","","","IEEE","IEEE Conferences"
"Multi-angle SAR image fusion algorithm based on visibility classification of non-layover region targets","S. Zhu; D. Ran","School of Space Information, Space Engineering University, Beijing, China; School of Space Information, Space Engineering University, Beijing, China","2017 International Conference on Security, Pattern Analysis, and Cybernetics (SPAC)","1 Mar 2018","2017","","","642","647","In order to reduce the layover region in traditional single-angle synthetic aperture radar (SA-SAR) image of mountainous area, a multi-angle synthetic aperture radar (MA-SAR) image fusion algorithm based on visibility classification of non-layover region targets is proposed. By defining a practical index named multi-angle visibility of non-layover region targets which used for automatic pixel classification, the proposed algorithm calculates the visibility index of every pixel of SA-SAR images obtained from different observation angles and fuses those pixels with the same visibility index to form the fused image. The fused image can effectively eliminate all adverse effects on target detection and classification which caused by the phenomenon of layover, and realize a precision MA-SAR imaging of mountainous area. The simulation results have verified the effectiveness of the proposed algorithm.","","978-1-5386-3016-7","10.1109/SPAC.2017.8304355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8304355","Multi-angle synthetic aperture radar (MA-SAR);Image fusion;Multi-angle visibility index;Automatic pixel classification","Filtering;Indexes;Synthetic aperture radar;Image fusion;Image segmentation;Speckle;Imaging","geophysical image processing;image classification;image fusion;object detection;radar imaging;radar resolution;remote sensing by radar;synthetic aperture radar;visibility","multiangle SAR image fusion algorithm;visibility classification;layover region;single-angle synthetic aperture radar image;mountainous area;multiangle synthetic aperture radar image fusion algorithm;multiangle visibility;automatic pixel classification;visibility index;SA-SAR images;target detection;precision MA-SAR imaging;observation angles;image fusion;nonlayover region targets","","4","","9","IEEE","1 Mar 2018","","","IEEE","IEEE Conferences"
"Hyperspectral and Multispectral Image Fusion: From Model-Driven to Data-Driven","Y. Zhao; H. Yan; S. Liu","School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; Shanghai Institute of Aerospace Technology, Shanghai, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","1256","1259","Hyperspectral image (HSI) provides rich spectral information, which has been used in object detection, environmental protection. However, HSI suffers from low spatial resolution owing to the limitations of imaging systems. Hyperspectral and multispectral image (MSI) fusion is an efficient way to enhance the spatial resolution of HSI. In past decades, many HSI and MSI fusion algorithms have been presented in the literature. In this paper, we present a comprehensive review for the HSI-MSI fusion methods. According to the characteristics and trend of HSI-MSI fusion methods, they are categorized as two classes: model-driven approaches and data-driven approaches. We clarify their characteristics, advantages and also make a comparison and discussion for the fusion methods in each category. Additionally, we analyze the existing challenges and present the potential research directions for HSI - MSI fusion method.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553692","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553692","hyperspectral and multispectral image fusion;model-driven;data-driven","Tensors;Imaging;Geoscience and remote sensing;Object detection;Market research;Matrices;Bayes methods","geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;image resolution;image sensors;object detection;remote sensing;sensor fusion","HSI-MSI fusion methods;model-driven approaches;data-driven approaches;MSI fusion method;multispectral image fusion;hyperspectral image;rich spectral information;low spatial resolution;imaging systems;MSI fusion algorithms","","1","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Construction and Application of a Post-Quake House Damage Model Based on Multiscale Self-Adaptive Fusion of Spectral Textures Images","R. Zhang; Y. Zhou; S. Wang; F. Wang; T. Zhang; Y. He; S. You","Land Satellite Remote Sensing Application Center, Ministry of Natural Resources, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Land Satellite Remote Sensing Application Center, Ministry of Natural Resources, Beijing, China; Land Satellite Remote Sensing Application Center, Ministry of Natural Resources, Beijing, China; Land Satellite Remote Sensing Application Center, Ministry of Natural Resources, Beijing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","6631","6634","In the disaster research field, extraction of post-disaster damaged house building information plays a critical role in post-disaster emergency rescue and disaster-induced damage assessment. In this study, we propose a method of automatically extracting house damage information from post-quake high-resolution optical remote-sensing imagery through multiscale fusion of spectral texture features. This is achieved in three steps. First, the texture features and spectral features of images are enhanced at the pixel level; then, the resulted feature images are fused at the feature level and the fused feature images are superpixel-segmented; finally, a post-quake house damage index model is constructed. The results show an overall accuracy of 76.75%, 75.35%, and 83.25% for the three different types of imagery studied. This suggests that our algorithm is applicable to extracting damage information from multisource remote sensing data and can provide useful guidance for post-disaster rescue and assessment based on regional house damage conditions.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323286","High resolution remote sensing;Superpixel;Damage modeling;Object oriented","Feature extraction;Remote sensing;Data mining;Indexes;Buildings;Image segmentation;Image color analysis","disasters;earthquakes;feature extraction;geophysical image processing;image classification;image fusion;image resolution;image segmentation;image texture;remote sensing","disaster research field;post-disaster damaged house building information;post-disaster emergency rescue;disaster-induced damage assessment;automatically extracting house damage information;post-quake high-resolution;remote-sensing imagery;multiscale fusion;spectral texture features;resulted feature images;post-quake house damage index model;multisource remote sensing data;post-disaster rescue;regional house damage conditions;post-quake house damage model;multiscale self-adaptive fusion;spectral textures","","","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A Novel Deep Feature Fusion Network For Remote Sensing Scene Classification","Y. Li; Q. Wang; X. Liang; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, Shaanxi Province, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","5484","5487","In this paper, we analyze the performance of a novel deep feature fusion network when applied to the problem of remote sensing scene classification. So far, many classical convolutional neural network models have shown remarkable performance in image classification. With the availability of the latest high resolution remote sensing image data, traditions CNN models‘ performance has been considerably reduced. In order to tackle this condition, the deep convolutional neural networks are used in recent studies. Their classification accuracy depends on the depth of the network, while a deeper network will bring about higher computational complexity. In this work, we employ a deep feature fusion model for remote sensing scene classification, which uses the features extracted from Deep ResNet50 and VGG16 which are pre-trained and fine-tuned. The analysis of the experimental data prove the feasibility of the feature fusion network in remote sensing scene classification.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898900","Remote sensing scene classification;ResNet;VGG16;feature fusion;fine-tuned CNN model","Feature extraction;Remote sensing;Image analysis;Convolutional neural networks;Visualization;Computational modeling;Data models","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing","deep feature fusion;remote sensing scene classification;image classification;deep convolutional neural networks;feature extraction;Deep ResNet50;VGG16;remote sensing image resolution","","8","","15","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Gradual Deep Residual Network for Remote Sensing Images Fusion","M. Ilyas Tchenar; Y. Wang; Q. Liu; M. El Amin Larabi","School of Computer Science and Engineering, Beihang University; School of Computer Science and Engineering, Beihang University; School of Computer Science and Engineering, Beihang University; Algerian Space Agency","2020 IEEE 5th International Conference on Signal and Image Processing (ICSIP)","4 Feb 2021","2020","","","269","274","Remote sensing image fusion (also known as pansharpening) aims to improve the spatial resolution of multispectral (MS) images using high frequency details extracted from Panchromatic (PAN) images. Recently, residual learning (ResNet) exhibits improved performance in many application domains. At the same time, numerous upsampling methods were developed, from the classical interpolation to deep learning based methods. In this paper, the Gradual Deep Residual Network (GDRN) is developed. The principle key of GDRN is that: instead of using one step upsampling layer, we progressively upscale and fuse MS and PAN images at two pyramid levels where each of them consists of a residual block and one step upsampling layer with a scale factor of 2. Experimental results of the proposed approach show better performance than classical methods and competitive performance against the state-of-the-art approaches.","","978-1-7281-6896-8","10.1109/ICSIP49896.2020.9339380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9339380","component;pansharpening;multispectral;panchromatic;residual network","Interpolation;Pansharpening;High frequency;Spatial resolution;Remote sensing;Image fusion;Residual neural networks","feature extraction;geophysical image processing;image fusion;image resolution;interpolation;learning (artificial intelligence);remote sensing","spatial resolution;multispectral images;high frequency details;Panchromatic images;residual learning;numerous upsampling methods;deep learning based methods;Gradual Deep Residual Network;GDRN;step upsampling layer;residual block;classical methods;Gradual deep residual network;remote sensing images fusion;sensing image fusion","","","","29","IEEE","4 Feb 2021","","","IEEE","IEEE Conferences"
"Hyperspectral-Multispectral Image Fusion Using Nndiffuse: Performance Assessment Using A Pixel Classification Task","R. Ducay; D. Messinger","Rochester Institute of Technology Chester F. Carlson Center for Imaging Science 54 Lomb Memorial Dr, Rochester, NY, U.S.A; Rochester Institute of Technology Chester F. Carlson Center for Imaging Science 54 Lomb Memorial Dr, Rochester, NY, U.S.A","2022 12th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","22 Nov 2022","2022","","","1","5","The spatial resolution of a hyperspectral image (HSI) can be enhanced using a higher-resolution co-registered multi-spectral image and an image fusion algorithm. Assessing the quality of the fused image is traditionally done by way of Wald’s protocol using image-wide quality metrics but these indices alone may not be enough to predict whether or not the enhanced image results in better performance on a task such as target detection or pixel classification. In this paper we demonstrate hyperspectral-multispectral image fusion using NNDIFFUSE (nearest neighbor-based diffusion algorithm) and assessment of fusion performance via pixel classification. Three classifiers are used (SVM and two 3D deep learning-based classifiers) and their performance on the reference HSI and fused imagery are compared using three post-classification metrics: overall accuracy, mean per-class F1 score, and kappa coefficient. Results show that among fusion algorithms compared, NNDIFFUSE provides the best image fusion product for a pixel classification task.","2158-6276","978-1-6654-7069-8","10.1109/WHISPERS56178.2022.9955122","National Geospatial-Intelligence Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955122","Hyperspectral;multispectral;image fusion;NNDIFFUSE;classification","Measurement;Support vector machines;Three-dimensional displays;Signal processing algorithms;Prediction algorithms;Classification algorithms;Task analysis","deep learning (artificial intelligence);geophysical image processing;hyperspectral imaging;image classification;image fusion;image registration;image resolution;image sensors;object detection;pattern classification;remote sensing;support vector machines","3D deep learning-based classifiers;fusion algorithms;hyperspectral image;hyperspectral-multispectral image fusion;image fusion algorithm;image fusion product;image-wide quality metrics;nearest neighbor-based diffusion algorithm;NNDIFFUSE;pixel classification task;post-classification metrics;spatial resolution;SVM","","","","14","IEEE","22 Nov 2022","","","IEEE","IEEE Conferences"
"Inter-Sensor Remote Sensing Image Enhancement for Operational Sentinel-2 and Sentinel-3 Data Products","R. Fernandez; R. Fernandez-Beltran; F. Pla","Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1504","1507","The recent availability of operational data from the Sentinel-2 and Sentinel-3 missions provides widespread opportunities to generate diverse high-level remote sensing products. However, the synergies between both multi-spectral instruments are often difficult to exploit from an operational perspective. Standard pansharpening algorithms may encounter important disadvantages due to the limited intersensor data availability in actual production environments. Moreover, the lack of a real high-resolution ground-truth for super-resolution techniques may affect the radiometric quality of the final result. In this scenario, this work investigates the viability of using the Multi-Spectral Instrument of Sentinel-2 for super-resolving data products acquired by the Ocean and Land Colour Instrument of Sentinel-3. Specifically, we define an inter-sensor image enhancement framework which combines a PCA-based component substitution pansharpening scheme with a CNN-based spatial enhancing super-resolution mapping. The conducted experiments reveal the suitability of the proposed approach for generating Level-4 data products within the Copernicus programme context.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324071","Sentinel-2 (S2);Sentinel-3 (S3);super-resolution (SR);pansharpening;image fusion","Spatial resolution;Pansharpening;Principal component analysis;Instruments;Remote sensing;Image enhancement;Superresolution","geophysical image processing;image enhancement;image fusion;image resolution;remote sensing;terrain mapping","inter-sensor remote sensing image enhancement;operational Sentinel-2;Sentinel-3 data products;recent availability;operational data;Sentinel-3 missions;diverse high-level remote sensing products;multispectral instruments;operational perspective;standard pansharpening algorithms;intersensor data availability;actual production environments;high-resolution ground-truth;super-resolution techniques;MultiSpectral Instrument;inter-sensor image enhancement framework;super-resolution mapping;Level-4 data products","","","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Otsu Thresholding based Image Fusion Framework using Contour-let Transform","S. K. Panguluri; L. Mohan","Department of Electronics and Communication Engineering, Vignan's Foundation for Science Technology, and Research Vadlamudi, Guntur, Andhra Pradesh, India; Department of Electronics and Communication Engineering, Vignan's Foundation for Science Technology, and Research Vadlamudi, Guntur, Andhra Pradesh, India","2021 6th International Conference on Inventive Computation Technologies (ICICT)","26 Feb 2021","2021","","","686","693","Nowadays the result of infrared (I-R) and visible (V-I) image fusion is significantly used in major applications such as surveillance, remote sensing, military, etc. in order to enhance visibility and improve situation awareness. Most importantly fusion algorithm is applied on those I-R image and V-I image that capture the same scene information during low light and adverse weather conditions such as fog, snow, and dust. This paper is presenting a novel image-fusion framework for improving scene information that is captured during low light and adverse weather conditions. In proposed algorithm the contrast of the source images is improved by using “morphology hat transform”. Contour-let transform has been used for decomposition of source images. The “Otsu thresholding based weighted fusion rule” has been introduced in this algorithm in order to combine “low-frequency coefficients” for improving the visual quality performance of fused image. In order to improve the edge information of final fused image, here “high frequency coefficients” are integrated by “max fusion rule”. Finally, fused image reconstruction is done by using inverse contour-let transform. Proposed method has produced much better results than latest similar existing methods both in visual quality comparison and also in metric-value comparison.","","978-1-7281-8501-9","10.1109/ICICT50816.2021.9358794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9358794","Infrared image;Visible image;Contour-let transform;Otsu thresholding based weighted fusion rule","Visualization;Surveillance;Snow;Transforms;Image fusion;Remote sensing;Image reconstruction","image fusion;image recognition;image reconstruction;image segmentation","inverse contour;fused image reconstruction;max fusion rule;final fused image;edge information;low-frequency coefficients;weighted fusion rule;morphology hat transform;source images;novel image-fusion framework;adverse weather conditions;low light;scene information;importantly fusion algorithm;situation awareness;visibility;remote sensing;visible image fusion;image fusion framework;Otsu thresholding","","","","19","IEEE","26 Feb 2021","","","IEEE","IEEE Conferences"
"Cross-Modal Feature Fusion Retrieval for Remote Sensing Image-Voice Retrieval","R. Yang; Y. Gu; Y. Liao; H. Zhang; Y. Sun; S. Wang; B. Hou; L. Jiao; H. Zhang","School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; School of Journalism and Communication, Northwest University, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2855","2858","With the increasing popularity of remote sensing technology applications, some emergency scenarios require rapid retrieval of remote sensing images, such as earthquake rescue, etc. Due to the high efficiency of voice input, researchers have focused on cross-modal remote sensing image-voice retrieval methods. However, these methods have two major drawbacks: speech input lacks discrimination and the intra-modal semantic information is under used. To address these drawbacks, we propose a novel cross-modal feature fusion retrieval model. Our model provides a more optimized cross-modal common feature space than previous models and thus optimizes the retrieval performance. First, our model adds the extra textual keyword information to the audio feature for remote sensing image retrieval. Second, it introduces inter-modality adversarial learning and intra-modality semantic discrimination into the remote sensing image-voice retrieval task. We conducted experiments on two datasets modified from the UCM-Captions dataset and the Remote Sensing Image Caption Dataset. The experimental results show that our model outperforms state-of-the-art models in this task.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554533","National Natural Science Foundation of China(grant numbers:61771379); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554533","cross-modal retrieval;remote sensing;feature fusion;adversarial learning","Visualization;Semantics;Image retrieval;Earthquakes;Adversarial machine learning;Sensors;Task analysis","feature extraction;geophysical image processing;image fusion;image retrieval;learning (artificial intelligence);remote sensing","remote sensing technology applications;rapid retrieval;remote sensing images;voice input;cross-modal remote sensing image-voice retrieval methods;optimized cross-modal;retrieval performance;remote sensing image retrieval;remote sensing image-voice retrieval task;remote sensing image caption dataset;intermodality adversarial learning;intramodality semantic discrimination;cross-modal feature fusion retrieval model;intramodal semantic information;UCM-Captions dataset","","2","","11","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Dual Lightweight Network with Attention and Feature Fusion for Semantic Segmentation of High-Resolution Remote Sensing Images","Y. Zhang; Y. Chen; Q. Ma; C. He; J. Cheng","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, P.R.China; School of Electronic Science and Engineering, University of Electronic Science and Technology of China, Chengdu, P.R.China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, P.R.China; Sichuan Jiuzhou Eletric Group Co., Ltd, Mianyang, P.R.China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, P.R.China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2755","2758","Semantic segmentation of high-resolution remote sensing (HRRS) images has been a long-term research topic in the field of remote sensing. Nowdays, many excellent networks based on deep learning have been applied in various remote sensing fields. However, these networks always have a large number of network parameters and rely on extensive computing resources. To solve the above problems, we propose a lightweight dual branches network with the attention modules and the feature fusion module. The backbone networks of dual branches, which have fewer parameters, are used to obtain the detail information and the context information respectively. The attention modules are used to establish full-image dependencies over the local feature representations. The feature fusion module is used to fuse the low-leve features and the high-leve features effectively. Compared to other popular networks, our network has better results evaluated on ISPRS Vaihingen Dataset while with fewer parameters(8M).","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553680","Beijing Natural Science Foundation(grant numbers:L182029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553680","Semantic segmentation;remote sensing;attention mechanism;feature fusion;deep learing","Deep learning;Image segmentation;Fuses;Semantics;Sensors;Remote sensing","feature extraction;geophysical image processing;image classification;image fusion;image representation;image resolution;image segmentation;learning (artificial intelligence);object detection;remote sensing;traffic engineering computing","dual lightweight network;semantic segmentation;high-resolution remote sensing images;long-term research topic;excellent networks;deep learning;remote sensing fields;network parameters;extensive computing resources;lightweight dual branches network;attention modules;feature fusion module;backbone networks;full-image dependencies;local feature representations;low-leve features;high-leve features;popular networks","","2","","12","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Multiscale Infrared and Visible Image Fusion Based on Phase Congruency and Saliency","J. Chen; K. Wu; L. Luo; X. Chen; Y. Gu; X. Tian","Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems; School of Mechanical Engineering and Electronic Information, China University of Geosciences; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems; Electronic Information School, Wuhan University","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","224","227","In this paper, in order to enhance the infrared target in infrared image and retain the edge and detail information in visible image, we propose a multi-scale decomposition fusion method based on phase congruency and saliency. In this method, the Laplacian pyramid is first used to decompose the source image into detail layers and base layers. Secondly, we use a method based on phase congruency for the fusion of detail layers. Thirdly, for the base layer, we decompose it into saliency map and residual map. The “max absolute” rule and “averag” rule are adopted for the fusion of saliency map and residual map, then the fused saliency map and residual map are added to attain the fused base image. Finally, we use the inverse transform of Laplacian pyramid to reconstruct the fused image. The experimental results show that the proposed method have better fusion effect than other methods. What's outstanding is that the infrared targets in the fused image are enhanced and abundant edges are preserved.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324363","NSFC(grant numbers:61971315,41977242,61973283); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324363","Image fusion;saliency;phase congruency","Image fusion;Image edge detection;Laplace equations;Image reconstruction;Discrete wavelet transforms;Target recognition;Fuses","decomposition;image fusion;image reconstruction;infrared imaging;inverse transforms","residual map;fused saliency map;fused base image;Laplacian pyramid;fused image;fusion effect;infrared target;visible image fusion;phase congruency;infrared image;multiscale decomposition fusion method;multiscale infrared image fusion;fused image reconstruction","","1","","19","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Remote Sensing Image Super-Resolution Via Attentional Feature Aggregation Generative Adversarial Network","F. Cai; K. -Y. Wu; F. Wang","Key Laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2598","2601","The extraction of high-frequency details is generally neglected in single image super-resolution (SISR) for remote sensing images. In this paper, we propose an attentional feature aggregation generative adversarial network (AFA-GAN) with the capability of strong feature extraction and attentional feature fusion to generate high-resolution remote sensing images. We adopt the residual feature aggregation framework for the feature extraction to make full use of the hierarchical features on the residual branches. To better fuse global and local features with inconsistent scales, an attentional feature fusion mechanism is utilized in residual feature aggregation modules. The comprehensive experiments with state-of-the-art SISR methods on the UC Merced dataset demonstrate the effectiveness and superiority of our AFA-GAN.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884863","National Natural Science Foundation of China(grant numbers:61901122); Natural Science Foundation of Shanghai(grant numbers:20ZR1406300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884863","Remote sensing images;single image super-resolution (SISR);attentional feature aggregation (AFA);generative adversarial network (GAN)","Visualization;Fuses;Superresolution;Feature extraction;Generative adversarial networks;Remote sensing;Image reconstruction","feature extraction;geophysical image processing;image fusion;image reconstruction;image resolution;remote sensing","high-resolution remote sensing images;residual feature aggregation framework;hierarchical features;global features;local features;attentional feature fusion mechanism;residual feature aggregation modules;AFA-GAN;remote sensing image super-resolution;high-frequency details;single image super-resolution;attentional feature aggregation generative adversarial network;strong feature extraction","","","","18","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Research on spatio-temporal fusion algorithm of remote sensing image based on GF-1 WFV and Sentinel-2 satellite data","S. Wang; X. Yang; G. Li; Y. Jin; C. Tian","Innovation Center, Hebei Aerospace Remote Sensing Information Technology, LangFang, China; Innovation Center, Hebei Aerospace Remote Sensing Information Technology, LangFang, China; Innovation Center, Hebei Aerospace Remote Sensing Information Technology, LangFang, China; Innovation Center, Hebei Aerospace Remote Sensing Information Technology, LangFang, China; Innovation Center, Hebei Aerospace Remote Sensing Information Technology, LangFang, China","2022 3rd International Conference on Geology, Mapping and Remote Sensing (ICGMRS)","18 Aug 2022","2022","","","667","678","The remote sensing data set with high spatial and temporal resolution is of great significance for monitoring surface change. However, the earth observation satellites at domestic and international cannot obtain high spatial resolution and high temporal resolution images at the same time. Remote sensing data spatio-temporal fusion technology is an effective means to solve this problem. In this paper, the multi-period GaoFen-1 WideField-View (GF-1 WFV) satellite images and Sentinel-2 satellite images of Beijing-Tianjin-Hebei region in 2020 are used to perform fusion simulation for the three spatio-temporal fusion algorithms of Spatial and Temporal Adaptive Reflectance Fusion Model (STARFM), Enhanced-STARFM (ESTARFM) and Enhanced Flexible Spatiotemporal Data Fusion Model (EFSDAF) and these simulation degree of the results are quantitatively evaluated. The results show that ESTARFM algorithm is more suitable for the construction of high spatio-temporal fusion data of GF-1 WFV and Sentinel-2 satellites in Beijing-Tianjin-Hebei region.","","978-1-6654-8595-1","10.1109/ICGMRS55602.2022.9849377","Natural Science Foundation of Hebei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9849377","STARFM;ESTARFM;EFSDAF;Temporal and spatial fusion;GF-1 WFV images","Adaptation models;Satellites;Time series analysis;Vegetation mapping;Lakes;Market research;Data models","geophysical image processing;image fusion;image resolution;remote sensing;sensor fusion;spatiotemporal phenomena","high spatio-temporal fusion data;GF-1 WFV;Sentinel-2 satellites;Beijing-Tianjin-Hebei region;spatio-temporal fusion algorithm;remote sensing image;Sentinel-2 satellite data;high spatial resolution;earth observation satellites;high temporal resolution images;remote sensing data spatio-temporal fusion technology;multiperiod GaoFen-1 WideField-View;Sentinel-2 satellite images;fusion simulation;Temporal Adaptive Reflectance Fusion Model;Enhanced Flexible Spatiotemporal Data Fusion Model;ESTARFM algorithm","","","","10","IEEE","18 Aug 2022","","","IEEE","IEEE Conferences"
"Review of Research on Registration of SAR and Optical Remote Sensing Image Based on Feature","L. Kai; Z. Xueqing","National Defense University of Technology, Information and Communication Engineering College, Wu Han, China; National Defense University of Technology, Information and Communication Engineering College, Wu Han, China","2018 IEEE 3rd International Conference on Signal and Image Processing (ICSIP)","3 Jan 2019","2018","","","111","115","Synthetic Aperture Radar(SAR) and optical remote sensing image registration is the prerequisite for image fusion and it is of important theoretical significance and practical value. The image registration methods are mainly divided into the methods based on feature, the methods based on Gray-scale and others. This article systematically sorts out feature-based optical and SAR remote sensing image registration techniques, summarizes all types of image registration, points out their advantages and disadvantages and predicts the prospects of their future.","","978-1-5386-6396-7","10.1109/SIPROCESS.2018.8600443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8600443","synthetic aperture radar(SAR);remote sensing;image registration;feature-based","Synthetic aperture radar;Optical sensors;Remote sensing;Optical imaging;Image registration;Adaptive optics;Feature extraction","geophysical image processing;geophysical techniques;image fusion;image registration;radar imaging;remote sensing by radar;synthetic aperture radar","SAR remote sensing image registration techniques;optical remote sensing image registration;image fusion;image registration methods;Synthetic Aperture Radar;Gray-scale methods","","3","","46","IEEE","3 Jan 2019","","","IEEE","IEEE Conferences"
"Evaluation model of rural protection and reconstruction effect based on remote sensing information fusion","F. Tang; J. Li","School of Intelligent Construction, Wuchang University of Technology, Wuhan, China; School of Intelligent Construction, Wuchang University of Technology, Wuhan, China","2021 5th International Conference on Electronics, Communication and Aerospace Technology (ICECA)","20 Jan 2022","2021","","","1267","1270","Evaluation model of the rural protection and reconstruction effect based on remote sensing information fusion is studied in this paper. The annotation accuracy of the data set has a direct impact on the prediction effect of the model. The higher the labeling accuracy, the better the prediction effect of the two models, and the higher the evaluation index values, hence, for the better evaluation of the model, the annocation model is considered to capture the remote sensing image data. The advantages of feature-level image fusion are that a large amount of the image information is compressed and real-time processing can be then realized, then the fusion model is used to construct the efficient framework. Through the experimental testing, the different aspects of performacne are tested.","","978-1-6654-3524-6","10.1109/ICECA52323.2021.9676112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9676112","Remote Sensing;Information Fusion;Rural Protection;Reconstruction Effect;Evaluation Model","Image coding;Predictive models;Data models;Real-time systems;Labeling;Indexes;Remote sensing","geophysical image processing;image fusion;remote sensing","prediction effect;evaluation index values;remote sensing image data;feature-level image fusion;image information;fusion model;evaluation model;rural protection;reconstruction effect;remote sensing information fusion;annotation accuracy;data set;labeling accuracy","","","","21","IEEE","20 Jan 2022","","","IEEE","IEEE Conferences"
"High Quality Remote Sensing Image Super-Resolution Using Deep Memory Connected Network","W. Xu; G. XU; Y. Wang; X. Sun; D. Lin; Y. WU","Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, China; Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","8889","8892","Single image super-resolution is an effective way to enhance the spatial resolution of remote sensing image, which is crucial for many applications such as target detection and image classification. However, existing methods based on the neural network usually have small receptive fields and ignore the image detail. We propose a novel method named deep memory connected network (DMCN) based on a convolutional neural network to reconstruct high-quality super-resolution images. We build local and global memory connections to combine image detail with environmental information. To further reduce parameters and ease time-consuming, we propose downsampling units, shrinking the spatial size of feature maps. We test DMCN on three remote sensing datasets with different spatial resolution. Experimental results indicate that our method yields promising improvements in both accuracy and visual performance over the current state-of-the-art.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518855","remote sensing image;super-resolution;convolutional neural network;image fusion","Spatial resolution;Remote sensing;Image reconstruction;Training;Convolutional neural networks;Satellites","convolution;feedforward neural nets;geophysical image processing;image reconstruction;image resolution;remote sensing","deep memory connected network;single image super-resolution;convolutional neural network;high-quality super-resolution images;remote sensing datasets;high quality remote sensing image super-resolution;spatial resolution;DMCN","","15","","6","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Remote Sensing Scene Classification Based on Decision-Level Fusion","X. Li; B. Jiang; T. Sun; S. Wang","Department of Electronic Engineering, Tsinghua University; Beijing Institute of Remote Sensing Information, Beijing, China; Beijing Institute of Remote Sensing Information, Beijing, China; Department of Electronic Engineering, Tsinghua University","2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC)","21 Jun 2019","2018","","","393","397","With the convenient acquisition of high spatial resolution remote sensing images, scene classification has attracted great attention in recent years, leading to the boom of scene classification methods. Among these methods, feature-level fusion strategies are widely used. Different from the existing methods, we propose a decision-level fusion method for remote sensing scene classification using the features extracted from convolutional neural networks (CNNs). The classification task is accomplished at the decision level through two steps. In the first step, the top-N possible classes of a test sample are obtained using the features extracted from the soft-max layer of a fine-tuned CNN. In the second step, the final class of the test sample is determined from the above N possible classes using the features extracted from the first fully-connected layers of two fine-tuned CNNs. Thorough experiments conducted on the UC Merced data set and AID data set demonstrate that the proposed method is effective, achieving comparable classification results with the state-of-the-art methods. Moreover, the decision-level fusion strategy is novel, providing a new idea for remote sensing scene classification.","","978-1-5386-5373-9","10.1109/ITOEC.2018.8740526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8740526","scene classification;convolutional neural network (CNN);remote sensing;decision-level fusion","Feature extraction;Image analysis;Remote sensing;Training;Spatial resolution;Fuses;Convolutional neural networks","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;remote sensing","feature extraction;convolutional neural network;CNN;top-N possible class;soft-max layer;AID data set;UC merced data set;decision-level fusion method;scene classification methods;high spatial resolution remote sensing images","","","","22","IEEE","21 Jun 2019","","","IEEE","IEEE Conferences"
"A Crystalview on Multi-Focus Image Fusion Methods","K. H. K. PRASAD; S. B. G. T. BABU; R. V. V. KRISHNA","Department of E.C.E, Aditya Engineering College, Surampalem, AP, India; Department of E.C.E, Aditya Engineering College, Surampalem, AP, India; Department of E.C.E, Aditya College of Engineering and Technology, Surampalem, AP, India","2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)","12 Apr 2021","2021","","","669","674","Multi Focus Image Fusion (MFIF) is used to compensate depth of field problem of cameras in which an amalgamation of the corresponding features from two are more images to a single image which gives all significant features of input images. According to the optical lens formula, in the captured image some objects are in focus some are out of focus due to depth of field problem of cameras. In MFIF two or more different images of same scene having diverse focuses are fused to generate all in focus image. The resultant fused image improved in terms of visual perception, efficiency. The applications of MFIF involves several fields like medical diagnostics, military, forensic, multi-focus image integration, pattern recognition, remote sensing, biomedical imaging etc. In this paper we mainly concentrate on various MFIF methods. By reviewing the applications, advantages, challenges and limitations in the fusion methods this review article provides wide range of references for the researchers working in the area of MFIF.","","978-1-7281-8529-3","10.1109/ICCCIS51004.2021.9397188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9397188","Depth of field;Multi Focus Image Fusion;Fusion methods","Feature extraction;Cameras;Pattern recognition;Image fusion;Remote sensing;Optimization;Visual perception","image fusion","multifocus image fusion;single image;optical lens formula;multifocus image integration;MFIF methods;crystalview","","","","22","IEEE","12 Apr 2021","","","IEEE","IEEE Conferences"
"Urban Blue-Green Factor Estimation In Fredrikstad, Norway From Hyperspectral And Lidar Remote Sensing Data Fusion - A Concept Study","V. O. Jonassen; D. Aarsten; J. Kailainathan; I. Maalen-Johansen","Terratec AS, Vækerøveien 3, Oslo, Norway; Terratec AS, Vækerøveien 3, Oslo, Norway; Department of Science and Technology, Norwegian University of Life Sciences, Ås, Norway; Department of Science and Technology, Norwegian University of Life Sciences, Ås, Norway","2019 10th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","5 Dec 2019","2019","","","1","5","Continuous land development and urbanization highlights the importance of improved surface water management to avoid flooding and delayed drainage. Thus, several cities around the world have used variations of the blue-green factor (BGF), a policy instrument to attain desired levels of vegetation and water surfaces in new property developments. However, estimating BGF for existing infrastructure is equally important in order to ensure proper water management of the entire city. The presented work shows a possible workflow to estimate BGF semi-automatically from remote sensing hyperspectral and lidar data in Norway. A set of urban features and areas were detected and analyzed for individual properties affecting the BGF, along with corresponding estimation accuracies. The results demonstrate the potential to extract information and calculate BGF automatically. Lastly, it is pointed out that the models used for BGF estimation need to be further investigated and developed to enhance the estimation accuracy.","2158-6276","978-1-7281-5294-3","10.1109/WHISPERS.2019.8921111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8921111","remote sensing;hyperspectral imagery;lidar;blue-green factor","Vegetation;Laser radar;Estimation;Vegetation mapping;Remote sensing;Land surface;Feature extraction","climate mitigation;geophysical image processing;image colour analysis;image fusion;optical radar;remote sensing by laser beam;town and country planning;vegetation;water conservation","estimation accuracy;BGF estimation need;corresponding estimation accuracies;urban features;lidar data;remote sensing hyperspectral;BGF semiautomatically;entire city;proper water management;property developments;water surfaces;vegetation;policy instrument;improved surface water management;urbanization highlights;lidar remote sensing data fusion;hyperspectral sensing data fusion;Norway;fredrikstad;urban blue-green factor estimation","","2","","13","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Fusion of multispectral images using a new subspace approach","S. Işık; G. G. Dordinejad; K. Özkan","Eskisehir Osmangazi Universitesi, Eskisehir, TR; Eskisehir Osmangazi Universitesi, Eskisehir, TR; Eskisehir Osmangazi Universitesi, Eskisehir, TR","2016 24th Signal Processing and Communication Application Conference (SIU)","23 Jun 2016","2016","","","381","384","Fusion of multispectral images is a major research problem in terms of surveillance, remote sensing, industrial automation, medical and defense applications. The main reason behind multispectral image fusion is that using single channel images does not meet requirements of classification, segmentation and related tasks of remote sensing applications. Therefore, a new solution is needed to combine multispectral images to get more accurate and good visualized image as well as preserving the important details behind them. With this purpose, we have a proposed a new image fusion technique by adopting the Common Vector Approach concept. Upon examining the results, one can observe that using the CVA method for image fusion promises good results when compared with Principal Component Analysis and Singular Value Decomposition.","","978-1-5090-1679-2","10.1109/SIU.2016.7495757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495757","multispectral image fusion;common vector approach","Image fusion;Face;Remote sensing;Paints;Principal component analysis;Painting;Discrete wavelet transforms","image fusion;remote sensing;video surveillance","multispectral image fusion;subspace approach;surveillance;remote sensing;industrial automation;medical application;defense application;single channel images;visualized image;common vector approach","","","","","IEEE","23 Jun 2016","","","IEEE","IEEE Conferences"
"Fusenet: End- to-End Multispectral Vhr Image Fusion and Classification","J. R. Bergado; C. Persello; A. Stein","Dept. of Earth Observation Science, University of Twente, Enschede, The Netherlands; Dept. of Earth Observation Science, University of Twente, Enschede, The Netherlands; Dept. of Earth Observation Science, University of Twente, Enschede, The Netherlands","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","2091","2094","Classification of very high resolution (VHR) satellite images faces two major challenges: 1) inherent low intra-class and high inter-class spectral similarities and 2) mismatching resolution of available bands. Conventional methods have addressed these challenges by adopting separate stages of image fusion and spatial feature extraction steps. These steps, however, are not jointly optimizing the classification task at hand. We propose a single-stage framework embedding these processing stages in a multiresolution convolutional network. The network, called FuseNet, aims to match the resolution of the panchromatic and multispectral bands in a VHR image using convolutional layers with corresponding downsampling and upsampling operations. We compared FuseNet against the use of separate processing steps for image fusion, such as pansharpening and resampling through interpolation. We also analyzed the sensitivity of the classification performance of FuseNet to a selected number of its hyperparameters. Results show that FuseNet surpasses conventional methods.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519214","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519214","Convolutional networks;image fusion;land cover classification;VHR image;deep learning","Satellites;Spatial resolution;Feature extraction;Tiles;Image fusion","feature extraction;image classification;image fusion;image matching;image resolution;image sampling","high interclass spectral similarities;end-to-end multispectral VHR image fusion;end-to-end multispectral VHR image classification;very high resolution satellite image classification;inherent low intraclass spectral similarities;image mismatching;FuseNet network;image downsampling operation;image upsampling operation;pansharpening;interpolation;high resolution satellite images;convolutional layers;multispectral bands;panchromatic bands;multiresolution convolutional network;single-stage framework;spatial feature extraction steps","","6","","9","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Single Image Cloud Detection via Multi-Image Fusion","S. Workman; M. U. Rafique; H. Blanton; C. Greenwell; N. Jacobs",DZYNE Technologies; University of Kentucky; University of Kentucky; University of Kentucky; University of Kentucky,"IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1468","1471","Artifacts in imagery captured by remote sensing, such as clouds, snow, and shadows, present challenges for various tasks, including semantic segmentation and object detection. A primary challenge in developing algorithms for identifying such artifacts is the cost of collecting annotated training data. In this work, we explore how recent advances in multi-image fusion can be leveraged to bootstrap single image cloud detection. We demonstrate that a network optimized to estimate image quality also implicitly learns to detect clouds. To support the training and evaluation of our approach, we collect a large dataset of Sentinel-2 images along with a per-pixel semantic labelling for land cover. Through various experiments, we demonstrate that our method reduces the need for annotated training data and improves cloud detection performance.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323759","weakly-supervised learning;multi-image fusion;segmentation;clouds","Image segmentation;Clouds;Training;Cloud computing;Semantics;Remote sensing;Training data","clouds;geophysical image processing;image classification;image fusion;image segmentation;image sensors;learning (artificial intelligence);object detection;remote sensing","object detection;annotated training data;multiimage fusion;single image cloud detection;image quality;Sentinel-2 images;cloud detection performance;semantic segmentation","","2","","16","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Image Fusion Based on Discrete Cosine Transform with High Compression","E. Alhatami; U. A. Bhatti; M. Huang; S. L. Feng","School of Information and Communication Engineering, Hainan University, Haikou, China; School of Information and Communication Engineering, Hainan University, Haikou, China; School of Information and Communication Engineering, Hainan University, Haikou, China; School of Information and Communication Engineering, Hainan University, Haikou, China","2022 7th International Conference on Signal and Image Processing (ICSIP)","19 Sep 2022","2022","","","606","610","The fusion of images is defined as an alignment of important information from diverse sensors using various mathematical models to generate a single compound image. The fusion of images is used for integrating the complementary multi-temporal, multi-view, and multi-sensor information into a single image with improved image quality and by keeping the integrity of essential features. It is considered a vital pre-processing phase for several applications such as robot vision, aerial, satellite imaging, medical imaging, and robot or vehicle guidance. This paper proposed an enhancement image compression algorithm on a discrete cosine transform (DCT). Then it uses the DCT transformation in the remote sensing image fusion domain and a remote sensing image fusion method with an improved proposed algorithm. The method in this paper has an acceptable compromise between enhancing the spatial resolution of the fused image and maintaining the spectral information. Besides, it is shown that there is no need for codec operations when performing image fusion in the compressed domain such as JPEG. This reduces the time to obtain the fused image and effectively suppresses the problem of dealing with blocky image processing; blockage often occurs. Furthermore, the improved algorithm has better spectral retention capability under the premise of more explicit fusion images and is more suitable for applications requiring higher spectral fidelity.","","978-1-6654-9563-9","10.1109/ICSIP55141.2022.9887300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887300","Image Fusion;DCT;Multi-Spectral Image (MSI);Panchromatic image (PAN)","Image coding;Satellites;Transform coding;Signal sampling;Sensor fusion;Sensors;Discrete cosine transforms","data compression;discrete cosine transforms;image coding;image enhancement;image fusion;image processing;image resolution;medical image processing;remote sensing;robot vision;sensor fusion","discrete cosine transform;high compression;single compound image;complementary multitemporal;multiview;multisensor information;single image;improved image quality;vital pre-processing phase;satellite imaging;medical imaging;enhancement image compression algorithm;DCT transformation;remote sensing image fusion domain;remote sensing image fusion method;fused image;blocky image processing;explicit fusion images","","","","15","IEEE","19 Sep 2022","","","IEEE","IEEE Conferences"
"Deep residual learning for remote sensed imagery pansharpening","Y. Wei; Q. Yuan","State Key Laboratory of Information Engineering, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China","2017 International Workshop on Remote Sensing with Intelligent Processing (RSIP)","26 Jun 2017","2017","","","1","4","We proposed a deep convolutional network for multi-spectral image pan-sharpening to overcome the drawbacks of traditional methods and improve the fusion accuracy. To break the performance limitation of deep networks, residual learning with specific adaption to image fusion tasks is applied to optimize the architecture of proposed network. Results of adequate experiments support that our model can yield high resolution multi-spectral images with state-of-the-art qualities, as the information in both spatial and spectral domains has been accurately preserved.","","978-1-5386-1990-2","10.1109/RSIP.2017.7958794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7958794","Pansharpening;image fusion;convolutional neural network;residual learning","Spatial resolution;Neural networks;Remote sensing;Machine learning;Feature extraction;Image fusion","computer vision;image fusion;learning (artificial intelligence);remote sensing","deep residual learning;remote sensed imagery pansharpening;deep convolutional network;multi-spectral image pan-sharpening;image fusion tasks;spatial domains;spectral domains","","15","","17","IEEE","26 Jun 2017","","","IEEE","IEEE Conferences"
"Hyperspectral image fusion based on non-factorization sparse representation and error matrix estimation","X. Han; J. Luo; J. Yu; W. Sun","Dept. of Electronic Engineering, Tsinghua Univ, Beijing, China; School of Opte-electronics, Beijing Institute of Technology, Beijing, China; Colg. of Computer Science and Technology, Beijing Univ. of Technology, Beijing, China; Dept. of Electronic Engineering, Tsinghua Univ, Beijing, China","2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","8 Mar 2018","2017","","","1155","1159","Matrix factorization with non-negative constrains is widely used in hyperspectral image fusion. Nevertheless, the non-negative restriction on the sparse coefficients limits the efficiency of dictionary representation. To solve this problem, a new hyperspectral image fusion method based on non-factorization sparse representation and error matrix estimation is proposed in this paper, for the fusion of remotely sensed high-spatial multi-bands image with low-spatial hyperspectral image in the same scene. Firstly, an efficient spectral dictionary learning method is specifically adopted for the construction of the spectral dictionary, which avoids the procedure of matrix factorization. Then, the sparse codes of the high-spatial multi-bands image with respect to the learned spectral dictionary are estimated using the alternating direction method of multipliers (ADMM) without non-negative constrains. For improving the quality of final fusion result, an error matrix estimation method is also proposed, exploiting the spatial structure information after non-factorization sparse representation. Experimental results both on simulated and real datasets demonstrate that, compared with the related state-of-the-art methods, our proposed method achieves the highest quality of hyperspectral image fusion, which can improve PSNR over 2.5844 and SAM over 0.3758.","","978-1-5090-5990-4","10.1109/GlobalSIP.2017.8309142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8309142","hyperspectral image fusion;non-factorization sparse representation;dictionary learning;error matrix estimation","Hyperspectral imaging;Sparse matrices;Dictionaries;Estimation;Interpolation;Image fusion","geophysical image processing;hyperspectral imaging;image fusion;image representation;matrix decomposition;remote sensing","sparse coefficients;dictionary representation;hyperspectral image fusion method;nonfactorization sparse representation;low-spatial hyperspectral image;matrix factorization;nonnegative constrains;error matrix estimation method;nonnegative restriction;spectral dictionary;remotely sensed high-spatial multiband image;spectral dictionary learning method;alternating direction method of multipliers;ADMM;PSNR","","3","","22","IEEE","8 Mar 2018","","","IEEE","IEEE Conferences"
"A Manifold Learning Approach of Land Cover Classification for Optical and SAR Fusing Data","X. Tan; S. Jiang; Z. Zheng; P. Zhang; M. Zhu; Y. He; Z. Yu; N. Wang; L. Jiang; G. Zhou; H. Zhang; J. Li","Yunnan Electric Power Research Institute, Yunnan Power Grid Co., Ltd., Kunming, Yunnan, PRC; State Key Laboratory of Remote Sensing Science, Jointly Sponsored by Beijing Normal University and the Institute of Remote Sensing and Digital Earth of Chinese Academy of Sciences, Beijing, PRC; State Key Laboratory of Remote Sensing Science, Jointly Sponsored by Beijing Normal University and the Institute of Remote Sensing and Digital Earth of Chinese Academy of Sciences, Beijing, PRC; State Key Laboratory of Remote Sensing Science, Jointly Sponsored by Beijing Normal University and the Institute of Remote Sensing and Digital Earth of Chinese Academy of Sciences, Beijing, PRC; Land and Resources Department of Sichuan Province, Chengdu, Sichuan, PRC; Sichuan Institute of Geo-Environment Monitoring, Chengdu, Sichuan, PRC; Center for Information and Geoscience, University of Electronic Science and Technology of China, Chengdu, Sichuan, PRC; State Key Laboratory of Remote Sensing Science, Jointly Sponsored by Beijing Normal University and the Institute of Remote Sensing and Digital Earth of Chinese Academy of Sciences, Beijing, PRC; Center for Information and Geoscience, University of Electronic Science and Technology of China, Chengdu, Sichuan, PRC; Guangxi Key Laboratory for Spatial Information and Geomatics, Guilin University of Technology, Guilin, Guangxi, PRC; Institute of Space and Earth Information Science, The Chinese University of Hong Kong, Shatin, New Territories, Hong Kong; Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","3567","3570","In the field of remote sensing, data acquired from a single sensor usually can't meet the needs of some special applications, because the information extracted from the data are often incomplete and limited. Data fusing can solve this problem, but it will lead to the redundant information. In this paper, we proposed a novel manifold learning approach to perform dimensionality reduction for the fusing optical and SAR data. And three typical manifold learning models, namely, ISOMAP, local linear embedding (LLE) and principle component analysis (PCA), were utilized to test the robustness of our method by comparing with the land cover classification results. Our experimental results showed that our proposed method obtained the best land cover classification results among these approaches for the fusing optical and SAR data.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517967","Land cover classification;Optical and SAR data;dimension reduction;manifold learning","Synthetic aperture radar;Optimization;Optical sensors;Manifolds;Remote sensing;Adaptive optics;Optical imaging","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);principal component analysis;remote sensing by radar;synthetic aperture radar","manifold learning models;land cover classification results;principle component analysis;SAR data;redundant information;data fusing;special applications;single sensor;remote sensing;SAR fusing data;manifold learning approach","","1","","8","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Advances and Opportunities in Remote Sensing Image Geometric Registration: A systematic review of state-of-the-art approaches and future research directions","R. Feng; H. Shen; J. Bai; X. Li","School of Geography and Tourism, Shaanxi Normal University, Xi’an, China; School of Resource and Environment Science and the Collaborative Innovation Center for Geospatial Technology, Wuhan University, Wuhan, China; School of Geography and Tourism, Shaanxi Normal University, Xi’an, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Magazine","10 Feb 2022","2021","9","4","120","142","Geometric registration is often an accuracy assurance for most remote sensing image processing and analysis, such as image mosaicking, image fusion, and time-series analysis. In recent decades, geometric registration has attracted considerable attention in the remote sensing community, leading to a large amount of research on the subject. However, few studies have systematically reviewed its current status and deeply investigated its development trends. Moreover, new approaches are constantly emerging, and some issues still need to be solved. Thus, this article presents a survey of state-of-the-art approaches for remote sensing image registration in terms of intensity-based, feature-based, and combination techniques. Optical flow estimation and deep learning-based methods are summarized, and software-operated registration and registration evaluation are introduced. Building on recent advances, promising opportunities are explored.","2168-6831","","10.1109/MGRS.2021.3081763","National Natural Science Foundation of China(grant numbers:41971303,41701394); Key Research and Development Program of Shaanxi Province(grant numbers:2020NY-166); Fundamental Research Funds for the Central Universities(grant numbers:GK202103143); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468875","","Remote sensing;Image registration;Feature extraction;Optical flow;Measurement;Learning systems;Time series analysis;Market research;Image fusion","geophysical image processing;geophysical signal processing;image fusion;image processing;image registration;image segmentation;image sequences;learning (artificial intelligence);remote sensing;time series","remote sensing image geometric registration;systematic review;accuracy assurance;remote sensing image processing;image mosaicking;image fusion;time-series analysis;remote sensing community;remote sensing image registration;software-operated registration;registration evaluation","","25","","188","IEEE","30 Jun 2021","","","IEEE","IEEE Magazines"
"Single Sensor Image Fusion Using a Deep Residual Network","F. Palsson; J. R. Sveinsson; M. O. Ulfarsson","Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland","2018 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","27 Jun 2019","2018","","","1","5","Single sensor fusion is the fusion of two or more spectrally disjoint reflectance bands that have different spatial resolution and have been acquired by the same sensor. An example is the Sentinel-2 constellation which can acquire multispectral bands of 10 m, 20 m and 60 m resolution from the visible to short-wave infrared (SWIR) regions of the electromagnetic spectrum. In this paper, we present a method based on a deep residual convolutional network to fuse the fine and coarse spatial resolution bands to obtain finer spatial resolution versions of the coarse bands. The benefits of the residual design are primarily that the network converges faster and it allows for deeper networks. Also, it improves the spectral consistency of the fused image. Using a real Sentinel-2 dataset, it is demonstrated that the proposed method gives good results when compared to state-of-the-art single sensor image fusion methods.","2158-6276","978-1-7281-1581-8","10.1109/WHISPERS.2018.8747059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747059","Image fusion;deep residual network;convolutional network;Sentinel-2","Training;Spatial resolution;Image fusion;Sensor fusion;MODIS","convolutional neural nets;geophysical image processing;geophysical techniques;image fusion;image resolution;image sensors","deep residual network;single sensor fusion;spectrally disjoint reflectance bands;Sentinel-2 constellation;multispectral bands;short-wave infrared regions;electromagnetic spectrum;deep residual convolutional network;fine resolution bands;coarse spatial resolution bands;finer spatial resolution versions;coarse bands;residual design;spectral consistency;fused image;Sentinel-2 dataset;single sensor image fusion methods","","","","15","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"Multi-Source Image Fusion for GNSS-Based Passive Radar","X. Zhou; P. Wang; J. Chen; H. Zeng","School of Electronics and Information Engineering, Beihang University, Beijing, China; School of Electronics and Information Engineering, Beihang University, Beijing, China; School of Electronics and Information Engineering, Beihang University, Beijing, China; National Key Laboratory of Science and Technology on Space Microwave, Xi'an, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","3708","3711","Global Navigation Satellite System (GNSS)-based Passive Radar (GPR) is gaining increasing attentions recently due to its potential on moving target detection (MTD). The main problem of the GPR is the low power density of GNSS signals. With the upgrades of GNSS, the total transmit power for the new launched satellites has been greatly improved. However, the total transmit power is allocated to several independent signals, which cannot be directly integrated. This paper proposes a multi-source image fusion method for the GPR which aims at coherently integrating the power allocated to different signals for the same satellite, and improving the detection performance. Validation experiments using GPS signals as illumination source and an airplane as target are conducted. The target is successfully detected by exploiting GPS L1 and L5 signals for the same satellite as illumination sources. Two-image fusion is performed utilizing the proposed method. Significant SNR improvement is achieved, which validates the feasibility of the proposed method.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883748","National Natural Science Foundation of China(grant numbers:62101014); National Key Laboratory of Science and Technology on Space Microwave(grant numbers:6142411203307); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883748","GNSS-based Passive Radar;MTD;image fusion;SNR improvement","Global navigation satellite system;Passive radar;Power system measurements;Satellites;Lighting;Geoscience and remote sensing;Object detection","artificial satellites;Global Positioning System;image fusion;object detection;passive radar;satellite navigation","GNSS-based Passive Radar;Global Navigation Satellite System-based Passive Radar;GPR;target detection;low power density;GNSS signals;launched satellites;independent signals;multisource image fusion method;detection performance;GPS signals;illumination source;two-image fusion","","","","9","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Multi-Scale Feature Fusion Network for Object Detection in VHR Optical Remote Sensing Images","W. Zhang; L. Jiao; X. Liu; J. Liu","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","330","333","In this paper, we propose a multi-scale feature fusion network (MS-FF Net) based on convolutional neural network (CNN) to deal with object detection in VHR images. In CNN, the low-level layers contain rich detail information and the high-level layers contain rich semantic information. Inspired by the idea of feature fusion, we propose an additional multi-scale feature fusion layer (MFL) to fuse the information between detail and semantic features. Then both large and small objects are considered by this network. Moreover, the network architecture and training strategies are designed to improve performance. Experiments on NWPU VHR-10 dataset demonstrate that the method with MFLs achieves significant improvement and outperforms compared methods in terms of mean average precision. Specially, the detection precision of airplane, baseball diamond, basketball court, ground track field and harbor categories exceeds 90% which is much higher than that of compared methods.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8897842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8897842","Remote sensing images;object detection;very high resolution optical remote sensing images;convolutional neural networks;feature fusion","Feature extraction;Proposals;Object detection;Remote sensing;Sports;Optical imaging;Optical sensors","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);object detection;remote sensing","semantic information;multiscale feature fusion layer;semantic features;network architecture;training strategies;multiscale feature fusion network;object detection;VHR optical remote sensing;convolutional neural network;CNN;VHR images;low-level layers;high-level layers","","10","","12","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Variational Bayesian image fusion based on combined sparse representations","B. Lin; X. Tao; S. Li; L. Dong; J. Lu","Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, China; Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, China; Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, China; Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, China; Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, China","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 May 2016","2016","","","1432","1436","Hyper-spectral image fusion has been a hot topic in medical imaging and remote sensing. This paper proposes a Bayesian fusion model which combines the panchromatic (PAN) image and the low spatial resolution hyper-spectral (HS) image under the same framework. Sparsity constraint is introduced as double ""spike-and-slab"" priors, and anisotropic Gaussian noise is adopted for accuracy. To achieve reduction in computational complexity, we turn the anisotropic Gaussian distribution into isotropic one with modified linear transformation and propose a variational Bayesian expectation maximization (EM) algorithm to calculate the result. Experiment results show that our solution can achieve comparable performance in pan-sharpening to other state-of-art algorithms while largely reducing the computational complexity.","2379-190X","978-1-4799-9988-0","10.1109/ICASSP.2016.7471913","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7471913","Hyper-spectral image fusion;sparse prior;computational complexity;anisotropic Gaussian distribution;variational Bayesian;algorithm","Bayes methods;Spatial resolution;Dictionaries;Manganese;Mathematical model;Image reconstruction;Image fusion","Bayes methods;computational complexity;expectation-maximisation algorithm;Gaussian noise;hyperspectral imaging;image fusion;image representation;image resolution;variational techniques","variational Bayesian image fusion model;combined sparse representations;hyper-spectral image fusion;medical imaging;remote sensing;panchromatic image;PAN image;low spatial resolution hyper-spectral image;HS image;sparsity constraint;double spike-and-slab priors;anisotropic Gaussian noise;computational complexity;anisotropic Gaussian distribution;modified linear transformation;variational Bayesian expectation maximization algorithm;EM algorithm","","8","","16","IEEE","19 May 2016","","","IEEE","IEEE Conferences"
"Airborne SAR and optical image fusion based on IHS transform and joint non-negative sparse representation","C. Liu; Y. Qi; W. Ding","State Key Laboratory of Virtual Reality Technology and System, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and System, Beihang University, Beijing, China; Research Institute of Unmanned Aerial Vehicle, Beihang University, Beijing, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","7196","7199","In this paper, a novel airborne synthetic aperture radar (SAR) and optical image fusion method based on Intensity-Hue-Saturation (IHS) and joint non-negative sparse representation (JNNSR) is proposed. Firstly, the color optical image is transformed into IHS space. Then, the intensity component of the optical image and the SAR image are decomposed by JNNSR into common component and innovation components. Based on the non-negative property of the sparse coefficients, the innovation component of the SAR image is fused with the intensity component of the optical image. Finally, the fused result is obtained by performing inverse IHS transform. The experimental result shows that our method is superior to the traditional methods in terms of several universal quality evaluation indexes, as well as in the visual quality.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730877","Airborne SAR image;Airborne optical image;Image fusion;IHS;Joint sparse representation","Dictionaries;Optical imaging;Optical sensors;Image fusion;Transforms;Synthetic aperture radar;Adaptive optics","airborne radar;image fusion;inverse transforms;synthetic aperture radar","airborne SAR image;inverse IHS transform;joint non-negative sparse representation;novel airborne synthetic aperture radar;optical image fusion method;intensity-hue-saturation transform;JNNSR;intensity component","","3","","11","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Road Material Information Extraction Based on Multi-Feature Fusion of Remote Sensing Image","C. Yang; Y. Li; B. Peng; Y. Cheng; L. Tong","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3943","3946","The extraction of road information has always played a quite important role in civil and military. With the gradual maturity of road extraction technology, how to automatically extract road pavement material information has also begun to attract the attention of researchers. Aiming at this problem, this paper proposes a road material analysis method based on image features and Support Vector Machine (SVM). The method extracts the road surface portion of the road based on the road binary image. Then we use the Rerinex algorithm [1] to denoise the image, the HSV (Hue, Saturation, Value) color model, Local Binary Patterns texture [2], and Gray Level Co-occurrence Matrix (GLCM) are used to extract the features of the road surface pixels. After the principal component analysis reducing the dimension, each feature vector is fused. Then we use the support vector machine (SVM) classifier to analyze the material (asphalt concrete road, cement concrete road and bare soil road) information of the road. This method that combines multiple image features is a new application extension of remote sensing image based on road extraction. The results of the classification experiments on remote sensing images confirmed that the method is effective.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899029","Road material;Remote Sensing Image;HSV feature;Texture features;SVM","Roads;Feature extraction;Image color analysis;Support vector machines;Remote sensing;Asphalt;Kernel","asphalt;building information modelling;concrete;feature extraction;geophysical image processing;image classification;image colour analysis;image denoising;image fusion;image texture;matrix algebra;principal component analysis;remote sensing;road building;support vector machines","road material information extraction;multifeature fusion;remote sensing image;road extraction technology;road material analysis method;road surface portion;road binary image;local binary pattern texture;road surface pixels;feature vector;support vector machine classifier;cement concrete road;bare soil road;multiple image features;road pavement material information extraction;Rerinex algorithm;SVM classifier;image denoising;hue, saturation, value color model;HSV color model;feature extraction;gray level co-occurrence matrix;principal component analysis","","5","","10","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Common Regions of Interest Extraction Based on Saliency Statistic Analysis for Multiple Remote Sensing Images","X. Lyu; L. Zhang; W. Zhu; L. Zhang","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4779","4782","Various landscape characteristics and irregular object boundaries often make object extraction more difficult. Automated analysis of remote sensing (RS) images is challenging and saliency detection is an effective solution. Yet, many traditional algorithms emphasize simply on a single image and would, therefore, neglect the similarity of an image set. In this paper, concerning the relationships among images, a region of interest extraction model based on common features analysis for remote sensing images is proposed. Firstly, multi-image saliency maps, showing the common salient objects, are generated by clustering in RGB and CIELab color spaces. Next, a method, highlighting the salient region, is based on global and local saliency statistics analysis. Finally, regions of interest are segmented from original images according to saliency maps which have been made boundaries holding by superpixels. Experimental evaluation shows that compared with six existing models, we get more accurate saliency maps.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553409","Beijing Natural Science Foundation(grant numbers:L182029); National Natural Science Foundation of China(grant numbers:41771407,61571050); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553409","Remote sensing image processing;information statistics;k-means;superpixels segmentation","Image segmentation;Analytical models;Image color analysis;Clustering algorithms;Feature extraction;Data mining;Remote sensing","feature extraction;geophysical image processing;image colour analysis;image fusion;image segmentation;object detection;remote sensing;statistical analysis","multiple remote sensing images;landscape characteristics;irregular object boundaries;object extraction;automated analysis;saliency detection;single image;image set;common features analysis;multiimage saliency maps;common salient objects;salient region;global saliency statistics analysis;local saliency statistics analysis;common regions of interest extraction;RGB;CIELab color spaces;image segmentation","","1","","13","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A comparative analysis of transformation methodologies in image fusion","S. V. A. Kumer; S. K. Srivatsa","Sri Chandrasekharendra Swaraswathi Viswa Maha Vidyalaya, Enathur, Tamil Nadu, IN; MIT, Anna University, Chennai, India","2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)","21 Jun 2018","2017","","","2911","2915","Image fusion is a process of combining a low resolution Multispectral image with the high resolution panchromatic image to obtain an output image which should be more informative than the images which have taken as input. This paper proposes the review of some image fusion methodologies like Principal Component Transform (PCT), Color Normalization Transform (CNT) and Gram Schmidt Transform (GST) to get the suitable output image. The images like Hyperion and Ikonos are taken as an input image and then the images are fused by the above transformations separately and perform the spectra comparison to classify an image with accuracy assessment.","","978-1-5386-1887-5","10.1109/ICECDS.2017.8389988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8389988","Principal Component Transform (PCT);Color Normalization Transform (CNT);Gram Schmidt Transform(GST);Image Fusion","Image fusion;Transforms;Image color analysis;Principal component analysis;Image resolution;Radiometry;Data analysis","geophysical image processing;geophysical techniques;image fusion;image resolution;principal component analysis;remote sensing","comparative analysis;transformation methodologies;low resolution Multispectral image;high resolution panchromatic image;image fusion methodologies;Principal Component Transform;Color Normalization Transform;Gram Schmidt Transform;input image;accuracy assessment","","1","","13","IEEE","21 Jun 2018","","","IEEE","IEEE Conferences"
"Multi Spectral Image Fusion with Deep Convolutional Network","S. Eghbalian; H. Ghassemian","Image processing and Information Analysis Laboratory Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Image processing and Information Analysis Laboratory Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2018 9th International Symposium on Telecommunications (IST)","7 Mar 2019","2018","","","173","177","A new multispectral image fusion method is proposed, based on deep convolutional neural networks. For pan-sharpening problem, the proposed method utilize the both super-resolution fusion methods and deep convolutional neural network. By the spatial information from the panchromatic (PAN) image the Multi-Spectral (MS) image is enhanced. In the other hand, the proposed method is independent from the number of MS bands because the spatial information directly estimated from PAN image. Experiments on images of the representative database are shown, proposed method can achieve better result competitive with the current well known methods.","","978-1-5386-8274-6","10.1109/ISTEL.2018.8661137","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661137","Multi-spectral;Image fusion;Pansharpening;Convolutional neural networks;Super-resolution","Image fusion;Remote sensing;Spatial resolution;Convolutional neural networks;Transforms;Principal component analysis","convolutional neural nets;image colour analysis;image enhancement;image fusion;image resolution","multispectral image fusion;superresolution fusion methods;multispectral image enhancement;panchromatic image;pan-sharpening problem;deep convolutional neural network;PAN image","","4","","27","IEEE","7 Mar 2019","","","IEEE","IEEE Conferences"
"A MRF-based Multi-agent System for Remote Sensing Image Segmentation","H. Bao; L. Zhou; L. Liu; Y. Wu","Chinese Academy of Sciences, Institute of Intelligent Machines, Hefei, China; Chinese Academy of Sciences, Institute of Intelligent Machines, Hefei, China; Chinese Academy of Sciences, Institute of Intelligent Machines, Hefei, China; Chinese Academy of Sciences, Institute of Intelligent Machines, Hefei, China","2017 International Conference on Computing Intelligence and Information System (CIIS)","29 Mar 2018","2017","","","246","249","Remote sensing is a technology for remote real-time monitoring surface information, remote sensing images are valuable in civil and military applications. In this paper, aiming at the design of remote sensing image segmentation algorithm, we propose a remote sensing image segmentation method based on multi-agent and MRF. The proposed method extracts and fuses the intensity, texture, edges, and the spatial information in collaboration and parallization, and it provides theoretical and technical support for remote sensing image segmentation task.","","978-1-5386-3886-6","10.1109/CIIS.2017.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327745","remote sensing image;segmentation;multi-agent system;markov random field (MRF)","Image segmentation;Remote sensing;Feature extraction;Multi-agent systems;Sensors;Data mining;Fuses","feature extraction;geophysical image processing;image fusion;image segmentation;multi-agent systems;remote sensing","remote real-time;surface information;remote sensing images;civil applications;military applications;remote sensing image segmentation algorithm;remote sensing image segmentation method;remote sensing image segmentation task;MRF-based multiagent system;intensity extration;texture extration;edge extration;intensity fusion;texture fusion;edges fusion","","1","","8","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"A Flexible Image Fusion Algorithm Applied on Fusion Between Optical and Thermal Data","S. Xu; M. Ehlers","Research Institute for Regional and Urban Development, Dortmund, Germany; Osnabrück University, Osnabrück, Germany","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","477","480","Classic image fusion algorithms fail to apply to thermal images because they require at least three bands as input but thermal images have only one band. Recent studies on fusing visible and thermal images restrict to close-range photographic images or require abundant image samples. This paper presents an image fusion algorithm that is flexible with different spectrum sensors, as well as the number of bands. This algorithm transfers the image into the frequency domain, where the spatial information from different sensors can be separated, extracted, and then synthesized. The algorithm was implemented in image fusion between an aerial photo and thermal image, as well as the space-borne thermal and visible band. Both cases realized spatial enhancement and spectral preservation.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883687","infrared and visible image fusion;thermal image sharpening;fast Fourier transform;filter in the frequency domain","Image sensors;Frequency-domain analysis;Geoscience and remote sensing;Sensor fusion;Optical imaging;Optical sensors;Data mining","image fusion;image sampling;infrared imaging","thermal image;close-range photographic images;image samples;flexible image fusion;aerial photo","","","","13","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Integrating modis and MTSAT-2 to generate high spatial-temporal-spectral resolution imagery for real-time air quality monitoring","Y. Zhao; B. Huang","Department of Geography and Resource Management, The Chinese University of Hong Kong, Hong Kong, China; Institute of Space and Earth Information Science, The Chinese University of Hong Kong, Hong Kong, China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","6122","6125","Plenty of researches have been presented to combine satellite remote sensing observations with ground-level air quality monitoring sites measurements together to accomplish spatially continuous observation of air pollutant, such as using Aerosol Optical Depth (AOD) derived from Moderate Resolution Imaging Spectroradiometer (MODIS) and ground observed fine particulate matter (PM2.5) to estimate the PM2.5 concentrations that has the same spatial coverage and continuity with MODIS AOD. However, because the temporal resolution of MODIS, i.e. twice in daytime, is too limited to accomplish real-time air quality monitoring. Hence, geostationary satellite with higher temporal resolution is introduced in this study, e.g., Multi-functional Transport Satellite 2 (MTSAT-2), which provides Earth observations for every half an hour or even more frequently. This paper presents an integrated spatial-temporal-spectral image fusion model to blend the spatial, temporal and spectral resolution of MODIS and MTSAT-2 images to generate the synthetic data with high spatial, temporal and spectral resolution simultaneously, which can provide more comprehensive data for real-time air quality monitoring. The fusion process involves MTSAT-2 visual band sharpening, spatial-temporal fusion, and temporal-spectral fusion, which aims to improve spatial, temporal, and spectral resolution respectively. The proposed image fusion model was tested on one set of MODIS and MTSAT-2 images within the Pearl River Delta (PRD) region, China. Experimental results indicate that, the advantages of the spatial, temporal, and spectral resolution of these two satellites are merged quite well, which can produce synthetic half-hourly MODIS-like images for real-time air quality monitoring.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128406","Air quality monitoring;image fusion;real-time;spatial resolution;spatial continuity;spectral resolution;temporal resolution","MODIS;Spatial resolution;Air quality;Satellites;Visualization;Image fusion","aerosols;air pollution;air quality;environmental monitoring (geophysics);geophysical image processing;image fusion;image resolution;remote sensing","spatial-temporal-spectral resolution imagery;satellite remote sensing observations;ground-level air quality monitoring sites measurements;MODIS AOD;Multifunctional Transport Satellite 2;spatial-temporal-spectral image fusion model;MTSAT-2 visual band sharpening;MODIS-like images;air pollutant;aerosol optical depth;Moderate Resolution Imaging Spectroradiometer;particulate matter;PM2.5 concentrations;geostationary satellite;MTSAT-2 images;Pearl River Delta;China","","3","","8","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Dense Dilated Convolutions Merging Network for Semantic Mapping of Remote Sensing Images","Q. liu; M. Kampffmeyer; R. Jenssen; A. -B. Salberg","SAMBA and Machine Learning Group, Norwegian Computing Center and UiT, Oslo, Norway; Machine Learning Group UiT, the Arctic University of Norway, Tromsø, Norway; Machine Learning Group UiT, the Arctic University of Norway, Tromsø, Norway; dept. SAMBA of NR, Norwegian Computing Center, Oslo, Norway","2019 Joint Urban Remote Sensing Event (JURSE)","22 Aug 2019","2019","","","1","4","We propose a network for semantic mapping called the Dense Dilated Convolutions Merging Network (DDCM-Net) to provide a deep learning approach that can recognize multi-scale and complex shaped objects with similar color and textures, such as buildings, surfaces/roads, and trees in very high resolution remote sensing images. The proposed DDCM-Net consists of dense dilated convolutions merged with varying dilation rates. This can effectively enlarge the kernels' receptive fields, and, more importantly, obtain fused local and global context information to promote surrounding discriminative capability. We demonstrate the effectiveness of the proposed DDCM-Net on the publicly available ISPRS Potsdam dataset and achieve a performance of 92.3% F1-score and 86.0% mean intersection over union accuracy by only using the RGB bands, without any post-processing. We also show results on the ISPRS Vaihingen dataset, where the DDCM-Net trained with IRRG bands, also obtained better mapping accuracy (89.8% F1-score) than previous state-of-the-art approaches.","2642-9535","978-1-7281-0009-8","10.1109/JURSE.2019.8809046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809046","Dense Dilated Convolutions Merging (DDCM);deep learning;semantic mapping;remote sensing","Semantics;Merging;Remote sensing;Buildings;Computer architecture;Training;Automobiles","feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;image resolution;image segmentation;image texture;learning (artificial intelligence);object detection;remote sensing","dilation rates;kernels;local context information;global context information;publicly available ISPRS Potsdam dataset;mapping accuracy;semantic mapping;Dense Dilated Convolutions Merging Network;deep learning approach;complex shaped objects;high resolution remote sensing images;multiscale objects;DDCM-Net","","5","","29","IEEE","22 Aug 2019","","","IEEE","IEEE Conferences"
"Change detection based on structural conditional random field framework for high spatial resolution remote sensing imagery","P. Lv; Y. Zhong; J. Zhao; A. Ma; L. Zhang","Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","1059","1062","In this paper, a structural conditional random field framework (SCRF) is proposed to detect the detailed change information from high spatial resolution (HSR) remote sensing imagery. Traditional random field based methods encounter the over-smoothing problem when deal with HSR images and the boundary of changed objects cannot be preserved well. To solve this problem, in SCRF, fuzzy c means (FCM) is used to model the unary potential while avoiding the independent assumption. Pairwise potentials with different shapes are selected as the structural set to model the spatial features of land cover such as buildings and roads. Based on SCRF, a set of change belief maps are generated to describe the observed image from different aspects. An object based fusion strategy is then followed to combine the belief maps to get the refined result. The results of the proposed method on two HSR data sets outperform some state-of-art algorithms.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127138","change detection;conditional random field;high spatial resolution;remote sensing","Remote sensing;Image segmentation;Spatial resolution;Shape;Roads;Buildings;Change detection algorithms","buildings (structures);fuzzy systems;geophysical image processing;image fusion;land cover;object detection;remote sensing;roads","change detection;structural conditional random field framework;high spatial resolution remote sensing imagery;SCRF;HSR images;object based fusion strategy;fuzzy c means;land cover;buildings;roads","","3","","10","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"MSPPF-Nets: A Deep Learning Architecture for Remote Sensing Image Classification","R. Yang; Y. Zhang; P. Zhao; Z. Ji; W. Deng","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3045","3048","Nowadays, deep learning has got a major success in computer vision, especially in image recognition. In this paper, a new architecture based on DenseNets which is referred to as Multi-Scale Input Spatial Pyramid Pooling Fusion Networks (MSPPF-nets) is proposed for the work of classification of local climate zones (LCZs). Multi-scale remote sensing images can be inputs of the networks by the benefit of Spatial Pyramid Pooling (SPP) layer, multi-scale features from different channels were extracted and fused by our multi-branch-input framework. The final classification results have illustrated the feasibility of this presented classification method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899068","Local Climate Zones (LCZs);MSPPF-nets;Classification;Remote sensing image;Feature fusion","Remote sensing;Fuses;Training;Computer architecture;Feature extraction;Image resolution;Deep learning","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);neural nets;remote sensing","MSPPF-nets;image recognition;spatial pyramid pooling layer;multibranch-input framework;multiscale input spatial pyramid pooling fusion networks;multiscale remote sensing image classification;deep learning architecture;computer vision;local climate zone classification;LCZ classification;SPP layer;feature extraction","","1","","10","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Object Detection for Remote Sensing Images based on Guided Anchoring and Feature Fusion","W. Wang; Z. Tian; R. Zhan; J. Zhang; Z. Zhuang","ATR Lab, College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China; ATR Lab, College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China; ATR Lab, College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China; ATR Lab, College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China; ATR Lab, College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2631","2634","Object detection for optical remote sensing images have undergone rapid development in recent years, due to the advanced techniques of deep learning. However, the diverse objects with different scales and aspect ratios increase the difficulty of detection. In this paper, we improve the detection performance by applying guided anchor generation and feature fusion. In specific, the guided anchoring block directly predicts the positions and shapes of the anchor boxes, taking the place of sliding windows and preset shapes in the original region proposal network (RPN). Then, it can obtain diverse and adaptable anchor boxes, avoiding the generation of redundant and monotonous anchor boxes. In addition, the RoI features obtained from different pyramid levels are fused together to predict the categories and locations of the objects. The experiments conducted on DOTA dataset demonstrate the effectiveness of the proposed method.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324652","Remote sensing;object detection;guided anchor;feature fusion","Feature extraction;Shape;Proposals;Object detection;Detectors;Computer vision;Remote sensing","feature extraction;geophysical image processing;image fusion;learning (artificial intelligence);object detection;remote sensing","anchor generation;feature fusion;guided anchoring block;original region proposal network;diverse anchor boxes;adaptable anchor boxes;redundant anchor boxes;monotonous anchor boxes;RoI features;different pyramid levels;object detection;optical remote sensing images;undergone rapid development;deep learning;diverse objects;aspect ratios;detection performance","","1","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A Learning-Based Multi-Type Noise Suppressing Method for Remote Sensing Images","Y. Li; X. Yu; J. Pei; W. Huo; Y. Zhang; Y. Huang; J. Yang","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","3299","3302","Remote sensing images (RSIs) play an important role in a wide range of applications. However, they are frequently contaminated by multiple kinds of noises and existing methods are mostly applied to suppressing single noise type and performs poorly for various noises. To deal with above deficiencies, we propose a learning-based multi-type noise suppressing method (MNSM). Firstly, “Parallel” denoising approach is utilized to obtain partially denoised images that supply sufficient information for the subsequent fusion task. Mean-while, the noise recognition net identifies noise type and adjusts the brightness of every partially denoised image, realizing the adaptivity for different noises. The fusion net lastly merges these images to acquire one clean image. Experimental results show that this approach obtains higher peak-signal-to-noise ratio (PSNR) than existing methods.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884897","National Natural Science Foundation of China(grant numbers:61901091,61901090); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884897","Remote sensing;multiple noises;denoising;parallel structure","Deep learning;Image recognition;Simulation;Noise reduction;Brightness;Task analysis;Remote sensing","geophysical image processing;image denoising;image fusion;learning (artificial intelligence);remote sensing;wavelet transforms","noise recognition net;partially denoised image;clean image;higher peak-signal-to-noise ratio;remote sensing images;suppressing single noise type;learning-based multitype noise suppressing method;parallel denoising approach;MNSM;fusion task;PSNR","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Multi-Level Strategy-Based Spatial Information Prediction for Spatiotemporal Remote Sensing Imagery Fusion","J. Chen; R. Feng; L. Wang; W. Han; J. Huang","School of Computer Science, China University of Geosciences, Wuhan, P.R.China; School of Computer Science, China University of Geosciences, Wuhan, P.R.China; School of Computer Science, China University of Geosciences, Wuhan, P.R.China; School of Computer Science, China University of Geosciences, Wuhan, P.R.China; School of Computer Science, China University of Geosciences, Wuhan, P.R.China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","637","640","Spatiotemporal fusion utilizes the complementarity of high-temporal-low-spatial (HTLS) and high-spatial-low-temporal (HSLT) resolution data to obtain high temporal and spatial (HTHS) resolution fusion data, which can effectively satisfy the demand for HTHS data. However, due to the difference of spatial resolution, it is difficult to obtain precise spatial information in spatiotemporal fusion. To solve this problem, a multi-level strategy-based spatial domain prediction algorithm is proposed to enhance the spatial information extraction in spatiotemporal remote sensing imagery fusion, which can reduce the noise superposition in the process of multiple reconstruction. By learning-based first and then interpolation-based Super resolution reconstruction, the proposed method can obtain better prediction of spatial information and improve the accuracy of spatiotemporal fusion.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323312","National Natural Science Foundation of China(grant numbers:41571413,41701429,U1711266,41571413,41925007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323312","Spatiotemporal fusion;Super-resolution reconstruction;EDSR;TPS;Remote sensing","Spatial resolution;Remote sensing;Spatiotemporal phenomena;Superresolution;Image reconstruction;Interpolation;Reconstruction algorithms","geophysical image processing;image fusion;image processing;image reconstruction;image resolution;interpolation;remote sensing","multilevel strategy-based spatial information prediction;spatiotemporal remote sensing imagery fusion;spatiotemporal fusion;HTHS data;spatial resolution;precise spatial information;multilevel strategy-based spatial domain prediction algorithm;spatial information extraction;learning-based;interpolation-based Super resolution reconstruction","","","","17","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Flood Mapping with SAR and Multi-Spectral Remote Sensing Images Based on Weighted Evidential Fusion","X. Chen; Y. Cui; C. Wen; M. Zheng; Y. Gao; J. Li","School of Earth and Space Sciences, Peking University, Beijing, China; School of Earth and Space Sciences, Peking University, Beijing, China; Information Center of Ministry of Civil Affairs of the People's Republic of China, Beijing, China; Information Center of Ministry of Civil Affairs of the People's Republic of China, Beijing, China; Information Center of Ministry of Civil Affairs of the People's Republic of China, Beijing, China; Faculty of Geographical Science, Beijing Normal University, Beijing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2519","2522","Synthetic Aperture Radar (SAR) and Multi-spectral (MS) remote sensing images are commonly used for flood mapping. SAR images can provide valid backscattering measurements of inundated areas through cloud cover, while MS data is able to monitor the spectral changes of ground surface, but usually affected by clouds. The complementary characteristics of the two data indicate the potential of their combining application for flood monitoring in emergency. This paper proposes a novel weighted evidential fusion method to take full advantages of the SAR and MS data for change detection during the flood. First, pre-processing and classification are performed with the SAR and MS data, independently. Second, a modified PCR6 rule for evidential fusion is proposed, which introduces the confusion matrixes to calculate the weight of evidences so that the conflicting degree in the fusion process can be reduced. Then, the flood inundating, standing and receding patterns are identified, which can be used to describe the flooding process in details. Practically, the proposed method is applied to flood mapping of the Typhoon Rumbia in 2018, in Shouguang City, China. The experiments show that the proposed fusion scheme efficiently use both of the SAR and MS data, and improve the flood mapping accuracy.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324158","SAR;multi-spectral;change detection;evidential fusion;flood monitoring;remote sensing","Floods;Synthetic aperture radar;Remote sensing;Land surface;Earth;Radar polarimetry;Monitoring","floods;geophysical image processing;hydrological techniques;image classification;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar","modified PCR6 rule;confusion matrixes;AD 2018;Typhoon Rumbia;China;Shouguang City;flooding process;flood inundating;fusion process;evidential fusion method;flood monitoring;spectral changes;cloud cover;backscattering measurements;SAR images;multispectral remote sensing images;flood mapping accuracy;MS data;fusion scheme","","","","6","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Adaptive Channel Attention and Feature Super-Resolution for Remote Sensing Images Spatiotemporal Fusion","S. Fang; S. Meng; Y. Cao; J. Zhang; W. Shi","Anhui Province Key Laboratory of Industry Safety and Emergency Technology, Hefei, China; Key Laboratory of Knowledge Engineering with Big Data (Hefei University of Technology), Ministry of Education, Hefei, China; University of Science and Technology of China, Hefei, Anhui, China; Key Laboratory of Knowledge Engineering with Big Data (Hefei University of Technology), Ministry of Education, Hefei, China; Macau University of Science and Technology, Macau, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2572","2575","CNN-based Spatiotemporal image fusion (STIF) methods have achieved better performance than traditional researches. However, most CNN-based methods fail to make full use of hierarchical features, and ignore the quality and distribution characteristics of feature maps in fine-grained STIF. In this paper, we propose a network with channel attention and feature super-resolution for STIF (CAFSRNet). First, our method uses the low resolution time-domain changing images as input to extract changes more accurately and simplify computational overhead. Second, channel attention mechanism is introduced into Cross-spatial Resolution Mapping module to make the network pay more attention to informative features. Third, by adding feature super-resolution into the supervision process, we enhance the distribution of feature maps and the quality of mapping results. The qualitative and quantitative experimental results on various datasets demonstrate the superiority of our proposed method over the state-of-the-art methods.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555093","National Key R&D Program of China(grant numbers:2018YFC0213104); National Natural Science Foundation of China(grant numbers:61872327,61175033); Fundamental Research Funds for the Central Universities(grant numbers:WK2380000001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555093","Spatiotemporal image fusion;learning-based;Convolutional neural network (CNN);channel attention;feature super-resolution","Superresolution;Neural networks;Visual effects;Feature extraction;Spatiotemporal phenomena;Indexes;Time-domain analysis","feature extraction;geophysical image processing;image fusion;image resolution;remote sensing;spatiotemporal phenomena","fine-grained STIF;feature super-resolution;low resolution time-domain changing images;channel attention mechanism;Cross-spatial Resolution Mapping module;network pay more attention;informative features;feature maps;adaptive channel attention;remote sensing images Spatiotemporal;CNN-based Spatiotemporal image fusion methods;CNN-based methods;hierarchical features","","","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Multispectral image and fullcolor remote sensing image processing technology","J. Cai; J. -x. Ma; J. -x. Wang","School of Information Science & Engineering, Hebei University of Science and Technology, China; School of Computer Science &Technology, Hebei University, China; School of Information Science & Engineering, Hebei University of Science and Technology, China","2017 9th International Conference on Modelling, Identification and Control (ICMIC)","22 Mar 2018","2017","","","551","555","Putting GF1. into use is a significant achievement of high scores, which is of great significance to promote the localization of high resolution earth observation data and serve the national economy. In this paper, the author introduces the processing technology of remote sensing image which was sent back from high score number satellite. The remote sensing image is divided into multi - spectral and full - color remote sensing image. This paper describes these two kinds of graphs by orthophoto correction and image registration. With the spectral quality and spatial resolution of the image processing, and such data were compared to infer the occurrence of the real significance of the changes.","","978-1-5090-6575-2","10.1109/ICMIC.2017.8321705","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8321705","Multispectral image;Panchromatic image;Remote Sensing Image","Remote sensing;Satellites;Spatial resolution;Image fusion;Earth","geophysical image processing;image registration;remote sensing","multispectral-color remote sensing image;full-color remote sensing image;image registration;orthophoto correction;high score number satellite;high resolution earth observation data;high scores;significant achievement;GF1;remote sensing image processing technology;multispectral image","","1","","10","IEEE","22 Mar 2018","","","IEEE","IEEE Conferences"
"Fusion of satellite images in transform domain","H. Venkatesh; K. Viswanath","Siddaganga Institute of Technology, Tumkur, Karnataka, IN; Telecommunication Engineering Department, Siddaganga Institute of Technology, Tumakuru, Karnataka","2016 International Conference on Communication and Signal Processing (ICCSP)","24 Nov 2016","2016","","","1884","1888","Image fusion taking into account the wavelet and fourier change grades rich multispectral points of importance yet provides fewer spatial subtle elements from basis images. Wavelet performs well at direct elements yet not at non-straight cutoffs since Wavelets don't utilize the geometric assets of assemblies. Curvelet overwhelmed such challenges in highlight representation. A novel Image fusion principle through high pass exploiting Local Magnitude Ratio (LMR) in (FDCT) Fast Discrete Curvelet Transforms area and Discrete wavelet transform (DWT) is characterized. Indian Remote Sensing Geo satellite pictures are utilized for MS and Pan pictures. This mixture guideline produces HR multispectral picture with high spatial determination resolution. This strategy is contrasted and wavelet, Principal Component Analysis (PCA), Fast Discrete Curvelet Transforms area combination techniques. The Proposed procedure results in multispectral information alongside spatial points of interest.","","978-1-5090-0396-9","10.1109/ICCSP.2016.7754497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7754497","Image Fusion;Discrete wavelet transforms Fast Discrete Curvelet Transforms;Principal Component Analysis;Guided Filtering","Image fusion;Discrete wavelet transforms;Satellites;Remote sensing;Spatial resolution","artificial satellites;curvelet transforms;discrete wavelet transforms;geophysical image processing;image filtering;image fusion;principal component analysis;remote sensing","satellite image fusion;multispectral points;local magnitude ratio;FDCT;fast discrete curvelet transforms;discrete wavelet transform;Indian Remote Sensing Geo satellite pictures;Pan pictures;MS pictures;HR multispectral picture;high spatial determination resolution;principal component analysis;PCA;multispectral information;transform domain","","1","","14","IEEE","24 Nov 2016","","","IEEE","IEEE Conferences"
"Infrared and Visible Image Fusion via Multi-discriminators Wasserstein Generative Adversarial Network","J. Li; H. Huo; K. Liu; C. Li; S. Li; X. Yang","School of Information Technology and Cyber Security, People’s Public Security University of China, BeiJing, China; School of Information Technology and Cyber Security, People’s Public Security University of China, BeiJing, China; Remote sensing center of public security, People’s Public Security University of China, BeiJing, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; School of Information Technology and Cyber Security, People’s Public Security University of China, BeiJing, China; School of Information Technology and Cyber Security, People’s Public Security University of China, BeiJing, China","2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)","17 Feb 2020","2019","","","2014","2019","Generative adversarial network (GAN) has been widely applied to infrared and visible image fusion. However, the existing GAN-based image fusion methods only establish one discriminator in the network to make the fused image capture gradient information from the visible image, which may result in the loss of some infrared intensity information and texture information on the fused images. To solve this problem and improve the performance of GAN, we extend GAN to multiple discriminators and propose an end-to-end multi-discriminators Wasserstein generative adversarial network (MD-WGAN). In this framework, the fused image can preserve major infrared intensity and detail information from the first discriminator, and keep more texture information that existing in visible image from the second discriminator. We also design a texture loss function via local binary patterns to preserve more texture from visible image. The extensive qualitative and quantitative experiments show the advantages of our method compared with other state-of-the-art fusion methods.","","978-1-7281-4550-1","10.1109/ICMLA.2019.00322","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999112","Wasserstein generative adversarial network, image fusion, infrared image, visible image","Generators;Generative adversarial networks;Image fusion;Gallium nitride;Machine learning;Security;Information technology","gradient methods;image fusion;image texture;infrared imaging;neural nets","visible image;fused image capture gradient information;infrared intensity information;texture information;end-to-end multidiscriminators Wasserstein generative adversarial network;detail information;GAN-based image fusion methods","","1","","29","IEEE","17 Feb 2020","","","IEEE","IEEE Conferences"
"High Spatial-spectral Resolution Image Fusion Algorithm Based on Spectral Mixture Analysis","L. Wang; Z. Xing; D. Zhao; Y. Li; X. Yang; C. Hou; P. Li; X. Zhao","National Marine Data and Information Service, Tianjin, China; National Marine Data and Information Service, Tianjin, China; China Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; National Marine Data and Information Service, Tianjin, China; National Marine Data and Information Service, Tianjin, China; National Marine Data and Information Service, Tianjin, China; National Marine Data and Information Service, Tianjin, China; National Marine Data and Information Service, Tianjin, China","2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC)","21 Jun 2019","2018","","","408","411","This paper proposed a high spatial-spectral resolution image fusion algorithm for multispectral and hyperspectral images based on spectral mixture analysis. For this purpose, correspondence was established between high-spectral-resolution data and high-spatial-resolution data by extracting the same endmember. Specifically, Linear mixture model was applied to unmixing images, and the inverse process was used to do the image fusion. To test the algorithm, Hyperion and SPOT data were chose to do the fusion experiment, taking PSNR and SA as evaluation indexes. Results demonstrate the proposed algorithm a good performance on both spectral fidelity and spatial enhancement ability.","","978-1-5386-5373-9","10.1109/ITOEC.2018.8740545","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8740545","high spatial-spectral resolution image fusion algorithm;multispectral and hyperspectral images;spectral mixture analysis","Hyperspectral imaging;Spatial resolution;Image fusion;Matrix decomposition;Mixture models","geophysical techniques;hyperspectral imaging;image fusion;image resolution","high-spatial-resolution data;spectral fidelity;high spatial-spectral resolution image fusion algorithm;spectral mixture analysis;multispectral images;hyperspectral images;high-spectral-resolution data;linear mixture model;unmixing images;inverse process;Hyperion data;SPOT data;fusion experiment;spatial enhancement ability","","","","6","IEEE","21 Jun 2019","","","IEEE","IEEE Conferences"
"Fusion of remote sensing images based on dictionary learning","M. Ghamchili; H. Ghassemian","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2017 Iranian Conference on Electrical Engineering (ICEE)","20 Jul 2017","2017","","","1895","1900","Remote sensing image pan-sharpening is an appropriate way to fuse panchromatic (PAN) image with high spatial/low spectral resolution and multispectral (MS) image with low spatial/high spectral resolution to obtain a high spatial/high spectral resolution MS image (HRMS). Spectral information preservation and spatial resolution enhancement are the key issues of pan-sharpening problem. We propose a novel pan-sharpening method to decrease the spectral distortion of traditional methods inspired by sparse representation theory. Unlike previous works, details which must be injected to MS image should be extracted directly from a so-called `detail dictionary', trained by high frequency information of PAN image. With two proposed assumptions, we prove that the sparse coefficients of details on the detail dictionary can be obtained from a low pass version of PAN image and the MS image. We use QuickBird and WorldView-2 data sets to evaluate the proposed method. The simulation results show that the proposed method can effectively preserve spectral information and also improve the spatial resolution. The results show that the performance of proposed method is better than the state-of-the-art and TSSC methods quantitatively and perceptually.","","978-1-5090-5963-8","10.1109/IranianCEE.2017.7985363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985363","Dictionary learning;image fusion;detail dictionary;approximation dictionary;sparse coefficient;remote sensing","Image resolution;Dictionaries;Sensors;Low pass filters;Satellites;Principal component analysis","compressed sensing;geophysical image processing;image enhancement;image fusion;image representation;image resolution;remote sensing","remote sensing image fusion;dictionary learning;remote sensing image pan-sharpening;panchromatic image;multispectral image;image resolution;spatial resolution enhancement;spectral distortion;sparse representation theory;PAN image;MS image;QuickBird data sets;WorldView-2 data sets;geoscience","","5","","25","IEEE","20 Jul 2017","","","IEEE","IEEE Conferences"
"Fusion of multi-frequency SAR data with THAICHOTE optical imagery for maize classification in Thailand","C. Sukawattanavijit; J. Chen","School of Electronics and Information Engineering, Beihang University, Beijing, CHINA; School of Electronics and Information Engineering, Beihang University, Beijing, CHINA","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","617","620","Remote sensing data have been commonly used for agricultural crop monitoring. This paper was assessed the quality of using SAR and optical data fusion for maize classification. Two different SAR data sets from different sensors including dual polarization (HH and VV) X-band COSMO-SkyMed (CSK) and quad polarization (HH, HV, VH and VV) C-band RADARSAT-2 images were fused with THAICHOTE (namely, THEOS, an Earth observation mission of Thailand) optical data. This paper describes a comparative study of multi-sensor image fusion techniques in order to maintain spectral quality of the fused images. Principal Component Analysis (PCA), Intensity-Hue-Saturation (IHS), Brovey Transform (BT) and High-pass filter (HPF) techniques are implemented for image fusion. For the supervised classification, maximum likelihood was applied to the fused images to identify maize crop. Finally, the accuracy assessment was done by comparing maize maps generated from fused images and THAICHOTE classification. The PCA fused RADARSAT-2 with THAICHOTE images consistently provide excellent classification accuracies, well over 85%. The results obtained not only improving of the classification accuracy, but also can be identified the growing cycle of maize crop.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325839","Fusion;THAICHOTE;Cosmo-SkyMed;RADARSAT-2;Maize classification","Principal component analysis;Remote sensing;Accuracy;Image fusion;Optical sensors;Synthetic aperture radar;Optical imaging","crops;geophysical image processing;high-pass filters;image classification;image fusion;maximum likelihood estimation;principal component analysis;radar polarimetry;remote sensing by radar;synthetic aperture radar;vegetation mapping","remote sensing data;agricultural crop monitoring;SAR data fusion;optical data fusion;maize classification;dual polarization X-band COSMO-SkyMed images;quadpolarization C-band RADARSAT-2 images;THEOS;Earth observation mission;Thailand;THAICHOTE optical data classification accuracy;multisensor image fusion techniques;spectral quality;principal component analysis;intensity-hue-saturation;Brovey Transform;high-pass filter techniques;supervised classification;maximum likelihood;maize crop;accuracy assessment;THAICHOTE optical imagery;maize crop growing cycle","","3","","16","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Graph Regularized L1/2-Sparsity Constrained Non-Negative Matrix Factorization for Hyperspectral and Multispectral Image Fusion","S. Kahraman; A. Ertürk; S. Ertürk","Kocaeli University Laboratory of Image and Signal Processing (KULIS), Kocaeli University, Turkey; Kocaeli University Laboratory of Image and Signal Processing (KULIS), Kocaeli University, Turkey; Kocaeli University Laboratory of Image and Signal Processing (KULIS), Kocaeli University, Turkey","2018 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","27 Jun 2019","2018","","","1","4","High spectral and high spatial resolution is paramount for high performance identification and classification of hyperspectal (HS) images. There are many different approaches in order to improve HS spatial resolution. Data fusion is the process of combining HS and multispectral (MS) images in order to obtain high spectral and high spatial resolution HS images. In this study, based on the coupled nonnegative matrix factorization (CNMF) framework for data fusion, L1/2-sparsity constrained graph regularized nonnegative matrix factorization (GLNMF) approach is investigated for HS and MS data fusion. Experimental results show that the GLNMF based fusion approach outperforms state-of-the-art CNMF based data fusion. Experimental results are illustrated on datasets synthesized according to Wald' s protocol from AVIRIS Indian Pines and HYDICE Washington D.C. datasets.","2158-6276","978-1-7281-1581-8","10.1109/WHISPERS.2018.8747026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747026","Data fusion;GLNMF;hyperspectral;multispectral;unmixing","Data integration;Hyperspectral imaging;Spatial resolution;Image fusion;Protocols","geophysical image processing;graph theory;image classification;image fusion;image resolution;matrix decomposition;remote sensing;spectral analysis","high spatial resolution;coupled nonnegative matrix factorization framework;sparsity constrained graph;MS data fusion;experimental results;GLNMF based fusion approach;hyperspectral image fusion;multispectral image fusion;high performance identification;classification;HS spatial resolution;multispectral images;Graph regularized L1/2-sparsity;AVIRIS Indian Pines;HYDICE Washington D.C. datasets","","","","15","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"Comparison of Several Hyperspectral Image Fusion Methods for Superresolution","H. Lin; J. Chen","Department of Information System, Dalian Naval Academy, Dalian, China; Department of Information System, Dalian Naval Academy, Dalian, China","2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC)","18 Oct 2018","2018","","","448","452","Hyperspectral image applications have been explored in various areas, but they are often suffered from coarser spatial resolutions. In recent years, many hyperspectral image fusion approaches which merge hyperspectral image with multi-spectral or panchromatic one have been presented to improve the spatial resolution of hyperspectral image. In this paper, we compared four state-of-the-art hyperspectral fusion methods, namely coupled nonnegative matrix factorization (CNMF) method, sparse matrix factorization (SPMF) method, hyperspectral Image superresolution (HySure) method and sparse representation (SPRE) method. The main idea of each method is depicted briefly, five statistical assessment parameters, namely cross correlation (CC), root-mean-square error (RMSE), spectral angle mapper (SAM), universal image quality index (UIQI), and relative dimensionless global error in synthesis (ERGAS) are adopted to comparatively analyze the fusion results. The experimental results show that the effect of method based on sparse representation is superior to the others one.","","978-1-5386-4991-6","10.1109/ICIVC.2018.8492889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8492889","hyperspectral image;multicalspectral image;fusion;superresolution","Spatial resolution;Hyperspectral imaging;Sparse matrices;Dictionaries;Image fusion","hyperspectral imaging;image fusion;image resolution;image sampling;matrix decomposition;mean square error methods;remote sensing;spectral analysis","hyperspectral image superresolution method;image quality index;hyperspectral fusion method;hyperspectral image fusion approach;coupled nonnegative matrix factorization;spectral angle mapper;sparse representation method;sparse matrix factorization method;nonnegative matrix factorization method;spatial resolution;hyperspectral image applications","","2","","8","IEEE","18 Oct 2018","","","IEEE","IEEE Conferences"
"An Unsupervised Hyperspectral Image Fusion Method Based on Spectral Unmixing and Deep Learning","K. Zheng; A. Khader; L. Xiao","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2398","2401","Due to the limitations of various hardware conditions, in practice, only high resolution multispectral and low resolution hyperspectral images are usually captured. In order to apply hyperspectral images in various fields, better quality hyperspectral images have become a problem to be solved. In this paper, we propose an image fusion method based on spectral unmixing, which effectively combines the advantages of multispectral images and low-resolution hyperspectral images to generate high-resolution hyperspectral images. To be specific, a deep learning model based on spectral decomposition is constructed, using multiplication iterative rules based on the traditional gradient descent algorithm to get initial high-resolution abundance and define degeneration networks to describe the spatial and spectral downsampling operations. Experiments show that this method can get fusion images better quality than other methods.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884850","hyperspectal image;multispectral image;deep learning;prior","Deep learning;Image resolution;Atmospheric modeling;Imaging;Iterative algorithms;Hardware;Image fusion","geophysical image processing;geophysical signal processing;gradient methods;hyperspectral imaging;image classification;image fusion;image resolution;iterative methods;learning (artificial intelligence);remote sensing","fusion images better quality;unsupervised hyperspectral image fusion method;spectral unmixing;low resolution hyperspectral images;quality hyperspectral images;multispectral images;low-resolution hyperspectral images;high-resolution hyperspectral images;deep learning model;spectral decomposition;initial high-resolution abundance;spatial downsampling operations;spectral downsampling operations","","","","12","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"A novel Image Fusion Framework based on Non-Subsampled Shearlet Transform (NSST) Domain","P. Li; H. Wang; X. Li; H. Hu; H. Wei; Y. Yuan; Z. Zhang; G. QI","College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, China; College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, China; China Electronics Technology Group Corporation, Chongqing Acoustic-Optic-Electronic Co., Ltd, Chongqing, China; College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, China; College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, China; China Electronics Technology Group Corporation, Chongqing Acoustic-Optic-Electronic Co., Ltd, Chongqing, China; China Electronics Technology Group Corporation, Chongqing Acoustic-Optic-Electronic Co., Ltd, Chongqing, China; Department of Mathematics and Computer Information Science, Mansfield University of Pennsylvania, Mansfield, PA, USA","2019 Chinese Control And Decision Conference (CCDC)","12 Sep 2019","2019","","","1409","1414","As an effective means to integrate the information contained in multiple images in different ways, multi-model image fusion provides more comprehensive and sophisticated information for modern medical diagnosis, remote sensing, video surveillance and other fields. Based on non-subsampled shearlet transform (NSST) domain, this paper proposed an general image fusion framework to well preserve image energy and detail. It can well solve the problem of limited direction in image decomposition. This fusion method applies NSST to decomposition source images into high-pass and low-pass subbands. Sum Modified-laplacian(SML) is employed to fused high-pass bands. A weighted sum of weighted local energy (WLE) and a modified Laplacian (WSEML) based on eight neighborhoods are used as a fusion strategy to fuse low-frequency components. Finally, employing NSST inverse transform to reconstruct the fused high frequency and low frequency components. The experimental results indicate that the framework can save more image details and improve the quality of fused images.","1948-9447","978-1-7281-0106-4","10.1109/CCDC.2019.8833211","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8833211","non-subsampled shearlet transform (NSST);image fusion;SML;multi-model images","Image fusion;Transforms;Medical diagnostic imaging;Computed tomography;Measurement;Laplace equations","image fusion;inverse transforms","image decomposition;weighted local energy;nonsubsampled shearlet transform domain;multimodel image fusion;remote sensing;video surveillance;medical diagnosis;sum modified-laplacian;NSST inverse transform;WSEML","","2","","31","IEEE","12 Sep 2019","","","IEEE","IEEE Conferences"
"Image Fusion Using Belief Propagation","P. Hill; D. Bull","The Department of Electrical and Electronic Engineering, The University of Bristol, Bristol; The Department of Electrical and Electronic Engineering, The University of Bristol, Bristol","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","1817","1821","This paper describes the application of belief propagation methods to image fusion within a complex wavelet decomposition (the Dual Tree Complex Wavelet Transform: DT-CWT). Belief propagation within each transform subband iterates through a lattice based Bayesian belief network. This leads to precisely controlled spatial coherence of subband coefficient fusion through the definition of belief graph probabilities. This results in a significant improvement in quantitatively measured fusion performance for a large database of over 160 fusion image pairs from a range of fusion applications including remote sensing, multi-focus and multi-modal sources. Improvements in qualitative image fusion performance is also demonstrated.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8461463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461463","Image Fusion;Wavelets;Bayesian Belief Networks;Bayesian Belief Propagation","Image fusion;Belief propagation;Measurement;Spatial coherence;Discrete wavelet transforms","belief networks;image fusion;probability;trees (mathematics);wavelet transforms","belief propagation methods;complex wavelet decomposition;DT-CWT;Bayesian belief network;precisely controlled spatial coherence;subband coefficient fusion;belief graph probabilities;quantitatively measured fusion performance;fusion applications;qualitative image fusion performance;dual tree complex wavelet transform;fusion image pairs;remote sensing","","1","","28","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Stepwise Refinement Of Low Resolution Labels For Earth Observation Data: Part 2","D. Cerra; N. Merkle; C. Henry; K. Alonso; P. d’Angelo; S. Auer; R. Bahmanyar; X. Yuan; K. Bittner; M. Langheinrich; G. Zhang; M. Pato; J. Tian; P. Reinartz","German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2021","2020","","","7066","7069","This paper describes the contribution of the DLR team ranking 2nd in Track 2 of the 2020 IEEE GRSS Data Fusion Contest. The semantic classification of multimodal earth observation data proposed is based on the refinement of low-resolution MODIS labels, using as auxiliary training data higher resolution labels available for a validation data set. The classification is initialized with a handcrafted decision tree integrating output from a random forest classifier, and subsequently boosted by detectors for specific classes. The results of the team ranking 3rd in Track 1 of the same contest are reported in a companion paper.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9547209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547209","","Earth;Meters;Semantics;Data integration;Training data;Geoscience and remote sensing;Detectors","geophysical image processing;geophysical techniques;image classification;image fusion;image resolution;remote sensing;terrain mapping","DLR team;2020 IEEE GRSS Data Fusion Contest;semantic classification;auxiliary training data higher resolution labels;handcrafted decision tree;stepwise refinement;multimodal Earth observation data","","","","5","IEEE","28 Sep 2021","","","IEEE","IEEE Conferences"
"Comparison of image fusion techniques in RGB & Kekre's LUV color space","H. B. Kekre; D. Mishra; R. S. Saboo","Computer Engg., MPSTME SVKM's NMIMS (Deemed to be University), Mumbai; Computer Engg., MPSTME SVKM's NMIMS (Deemed to be University), Mumbai; Computer Engg., MPSTME SVKM's NMIMS (Deemed to be University), Mumbai","2015 International Conference on Futuristic Trends on Computational Analysis and Knowledge Management (ABLAZE)","13 Jul 2015","2015","","","114","120","With the advent and augmentation of image processing, image fusion has emerged as a topic of great importance in many research areas such as automatic object detection, medical imaging, computer vision, remote sensing, robotics, military and law enforcement, image classification and so on. This dissertation discusses the basic concepts of image fusion, different spatial and frequency domain algorithms for image fusion. We propose techniques that make use of other simple spatial pixel methods in combination with different transforms & hybrid wavelets instead of only using only averaging method to fuse the images. The transform and wavelet based methods are implemented using row, column and full transforms. A comparative assessment of pixel and block level image fusion methods is performed using six image quality metrics. The objective is to find which method gives better results for multi-focus color image fusion in RGB and LUV color space.","","978-1-4799-8433-6","10.1109/ABLAZE.2015.7154979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7154979","Image Fusion;Pixel Level Fusion Methods;Principal Component Analysis;Kekre's Wavelet Transform;Walsh PCA;DCT PCA;Kekre's LUV Color;Image Quality Metrics","Image fusion;Image color analysis;Wavelet domain;Discrete cosine transforms;Discrete wavelet transforms","image colour analysis;image fusion;wavelet transforms","RGB color space;Kekres LUV color space;frequency domain algorithms;spatial domain algorithms;spatial pixel methods;hybrid wavelets;wavelet based methods;full transforms;column transforms;row transforms;block level image fusion methods;pixel level image fusion methods;image quality metrics","","1","","14","IEEE","13 Jul 2015","","","IEEE","IEEE Conferences"
"Multi-sensor mars image fusion based on IHS combined robust PCA","J. -L. Wu; X. -L. Tian; Y. -X. Tian","Faculty of Information Technology, Macau University of Science and Technology, Macao, China; Faculty of Information Technology, Macau University of Science and Technology, Macao, China; Faculty of Information Technology, Macau University of Science and Technology, Macao, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","2594","2597","This paper proposes a fusion based on IHS combined robust PCA for remotely sensed Mars image data obtained by the MRO and Odyssey (THEMIS). Experiments show that the proposed algorithm, compares with conventional fusion techniques, can significantly inhibit the blocking artifacts and better extract the spatial details from the source images.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729670","image fusion;Mars image data;IHS;robust PCA;SSIM","Mars;Image fusion;Spatial resolution;Principal component analysis;Transforms;Robustness","astronomical image processing;image fusion;Mars","multisensor Mars image fusion;IHS combined robust PCA;MRO observations;Odyssey observations;THEMIS observations","","","","7","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Remote sensing image data fusion using spatial PCA and average block-DCT","A. Saboori; J. Birjandtalab","Islamic Azad University, Tehran, Tehran, IR; University of Texas at Dallas, Richardson, TX, US","2016 10th International Conference on Signal Processing and Communication Systems (ICSPCS)","6 Feb 2017","2016","","","1","7","Recently, with rapid development of imaging technology, there is a need for integrating images from different sources and extracting useful information. Remote sensing provides high quality multi-spectral and panchromatic images in terms of spectral and spatial information, respectively. So it is quite important to develop robust techniques to integrate images of the same scene with different characteristics in order to extract maximum spectral and spatial information. In this work, we provide a novel image fusion technique by combining block Discrete Cosine Transform (DCT) and spatial Principal Component Analysis (PCA) to improve the spectral quality of multi-spectral images while preserving the spatial resolution. We compare our technique with existing image fusion methods using different quality measures. The experimental results show that block-DCT based image fusion visually and quantitatively outperforms other techniques in terms of preserving spectral quality.","","978-1-5090-0941-1","10.1109/ICSPCS.2016.7843352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7843352","","Principal component analysis;Spatial resolution;Image fusion;Discrete cosine transforms;Remote sensing;Satellites","discrete cosine transforms;image fusion;principal component analysis;remote sensing","remote sensing image data fusion;spatial PCA;average block-DCT;quality multispectral images;panchromatic images;spectral information;spatial information;discrete cosine transform;principal component analysis;spectral quality;spatial resolution","","1","","19","IEEE","6 Feb 2017","","","IEEE","IEEE Conferences"
"Hyperspectral and Multispectral Image Fusion Using a Multi-Level Propagation Learning Network","C. A. Theran; M. A. Alvarez; E. Arzuaga; H. Sierra","University of Puerto Rico Mayaguez, Mayaguez, PR; Department of Electrical and Computer Engineering, University of Puerto Rico Mayaguez, Mayaguez, PR, PR; Department of Electrical and Computer Engineering, University of Puerto Rico Mayaguez, Mayaguez, PR, PR; University of Puerto Rico Mayaguez, Mayaguez, PR","2021 11th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Jul 2021","2021","","","1","5","Data fusion techniques have gained special interest in remote sensing due to the available capabilities to obtain measurements from the same scene using different instruments with varied resolution domains. In particular, multispectral (MS) and hyperspectral (HS) imaging fusion is used to generate high spatial and spectral images (HSEI). Deep learning data fusion models based on Long Short Term Memory (LSTM) and Convolutional Neural Networks (CNN) have been developed to achieve such task.In this work, we present a Multi-Level Propagation Learning Network (MLPLN) based on a LSTM model but that can be trained with variable data sizes in order achieve the fusion process. Moreover, the MLPLN provides an intrinsic data augmentation feature that reduces the required number of training samples. The proposed model generates a HSEI by fusing a high-spatial resolution MS image and a low spatial resolution HS image. The performance of the model is studied and compared to existing CNN and LSTM approaches by evaluating the quality of the fused image using the structural similarity metric (SSIM). The results show that an increase in the SSIM is still obtained while reducing of the number of training samples to train the MLPLN model.","2158-6276","978-1-6654-3601-4","10.1109/WHISPERS52202.2021.9484034","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9484034","Hyperspectral image;Multispectral image;Long Short Term Memory;Data fusion;Deep learning","Training;Instruments;Data integration;Imaging;Particle measurements;Data models;Spatial resolution","backpropagation;convolutional neural nets;hyperspectral imaging;image fusion;image resolution;learning (artificial intelligence);recurrent neural nets","LSTM model;fusion process;intrinsic data augmentation feature;HSEI;high-spatial resolution MS image;fused image;MLPLN model;data fusion techniques;remote sensing;varied resolution;spatial images;spectral images;deep learning data fusion models;long short term memory;convolutional neural networks;low-spatial resolution HS image;multilevel propagation learning network;structural similarity metric;SSIM;image quality","","","","18","IEEE","19 Jul 2021","","","IEEE","IEEE Conferences"
"Remote Sensing Image Scene Classification Based on Gated Pyramidal Convolution","H. Liu; S. Bian; Z. Wang; B. Liu; Q. Hu; D. -e. Guo","Chongqing MeiAnSen Technology Co., Ltd, Chongqing, China; Chongqing MeiAnSen Technology Co., Ltd, Chongqing, China; Chongqing MeiAnSen Technology Co., Ltd, Chongqing, China; Chongqing MeiAnSen Technology Co., Ltd, Chongqing, China; Chongqing MeiAnSen Technology Co., Ltd, Chongqing, China; Chongqing Spatial Big Data Intelligent Engineering Research Center, Chongqing University of Posts and Telecommunications, Chongqing, China","2020 IEEE International Conference on Progress in Informatics and Computing (PIC)","16 Feb 2021","2020","","","154","158","Deep learning technology has demonstrated superior performance in remote sensing image scene classification. However, the complex background and diverse target objects in remote sensing images have always been a major challenge in scene classification. To solve this problem, we propose a novel remote sensing scene classification method based on Gated Pyramidal Convolution (GPC). Specifically, pyramidal convolution (PyConv) is first introduced into the residual block to capture different levels of detailed features. Then, the Gated Unit (GU) is placed behind the last three residual units to filter useless background information. Finally, a multi-level feature fusion structure is introduced to fuse the features of the last three residual units to further enhance the feature discriminant ability. Extensive experiments on AID and NWPU-RESISC45 datasets demonstrate that the proposed GPC model outperforms the state-of-the-art baselines.","2329-6259","978-1-7281-7086-2","10.1109/PIC50277.2020.9350803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9350803","feature fusion;remote sensing image scene classification;Gated unit;remote sensing image;pyramidal convolution","Image analysis;Convolution;Layout;Logic gates;Sensors;Resource management;Remote sensing","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing","residual units;remote sensing image scene classification;Gated Pyramidal Convolution;remote sensing images;novel remote sensing scene classification method","","","","14","IEEE","16 Feb 2021","","","IEEE","IEEE Conferences"
"Remote Sensing Satellite Jitter Detection Based on Image Registration and Convolutional Neural Network Fusion","Z. Zhang; A. Iwasaki; G. Xu",The University of Tokyo and Harbin Institute of Technology; The University of Tokyo and Harbin Institute of Technology; The University of Tokyo and Harbin Institute of Technology,"IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","10035","10038","Attitude jitter of observation satellites is a problem that degenerates the imaging quality in high-resolution (HR) remote sensing. In order to retrieve jitters with high accuracy, a new way to fuse image registration and convolutional neural network by a Kalman filter is described. In this method, a novel and complete framework based on image registration are proposed to estimate the on-orbit attitude variations from multi-spectrum remote sensing images. Then a convolutional neural network(CNN) is proposed to estimate the attitude jitter by automatically learning essential scene features from a single image. A Kalman filter is derived to fuse the jitter obtained from the two algorithms. The proposed methodology is trained and tested with PatternNet dataset. Simulation results show that the attitude variations are detected with high accuracy and the deformed images are compensated.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899150","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899150","CNN;Image registration;Kalman filter;Jitter detection","Jitter;Image registration;Remote sensing;Cameras;Deconvolution;Satellites;Convolutional neural networks","artificial satellites;convolutional neural nets;feature extraction;image filtering;image fusion;image registration;jitter;Kalman filters;learning (artificial intelligence);remote sensing","image registration;convolutional neural network fusion;attitude jitter;scene features learning;on-orbit attitude variations estimation;remote sensing satellite jitter detection;multispectrum remote sensing images;Kalman filter;high-resolution remote sensing;imaging quality;observation satellites","","","","13","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Multiscale image fusion for pansharpening of multispectral images using saliency detection","Shruti; S. Budhiraja","Electronics and Communication Engineering, Punjab University, Chandigarh, India; Electronics and Communication Engineering, Punjab University, Chandigarh, India","2016 Ninth International Conference on Contemporary Computing (IC3)","20 Mar 2017","2016","","","1","6","Pansharpening is a technique used for fusion of low spatial resolution multispectral images with high spatial resolution panchromatic image. The objective of pansharpening is to enhance the spatial resolution of multispectral image while preserving its spectral information. The commonly used pansharpening methods like IHS, PCA, Gram Schmidt and Wavelet based method compromise either on spatial or spectral resolution. This paper presents a modified multiresolution analysis based pansharpening technique that is based on saliency detection of an image. This method fuses the spatial and spectral information of the images independently; therefore the fused image is obtained with high spatial as well as spectral information. The technique has been implemented on different datasets of multispectral and panchromatic images using different sets of fusion rules. The results reveal that the modified saliency detection based pansharpening technique performs better than the existing pansharpening technique and other commonly used pansharpening techniques.","","978-1-5090-3251-8","10.1109/IC3.2016.7880253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7880253","MS image;Pan image;fusion;pansharpening;saliency detection","Spatial resolution;Information filters;Image fusion;Image reconstruction;Remote sensing","image fusion;image resolution","multiscale image fusion;saliency detection;spatial resolution multispectral images;high spatial resolution panchromatic images;spectral information;pansharpening methods;IHS;PCA;Gram Schmidt;wavelet based method;spectral resolution;multiresolution analysis;fusion rules","","1","","19","IEEE","20 Mar 2017","","","IEEE","IEEE Conferences"
"Image Fusion Based on Morphological Component Analysis via Gradient","S. Chen; Y. Lu; Q. Gao; D. Sun; X. Peng","School of Electrical Engineering and Automation, Anhui University, Hefei, China; School of Electrical Engineering and Automation, Anhui University, Hefei, China; School of Electrical Engineering and Automation, Anhui University, Hefei, China; School of Electrical Engineering and Automation, Anhui University, Hefei, China; School of Electrical Engineering and Automation, Anhui University, Hefei, China","2018 2nd IEEE Advanced Information Management,Communicates,Electronic and Automation Control Conference (IMCEC)","23 Sep 2018","2018","","","1034","1037","I306 fusion is a process to strengthen the human perception of individual source images from the same scene. We propose a novel method to address fusion based on morphological component analysis (MCA) and gradient comparison. We first use MCA to separate source images into multi-components (cartoon, texture and few residual images). Then computing the sum of gradients for every pixel in each image component and select suitable pixels in the fused image. In the experiments, the proposed method brings a state-of-the-art fused performance compared with traditional algorithms in terms of the visual perceptions and objective evaluations.","","978-1-5386-1803-5","10.1109/IMCEC.2018.8469385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8469385","Image fusion;MCA;Gradient;Multi-components","Image fusion;Dictionaries;Fuses;Discrete wavelet transforms;Remote sensing","gradient methods;image fusion;image texture;visual perception","image fusion;morphological component analysis;human perception;visual perceptions;MCA;image texture;residual images;gradient comparison","","","","22","IEEE","23 Sep 2018","","","IEEE","IEEE Conferences"
"Attention-based F-UNet for Remote Sensing Image Fusion","P. Zhang; Q. Jiang; L. Cai; R. Wang; P. Wang; X. Jin","School of Software, Yunnan University, Kunming, China; School of Software, Yunnan University, Kunming, China; School of Software, Yunnan University, Kunming, China; School of Software, Yunnan University, Kunming, China; School of Software, Yunnan University, Kunming, China; School of Software, Yunnan University, Kunming, China","2021 IEEE 23rd Int Conf on High Performance Computing & Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)","30 May 2022","2021","","","81","88","With the development of modern remote sensing techniques, various earth observation satellites continuously provide remote sensing images with different spatial resolution, time resolution, and spectral resolution. In order to make full use of the information of a variety of remote sensing images, these images needs to be fused to obtain richer information than anyone of the source image. This paper proposes a FUNet remote sensing image fusion model based on UNet++ structure and attention mechanism. Our model is composed of three modules: multi-scale feature extraction, feature fusion, and image reconstruction. We introduce two attention mechanisms in the image feature extraction, fusion and reconstruction modules to make the network pay more attention to the key information in the feature image. Experimental results show that F - UN et outperforms the existing methods under both full reference and no reference indicators.","","978-1-6654-9457-1","10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9781196","pan-sharpening;image fusion;multispectral images;panchromatic images;convolutional neural networks;attention","Earth;Satellites;Feature extraction;Distortion;Spatial resolution;Remote sensing;Image reconstruction","feature extraction;geophysical image processing;image fusion;image reconstruction;image resolution;remote sensing","remote sensing images;FUNet remote sensing image fusion model;multiscale feature extraction;image reconstruction;image feature extraction;modern remote sensing techniques;Earth observation satellites","","","","31","IEEE","30 May 2022","","","IEEE","IEEE Conferences"
"Fusion between UAV-SFM and terrestrial laser scanner for field validation of satellite remote sensing","A. Kato; H. Obanawa; Y. Hayakawa; M. Watanabe; Y. Yamaguchi; T. Enoki","Graduate School of Horticulture, Chiba University, Chiba, Japan; Center for Environmental Remote Sensing, Chiba University, Chiba, Japan; Tokyo Daigaku, Bunkyo-ku, Tokyo, JP; Earth Observation Research Center, Japan Aerospace Exploration Agency, Ibaraki, Japan; Graduate School of Science & Technology, Niigata University, Niigata, Japan; Kyushu Daigaku, Fukuoka, JP","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","2642","2645","Field tree measurement to collect ground truth data is labor intensive. To make it efficient to collect the field data in a large scale, Unmanned Aerial Vehicle is introduced in this study. An automated photogrammetric technique called Structure from Motion (SfM) is used to create 3D data out of photos collected by UAV. Tree structure created by SfM is compared with the 3D data obtained by several Terrestrial Laser Scanners to see the difference in vertical forest structure. Canopy structure is mainly captured by UAV-SfM and there is more point density around tree canopy than 3D data obtained by airborne laser. The point density of UAV around tree canopy is equivalent to that of TLS data, but tree trunk underneath the canopy cannot be reconstructed well, because the trunk is obscured by tree canopy and it is not visible on photos.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326355","SfM;Laser;TLS;Forest;3D","Three-dimensional displays;Vegetation;Sensors;Remote sensing;Global Positioning System;Cameras;Satellites","autonomous aerial vehicles;geophysical image processing;image fusion;photogrammetry;remote sensing by laser beam;vegetation;vegetation mapping","field tree measurement;ground truth data;field data;unmanned aerial vehicle;automated photogrammetric technique;structure from motion;tree structure;terrestrial laser scanners;vertical forest structure;canopy structure;UAV-SfM;tree canopy;airborne laser;point density;trunk;field validation;satellite remote sensing","","4","","10","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Sparse Representations and Dictionary Learning: from Image Fusion to Motion Estimation","J. . -Y. Tourneret; A. Basarab; N. Ouzir; Q. Wei","University of Toulouse/INP-ENSEEIHT/IRIT/TSA, Toulouse, France; University of Toulouse, Université Paul Sabatier Toulouse 3, IRIT, CNRS UMR, France; University of Paris-Saclay/Inria/CentraleSupelec/CVN, Gif Sur Yvette, France; Machine Learning Center of Excellence, J. P. Morgan, New York, NY, USA","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","25","28","The first part of this paper presents some works conducted with Jose Bioucas Dias for fusing high spectral resolution images (such as hyperspectral images) and high spatial resolution images (such as panchromatic or multispectral images) in order to build images with improved spectral and spatial resolutions. These works are related to Bayesian fusion strategies exploiting prior information about the target image to be recovered constructed by dictionary learning. Interestingly, these Bayesian image fusion methods can be adapted with limited changes to motion estimation in pairs or sequences of images. The second part of this paper explains how the work of Jose Bioucas Dias has been a source of inspiration for developing new Bayesian motion estimation methods for ultrasound images.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554890","Image fusion;motion estimation;sparse representations;dictionary learning","Ultrasonic imaging;Dictionaries;Motion estimation;Geoscience and remote sensing;Machine learning;Bayes methods;Spatial resolution","Bayes methods;hyperspectral imaging;image fusion;image reconstruction;image representation;image resolution;motion estimation;sensor fusion;ultrasonic imaging","Bayesian image fusion methods;multispectral images;panchromatic images;high spatial resolution images;hyperspectral images;high spectral resolution images;dictionary learning;sparse representations;ultrasound images;Bayesian motion estimation methods","","","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A Full-Resolution Training Framework for Sentinel-2 Image Fusion","M. Ciotola; M. Ragosta; G. Poggi; G. Scarpa","Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione, Università Federico II, Napoli, (I); Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione, Università Federico II, Napoli, (I); Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione, Università Federico II, Napoli, (I); Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione, Università Federico II, Napoli, (I)","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","1260","1263","This work presents a new unsupervised framework for training deep learning models for super-resolution of Sentinel-2 images by fusion of its 10-m and 20-m bands. The proposed scheme avoids the resolution downgrade process needed to generate training data in the supervised case. On the other hand, a proper loss that accounts for cycle-consistency between the network prediction and the input components to be fused is proposed. Despite its unsupervised nature, in our preliminary experiments the proposed scheme has shown promising results in comparison to the supervised approach. Besides, by construction of the proposed loss, the resulting trained network can be ascribed to the class of multi-resolution analysis methods.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553199","Super-resolution;data-fusion;convolutional neural network;machine learning;Sentinel-2","Training;Deep learning;Superresolution;Training data;Geoscience and remote sensing;Image fusion","geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);neural nets","full-resolution training framework;Sentinel-2 image fusion;unsupervised framework;deep learning models;super-resolution;Sentinel-2 images;resolution downgrade process;training data;supervised case;proper loss;cycle-consistency;network prediction;input components;unsupervised nature;supervised approach;resulting trained network;multiresolution analysis methods","","6","","20","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Assessment of pan-sharpening methods applied to WorldView-2 image fusion","H. Li; L. Jing; Y. Tang; Q. Liu; H. Ding; Z. Sun; Y. Chen","Key Laboratory of Digital Earth Science, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Chinese Academy of Sciences, Beijing, China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","3302","3305","Various multispectral (MS) and panchromatic (PAN) fusion (or pan-sharpening) algorithms were developed to produce an enhanced MS image of high spatial resolution. Regarding the novelty in both the PAN and MS bands of the WV-2 imagery, the objective of this study is to assess the performance of nine state-of-the-art pan-sharpening methods for the WV-2 imagery, using both image quality indices and information indices that used for urban information extraction. The comparison of the four quality indices (RASE, ERGAS, SAM, and Q4) demonstrated that the HR method performed the best for the WV-2 MS and PAN images. However, the comparison of the four information indices showed that a higher quality at data level does not signify better information preservation for object recognition.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326524","Pan-sharpening;WorldView-2 two;quality assessment","Spatial resolution;Remote sensing;Principal component analysis;Image quality;Indexes;Image fusion","geophysical image processing;image fusion;image reconstruction;image resolution;terrain mapping","WorldView-2 image fusion;pan-sharpening methods;multispectral fusion algorithms;panchromatic fusion algorithms;high spatial resolution;panchromatic bands;multispectral bands;WV-2 imagery;information indices;urban information extraction;RASE image quality index;ERGAS image quality index;SAM image quality index;Q4 image quality index;WV-2 multispectral images;WV-2 panchromatic images;data level;object recognition","","4","","14","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Integrating time-series and high-spatial remote sensing data based on multilevel decision fusion","G. Xudong; C. Huang; L. Gaohuan; L. Qingsheng","Research Center for Digital Mountain and Remote Sensing Application, Institute of Mountain Hazards and Environment, CAS, Chengdu, CO, China; State Key Laboratory of Resources and Environmental Information System, Institute of Geographic Sciences and Natural Resources Research, CAS, Beijing, CO, China; State Key Laboratory of Resources and Environmental Information System, Institute of Geographic Sciences and Natural Resources Research, CAS, Beijing, CO, China; State Key Laboratory of Resources and Environmental Information System, Institute of Geographic Sciences and Natural Resources Research, CAS, Beijing, CO, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","212","215","Due to the low spatial resolution of MODIS data, the accuracy of small-area plaque extraction with high degree of landscape fragmentation is greatly limited. To this end, the study combines Landsat data with higher spatial resolution and MODIS data with higher temporal resolution for decision-level fusion. Considering the importance of the land heterogeneity factor in the fusion process, it is superimposed with the weighting factor, which is to linearly weight the Landsat classification result and the MOIDS classification result. Three levels were used to complete the process of data fusion, that are the pixel of MODIS data, the pixel of Landsat data, and objects level that connect between these two levels. The multilevel decision fusion scheme was tested in two sites of the lower Mekong basin. We put forth a comparison test, and it was proved that the classification accuracy was improved compared with the single data source classification results in terms of the overall accuracy. The method was also compared with the two-level combination results and a weighted sum decision rule-based approach. The decision fusion scheme is extensible to other multi-resolution data decision fusion applications.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323564","National Natural Science Foundation of China(grant numbers:41901309); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323564","image classification;decision fusion;multi-temporal;remote sensing","Remote sensing;Earth;Artificial satellites;MODIS;Time series analysis;Image segmentation;Data mining","geophysical image processing;image fusion;image resolution;terrain mapping","integrating time-series;high-spatial remote sensing data;low spatial resolution;MODIS data;small-area plaque extraction;Landsat data;higher spatial resolution;higher temporal resolution;decision-level fusion;land heterogeneity factor;fusion process;weighting factor;Landsat classification result;MOIDS classification result;multilevel decision fusion scheme;classification accuracy;single data source classification results;two-level combination results;weighted sum decision rule-based approach;multiresolution data decision fusion applications","","","","11","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Satellite Image Fusion using FDCT for Land Cover Classification","S. K. Naik; H. Ramesh","Water Resources and Ocean Engineering National Institute of Technology Karnataka, Mangalore, India; Water Resources and Ocean Engineering National Institute of Technology Karnataka, Mangalore, India","2021 IEEE 6th International Conference on Computing, Communication and Automation (ICCCA)","10 Jan 2022","2021","","","81","86","Remote sensing is a fast developing field of science involving repetitive collection of data from earth observing satellites. However each satellite system has one or more limitations, giving rise to the need of data collection from multiple sources and their fusion. Landsat 8 collects images in a broad spectrum but at a coarser spatial resolution of 30m. Cartosat-1 collects images at a high spatial resolution of 2.5m but lacks color details. Good visually interpretable images are indispensable for land cover classification. In this paper, the Landsat 8 and Cartosat-1 images are fused by using the Fast Discrete Curvelet Transform (FDCT) method. Supervised classification using the Random Forest (RF) classifier is performed on the Landsat 8 multispectral image and the fused image. The results showed high quality of image fusion based on the entropy, RMSE and CC values obtained for the given dataset. The fusion process also improved the overall accuracy of the land cover classification.","2642-7354","978-1-6654-1473-9","10.1109/ICCCA52192.2021.9666283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9666283","Satellite image fusion;Land cover classification","Earth;Satellites;Artificial satellites;Stereo image processing;Transforms;Entropy;Spatial resolution","curvelet transforms;geophysical image processing;image classification;image fusion;image resolution;land cover;remote sensing","fused image;fusion process;land cover classification;satellite image fusion;FDCT;remote sensing;earth observing satellites;satellite system;data collection;coarser spatial resolution;high spatial resolution;good visually interpretable images;Cartosat-1 images;Fast Discrete Curvelet Transform method;supervised classification;Landsat 8 multispectral image;size 30.0 m;size 2.5 m","","","","29","IEEE","10 Jan 2022","","","IEEE","IEEE Conferences"
"Unrolled Projected Gradient Descent for Multi-spectral Image Fusion","S. Lohit; D. Liu; H. Mansour; P. T. Boufounos","Arizona State University, Tempe, AZ, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","7725","7729","In this paper, we consider the problem of fusing low spatial resolution multi-spectral (MS) aerial images with their associated high spatial resolution panchromatic image. To solve this problem, various methods have been proposed, using either model-based or model-agnostic algorithms such as deep learning techniques. In this paper, we aim to utilize more interpretable architectures to solve the MS fusion problem by integrating existing ideas from image processing with deep learning. In particular, we develop a signal processing-inspired learning solution, where we unroll the iterations of the projected gradient descent (PGD) algorithm, and each iteration contains a projection operation carried out by a deep convolutional neural network. We observe that our proposed method provides a new perspective on existing deep-learning solutions, and under certain circumstance it reduces to current black-box deep learning methods. Our extensive experimental results show significant improvements of the proposed approach over several baselines.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8683124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8683124","multi-spectral image fusion;deep learning;projected gradient descent","Deep learning;Image fusion;Spatial resolution;Signal processing algorithms;Imaging;Inverse problems","convolutional neural nets;geophysical image processing;gradient methods;image fusion;image resolution;learning (artificial intelligence);remote sensing","unrolled projected gradient descent;model-agnostic algorithms;deep learning techniques;MS fusion problem;image processing;signal processing-inspired learning solution;iteration;projected gradient descent algorithm;projection operation;deep convolutional neural network;black-box deep learning methods;high spatial resolution panchromatic image;low spatial resolution multispectral aerial image fusion;model-based algorithms","","11","","17","IEEE","17 Apr 2019","","","IEEE","IEEE Conferences"
"Mineral exploration with hyperspectral image fusion","E. Saralıoğlu; E. T. Görmüş; O. Güngör","Karadeniz Teknik Universitesi, Trabzon, TR; Karadeniz Teknik Universitesi, Trabzon, TR; Karadeniz Teknik Universitesi, Trabzon, TR","2016 24th Signal Processing and Communication Application Conference (SIU)","23 Jun 2016","2016","","","1281","1284","Hyperspectral images can identify minerals better than Multispectral images because of their high spectral resolutions. However, a pixel might include more than one mineral, as hyperspectral images have low spatial resolution. In these situations, the number of minerals can be estimated in mixed pixels but their spatial positions cannot be known. This is one of the biggest obstacles that prevents effective use of hyperspectral images in mineral exploration. In order to overcome that obstacle, it is known that, fusing hyperspectral images with a better spatial resolution images generates a better content in the fused images. In this study, well known and publicly available Cuprite AVIRIS image is fused with Landsat 8- Panchromatic band. Then, the minerals in the Cuprite site are classified both on the fused image and the original AVIRIS image. When we compared the classification results, it is found that, minerals are classified with higher accuracy on the fused image compared to the AVIRIS image.","","978-1-5090-1679-2","10.1109/SIU.2016.7495981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495981","Hyperspectral;Image Fusion;Classification;Quality Assessment;Mineral Exploration","Minerals;Satellites;Earth;Hyperspectral imaging;Image fusion","geophysical image processing;image classification;image fusion;image resolution;minerals;remote sensing","Landsat 8-panchromatic band;Cuprite AVIRIS image;mixed pixel estimation;low spatial resolution images;high spectral resolution images;hyperspectral image fusion;mineral exploration","","10","","","IEEE","23 Jun 2016","","","IEEE","IEEE Conferences"
"Hyperspectral and multispectral image fusion using dual-source localized dictionary pair","J. Liang; Y. Zhang; S. Mei","Shaanxi Key Laboratory of Information Acquisition and Processing, Northwestern Polytechnical University, Xi'an, China; Shaanxi Key Laboratory of Information Acquisition and Processing, Northwestern Polytechnical University, Xi'an, China; Shaanxi Key Laboratory of Information Acquisition and Processing, Northwestern Polytechnical University, Xi'an, China","2017 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS)","22 Jan 2018","2017","","","261","264","Hyperpsectral image (HSI) provides abundant and detailed spectral information with limited spatial resolution. When a multispectral image (MSI) with higher spatial resolution of the same observed scene is available, this limitation on spatial resolution can be handled by applying fusion techniques. In this work, a novel HSI and MSI fusion approach using dictionary-based reconstruction is proposed. To incorporate more effective information for fusion, dual-source dictionary pair including a dictionary on low spatial resolution and a dictionary on high spatial resolution, is constructed using both HSI and MSI. Furthermore, to reduce the calculation cost, a localized strategy is applied instead of the global one. Finally, the fused result is reconstructed with dictionary using collaborative representation. Simulative experiments illustrate its outperformance over some state-of-the-art HSI and MSI fusion approaches.","","978-1-5386-2159-2","10.1109/ISPACS.2017.8266485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8266485","Collaborative representation;dictionary pair;hyperspectral;image fusion;multispectral","Dictionaries;Spatial resolution;Hyperspectral imaging;Image reconstruction;DH-HEMTs;Image fusion","geophysical image processing;image classification;image fusion;image reconstruction;image representation;image resolution;image sensors;remote sensing","dual-source localized dictionary pair;hyperpsectral image;spectral information;higher spatial resolution;fusion techniques;MSI fusion approach;dual-source dictionary pair;low spatial resolution;high spatial resolution;localized strategy;multispectral image fusion;HSI","","2","","7","IEEE","22 Jan 2018","","","IEEE","IEEE Conferences"
"Hyperspectral image fusion by hybrid regularizations with local low-rank","Z. Liu; M. Ma; Z. Wu","School of Information Engineering Nanchang Institute of Technology, Nanchang, China; College of Science Nanchang Institute of Technology, Nanchang, China; School of Information Engineering Nanchang Institute of Technology, Nanchang, China","2021 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI)","4 Oct 2021","2021","","","116","120","Hyperspectral images usually have higher spectral resolution but lower spatial resolution, compared with the multispectral images. Low spatial resolution brings difficulties to the practical applications of hyperspectral images. Therefore, to get high spatial resolution hyperspectral image, it is very important to fuse low spatial resolution hyperspectral image with the high spatial resolution multispectral image in the same scene. In this paper, we propose a hybrid regularization model by integrating sparse prior, local low-rank regularization and total variation based on l2 norm to reconstruct high spatial resolution hyperspectral images. In addition, we design an alternating direction method of multipliers (ADMM) to solve it. The experimental results show the superiority and competitiveness of our method over the state-of-the-art methods.","","978-1-6654-3960-2","10.1109/ICCEAI52939.2021.00022","Natural Science Foundation of Jiangxi Province(grant numbers:20192BAB211005); NNSF of China(grant numbers:61865012,61861032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9544289","Hyperspectral image fusion;total variation based on l2 norm;local low-rank;ADMM","Fuses;Convex functions;Spatial resolution;Artificial intelligence;Image fusion;Image reconstruction;Hyperspectral imaging","geophysical image processing;hyperspectral imaging;image denoising;image fusion;image reconstruction;image resolution;remote sensing","hyperspectral image fusion;hyperspectral images;higher spectral resolution;lower spatial resolution;multispectral images;high spatial resolution hyperspectral image;low spatial resolution hyperspectral image;high spatial resolution multispectral image;low-rank regularization","","","","16","IEEE","4 Oct 2021","","","IEEE","IEEE Conferences"
"Image Fusion and Reconstruction of Compressed Data: A Joint Approach","D. Picone; L. Condat; F. Cotte; M. Dalla Mura","CNRS, Institute of Engineering Univ. Grenoble Alpes, Grenoble, France; CNRS, Institute of Engineering Univ. Grenoble Alpes, Grenoble, France; CNRS, Institute of Engineering Univ. Grenoble Alpes, Grenoble, France; CNRS, Institute of Engineering Univ. Grenoble Alpes, Grenoble, France","2018 25th IEEE International Conference on Image Processing (ICIP)","6 Sep 2018","2018","","","878","882","In the context of data fusion, pansharpening refers to the combination of a panchromatic (PAN) and a multispectral (MS) image, aimed at generating an image that features both the high spatial resolution of the former and high spectral diversity of the latter. In this work we present a model to jointly solve the problem of data fusion and reconstruction of a compressed image; the latter is envisioned to be generated solely with optical on-board instruments, and stored in place of the original sources. The burden of data downlink is hence significantly reduced at the expense of a more laborious analysis done at the ground segment to estimate the missing information. The reconstruction algorithm estimates the target sharpened image directly instead of decompressing the original sources beforehand; a viable and practical novel solution is also introduced to show the effectiveness of the approach.","2381-8549","978-1-4799-7061-2","10.1109/ICIP.2018.8451631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451631","Image fusion;data compression;remote sensing;inverse problems;optical devices","Image coding;Optical imaging;Spatial resolution;Image reconstruction;Optical sensors;Image fusion","data compression;image coding;image colour analysis;image fusion;image reconstruction;image resolution","image fusion;compressed data;data fusion;multispectral image;high spatial resolution;high spectral diversity;compressed image;data downlink;reconstruction algorithm;sharpened image","","1","","30","IEEE","6 Sep 2018","","","IEEE","IEEE Conferences"
"PCA-based Wavelet Remote Sensing Image Synthesis Simulation Method","G. Li; H. Yang; J. Wang; Y. Li; C. Zhang; H. Xie; B. Feng","Graduate School, Space Engineering University, Beijing, China; School of Space Information, Space Engineering University, Beijing, China; Graduate School, Space Engineering University, Beijing, China; Graduate School, Space Engineering University, Beijing, China; Graduate School, Space Engineering University, Beijing, China; Graduate School, Space Engineering University, Beijing, China; Graduate School, Space Engineering University, Beijing, China","2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","5 Apr 2021","2021","","","1042","1046","The traditional image mosaic method directly replaces the remote sensing target image into the corresponding area of the background image, but ""splicing traces"" are prone to appear. This paper proposes a method of fusion of remote sensing background and target images under wavelet multi-frequency based on principal component analysis. First, the remote sensing background and target image are decomposed by wavelet, and then the fusion weight of the image wavelet low-frequency coefficient is calculated by the principal component analysis method, and the high-frequency coefficient is fused by the absolute maximum method, and finally the wavelet fusion coefficient is inversely transformed to realize the image synthesis simulation. The simulation results show that the fusion image can not only highlight the target features, but also has no stitching traces.","2689-6621","978-1-7281-8028-1","10.1109/IAEAC50856.2021.9390650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9390650","image geometric transformation;image fusion;principal component analysis;wavelet transform","Wavelet transforms;Analytical models;Image synthesis;Splicing;Wavelet analysis;Remote sensing;Principal component analysis","image fusion;image segmentation;inverse transforms;principal component analysis;remote sensing;wavelet transforms","absolute maximum method;wavelet fusion coefficient;fusion image;PCA-based wavelet remote sensing image synthesis simulation method;traditional image mosaic method;remote sensing target image;background image;remote sensing background;wavelet multifrequency;image wavelet low-frequency coefficient;principal component analysis method","","1","","6","IEEE","5 Apr 2021","","","IEEE","IEEE Conferences"
"Object based fusion of polarimetric SAR and hyperspectral imaging for land use classification","J. Hu; P. Ghamisi; A. Schmitt; X. X. Zhu","Remote Sensing Technology Institute, DLR German Aerospace Center, Wessling, Germany; Remote Sensing Technology Institute, DLR German Aerospace Center, Wessling, Germany; Remote Sensing Data Center, DLR German Aerospace Center, Wessling, Germany; Signal Processing in Earth Observation (SIPEO), Technische Universität München (TUM), Germany","2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Oct 2017","2016","","","1","5","In this paper, we propose an object-based fusion approach for the joint use of polarimetric synthetic aperture radar (PolSAR) and hyperspectral data. The proposed approach extracts information from both datasets based on an object-level, which is used here for land use classification. The achieved classification result infers that the proposed methodology improves the classification performance of both hyperspectral and PolSAR data and can properly gather complementary information of the two kinds of dataset. The fusion approach also considers that only limited training samples are available, which is often the case in remote sensing.","2158-6276","978-1-5090-0608-3","10.1109/WHISPERS.2016.8071752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071752","PolSAR;hyperspectral image;fusion;classification","Hyperspectral imaging;Support vector machines;Training;Buildings;Image segmentation","geophysical image processing;image classification;image fusion;radar imaging;radar polarimetry;remote sensing;remote sensing by radar;synthetic aperture radar","polarimetric synthetic aperture radar;limited training samples;remote sensing;object-based fusion approach;hyperspectral imaging;polarimetric SAR;object based fusion;complementary information;hyperspectral PolSAR data;classification performance;classification result infers;land use classification;object-level;hyperspectral data","","3","","8","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"A Maximum Likelihood Method for Remotely Sensed Image Fusion Using Generalized IHS Transform","S. Shaohong; L. Longbing; L. Dongye; S. Aiye","Change Jiang river research institute, Wuhan, China; Hainan hydrology and Water Resources Survey Bureau, Haikou, China; Anhui traffic survey and Design Institute Co., Ltd, Hefei, China; College of Computer and Information Engineering, Hohai Univeristy, Nanjing, China","2018 11th International Symposium on Computational Intelligence and Design (ISCID)","25 Apr 2019","2018","02","","29","32","Image fusion is an effective approach for improving spatial resolution of multispectral(MS) images by incorporating the detailed information of panchromatic (Pan) image, which need not change the original imaging system hardware. In this paper, we propose a fusion method for IKONOS/QuickBird images using generalized IHS (GIHS) technique based on maximum likelihood (ML) estimation. First, the regression coefficients are computed by the regression function between Pan and MS images. Then, the intensity component of the MS images is obtained by those coefficients. After that, a new Pan image is acquired using ML framework based on the observed model. Finally, the fused images are obtained by the GIHS method. Experiments on spatially degraded IKONOS/QuickBird MS images, whose original MS images are available for reference, show that the reconstructed results applying to the proposed approach have better spectral quality and spatial quality.","2473-3547","978-1-5386-8527-3","10.1109/ISCID.2018.10108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695523","image fusion;multispctral image;maximum likelihood;IHS transform","Spatial resolution;Maximum likelihood estimation;Image reconstruction;Remote sensing;Transforms","geophysical image processing;geophysical signal processing;image fusion;image resolution;remote sensing;wavelet transforms","original imaging system hardware;fusion method;IKONOS/QuickBird images;generalized IHS technique;maximum likelihood estimation;regression coefficients;Pan image;fused images;GIHS method;IKONOS/QuickBird MS images;original MS images;maximum likelihood method;remotely sensed image fusion;generalized IHS transform;spatial resolution;multispectral(MS) images;panchromatic image","","","","9","IEEE","25 Apr 2019","","","IEEE","IEEE Conferences"
"The Effect of SAR Speckle Removal in SAR-Optical Image Fusion","S. Gençay; C. Özcan","Bilgisayar Teknolojileri, Manisa Celal Bayar Üniversitesi, Manisa, Türkiye; Yazılım Mühendisliği, Karabük Üniversitesi, Karabük, Türkiye","2022 30th Signal Processing and Communications Applications Conference (SIU)","29 Aug 2022","2022","","","1","4","Due to the imaging mechanism of Synthetic Aperture Radar (SAR) and the noise in the images, visual identification of objects in the scene is not as easy as in optical images. SAR images have limited color information and cannot reflect the spectral information of objects. Optical images, on the other hand, have rich spectral information. SAR-Optical image fusion is an important area of study so that SAR data can be easily evaluated by anyone, but it is difficult to find a matching SAR and optical image of the same scene. In order to overcome this difficulty, Sentinel-1 and Sentinel-2 datasets have been published and image fusion studies have been carried out with various methods. However, it has been observed that the effect of SAR noise removal before merging on image fusion methods has not been investigated. In the studies conducted to investigate this effect, five different fusion algorithms used in the literature were tested with twenty different image groups using different noise reduction ratios. The success of the fusion results obtained was compared with five different metrics that are widely used in the literature. The images and metric results obtained as a result of the tests showed that the removal of speckle noise in the SAR data has a positive effect on the fusion results.","2165-0608","978-1-6654-5092-8","10.1109/SIU55565.2022.9864861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864861","optical image;SAR image;image fusion;remote sensing","Measurement;Visualization;Speckle;Optical imaging;Adaptive optics;Optical sensors;Optical reflection","image fusion;optical images;radar imaging;speckle;synthetic aperture radar","SAR speckle removal;SAR-Optical image fusion;imaging mechanism;SAR images;Optical images;rich spectral information;SAR data;matching SAR;image fusion studies;SAR noise removal;image fusion methods;different fusion algorithms;different image groups;fusion results","","","","0","IEEE","29 Aug 2022","","","IEEE","IEEE Conferences"
"The Infrared and Visible Light Image Fusion Based on the Non-subsample Shearlet Transform and Heat Source Concentration Ratio","J. Luo; W. Kong","Department of Information and Engineering, Engineering University of PAP, Xian, China; Department of Information and Engineering, Engineering University of PAP, Xian, China","2016 International Conference on Intelligent Networking and Collaborative Systems (INCoS)","27 Oct 2016","2016","","","544","547","Image fusion is the technology that use image processing algorithm to integrate different and complementary information form two or more images to create a new collection which is more accurate and convenient for the further use. As one significant component of the Image Processing, Image fusion is widely employed in many aspects, such as medical diagnose, military reconnaissance and remote sensing survey. To get a fused image which combines the target information and features of infrared and visible light image, a fusion method based on the Non-subsample Shearlet Transform (NSST) and heat source concentration ratio is presented in this paper. Compared with the traditional contourlet transform, NSST can overcome the limitation in directional decomposition and has excellent shift invariance. The input is decomposed to two parts by the transform, a new fusion rule which adopts the heat source concentration ratio and space frequency to retain as more important information as possible is presented in the low-frequency part and information entropy serves as the measurement in the high-frequent part. Simulation shows that the new fusion scheme can obviously improve the quality of fusion image and makes the heat source information prominent.","","978-1-5090-4124-4","10.1109/INCoS.2016.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7695237","image fusion;NSST;infrared and visible light images;heat source concentrataion","Image fusion;Infrared heating;Wavelet transforms;Information entropy;Biomedical imaging;Frequency measurement","entropy;image fusion;infrared imaging;transforms","nonsubsample Shearlet transform;heat source concentration ratio;visible light image fusion;Infrared light image fusion;image processing;NSST;space frequency;information entropy;fusion image quality","","1","","16","IEEE","27 Oct 2016","","","IEEE","IEEE Conferences"
"Infrared and visible image fusion based on innovation feature simultaneous decomposition","G. He; D. Dong; S. Xing; X. Zhao","Northwestern Polytechnical University, Xi'an, China; Northwestern Polytechnical University, Xi'an, China; Northwestern Polytechnical University, Xi'an, China; Qilu Institute of Technology","2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","8 Feb 2018","2017","","","1174","1177","In the field of image fusion, for the problem of large difference between infrared and visible image, these two are decomposed by joint sparse representation and get the common feature and innovation feature; Furthermore, a new fusion method based on innovation feature simultaneous decomposition is proposed to solve the problem of feature rich in infrared and visible light images. Based on the joint sparse representation model, the multi-source image data is combined into a new signal at the corresponding position, and the innovation feature of the two is decomposed into the same atom by Simultaneous Orthogonal Matching Pursuit. Experimental results and analysis show that the proposed method is superior to the popular sparse representation method, the simultaneous orthogonal matching pursuit method and the traditional joint sparse representation method, and the subjective visual evaluation and the objective index evaluation is both superior. The relevant research can be extended to the specific feature rich remote sensing image or medical image fusion field.","","978-1-5386-1542-3","10.1109/APSIPA.2017.8282206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8282206","infrared and visible image fusion;joint sparse representation;innovation feature and simultaneous orthogonal matching pursuit","Technological innovation;Dictionaries;Image fusion;Indexes;Discrete wavelet transforms;Matching pursuit algorithms;Data models","compressed sensing;image fusion;image representation;iterative methods","sparse representation;simultaneous orthogonal matching pursuit;multisource image data;joint sparse representation model;innovation feature simultaneous decomposition;visible image fusion;infrared image fusion","","","","8","IEEE","8 Feb 2018","","","IEEE","IEEE Conferences"
"Multi-Source Remote Sensing Data Classification via Fully Convolutional Networks and Post-Classification Processing","Y. Xu; B. Du; L. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, P. R. China; School of Computer, Wuhan University, Wuhan, P. R. China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, P. R. China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","3852","3855","This paper presents a new data fusion methodology named Fusion-FCN for the classification of multi-source remote sensing data using fully convolutional networks (FCNs). Three different types of data including LiDAR data, hyperspectral images and very high resolution images are utilized in the proposed framework. Considering the confusions between similar categories (e.g., road and highway), we further implement post-classification processing with the topological relationship among different objects based on the result yielded by the proposed Fusion-FCN. The proposed method achieved an overall accuracy of 80.78% and a kappa coefficient of 0.80, which ranked first in the 2018 IEEE GRSS Data Fusion Contest.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518295","Data fusion;deep learning;fully convolutional network;image segmentation","Roads;Data integration;Training;Laser radar;Hyperspectral sensors","geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;image fusion;optical radar;remote sensing by laser beam;remote sensing by radar;terrain mapping","hyperspectral images;high resolution images;post-classification processing;Fusion-FCN;2018 IEEE GRSS Data Fusion Contest;multisource remote sensing data classification;fully convolutional networks;data fusion methodology;LiDAR data","","18","","7","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Cascade Detector With Feature Fusion For Arbitrary-Oriented Objects In Remote Sensing Images","L. Hou; K. Lu; J. Xue; L. Hao","School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China; School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China; School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China; School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China","2020 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2020","2020","","","1","6","Detection of multi-class rotated objects is a challenging task in optical remote sensing images because of large-scale variations, arbitrary orientations and complex backgrounds, etc. Most of the state-of-the-art object detectors for natural images, that use horizontal bounding boxes, are not suitable for oriented objects in remote sensing images. In this paper, we propose an end-to-end cascade detector that can effectively detect rotated objects in complex remote sensing images. Specifically, a feature fusion block is designed to capture features with more details. Meanwhile, a supervised spatial attention mechanism is adopted to improve performance in detecting objects with complex backgrounds by weakening noise and enhancing object regions. Finally, to obtain more accurate object position, a cascade of multi-step detection subnet is implemented to refine anchors. Experiments using a publicly available remote sensing dataset DOTA show that our object detector achieves superior performance over other state-of-the-art approaches.","1945-788X","978-1-7281-1331-9","10.1109/ICME46284.2020.9102807","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9102807","Object detection;Deep learning;Remote sensing image;Convolutional neural network","Remote sensing;Feature extraction;Detectors;Semantics;Object detection;Task analysis;Marine vehicles","geophysical image processing;image fusion;object detection;remote sensing","complex backgrounds;natural images;horizontal bounding boxes;end-to-end cascade detector;complex remote sensing images;feature fusion block;multistep detection subnet;object detector;arbitrary-oriented objects;optical remote sensing images;object regions enhancement;multiclass rotated object detection;supervised spatial attention mechanism;publicly available remote sensing dataset DOTA","","11","","21","IEEE","9 Jun 2020","","","IEEE","IEEE Conferences"
"Research on fusion model of multi-temporal remote sensing bathymetry around island","Z. Jingyu; M. Yi; S. Weifu","First Institute of Oceanography, State Oceanic Administration, Qingdao, China; First Institute of Oceanography, State Oceanic Administration, Beijing, Beijing, CN; First Institute of Oceanography, State Oceanic Administration, Qingdao, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","3799","3801","Water depth is necessary for shipping security, port and marine engineering construction and planning utilization of coastal zone and island. Traditional bathymetry methods can provide accurate measurements of the water depth. But remote sensing has characteristics such as speediness, synchronization, large area, high resolution and so on, and it can solve many problems which are hard to overcome in traditional fathoming method. To fully utilize the existing resources of remote sensing images and excavate multi-temporal information effectively, the paper proposed the fusion model of multi-temporal remote sensing bathymetry inversion in Beidao Island based on fuzzy membership degree. By the help of decision fusion, both the mean relative error and the mean absolute error of the multi-temporal fusion result are lower than those of the single-temporal inversion results.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729985","bathymetry;decision fusion;multitemporal;Beidao Island","Remote sensing;Sea measurements;Clouds;Atmospheric modeling;Monitoring;Oceanography;Satellites","bathymetry;geophysical image processing;image fusion;oceanography;remote sensing","fusion model;multitemporal remote sensing bathymetry;island;shipping security;port;marine engineering construction;planning utilization;coastal zone;bathymetry method;water depth measurements;fathoming method;remote sensing image;Beidao Island;China;fuzzy membership degree;multitemporal fusion;single-temporal inversion","","","","10","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Spectral-Spatial Transformer for Hyperspectral Image Sharpening","L. Chen; G. Vivone; J. Qin; J. Chanussot; X. Yang","Sichuan University, Chengdu, China; CNR-IMAA, Tito Scalo, Italy; Sichuan University, Chengdu, China; INP Grenoble, Grenoble, France; Sichuan University, Chengdu, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1452","1455","Convolutional neural networks (CNNs) have achieved impressive performance for hyperspectral (HS) and multispectral (MS) image fusion in recent years. They extract features by local filters, which is limited to explore long-range dependency in input images. However, long-range dependence is an import cue for HS and MS image fusion, as it contributes to exploration of spatial self-similarity and spectral dependence. To take advantage of long-range dependence, we propose a spectral-spatial transformer (SST) for MS and HS image fusion. The experimental results demonstrate the high performance of the proposed approach compared to some state-of-the-art methods.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884194","Deep learning;image fusion;hyperspectral image;multispectral image;transformer;remote sensing","Geoscience and remote sensing;Transformers;Feature extraction;Sensors;Convolutional neural networks;Image fusion;Hyperspectral imaging","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image enhancement;image fusion;image resolution","input images;long-range dependence;spatial self-similarity;spectral dependence;spectral-spatial transformer;hyperspectral image sharpening;convolutional neural networks;HS;MS;long-range dependency","","","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Dual-Stream High Resolution Network for Multi-Source Remote Sensing Image Segmentation","B. Ren; S. Ma; B. Hou; D. Hong","School of Artificial Intelligence, Xidian University, Xian, China; School of Artificial Intelligence, Xidian University, Xian, China; School of Artificial Intelligence, Xidian University, Xian, China; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Wessling, Germany","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","3440","3443","Recently, the image segmentation has been a significant research direction in the field of optical remote sensing data processing. However, due to the limitation of the optical imaging mechanism, traditional image segmentation methods are not efficient for processing the optical remote sensing images, especially influencing by the complex weather conditions. In order to ensure the classification performance, synthetic aperture radar (SAR) data are employed as complementary to the data procedure for enhancing the capability of land cover interpretation. Then a dual-stream high-resolution network (HRNet) is proposed to combine two types of heterogeneous data (SAR and optical image), and a multi-modal squeeze-and-excitation (SE) module is exploited to make feature maps fused. Experiments show that the proposed method has excellent performance on the remote sensing data acquired by GF2 and GF3 satellites.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553947","National Natural Science Foundation of China(grant numbers:62001355); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553947","Images Segmentation;Heterogeneous Data Fusion;Multi-modalities","Image segmentation;Satellites;Optical fiber networks;Optical imaging;Adaptive optics;Optical sensors;Task analysis","geophysical image processing;geophysical techniques;image fusion;image resolution;image segmentation;optical images;radar imaging;remote sensing;synthetic aperture radar","dual-stream high resolution network;multisource remote sensing image segmentation;significant research direction;optical remote sensing data processing;optical imaging mechanism;traditional image segmentation methods;optical remote sensing images;complex weather conditions;synthetic aperture radar data;data procedure;high-resolution network;heterogeneous data;optical image","","3","","5","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Improving SAR and Optical Image Fusion for Lulc Classification with Domain Knowledge","K. R. Prabhakar; V. H. Nukala; J. Gubbi; A. Pal; B. P","TCS Research, INDIA; TCS Research, INDIA; TCS Research, INDIA; TCS Research, INDIA; TCS Research, INDIA","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","711","714","Fusing SAR and multi-spectral images to generate a precise land cover map in a weakly supervised setting is a challenging yet essential problem. The inaccurate, noisy, and inexact ground truth labels pose difficulty training any machine learning models. In this paper, we make a fundamental and pivotal contribution towards improving the ground truth label quality using domain knowledge. We present a simple yet effective mechanism to refine the low-resolution noisy ground truth labels. The proposed approach is trained and tested on a publicly available DFC2020 dataset. Through experiments, we show the effectiveness of our method by training a deep learning model on the refined labels that outperform even the models trained with clean ground truth.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884283","CNN;SAR;multispectral;image fusion","Training;Deep learning;Training data;Geoscience and remote sensing;Optical imaging;Noise measurement;Reliability","geophysical image processing;image classification;image fusion;land cover;learning (artificial intelligence);optical images;radar imaging;synthetic aperture radar;terrain mapping","SAR;multispectral images;precise land cover map;weakly supervised setting;challenging yet essential problem;inaccurate ground truth labels;noisy, ground truth labels;inexact ground truth labels;machine learning models;fundamental contribution;pivotal contribution;domain knowledge;simple yet effective mechanism;low-resolution noisy ground truth labels;publicly available DFC2020 dataset;deep learning model;refined labels;clean ground truth;optical image fusion;lulc classification","","","","8","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Sparse Representation of Injected Details for MRA-Based Pansharpening","M. Maneshi; H. Ghassemian; M. Imani","Image Processing and Information Analysis Lab. Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Lab. Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Lab. Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2020 IEEE India Geoscience and Remote Sensing Symposium (InGARSS)","23 Feb 2021","2020","","","86","89","Pansharpening is a notable remote sensing topic in which high spatial resolution panchromatic image and low spatial resolution multi-spectral image are being fused in order to receive the high spatial resolution multi-spectral image. This paper presents a hybrid pansharpening method based on MRA framework and the sparse representation of injected details. To add spatial details of the panchromatic image into the multispectral image more effectively, the injection gains are computed through an iterative full-scale model in which the gains are updated at each iteration relying on its previous iteration's fusion product. The proposed method is compared with five pansharpening approaches to investigate the effectiveness. Experiments have been implemented on two data sets from the Pleiades and GeoEye-1 satellites both at reduced and full scale. In terms of visual and quantity assessment, the high-resolution MS image produced by the proposed method is more acceptable than those images fused by other rival approaches.","","978-1-7281-3114-6","10.1109/InGARSS48198.2020.9358956","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9358956","Sparse representation;Image fusion;Pansharpening;Remote sensing","Visualization;Satellites;Computational modeling;Pansharpening;Sensors;Spatial resolution;Remote sensing","geophysical image processing;image fusion;image resolution;remote sensing","hybrid pansharpening method;MRA framework;sparse representation;injected details;spatial details;injection gains;iterative full-scale model;high-resolution MS image;MRA-based pansharpening;high spatial resolution panchromatic image;low spatial resolution multispectral image;high spatial resolution multispectral image;remote sensing topic;Pleiades satellite;GeoEye-1 satellite","","1","","16","IEEE","23 Feb 2021","","","IEEE","IEEE Conferences"
"Multi-dimension and multi-granularity segmentation of remote sensing image based on improved Otsu algorithm","Dongmei Huang; Jingqi Sun; Shuang Liu; Shoujue Xu; Suling Liang; Cong Li; Zhenhua Wang","College of Information Technology, Shanghai Ocean University, Shanghai, China; College of Information Technology, Shanghai Ocean University, Shanghai, China; College of Information Technology, Shanghai Ocean University, Shanghai, China; College of Information Technology, Shanghai Ocean University, Shanghai, China; College of Information Technology, Shanghai Ocean University, Shanghai, China; College of Information Technology, Shanghai Ocean University, Shanghai, China; College of Information Technology, Shanghai Ocean University, Shanghai, China","2017 IEEE 14th International Conference on Networking, Sensing and Control (ICNSC)","3 Aug 2017","2017","","","679","684","An increasing number of unknown islands, an important resource for human development, is identified based on segmentation of remote sensing image. Different from traditional digital image, remote sensing image has significant characteristics, such as multi-band, multi-source, and multi-granularity. Thus, the segmentation theory based on traditional digital image is not suitable for remote sensing image. Here, the segmentation algorithm (Otsu), which is a common method for traditional digital image, was improved in two aspects: (1) Based on PCA and band fusion, the Otsu algorithm with one-dimensional image was extended to multi-dimensional ones; (2) By optimizing the threshold value, the Otsu algorithm for single feature extraction was extended to multi-granularity extraction. Taking the island segmentation from remote sensing image as an example, the improved Otsu algorithm was compared with the traditional Otsu: 1) Through using PCA algorithm, multi-band remote sensing image was reduced to effective 3-4 new bands; 2) Through different threshold settings, the objects in the remote sensing image are divided into different classes; 3) The improved Otsu algorithm reduces the computational complexity, taking the threshold value of 2 as an example, the time efficiency is improved by 42.15%.","","978-1-5090-4429-0","10.1109/ICNSC.2017.8000172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8000172","multi-dimension;multi-granularity;remote sensing image;Otsu algorithm","Image segmentation;Remote sensing;Algorithm design and analysis;Digital images;Principal component analysis;Feature extraction;Computational complexity","geophysical image processing;image fusion;image segmentation;principal component analysis;remote sensing","multidimension segmentation;multigranularity segmentation;remote sensing image;improved Otsu algorithm;human development;digital image;band fusion;one-dimensional image;threshold value;multigranularity extraction;PCA algorithm;multiband remote sensing image","","","","15","IEEE","3 Aug 2017","","","IEEE","IEEE Conferences"
"Bidimentional emphirical mode decomposition based image fusion","A. Khan; P. Agrawal; H. Sainthiya","Digital and Communication Department, Jaipur Institute of Technology, Jaipur; Digital and Communication Department, Jaipur Institute of Technology, Jaipur; Electronics and Comm. Department, BIET, Jhansi","2017 International Conference on Recent Innovations in Signal processing and Embedded Systems (RISE)","11 Jun 2018","2017","","","212","217","The image fusion plays a crucial role in many fields such as remote sensing, medical and robotics applications. This paper is focused on image fusion of images of different focus depth. The aim is to study these concepts and provide simulations and evaluations on various implementations. When performing image fusion the images are decomposed by bi-dimensional Empirical mode decomposition (BEMD) to obtain high frequency coefficients which is used to determine which parts of the input images that makes it into the fused image. The same technique is tested on images of different modality. In this thesis, a novel bi-dimensional Empirical mode decomposition (BEMD) based image fusion scheme is proposed. The BEMD decomposes the source images into intrinsic mode functions (IMFs) and residual components. IMF components of the first signal in the decomposition of the source images are used to generate the fused images using appropriate fusion rule. Performance evaluation of fused images is done by computing fusion quality metrics and the fusion results are compared with other existing fusion schemes. It is seen that the performance of the proposed scheme is better as compared with the existing fusion schemes.","","978-1-5090-4760-4","10.1109/RISE.2017.8378156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8378156","Image fusion;BEMD;Medical images;Multi-spectral images;Multifocus image","Image fusion;Image edge detection;Empirical mode decomposition;Biomedical imaging;Loss measurement","Hilbert transforms;image fusion","BEMD;input images;fused image;source images;residual components;fusion quality metrics;focus depth;bidimensional Empirical mode decomposition based image fusion scheme;intrinsic mode functions;fusion rule;high frequency coefficients","","1","","","IEEE","11 Jun 2018","","","IEEE","IEEE Conferences"
"A Deep Learning Framework for Fusion of Sar and Optical Satellite Imagery","N. Gupta; H. S. Srivastava; T. Sivasankar; P. Patel","NIIT University, Neemrana, India; Indian Institute of Remote Sensing, ISRO, Dehradun, India; NIIT University, Neemrana, India; Space Applications Centre, ISRO, Ahmedabad, India","2021 IEEE International India Geoscience and Remote Sensing Symposium (InGARSS)","13 Jun 2022","2021","","","488","491","In remote sensing, image fusion is the process of converting information from various source images to a single image such that the features of the source are preserved and relevant information is being highlighted. Through this research work, we propose an unsupervised deep learning Generative Adversarial Network (GAN) for the fusion process of SAR and optical Images. For SAR image, we chose VV, VH, VV-VH bands and for optical image we did Principal Component Analysis (PCA) on its image bands to extract the top three principal components and compose an image out of it. Images were then converted into HSV space. The GAN is primarily trained to capture the maximum gradient features from both the images and secondarily to capture other noticeable features. Experimental results on both training and test samples indicate that the proposed method is able to preserve gradient features and other details of the images with respect to input images.","","978-1-6654-4249-7","10.1109/InGARSS51564.2021.9792062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792062","SAR;optical;image fusion;deep learning;GAN","Deep learning;Training;Neural networks;Optical computing;Optical imaging;Generative adversarial networks;Optical sensors","deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;principal component analysis;remote sensing;synthetic aperture radar;unsupervised learning","remote sensing;image fusion;GAN;optical Images;SAR image;VV-VH bands;optical image;principal component analysis;image bands;maximum gradient features;optical satellite imagery;unsupervised deep learning generative adversarial network;PCA","","","","18","IEEE","13 Jun 2022","","","IEEE","IEEE Conferences"
"Hyperspectral Remote Sensing Image Processing and Information Extraction Technology Research in Geological Recognition Application","X. Bi; L. Jiang; Q. Dong; D. Wang","Aerospace Information Research Institute Chinese, Academy of Sciences, Chengdu, China; China Railway Eryuan, Engineering Group. Co. Ltd, Chengdu, China; Aerospace Information Research Institute Chinese, Academy of Sciences, Chengdu, China; China Railway Eryuan, Engineering Group. Co. Ltd, Chengdu, China","2022 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)","23 May 2022","2022","","","936","938","Application of hyperspectral remote sensing technology in geological identification has highlighted its advantages, but the identification accuracy of metal mineral cations is limited. In this paper, the dominant spectrum of domestic HJ-1A data and Hyperion data are used for collaborative processing, realize multi-source hyperspectral image fusion and information extraction and recognition, mapping of mineral alteration information in the Qinghai-Tibet Plateau. The adopted method improves the identification accuracy of metal mineral cations in the visible light and near-infrared range. After comparative verification analysis and statistical evaluation of results, it is confirmed that this method has important application value for the identification of metal mineral cations in complex geological environment.","","978-1-6654-0902-5","10.1109/IPEC54454.2022.9777528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9777528","hyperspectral remote sensing;image fusion;information extraction;geological identification","Image recognition;Conferences;Metals;Information retrieval;Ions;Minerals;Data mining","geology;geophysical image processing;geophysical techniques;geophysics computing;image fusion;image processing;minerals;remote sensing","identification accuracy;metal mineral cations;dominant spectrum;domestic HJ-1A;collaborative processing;mineral alteration information;Qinghai-Tibet Plateau;complex geological environment;hyperspectral remote sensing image processing;information extraction technology research;geological recognition application;hyperspectral remote sensing technology;geological identification;wavelength 1.0 A","","","","5","IEEE","23 May 2022","","","IEEE","IEEE Conferences"
"A Unified Multimodal Deep Learning Framework For Remote Sensing Imagery Classification","D. Hong; L. Gao; X. Wu; J. Yao; N. Yokoya; B. Zhang","Germany Aerospace Center (DLR), Remote Sensing Technology Institute (IMF), Wessling, Germany; Key Laboratory of Digital Earth Science, Aerospace Information Research Institute (AIR), CAS, Beijing, China; School of Information and Electronics, Beijing Institute of Technology (BIT), Beijing, China; Key Laboratory of Digital Earth Science, Aerospace Information Research Institute (AIR), CAS, Beijing, China; RIKEN Center for Advanced Intelligence Project (AIP), RIKEN, Tokyo, Japan; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China","2021 11th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Jul 2021","2021","","","1","5","In this paper, we present a unified deep learning framework for multimodal remote sensing image classification, U-MDL for short. U-MDL attempts to develop a general network architecture that consists of two subnetworks for feature extraction and feature fusion, respectively, with a focus on ""which"", ""when"", and ""how"" to fuse. For this purpose, we detail several common but effective fusion modules in the networks, e.g., early fusion, middle fusion, late fusion, and encoder-decoder fusion. These modules can be generalized well into our U-MDL framework. More significantly, we also emphasize to investigate a special case of multi-modality learning (MML), that is, cross-modality learning (CML) which widely exists in real applications. Moreover, extensive experiments are conducted to demonstrate the superiority and effectiveness of the proposed U-MDL framework in the remote sensing image classification task. The codes and datasets are available at: https://github.com/danfenghong/IEEE_TGRS_MDL-RS for the sake of reproducibility.","2158-6276","978-1-6654-3601-4","10.1109/WHISPERS52202.2021.9484057","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9484057","Classification;cross-modality;deep learning;feature extraction;feature fusion;hyperspectral;multi-modality","Deep learning;Fuses;Conferences;Network architecture;Feature extraction;Reproducibility of results;Task analysis","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing","feature extraction;feature fusion;common but effective fusion modules;early fusion;middle fusion;late fusion;encoder-decoder fusion;U-MDL framework;multimodality learning;cross-modality learning;remote sensing image classification task;unified multimodal deep learning framework;remote sensing imagery classification;unified deep learning framework;multimodal remote sensing image classification;general network architecture","","","","21","IEEE","19 Jul 2021","","","IEEE","IEEE Conferences"
"HFGAN: A Heterogeneous Fusion Generative Adversarial Network for Sar-to-Optical Image Translation","N. Yu; A. Ma; Y. Zhong; X. Gong","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Beijing Institute of Remote Sensing Information, Beijing, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2864","2867","Due to the influence of the imaging mechanism of SAR images, it is difficult to interpret ground information through SAR images without expert knowledge. On the contrary, optical images have rich spatial and color information, so it is necessary to conduct research on the translation of SAR to optical remote sensing images. In this end, we propose a heterogeneous fusion generative adversarial network (HFGAN) for SAR-to-optical image translation. There are two main improvements: (1) Complementary generation of global structure and texture information. A heterogeneous fusion generator and a multi-scale discriminator are proposed to ensure that the global and detailed features of the generated image are more accurate and rich. (2) Color fidelity. Chromatic aberration loss are introduced to reduce the color difference between the generated image and the real optical image. Through qualitative and quantitative experiments, it is proved that the proposed method not only obtains better visual effects, but also has certain progress in the evaluation metrics, which proves that the proposed method is superior to the previous advanced methods in SAR-to-optical image translation.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883519","Remote sensing;Generative Adversarial Network;Image tranlation","Optical losses;Image color analysis;Optical imaging;Generative adversarial networks;Visual effects;Generators;Radar polarimetry","aberrations;geophysical image processing;image classification;image colour analysis;image fusion;image resolution;image texture;optical images;radar imaging;remote sensing;synthetic aperture radar","optical remote sensing images;heterogeneous fusion generative adversarial network;sar-to-optical image translation;heterogeneous fusion generator;imaging mechanism;SAR images;rich spatial;color information","","","","6","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Weak Moving Object Detection In Optical Remote Sensing Video With Motion-Drive Fusion Network","Y. Li; L. Jiao; X. Tang; X. Zhang; W. Zhang; L. Gao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, Shaanxi Province, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","5476","5479","Object detection in optical remote sensing video (ORSV) is a new trend which makes it possible for obtaining richer information in more complicated and diverse situations. However, the small objects are blurred in the videos captured from optical sensor assembled in satellite, limited by the devices and natural weather. The concept of weak object is defined in this situation that the objects are extremely small and hardly detected with only one static image. Therefore, we propose a simple but efficient method for weak moving object detection in ORSV by combining the temporal information from neighbor frames and spatial features from image pixels. First, we compute the difference map between two adjacent frames, and stack it with original RGB channel so that a (1+3)-channel input data is made. Then, a motion-drive D-RGB (difference map with RGB image) fusion network is developed to obtain the feature map of this (1+3)-channel data. To adapt unusual scale in ORSV images, based on statistical prior objects size, we change the size of anchor box in original Faster R-CNN. The proposed method is demonstrated to improve the mean average precision on detecting weak moving remote sensing objects.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900412","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900412","weak moving object detection;optical remote sensing video;differencing map","Object detection;Feature extraction;Remote sensing;Shape;Data models;Optical imaging;Optical sensors","image colour analysis;image motion analysis;object detection;remote sensing;sensor fusion;statistical analysis;video signal processing","weak moving object detection;RGB image;ORSV images;statistical prior objects size;weak moving remote sensing objects;optical remote sensing video;motion-drive fusion network;optical sensor;faster R-CNN;motion-drive D-RGB;static image;image pixels;image detection","","4","","10","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Deep Learning approach for Multiple Source Classification in Remote Sensing Imagery","H. Alhawiti; Y. Bazi; M. M. Al Rahhal; H. Alhichri; M. A. Zuair","Computer Engineering Department, King Saud University, Riyadh, Saudi Arabia; Computer Engineering Department, King Saud University, Riyadh, Saudi Arabia; Information System Department, King Saud University, Riyadh, Saudi Arabia; Computer Engineering Department, King Saud University, Riyadh, Saudi Arabia; Computer Engineering Department, King Saud University, Riyadh, Saudi Arabia","2020 3rd International Conference on Computer Applications & Information Security (ICCAIS)","20 May 2020","2020","","","1","4","In this paper, we present a deep learning approach for learning for multiple remote sensing sources. The method starts by eliminating the distribution shift between the different sources and the target dataset using an adversarial learning approach based on min-max entropy optimization. After convergence, the results are aggregated using an average fusion layer. As pre-trained CNN we use in the work the recent state-of-the-art EfficientNet models. In the experiments, we assess the method on four remote sensing datasets acquired over different locations of the earth's surface and are labeled by different experts. The obtained results confirm the promising capability of the proposed method.","","978-1-7281-4213-5","10.1109/ICCAIS48893.2020.9096746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9096746","remote sensing;domain adaptation;deep learning;multiple sources","Remote sensing;Training;Machine learning;Entropy;Adaptation models;Image analysis;Neural networks","convolutional neural nets;entropy;geophysical image processing;image classification;image fusion;learning (artificial intelligence);minimax techniques;object detection;remote sensing","deep learning approach;multiple source classification;remote sensing imagery;multiple remote sensing sources;target dataset;adversarial learning approach;min-max entropy optimization;remote sensing datasets;average fusion layer;pre-trained CNN;EfficientNet models;earth surface location","","","","16","IEEE","20 May 2020","","","IEEE","IEEE Conferences"
"Transfering Super Resolution Convolutional Neural Network For Remote Sensing Data Sharpening","M. Iftene; M. E. A. Arabi; M. S. Karoui","Agence Spatiale Algérienne, Centre des Techniques Spatiales, Algeria; Agence Spatiale Algérienne, Centre des Techniques Spatiales, Algeria; Agence Spatiale Algérienne, Centre des Techniques Spatiales, Algeria","2018 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","27 Jun 2019","2018","","","1","4","Pansharpening process aims at fusing low-spatial/high-spectral resolutions multispectral/hyperspectral (MS/HS) remote sensing data with high-spatial resolution and without spectral diversity panchromatic (PAN) ones. This paper explores different data preparation possibilities, learning strategies and architectures, used in the convolutional neural network (CNN) approaches, for improving the performance of the pansharpening process of remote sensing MS/HS data. Also, in this paper, the super resolution CNN (SRCNN) architecture is adapted by adding a normalization step in the training phase of the CNN-based pansharpening process. Then, training datasets are prepared for fitting the generalization need. Experiments based on multi-source datasets are performed to evaluate the performance of the proposed SRCNN-based pansharpening architecture. The preliminary results are promising since they show that the proposed approach is competitive with some literature methods.","2158-6276","978-1-7281-1581-8","10.1109/WHISPERS.2018.8747223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747223","Multispectral/Hyperspectral imaging;pansharpening;fusion;deep learning;convolutional neural networks;super-resolution","Remote sensing;Conferences;Indexes;Biological neural networks;Machine learning","geophysical image processing;hyperspectral imaging;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","high-spatial resolution;pansharpening process;remote sensing MS/HS data;super resolution CNN architecture;CNN-based;super resolution convolutional neural network;remote sensing data sharpening;data preparation possibilities;spectral diversity panchromatic;SRCNN-based pansharpening architecture","","1","","12","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"Survey on multifocus image fusion techniques","G. Kaur; P. Kaur","Department of Computer engineering and technology, Guru Nanak Dev University, Amritsar, India; Department of Computer engineering and technology, Guru Nanak Dev University, Amritsar, India","2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)","24 Nov 2016","2016","","","1420","1424","Multi-focus image fusion is considered to be a vast research topic. Image fusion is the process in which source images are combined to get a single focused image. This focused image obtained contains relatively more information with all objects in focus and better description of scene. It is applied in various applications like medical imaging, remote sensing etc. Various multi-focus image fusion techniques are discussed in this paper, using focus measures such as energy of gradient of image, spatial frequency etc. The performance of these techniques depends on the methods used to determine the focused regions to get a fused image.","","978-1-4673-9939-5","10.1109/ICEEOT.2016.7754918","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7754918","image fusion;multi-focus;focus measure;fused image;performance evaluation","Image fusion;Feature extraction;Sparse matrices;Performance evaluation;Transforms;Image edge detection","image fusion","multifocus image fusion techniques;source images;single focused image;scene description;focus measures","","14","","25","IEEE","24 Nov 2016","","","IEEE","IEEE Conferences"
"Fusion of Hyperspectral and Panchromatic Images Based on Matting Model","W. Dong; X. Song; J. Qu; H. Gan","State Key Lab. of Integrated Service Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Service Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Service Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Service Networks, Xidian University, Xi'an, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","7204","7207","In this paper, a novel hyperspectral (HS) image fusion method using matting model is presented. Matting model refers to each band of an HS image that can be decomposed into three components, i.e., alpha channel, spectral foreground, and spectral background. First, panchromatic (PAN) image is sharpened to enhance details, and the spatial information of each band of HS image is obtained by weighted least squares filtering. Different from traditional matting model based methods that PAN image is served as the alpha channel, we do the PCA transformation to PAN image and spatial information of each band to obtain the first principal component channel which is selected for the alpha channel. This processing reduces spatial distortion. Finally, HS foreground and HS background are estimated by the alpha channel, and the fused HS image is reconstructed nearly perfectly. Experiments reveal that the proposed method is superior to the state-of-the-art methods.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518489","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518489","Hyperspectral image;panchromatic image;image fusion;matting model","Principal component analysis;Bayes methods;Hyperspectral imaging;Image fusion;Hafnium;Distortion","geophysical image processing;image colour analysis;image fusion;image reconstruction;principal component analysis;remote sensing","PAN image;spatial information;alpha channel;panchromatic image;novel hyperspectral image fusion method;spectral foreground;spectral background;traditional matting model;HS image fusion;PCA transformation;first principal component channel;weighted least squares filtering","","","","8","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"2D-DFrFT Based Deep Network for Ship Classification in Remote Sensing Imagery","Q. Shi; W. Li; R. Tao","College of Information Science & Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science & Technology, Beijing University of Chemical Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology","2018 10th IAPR Workshop on Pattern Recognition in Remote Sensing (PRRS)","11 Oct 2018","2018","","","1","5","Ship classification in optical remote sensing images is a fundamental but challenging problem with wide range of applications. Deep convolutional neural network (CNN) has shown excellent performance in object classification; however, limited available training samples prevent CNN for ship classification. In this paper, a novel ship-classification framework consisting of two-branch CNN and two dimensional discrete fractional Fourier transform (2D-DFrFT) is proposed. Firstly, the amplitude and phase information of ship image in 2D-DFrFT is extracted. Due to the fact that different orders of 2D-DFrFT have different contribution on the process of feature extraction of ship image. Thus the amplitude (M) and phase (P) value obtained in different orders are regarded as the input of two-branch CNN that can learn the high-level features automatically. After multiple features learning, decision-level fusion is adopted for final classification. The remote sensing image data, named as BCCT200-resize, is utilized for validation. Compared to the existing state-of-art algorithms, the proposed method has superior performance.","2377-0198","978-1-5386-8479-5","10.1109/PRRS.2018.8486413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8486413","Ship classification;Optical imagery;Convolutional neural Network;2D-DFrFT","Marine vehicles;Feature extraction;Remote sensing;Two dimensional displays;Optical imaging;Optical sensors;Training","convolution;discrete Fourier transforms;feature extraction;feedforward neural nets;geophysical image processing;image classification;image fusion;learning (artificial intelligence);principal component analysis;remote sensing;ships","remote sensing imagery;ship classification;optical remote sensing images;object classification;two-branch CNN;2D-DFrFT;ship image;deep convolutional neural network;ship-classification framework;deep network;feature extraction;phase information;amplitude information;multiple features learning;decision-level fusion;two dimensional discrete fractional Fourier transform;principal components analysis","","5","","16","IEEE","11 Oct 2018","","","IEEE","IEEE Conferences"
"An adaptive multispectral image fusion using particle swarm optimization","A. Azarang; H. Ghassemian","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2017 Iranian Conference on Electrical Engineering (ICEE)","20 Jul 2017","2017","","","1708","1712","In this paper, a novel image fusion method for remote sensing applications is proposed. In order to estimate the primitive detail map, the band coefficients of multispectral images are computed using least squares method. To refine the detail map and better image fusion, an adaptive method is proposed to compute the weights of linear combinations of panchromatic (Pan) and multispectral (MS) gradients. The injected weights are calculated using particle swarm optimization (PSO). Two data sets obtained by WorldView-3 and QuickBird satellites are employed for testing the assessment and comparing with state-of-art methods.","","978-1-5090-5963-8","10.1109/IranianCEE.2017.7985325","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985325","Pansharpening;adaptive computation;particle swarm optimization;remote sensing","Image resolution;Optimization","geophysical image processing;image fusion;particle swarm optimisation;remote sensing","adaptive multispectral image fusion method;particle swarm optimization;remote sensing applications;least squares method;PSO;multispectral gradients;panchromatic gradient","","6","","21","IEEE","20 Jul 2017","","","IEEE","IEEE Conferences"
"An efficient approach for spattotemporal image fusion with application to HSHT land cover change simulation","M. Ding; H. Wang; L. Sui","Chang'an Universtiy, Xi'an, China; Xi'an University of Finance and Economics, Xi'an, China; Chang'an Universtiy, Xi'an, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","2578","2581","Monitoring fast land-cover changes at fine special resolution is an important requirement for global environmental change research. In this paper, we present a novel spatiotemporal fusion framework to efficiently simulate surface reflectance with both high-spatial and high-temporal (HSHT) resolution. The main contribution may be divided into two parts. First, a semi-physical unmixing algorithm is developed to super-resolve the heterogeneous landscapes with phenology change. Second, a super-resolution method is proposed for the land-cover-type changed landscape based on nonlocal-crossing-similarity property. The method is demonstrated on two types of data: images primarily with phenology changes and images primarily with land-cover-type changes. By comparing with other well-known spatiotemporal fusion algorithms, we evaluate the precision of the proposed approach.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729666","Spatiotemporal image fusion;super-resolution;land-cover change;nonlocal-self-similarity","Remote sensing;Satellites;Earth;MODIS;Spatial resolution;Reflectivity","land cover;remote sensing","spatiotemporal image fusion;HSHT land cover change simulation;fine special resolution;global environmental change research;novel spatiotemporal fusion framework;high-spatial and high-temporal resolution;semiphysical unmixing algorithm;land-cover-type changed landscape;nonlocal-crossing-similarity property","","","","8","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"MoatNet: Registration for Multi-Temporal Optical Remote Sensing Images Using Deep Convolutional Features","C. Li; Y. You; J. Cao; W. Zhou","School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2154","2157","Image registration is an important technique that has been widely used in many areas. It is an indispensable premise for remote sensing image tasks like change detection and image fusion. In this paper, we propose a deep learning framework to generate descriptors for key points and then combine the descriptors constructed with FAST key points for accurate image registration. During the process of training, we adopt a novel loss function named Moat Loss (ML) to train our model, which is accordingly called MoatNet. Experiments show that our method is more robust than traditional algorithms like SIFT and is more accurate than the end-to-end deep learning methods in much more complex cases.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553197","Beijing Natural Science Foundation, China(grant numbers:4214058); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553197","Image registration;CNN;MoatNet;FAST","Optical losses;Deep learning;Training;Image registration;Optical imaging;Adaptive optics;Optical sensors","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;image registration;remote sensing","deep convolutional features;remote sensing image tasks;change detection;image fusion;deep learning framework;FAST key points;accurate image registration;MoatNet;end-to-end deep learning methods;multitemporal optical remote sensing image;loss function;moat loss","","","","8","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"An object approach to the assessment of the spatial quality of pan-sharpened remote sensing images","D. Rodríguez-Esparragón; J. Marcello-Ruiz; F. Eugenio-González; Á. García-Pedrero; C. Gonzalo-Martín","Instituto de Oceanografía y Cambio Global (IOCAG), University of Las Palmas de Gran Canaria, Las Palmas de Gran Canaria, Spain; Instituto de Oceanografía y Cambio Global (IOCAG), University of Las Palmas de Gran Canaria, Las Palmas de Gran Canaria, Spain; Universidad de Las Palmas de Gran Canaria, Las Palmas de Gran Canaria, Islas Canarias, ES; Centro de Tecnología Biomédica (CTB), Universidad Politécnica de Madrid (UPM), Madrid, Spain; Centro de Tecnología Biomédica (CTB), Universidad Politécnica de Madrid (UPM), Madrid, Spain","2015 4th International Work Conference on Bioinspired Intelligence (IWOBI)","16 Jul 2015","2015","","","49","54","A correct assessment of pan-sharpened remote sensing imagery (fusion of panchromatic and multispectral images) enables the improvement and ranking of pan sharpening algorithms. For this purpose, numerous quality indicators have been published in the literature; however, there is no consensus on their performance. In general, the pixel is used as a reference for the computation of these indicators and the result is a unique overall value that hardly represents the perceived quality by a human.","","978-1-4673-7846-8","10.1109/IWOBI.2015.7160143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7160143","","Image segmentation;Indexes;Remote sensing;Frequency measurement;Principal component analysis;Transforms;Algorithm design and analysis","geophysical image processing;image fusion;remote sensing","frequency comparison index;image fusion algorithm;pansharpening process;spatial quality map;image objects;suprapixel approach;pan sharpening algorithm;multispectral image fusion;panchromatic image fusion;pan-sharpened remote sensing imagery;spatial quality assessment","","","","10","IEEE","16 Jul 2015","","","IEEE","IEEE Conferences"
"A Training-Free, One-Shot Detection Framework for Geospatial Objects in Remote Sensing Images","T. Zhang; X. Sun; Y. Zhang; M. Yan; Y. Wang; Z. Wang; K. Fu","Chinese Academy of Sciences, Institute of Electronic, Beijing, China; Chinese Academy of Sciences, Institute of Electronic, Beijing, China; Chinese Academy of Sciences, Institute of Electronic, Beijing, China; Chinese Academy of Sciences, Institute of Electronic, Beijing, China; Chinese Academy of Sciences, Institute of Electronic, Beijing, China; Chinese Academy of Sciences, Institute of Electronic, Beijing, China; Chinese Academy of Sciences, Institute of Electronic, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","1414","1417","Deep learning based object detection has achieved great success. However, these supervised learning methods are data-hungry and time-consuming. This restriction makes them unsuitable for limited data and urgent tasks, especially in the applications of remote sensing. Inspired by the ability of humans to quickly learn new visual concepts from very few examples, we propose a training-free, one-shot geospatial object detection framework for remote sensing images. It consists of (1) a feature extractor with remote sensing domain knowledge, (2) a multi-level feature fusion method, (3) a novel similarity metric method, and (4) a 2-stage object detection pipeline. Experiments on sewage treatment plant and airport detections show that proposed method has achieved a certain effect. Our method can serve as a baseline for training-free, one-shot geospatial object detection.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898679","Training-free;one-shot;object detection","Feature extraction;Object detection;Remote sensing;Geospatial analysis;Task analysis;Pipelines;Convolution","feature extraction;geophysical image processing;image classification;image fusion;object detection;remote sensing;supervised learning","geospatial objects;remote sensing images;deep learning based object detection;multilevel feature fusion;similarity metric method;training-free framework;one-shot geospatial object detection;supervised learning;feature extractor;image classification","","3","","7","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Hierarchical multinomial latent model with G0 distribution for remote sensing image semantic segmentation","Y. Duan; X. Tao; C. Han; J. Lu","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","8 Mar 2018","2017","","","254","258","Considering the scattering statistics and multi-scale characteristics of the remote sensing images, this paper presents a hierarchical multinomial latent model with G0 distribution (HML-G0) for remote sensing image semantic segmentation. In the proposed approach, hierarchical multinomial latent model is proposed to capture the multi-scale information of the remote sensing images. Moreover, the flexibility of G0 distribution is plugged into the hierarchical multinomial latent model for the segmentation of various types of land covers. Then, the developed Bayesian inference on the quadtree is incorporated in our approach, and the semantic segmentation map is achieved by bottom-up and top-down probability computation. Experimental results demonstrate that our proposed hierarchical scheme produces the semantic segmentation maps, and the exhibiting performance improvements in terms of labeling consistency and the detail preservation.","","978-1-5090-5990-4","10.1109/GlobalSIP.2017.8308643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8308643","Remote sensing images;semantic segmentation;hierarchical multinomial latent model;G0;distribution;Bayesian inference","Image segmentation;Remote sensing;Semantics;Synthetic aperture radar;Context modeling;Bayes methods;Microsoft Windows","Bayes methods;geophysical image processing;image classification;image fusion;image representation;image segmentation;inference mechanisms;probability;quadtrees;remote sensing","top-down probability computation;semantic segmentation maps;labeling consistency;detail preservation;scattering statistics;remote sensing image semantic segmentation;hierarchical multinomial latent model;remote sensing images","","4","","15","IEEE","8 Mar 2018","","","IEEE","IEEE Conferences"
"Supervised Hashing with Kernel Based on Feature Fusion for Remote Sensing Image Retrieval","Z. Yang; Y. Ge; Z. Huang; C. Xiong","School of Software, Nanchang Hangkong University, NanChang, China; School of Software, Nanchang Hangkong University, NanChang, China; School of Software, Nanchang Hangkong University, NanChang, China; School of Software, Nanchang Hangkong University, NanChang, China","2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)","2 Apr 2021","2021","","","470","475","Hashing has emerged as an influential solution to improve high-resolution remote sensing images retrieval (HRRSIR) performance. Most of the existing hashing methods adopt single feature to retrieve images. However, it is difficult for single feature to express the highly complex geometrical structures and spatial patterns of high-resolution remote sensing images. To address these issues, we propose a supervised hashing with kernel based on feature fusion method, which is called FKSH. FKSH mainly includes feature extraction, feature fusion and hash learning. Firstly, GoogLeNet and VGG16 are selected to learn features. In order to keep more spatial information, the features are extracted with the original input size and keep the output form of the three-dimensional tensor. Then max-pooling is performed on the tensor to retain the salient features. Secondly, the features from GoogLeNet are copied to uniform the dimension with that from VGG16. Thus, the features from GoogLeNet and VGG16 can be fused by element-wise addition. Finally, the high-dimensional fused features are mapped to compact binary codes by hashing, and the compact binary codes are adopted to retrieve remote sensing images. Experiments on the two benchmarked datasets clearly shows our superiority.","","978-1-6654-1540-8","10.1109/ICBAIE52039.2021.9389931","National Natural Science Foundation of China; Natural Science Foundation of Jiangxi Province; Nanchang Hangkong University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9389931","Image Retrieval;Feature Fusion;Supervised Hashing with Kernels;Remote Sensing;Supervised Hashing","Tensors;Image retrieval;Binary codes;Transforms;Feature extraction;Kernel;Remote sensing","binary codes;feature extraction;image classification;image fusion;image retrieval;learning (artificial intelligence);remote sensing","hash learning;GoogLeNet;VGG16;salient features;high-dimensional fused features;supervised hashing;remote sensing image retrieval;influential solution;high-resolution remote sensing images retrieval performance;existing hashing methods;highly complex geometrical structures;spatial patterns;feature fusion method;FKSH;feature extraction","","","","19","IEEE","2 Apr 2021","","","IEEE","IEEE Conferences"
"Multi-Scale Feature Extraction and Total Variation Based Fusion Method For HSI and Lidar Data Classification","Y. Tong; Y. Quan; W. Feng; G. Dauphin; Y. Wang; P. Wu; M. Xing","Research Institute of Advanced Remote Sensing Technology, Xidian University, Xi'an, China; Research Institute of Advanced Remote Sensing Technology, Xidian University, Xi'an, China; Research Institute of Advanced Remote Sensing Technology, Xidian University, Xi'an, China; Laboratory of Information Processing and Transmission, L2TI, Institut Galilée, University Paris XIII, France; Research Institute of Advanced Remote Sensing Technology, Xidian University, Xi'an, China; Key Laboratory of State Forestry Administration on Soil Land Water Conservation & Ecological Restoration of the Loess Plateau, Shaan Xi Academy of Forestry, Xi'an, P. R. China; Academy of Advanced Interdisciplinary Research, Xidian University, Xi'an, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","5433","5436","The fusion of hyperspectral image (HSI) and light detection and ranging (LiDAR) data can provide complementary information and improve the accuracy of land cover classification. In this paper, a novel fusion method is proposed to fuse the HSI and LiDAR dataset based on multi-scale feature extraction and total variation. In the method, the extended multi-attribute profile (EMAP) is utilized to automatically extract structural information from HSI and LiDAR elements. The extracted features are then estimated in a lower-dimensional space by multi-scale total variation (MSTV). Finally, the classification map is generated by applying random forest classifiers on the fused data. In the experiment, the performance of the proposed method is evaluated on an urban dataset of Houston. The results demonstrate that classification accuracy could be significantly improved by the proposed method compared with other methods.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554337","National Natural Science Foundation of China(grant numbers:61772397,12005169); National Key R&D Program of China(grant numbers:2016YFE0200400); Open Research Fund of Key Laboratory of Digital Earth Science(grant numbers:2019LDE005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554337","Feature fusion;extended multi-attribute profiles;multi-scale total variation;random forest;remote sensing","Laser radar;Fuses;Redundancy;Geoscience and remote sensing;Interference;Feature extraction;Distance measurement","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;optical radar;pattern classification","HSI;LiDAR dataset;multiscale feature extraction;extended multiattribute profile;LiDAR elements;multiscale total variation;classification map;fused data;classification accuracy;fusion method;lidar data classification;light detection;complementary information;land cover classification","","3","","12","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A Semi-Supervised Siamese Network with Label Fusion for Remote Sensing Image Scene Classification","W. Miao; J. Geng; X. Deng; W. Jiang","School of Electronics and Information, Northwestern Polytechnical University, Xi'an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4932","4935","Remote sensing image scene classification, which requires large amounts of labeled data, plays a critical role in a range of fields.However, in the actual complex environment, the obtained remote sensing images are sometimes unlabeled due to data perturbation and the cost of manual labeling, which limits the training effect and generalization ability. To solve this issue, a semi-supervised siamese network with label fusion is proposed for remote sensing image scene classification. The siamese network is developed to extract features from remote sensing image, where loss function based on the low-entropy principle is constructed to select the unlabeled data as pseudo-label samples. The labeled and pseudo-label samples are mixed to further train the siamese network. The results on UC Merced dataset and WHU-RS19 show that our method is capable to achieve excellent performance compared with other semi-supervised learning methods.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553501","National Natural Science Foundation of China(grant numbers:61703338,61901376); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553501","semi-supervised learning;remote sensing image;classification;deep neural networks","Training;Perturbation methods;Neural networks;Manuals;Semisupervised learning;Feature extraction;Sensors","entropy;feature extraction;image classification;image fusion;remote sensing;supervised learning","label fusion;remote sensing image scene classification;semisupervised Siamese network;data perturbation;loss function;low-entropy principle;unlabeled data;pseudo-label samples;UC Merced dataset;WHU-RS19;semisupervised learning methods","","","","11","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Deep learning for semantic segmentation of remote sensing images with rich spectral content","A. Ben Hamida; A. Benoit; P. Lambert; L. Klein; C. Ben Amar; N. Audebert; S. Lefèvre","Univ. Savoie Mont Blanc, LISTIC, Annecy, France; Univ. Savoie Mont Blanc, LISTIC, Annecy, France; Univ. Savoie Mont Blanc, LISTIC, Annecy, France; Univ. Savoie Mont Blanc, LISTIC, Annecy, France; REGIM, Ecole Nationale d'Ingénieurs de Sfax, Sfax, Tunisie; Univ. Bretagne-Sud, IRISA, Vannes, France; Univ. Bretagne-Sud, IRISA, Vannes, France","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","2569","2572","With the rapid development of Remote Sensing acquisition techniques, there is a need to scale and improve processing tools to cope with the observed increase of both data volume and richness. Among popular techniques in remote sensing, Deep Learning gains increasing interest but depends on the quality of the training data. Therefore, this paper presents recent Deep Learning approaches for fine or coarse land cover semantic segmentation estimation. Various 2D architectures are tested and a new 3D model is introduced in order to jointly process the spatial and spectral dimensions of the data. Such a set of networks enables the comparison of the different spectral fusion schemes. Besides, we also assess the use of a “noisy ground truth” (i.e. outdated and low spatial resolution labels) for training and testing the networks.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127520","Remote Sensing;Multispectral;Deep Learning;Semantic Segmentation;Noisy Training","Three-dimensional displays;Convolution;Estimation;Semantics;Neurons;Image resolution;Decoding","geophysical image processing;image fusion;image resolution;image segmentation;land cover;learning (artificial intelligence);remote sensing","spectral fusion schemes;remote sensing image semantic segmentation;remote sensing acquisition techniques;deep learning approach;2D architectures;3D model;spectral dimensions;spatial dimensions;coarse land cover semantic segmentation estimation;fine land cover semantic segmentation estimation;training data;data volume","","9","","14","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Image fusion at pixel and feature levels based on pyramid imaging","B. Ashalatha; M. B. Reddy","CSE Dept., Krishna University, Machilipatnam, Andhra Pradesh, India; CSE Dept., Krishna University, Machilipatnam, Andhra Pradesh, India","2017 IEEE International Conference on Smart Technologies and Management for Computing, Communication, Controls, Energy and Materials (ICSTM)","30 Oct 2017","2017","","","258","263","Advancement in the recent technology affects a wide research in the field of Image Fusion. It is the more researched challenges in Computer vision, Remote sensing, Medical Imaging, and Target Recognition. The idea behind the image fusion is merging complementary and redundant information from multiple images in such a way, as to retain the most desirable characteristics of every image. The single fused image is relatively high informative when compared to the original images. The Laplacian pyramid method is a Multi-resolution method, in which low resolution images are fused to produce a high resolution image. This paper mainly worked on multi resolution images of same scene and provides results of performance measures namely mean, standard deviation, entropy, Peak Signal to Noise Ratio at Pixel-Level fusion, Feature-Level fusion using techniques like Simple average method, Principle Component Analysis method based on Laplacian pyramid levels of images. Results also include Histograms of both source images and fused output image. These results show higher resolution and better features than the original images.","","978-1-5090-5905-8","10.1109/ICSTM.2017.8089164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8089164","","Image fusion;Image resolution;Feature extraction;Remote sensing;Standards;Entropy;Principal component analysis","feature extraction;image fusion;image resolution","image fusion;pyramid imaging;Medical Imaging;Laplacian pyramid method;image resolution;pixel-level fusion;feature-level fusion;performance measures","","1","","13","IEEE","30 Oct 2017","","","IEEE","IEEE Conferences"
"MSFNET: Multi-Stage Fusion Network for Semantic Segmentation of Fine-Resolution Remote Sensing Data","X. Ma; X. Zhang; M. -O. Pun; M. Liu","School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China; School of Mathe-matical Sciences, University of Science and Technology of China, Hefei, China; Shenzhen Research Institute of Big Data and the Pengcheng Lab-oratory, Shenzhen, China; MizarVision, Shanghai, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2833","2836","This work proposes a Multi-Stage Fusion Network (MSFNet) for semantic segmentation of fine-resolution remote sensing data by exploiting a multi -stage transformer architec-ture. The proposed MSFNet fuses information of different scales and modalities using a multi-stage scheme based on cross-attention mechanism. More specifically, the proposed MSFNet is composed of two Multi-Level Transformers (ML-Trans), one Crossmodal Fusion Transformer (CFTrans) and one Global-Context Augmented Transformer (GCATrans). DMLTrans and CFTrans are designed to fuse features in dif-ferent levels in each modality and high-level crossmodal ab-stract features, respectively, whereas GCATrans enhances the fusion feature of the main modal. Capitalizing on MSFNet, this work demonstrates the fusion of red-green-blue (RGB) remote sensing images and digital surface model (DSM) data. Extensive experiments on large-scale fine-resolution remote sensing data sets, namely the ISPRS Vaihingen, confirm the excellent performance of the proposed architecture as compared to conventional multimodal methods.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883789","","Image segmentation;Fuses;Semantics;Transformers;Data models;Service-oriented architecture;Remote sensing","feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;image resolution;image segmentation;object detection;remote sensing;sensor fusion","digital surface model data;large-scale fine-resolution remote sensing data sets;MultiStage Fusion Network;semantic segmentation;multi-stage transformer architec-ture;MSFNet fuses information;multistage scheme;MultiLevel Transformers;Crossmodal Fusion Transformer;Global-Context Augmented Transformer;high-level crossmodal ab-stract features;fusion feature","","","","7","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Application of Domestic Satellite Image Fast Ortho-Rectification Method in Dynamic Remote Sensing Monitoring of Sea Area","J. Chu; J. Zhao; N. Gao; D. Song; J. Fan; X. Wang","National Marine Environmental Monitoring Center, Dalian, China; National Marine Environmental Monitoring Center, Dalian, China; National Marine Environmental Monitoring Center, Dalian, China; National Marine Environmental Monitoring Center, Dalian, China; National Marine Environmental Monitoring Center, Dalian, China; National Marine Environmental Monitoring Center, Dalian, China","2018 Ninth International Conference on Intelligent Control and Information Processing (ICICIP)","10 Jan 2019","2018","","","134","138","To cater for the need for dynamic remote sensing monitoring in the sea area and the characteristics of the domestic high-resolution satellite imagery in coastal areas, a set of effective and fast ortho-rectification methods based on PCI Geoimaging Accelerator (GXL) system are developed in this paper. The technical flow generated and parameters setting of key technological linkages are described in details. Upon the application in actual production and the analysis of production efficiency of GXL, the automatic and fast ortho-rectification method of domestic satellite imagery based on dynamic remote sensing monitoring in the sea area is further demonstrated to meet the business needs.","","978-1-5386-5860-4","10.1109/ICICIP.2018.8606723","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606723","Dynamic remote sensing monitoring of sea area;high-resolution satellite imagery;PCI GeoImaging accelerator;fast Ortho-Rectification","Satellites;Hafnium;Geomagnetism;Information technology;Remote sensing;Monitoring;Image fusion","image resolution;oceanographic techniques;remote sensing","dynamic remote sensing monitoring;sea area;domestic high-resolution satellite imagery;coastal areas;PCI Geoimaging Accelerator system;domestic satellite image fast ortho-rectification method;technical flow;technological linkages;GXL production efficiency","","","","8","IEEE","10 Jan 2019","","","IEEE","IEEE Conferences"
"Construction and Implementation of a Data-Oriented Platform for Rural Ecological Entrepreneurship based on Rural Remote Sensing Image Change Detection Algorithm","Z. Liang; X. Xia; B. Zhou; H. Wu","School of Economics and Management, Jiangxi Agricultural University, Nanchang, China; School of Economics and Management, Jiangxi Agricultural University, Nanchang, China; School of Economics and Management, Jiangxi Agricultural University, Nanchang, China; School of Economics and Management, Jiangxi Agricultural University, Nanchang, China","2022 International Conference on Inventive Computation Technologies (ICICT)","16 Aug 2022","2022","","","1011","1015","Construction and implementation of a data-oriented platform for rural ecological entrepreneurship based on rural remote sensing image change detection algorithm is studied in the paper. The image fusion method is used to detect the change information, and only the multispectral data of one phase and the high geometric resolution data of the other phase are used, which saves the cost of dynamic monitoring and information processing. In addition, the advantages of multi-source remote sensing information technology that can obtain thematic data efficiently, accurately, in a large area and in real time are used. With these advantages, this paper designs the novel data-oriented platform for rural ecological entrepreneurship. The system is implemented, and the data algorithm is tested.","2767-7788","978-1-6654-0837-0","10.1109/ICICT54344.2022.9850586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9850586","Remote sensing;change detection;image processing;ecological entrepreneurship;data-oriented platform","Image resolution;Computational modeling;Biological system modeling;Entrepreneurship;Information processing;Real-time systems;Detection algorithms","ecology;geophysical image processing;image fusion;remote sensing","data algorithm;rural ecological entrepreneurship;rural remote sensing image change detection algorithm;image fusion method;change information;multispectral data;high geometric resolution data;multisource remote sensing information technology;novel data-oriented platform","","","","28","IEEE","16 Aug 2022","","","IEEE","IEEE Conferences"
"Weakly Supervised Semantic Change Detection via Label Refinement Framework","Z. Zheng; Y. Liu; S. Tian; J. Wang; A. Ma; Y. Zhong","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing Wuhan University, Wuhan, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2066","2069","Semantic change detection is a meaningful but challenging task in the remote sensing community. The currently dominant approaches are mainly based on deep learning. However, the lack of high-resolution annotations is the main bottleneck for semantic change detection at scale when using these state-of-the-art deep learning models. In this paper, the label refinement framework is proposed for weakly-supervised semantic change detection, which allows the deep network to learn from low-resolution labels and produce high-resolution semantic change maps, thus alleviating the data-hungry problem. This framework contains four parts: coarse label training’ pseudo-label refinement, multitask change detection and post-process. The experimental results on 2021 IEEE GRSS Data Fusion Contest Track MSD dataset confirmed the effectiveness of the proposed method. Additionally, our method wins 4th place in the 2021 IEEE GRSS Data Fusion Contest Track MSD (DFC21-MSD).","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553768","National Key Research and Development Program of China(grant numbers:41771385,41801267); China Postdoctoral Science Foundation(grant numbers:2017M622522); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553768","Land cover change detection;weakly supervision;convolutional neural network;semantic change detection","Deep learning;Training;Annotations;Semantics;Data integration;Task analysis;Remote sensing","geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);remote sensing;sensor fusion","label refinement framework;weakly-supervised semantic change detection;low-resolution labels;high-resolution semantic change maps;coarse label training pseudolabel refinement;multitask change detection;weakly supervised semantic change detection;high-resolution annotations;state-of-the-art deep learning models","","3","","15","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Robust Semantic Segmentation By Dense Fusion Network On Blurred VHR Remote Sensing Images","Y. Peng; S. Sun; Z. Wang; Y. Pan; R. Li","Shenzhen Institutes of Advanced Technology, CAS, Shenzhen, China; Beijing Futong Dongfang Technology Co, Ltd, Beijing, China; Shenzhen Institutes of Advanced Technology, CAS, Shenzhen, China; College of Information Science & Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science & Technology, Beijing University of Chemical Technology, Beijing, China","2020 6th International Conference on Big Data and Information Analytics (BigDIA)","2 Apr 2021","2020","","","142","145","Robust semantic segmentation of VHR remote sensing images from UAV sensors is critical for earth observation, land use, land cover or mapping applications. Several factors such as shadows, weather disruption and camera shakes making this problem highly challenging, especially only using RGB images. In this paper, we propose the use of multi-modality data including NIR, RGB and DSM to increase robustness of segmentation in blurred or partially damaged VHR remote sensing images. By proposing a cascaded dense encoder-decoder network and the SELayer based fusion and assembling techniques, the proposed RobustDenseNet achieves steady performance when the image quality is decreasing, compared with the state-of-the-art semantic segmentation model.","","978-1-6654-2232-1","10.1109/BigDIA51454.2020.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9384562","robustness;fusion;remote sensing image;segmentation;multi-modality","Image segmentation;Semantics;Feature extraction;Robustness;Sensors;Remote sensing;Meteorology","geophysical image processing;image classification;image colour analysis;image fusion;image resolution;image segmentation;remote sensing","dense fusion network;robust semantic segmentation;UAV sensors;earth observation;land use;land cover;weather disruption;camera;RGB images;multimodality data;blurred VHR remote sensing images;partially damaged VHR remote sensing images;cascaded dense encoder-decoder network;SELayer based fusion;assembling techniques;image quality;state-of-the-art semantic segmentation model","","2","","9","IEEE","2 Apr 2021","","","IEEE","IEEE Conferences"
"Query-Adaptive Feature Fusion Base on Convolutional Neural Networks for Remote Sensing Image Retrieval","F. Ye; S. Chen; X. Meng; J. Xin","Key Laboratory of Mine Environmental Monitoring and Improving Around Poyang Lake, Ministry of Natural Resources, Nanchang, China; School of Surveying and Mapping Engineering, East China University of Technology, Nanchang, China; School of Surveying and Mapping Engineering, East China University of Technology, Nanchang, China; School of Surveying and Mapping Engineering, East China University of Technology, Nanchang, China","2021 International Conference on Digital Society and Intelligent Systems (DSInS)","10 Jan 2022","2021","","","148","151","Content-based Remote sensing image retrieval (CBRSIR) becomes important research with the volume of remote sensing images rapidly expanding. Many image features have been proposed for CBRSIR, hence it has become a big challenge to effectively fuse these features for alleviating the huge variation in retrieval performance among different image queries when a single image feature is used. We proposed a query-adaptive feature fusion method based on a convolutional neural networks (CNN) regression model. We use the CNN regression model to estimate the DCG value for each feature and assign different features with different weights for each query according to these DCG values. Meanwhile, we use the image-to-query-class distance to further improve retrieval performance. Experiments on UCMD show that the proposed method can improve the CBRSIR performance.","","978-1-6654-0630-7","10.1109/DSInS54396.2021.9670607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9670607","Remote Sensing Image Retrieval;query-adaptive feature fusion;image-to-query-class distance;CNN regression model","Fuses;Image retrieval;Predictive models;Convolutional neural networks;Intelligent systems;Remote sensing","content-based retrieval;feature extraction;geophysical image processing;image fusion;image retrieval;neural nets;remote sensing","query-adaptive feature fusion base;Remote sensing image retrieval;CBRSIR;remote sensing images;image features;retrieval performance;different image queries;single image feature;query-adaptive feature fusion method;convolutional neural networks regression model;image-to-query-class distance","","1","","23","IEEE","10 Jan 2022","","","IEEE","IEEE Conferences"
"Intellectual Property for DWT based Image Fusion","B. Rajeshwari; Swarna; T. Kulkarni; Varsha; B. Bajarangbali","Dept.of ECE, PES University, Bengaluru; Dept.of ECE, PES University, Bengaluru; Dept.of ECE, PES University, Bengaluru; Dept.of ECE, PES University, Bengaluru; Dept.of ECE, PES University, Bengaluru","2021 International Conference on Smart Generation Computing, Communication and Networking (SMART GENCON)","21 Dec 2021","2021","","","1","6","In Recent times, the requirement of high quality images and application of advanced image processing techniques in medical and remote sensing fields has radically increased. Therefore, there is an increase in the data storage requirements to accommodate these high quality images. With image fusion techniques, the data stored in two images is fused into a single image while retaining the necessary information from both the source images, thus reducing the data storage. Implementation of image fusion technique requires a huge amount of computation, for which a dedicated hardware can be designed. To make further enhancements to the image fusion algorithms, Field Programmable Gate Array(FPGA) is a suitable target as it supports reconfigurable computing. Simulation is done using Xilinx System Generator 2019.1 software where the implementation utilizes Zedboard Zynq-7000XC 7Z 020-CLG 484-1 device. The fused image outputs are assessed using well-known image quality metrics like Structural Similarity Index(SSIM), Entropy etc.","","978-1-6654-2503-2","10.1109/SMARTGENCON51891.2021.9645870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645870","Image fusion;Discrete Wavelet Transform;System Generator","Measurement;Image quality;Software algorithms;Memory;Intellectual property;Logic gates;Generators","discrete wavelet transforms;field programmable gate arrays;image fusion;image processing;reconfigurable architectures","image quality metrics;DWT based image fusion;high quality images;advanced image processing techniques;medical sensing fields;remote sensing fields;data storage requirements;image fusion technique;single image;source images;image fusion algorithms;fused image outputs","","","","8","IEEE","21 Dec 2021","","","IEEE","IEEE Conferences"
"Spectral Modulation for Fusion of Hyperspectral and Multispectral Images","X. Lu; X. Yu; W. Tang; B. Zhu","Shanghai Radio Equipment Research Institute, Shanghai, China; Shanghai Radio Equipment Research Institute, Shanghai, China; Shanghai Radio Equipment Research Institute, Shanghai, China; Shanghai Radio Equipment Research Institute, Shanghai, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3149","3152","Hyperspectral (HS) and multispectral (MS) image fusion has attracted great attention during the past decades. Numerous of fusion methods have been developed and shown their effectiveness particularly on simulated data. Nonetheless, for real remote sensing data, the different acquisition times or conditions result in a serious spectral distortion and severely affect the fusion quality. Yet very few works have considered this issue. In this paper, a spectral modulation (SM) method is proposed to better maintain the spectral information of the HS data when fusing with MS data. The goal is to generate an adjusted MS image that would have been observed under the same imaging conditions with the corresponding HS sensor. Experiments on two HS and MS data sets acquired by different platforms demonstrate that the proposed method is beneficial to the spectral fidelity and spatial enhancement of the fused image compared with some state-of-the-art fusion techniques.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898754","Hyperspectral;multispectral;image fusion;spectral modulation","Spatial resolution;Modulation;Transforms;Image fusion;Distortion;Imaging","geophysical image processing;image fusion;image resolution;remote sensing","hyperspectral images;multispectral images;fusion methods;remote sensing data;acquisition times;serious spectral distortion;fusion quality;spectral modulation method;spectral information;HS data;adjusted MS image;imaging conditions;HS sensor;MS data sets;spectral fidelity;fused image;state-of-the-art fusion techniques","","","","9","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Satellite Image Fusion Using an Iterative Ihs-Based Approach","M. Ghadjati; A. Benazza-Benyahia; A. Moussaoui","Laboratoire de Génie Electrique de Guelma (LGEG), Université 8 mai 45 de Guelma, BP 401 Guelma, ALGERIA; LR11TIC01, COSIM Lab, Cité Technologique des Communications, University of Carthage SUP’COM, Rte de Raoued Km 3,5, Ariana, TUNISIA; Laboratoire de Génie Electrique de Guelma (LGEG), Université 8 mai 45 de Guelma, BP 401 Guelma, ALGERIA","2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium (M2GARSS)","2 Jun 2020","2020","","","133","136","Intensity-Hue-Saturation (IHS) method is a pansharpening method belonging to the spectral class methods. In this class of methods, the multispectral image undergoes a spectral transformation and then one of the resulting components is totally replaced by the panchromatic image, hence leading to a significant color distortion. To alleviate this shortcoming, in the literature, the wavelet transform is often integrated to the spectral methods in order to transfer only the spatial details of the panchromatic image. Furthermore, the spatial information quantity transferred during the fusion is usually defined by the resolution ratio between the multispectral and panchromatic images. However, this is not necessarily the optimal way to generate the best fused image. In this paper, we propose an iterative IHS-based method (called IIHS), to continuously transfer the spatial information from the panchromatic image to the multispectral image while controling the perceptual quality of the fused image. Experiments on remote sensing images show that the proposed method presents the best visual and objective performances comparatively to the state-of-art IHS methods.","","978-1-7281-2190-1","10.1109/M2GARSS47143.2020.9105197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9105197","Pansharpening;IHS method;iterative IHS;spatial information;quality with no reference index","Wavelet transforms;Visualization;Satellites;Image color analysis;Pansharpening;Distortion;Iterative methods","geophysical image processing;hyperspectral imaging;image colour analysis;image fusion;image resolution;iterative methods;remote sensing;wavelet transforms","IIHS;remote sensing images;panchromatic images;color distortion;panchromatic image;spectral transformation;multispectral image;spectral class methods;pansharpening method;Intensity-Hue-Saturation method;iterative IHS-based approach;satellite image fusion","","","","8","IEEE","2 Jun 2020","","","IEEE","IEEE Conferences"
"Multi-temporal optical VHR image fusion for Land-Cover mapping","M. Paget; A. Gressin; C. Mallet","IGN, SRIG, Université Paris-Est, Saint Mandé, France; IGN, SRIG, Université Paris-Est, Saint Mandé, France; IGN, SRIG, Université Paris-Est, Saint Mandé, France","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","1913","1916","Land-Cover databases (LC-DB) are very useful for environmental purposes, but need to be semantically detailed to provide robust and instructive spatial indicators. Moreover, remote sensed data allow to cover large areas with high temporal resolution. Such multi-temporal data are very useful input to discriminate LC classes. Nevertheless, automatic fusion method need to be developed to provide high quality LC-DB. In this paper, several fusion methods are proposed and introduced in an existing Land-Cover mapping framework. Those fusion methods allow to take advantage of multi-temporal data. Those methods are compared, and assessed thanks to a very high resolution LC-DB.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326168","Remote sensing;change detection;land cover;satellite imagery","Buildings;Roads;Accuracy;Remote sensing;Satellites;Image resolution;Sensors","land cover;remote sensing;terrain mapping","multitemporal optical VHR image fusion;land-cover mapping;land-cover databases;instructive spatial indicators;remote sensed data;discriminate LC classes","","","","4","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Fusion Detection of Closed Water in Medium-Low Resolution Remote Sensing Imagery","Y. Ning; Y. You; J. Cao; F. Liu; Q. Yan; Y. Zhang","School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4027","4030","Aiming at the closed water detection in remote sensing imagery at medium-low resolution, this paper proposes a novel method for closed water detection based on fusion detection which conducts detection via informative fused images blended by Synthetic Aperture Radar (SAR) and optical images. Firstly, it utilizes SAR and optical image pairs containing the same closed water object to generate aligned image pairs according to latitude and longitude information. Next, generative adversarial network (GAN) is adopted to fuse two categories of images. At last, a target detection network driven by optical image samples is used to detect the closed water on the fused image. The experiment result on Sentinel-1&2 shows that the proposed method can effectively make up for the shortage of SAR image in closed water detection and improve the detection performance.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553554","Beijing Natural Science Foundation, China(grant numbers:4214058); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553554","Closed water detection;Image fusion;GAN","Image resolution;Object detection;Optical fiber networks;Optical imaging;Generative adversarial networks;Adaptive optics;Radar polarimetry","geophysical image processing;geophysical techniques;image fusion;image resolution;object detection;optical images;radar imaging;remote sensing;synthetic aperture radar","target detection network;optical image samples;fused image;SAR image;closed water detection;detection performance;fusion detection;medium-low resolution remote sensing imagery;informative fused images;optical images;optical image pairs;closed water object;aligned image pairs","","","","7","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Fusion of CNN ensemble for Remote Sensing Scene Classification","N. Alosaimi; H. Alhichri","Department of Computer Engineering, King Saud University, Riyadh, Saudi Arabia; Advanced Lab for Intelligent Systems Research (ALISR), King Saud University, Riyadh, Saudi Arabia","2020 3rd International Conference on Computer Applications & Information Security (ICCAIS)","20 May 2020","2020","","","1","6","Scene classification problem in remote sensing (RS) images has attracted many researchers recently. Different fusion methods have been widely used by the machine learning community to fuse classifiers. In this paper, a decision-level fusion method has been proposed to fuse a set of stat-of-the-art CNN classifiers, namely VGG-16, SqueezeNet, and DenseNet. First, the experiment proves that these classifiers do not make the same classification mistakes, i.e. most of the time at least one of them provides correct classification. Thus these three classifiers are diverse and hence complement each other. To exploit this discovery, a novel decision-level fusion method that combines the classification decisions using voting and confidence fusion techniques has been developed. To show the effectiveness of the proposed fusion method, the results demonstrate how the accuracy of the classification can be enhanced using fusion versus training individual networks. The preliminary results for the UC Merced dataset, the KSA multisensor dataset, Aerial Image Datasets (AID), Optimal31 dataset and Whurs19 dataset have been presented. Preliminary comparison to state-of-the-art methods show the promising capabilities of this solution and encourages to investigate this method further.","","978-1-7281-4213-5","10.1109/ICCAIS48893.2020.9096721","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9096721","Remote sensing;Scene classification;deep learning;Convolutional Neural Network;Fusion of multiple CNN","Computational modeling;Training;Predictive models;Feature extraction;Remote sensing;Testing;Load modeling","convolutional neural nets;image classification;image fusion;learning (artificial intelligence);remote sensing","novel decision-level fusion method;confidence fusion techniques;UC Merced dataset;KSA multisensor dataset;Aerial Image Datasets;Optimal31 dataset;remote sensing images;CNN classifiers;remote sensing scene classification problem","","3","","27","IEEE","20 May 2020","","","IEEE","IEEE Conferences"
"Comprehensive Analysis of Characteristics of Debris Flow Fans in Xiaojiang Valley by Using Remote Sensing Method","X. Zhang; S. Gan; X. Yuan; L. Wang; M. Yang","Faculty of Land Resource Engineering, Kunming University of Science and Technology, Kunming, China; Plateau Mountain Area Spatial Information Survey Technique Application Engineering Research Center at Yunnan Province’s Universities, Kunming University of Science and Technology, Kunming, China; Plateau Mountain Area Spatial Information Survey Technique Application Engineering Research Center at Yunnan Province’s Universities, Kunming University of Science and Technology, Kunming, China; Surveying and Mapping Engineering Institute of Yunnan Province, Kunming, China; Faculty of Land Resource Engineering, Kunming University of Science and Technology, Kunming, China","2019 IEEE 4th International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)","30 May 2019","2019","","","423","427","With the continuous improvement of the spatial resolution of remote sensing images, its application in geosciences is more and more extensive. Xiaojiang Valley is a place where debris flow often breakout in China. Due to the frequent occurrence of debris flow, many debris flow fans have been formed in the valley. In this paper, we use the methods of visual interpretation by using GF2 images and field investigation to find that there are a total of 196 debris flow fans in the Xiaojiang River Valley. The GIS technology is used to analyze the area, spatial distribution, slope and plane shape of these debris flow fans. The results show that the debris flow fans in the study area are mainly medium-size and large-size debris flow fans, and the total area of 196 debris flow fans is 65.709km2; In terms of spatial plane distribution characteristics, 64% of the debris flow fans are distributed along the main river and the branch Kuaihe River of Xiaojiang River; In terms of elevation and percentage slope characteristics, debris flow fans are the most widely distributed in the areas which the elevation between 1000m-1500m or the percentage slope between 15% -30%; In terms of morphological characteristics, the length-width ratio of debris flow fans in the range of 0.5-1.0 is the most.","","978-1-7281-1410-1","10.1109/ICCCBDA.2019.8725740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8725740","debris flow fans;Xiaojiang Valley;remote sensing interpretation;stacking characteristics;spatial distribution","Fans;Rivers;Remote sensing;Shape;Spatial resolution;Geology;Graphical models","geographic information systems;geomorphology;geophysical image processing;hydrological techniques;image fusion;image segmentation;remote sensing;rivers","China;Kuaihe River;Xiaojiang River Valley;GF2 images;geosciences;remote sensing images;remote sensing method;debris flow fans;size 1000.0 m to 1500.0 m","","","","13","IEEE","30 May 2019","","","IEEE","IEEE Conferences"
"Change Monitoring of Urban Typical Facilities for Remote Sensing Images Fusion","L. Huafeng; X. Guibin; Z. Yuwei; X. Lianke; B. Xiaochun; W. Jie","Business Department, Hubei Huazhong Electric Power Technology Development Co, Ltd, Wuhan, China; Business Department, Electric Power Research Institute, State Grid Shandong Electric Power Company, Jinan, China; Business Department, Electric Power Research Institute, State Grid Shandong Electric Power Company, Jinan, China; Business Department, Electric Power Research Institute, State Grid Shandong Electric Power Company, Jinan, China; Business Department, Electric Power Research Institute, State Grid Shaanxi Electric Power Company, Xi'an, China; Business Department, Electric Power Research Institute, State Grid Shandong Electric Power Company, Jinan, China","2020 IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS)","22 Sep 2020","2020","","","798","802","Most of the traditional remote sensing image change detection adopts the band ratio method and the image difference method. This type of method is simple to operate, and only the image pixel difference value and the band ratio value are used for calculation, and the detection accuracy cannot be guaranteed. Aiming at this problem, this paper proposes a kind of spectral product-based fusion algorithm based on algebraic weighting. The algorithm combines the ratio image and the difference image, and then selects the Ostu algorithm to iteratively calculate the fusion to extract the image change region. The experimental results show that the detection accuracy of this method is better than the traditional change detection method, and it has certain stability and intelligence.","","978-1-7281-9874-3","10.1109/ICPICS50287.2020.9202111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202111","remote sensing image;spectral fusion;change detection;principal component analysis","Power systems;Remote sensing;Principal component analysis;Electronic mail;Gray-scale;Companies","feature extraction;geophysical image processing;image fusion;image segmentation;object detection;remote sensing","change monitoring;urban typical facilities;remote sensing images fusion;remote sensing image change detection;band ratio method;image difference method;image pixel difference value;band ratio value;spectral product-based fusion algorithm;ratio image;difference image;Ostu algorithm;image change region;change detection method;algebraic weighting","","","","12","IEEE","22 Sep 2020","","","IEEE","IEEE Conferences"
"Research on cloud removal based on fusing Multi-temporal Remote Sensing Images","X. Zou; Y. Zhao; P. Tao; Y. Cui","Department of Computer Technology and Applications, Qinghai University, Xining, China; Department of Computer Technology and Applications, Qinghai University, Xining, China; Department of Computer Technology and Applications, Tsinghua University, Beijing, China; Department of Computer Technology and Applications, Qinghai University, Xining, China","2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","3 Aug 2022","2022","10","","1296","1300","Cloud and cloud shadow are the primary factors that affect the application of remote sensing images, and they have always been problems encountered in remote sensing image processing. This article puts forward a new cloud removal strategy, whose data is from the Landsat multi-source remote sensing images and based on an improved BP neural network. Compared with the previous cloud removal methods, the selection value of BP neural network training is changed to reduce human participation. The previous gray-scale value group marked by classification (Vegetation, water body, bare land, residential land, and fields, etc.) is changed to the gray-scale value group of the two images' common areas without cloud. The experimental results show that the de-cloud image got by our method has higher SSIM and Cosine similarity with the reference image.","2693-2865","978-1-6654-2207-9","10.1109/ITAIC54216.2022.9836831","Qinghai University(grant numbers:2019-QGY-4); Natural Science Foundation of China(grant numbers:61863031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9836831","Remote Sensing;Cloud Removal;Multi Temporal;Image Fusion","Training;Earth;Artificial satellites;Conferences;Neural networks;Vegetation mapping;Gray-scale","backpropagation;geophysical image processing;geophysical techniques;image classification;image fusion;neural nets;remote sensing","cloud shadow;Landsat multisource remote sensing images;cloud removal;BP neural network training;gray-scale value group;vegetation;water body;bare land;residential land","","","","21","IEEE","3 Aug 2022","","","IEEE","IEEE Conferences"
"A Fusion Method of SAR Image and Optical Image Based on NSCT and Gram-Schmidt Transform","B. Yan; Y. Kong","College of Electrical and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electrical and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2332","2335","The purpose of remote sensing image fusion is to synthesize the characteristics of multi-source remote sensing images and generate a composite image with new spatial, spectral and temporal features. The Gram-Schmidt (GS) transform can better improve the spatial features and maintain the spectral characteristics of the original images to a large extent. However, for the fusion of Synthetic aperture radar (SAR) image and optical image, there are still obvious spectral distortion and detail blurring. In this paper, the GS transform method is improved with non-subsampled contourlet transform (NSCT) which is used to obtain a high-resolution image that contains both spectral information and SAR image detail information. The results show that the spectral information of the fusion image is well preserved and the indicators are better than other methods.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323158","Aeronautical Science Foundation of China(grant numbers:20182052012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323158","Gram-Schmidt transform;NSCT;SAR;image fusion;optical image","Transforms;Radar polarimetry;Optical imaging;Optical distortion;Optical reflection;Optical sensors;Optical filters","geophysical image processing;geophysical techniques;image fusion;image resolution;radar imaging;remote sensing;synthetic aperture radar","nonsubsampled contourlet transform;fusion image;spectral information;high-resolution image;GS transform method;detail blurring;spectral distortion;synthetic aperture radar image;spectral characteristics;composite image;multisource remote sensing images;remote sensing image fusion;Gram-Schmidt transform;NSCT;optical image;SAR image;fusion method","","2","","6","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A Fusion Method of Multi-spectral Image and Panchromatic Image Based on NSCT Transform and Adaptive Gamma Correction","L. Jiahuan; Z. Jian; D. Yunfei","UCAS Beijing, China, Xi'an, China; Chinese Academy of Science, Institute of Optics and Precision Mechanics, Xi'an, China; Chinese Academy of Science, Xi'an Institute of Optics and Precision Mechanics, Xi'an, China","2018 3rd International Conference on Information Systems Engineering (ICISE)","17 Jan 2019","2018","","","10","15","The purpose of remote sensing images fusion is to produce a fused image that contains more clear, accurate and comprehensive information than any single image. an image fusion algorithm incorporating gamma-corrected is proposed based on non-subsampled Contourlet transform (NSCT). Firstly, the multispectral image is transformed to intensity-hue-saturation (IHS) system. Secondly, the panchromatic image and the component intensity of the multispectral image are decomposed by NSCT. Then the NSCT coefficients of high and low frequency subbands are fused by different rules, respectively. For the low frequency subbands, an adaptive gamma correction was used and mutual information as the threshold for the weighted coefficient fusion. The max-abs-based fusion rule is used to fuse the high frequency coefficients. Finally, the fusion image can be obtained by performing inverse NSCT and inverse IHS transform. Experiment results demonstrate that the fused image contains abundant detailed contents and preserves the saliency structure. Our proposed method achieves better visual quality than the current methods.","2160-1291","978-1-5386-6259-5","10.1109/ICISE.2018.00010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8614472","Multispectral and panchromatic images;image fusion;NSCT;gamma correction","Image fusion;Filter banks;Spatial resolution;Discrete wavelet transforms;Mutual information","geophysical image processing;image colour analysis;image enhancement;image fusion;remote sensing;transforms","low frequency subbands;adaptive gamma correction;weighted coefficient fusion;max-abs-based fusion rule;high frequency coefficients;fusion image;inverse NSCT;fused image;fusion method;multispectral image;panchromatic image;NSCT transform;remote sensing images fusion;nonsubsampled Contourlet;intensity-hue-saturation system;NSCT coefficients;image fusion algorithm","","3","","14","IEEE","17 Jan 2019","","","IEEE","IEEE Conferences"
"A Two-step Spatio-Temporal satellite image Fusion Model for temporal changes of various LULC under one-pair prior images scenario","Yongquan Zhao; B. Huang","Department of Geography and Resource Management, The Chinese University of Hong Kong, Hong Kong, China; Department of Geography and Resource Management, The Chinese University of Hong Kong, Hong Kong, China","2016 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","24 Nov 2016","2016","","","1","5","This paper proposes a two-step spatio-temporal fusion model (TSTFM) for generating synthetic satellite remote sensing images with high-spatial and high-temporal resolution (HSaHTeR) based on one pair of prior images, which contain one low-spatial but high-temporal resolution (LSaHTeR) image and one high-spatial but low-temporal resolution (HSaLTeR) image. Considering both phenology and type surface temporal changes, the two steps in TSTFM are adopted to handle these two kinds of changes respectively, which are based on weighted mean and example-based image super-resolution approaches accordingly. In addition, a relative radiometric normalization process is conducted before performing the two-step spatio-temporal fusion (STF) process, which aims to calibrate radiometric differences of different kinds of satellite sensors. The proposed method was tested on two sets of test data: surface with mainly LULC phenology changes and surface with primarily LULC type changes. Experimental results show that TSTFM can capture both phenology and type changes efficiently and precisely even with one-pair prior images, and it can also maintain its robustness when facing extremely complex LULC.","","978-1-5090-2708-8","10.1109/ICSPCC.2016.7753699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7753699","Spatio-temporal fusion;weighted mean;image super-resolution;phenology change;type change;various LULC","MODIS;Satellites;Remote sensing;Earth;Spatial resolution;Satellite broadcasting","geophysical image processing;image fusion;image resolution;land use;radiometry;remote sensing","two-step spatio-temporal satellite image fusion model;LULC;one-pair prior images scenario;synthetic satellite remote sensing images;high-spatial and high-temporal resolution image;low-spatial but high-temporal resolution image;high-spatial but low-temporal resolution image;type surface temporal changes;phenology changes;TSTFM;weighted mean;example-based image super-resolution;relative radiometric normalization process;STF process;HSaHTeR image;LSaHTeR image;HSaLTeR image","","","","9","IEEE","24 Nov 2016","","","IEEE","IEEE Conferences"
"Light-Weight Attention Semantic Segmentation Network for High-Resolution Remote Sensing Images","S. Liu; C. He; H. Bai; Y. Zhang; J. Cheng","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China; Sichuan Jiuzhou Eletric Group Co., Ltd, Mianyang, P.R. China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2595","2598","Semantic segmentation of high-resolution remote sensing (HRRS) images becomes more and more important at present. Popular approaches use deep learning to solve this task, which depends on a large amount of labeled data and powerful computing resources. When computing resources or the labeled data are insufficient, their performance will be severely degraded. To deal with this problem, we proposed a light-weight network with attention modules for semantic segmentation of HRRS images. The depth and width of the network are designed, which has a small number of parameters to ensure the efficiency of training. The network adopts an encoder-decoder architecture. The feature maps of different scales from the encoder are concatenated together after resizing to carry out multi-scale feature fusion. To capture the global semantic information from the context, the attention mechanism is employed in the decoder. With one GTX2080Ti GPU and only 15 MB parameters the model owns, our light-weight network has quality results evaluated on ISPRS Vaihingen Dataset with fewer parameters compared to other popular approaches.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324723","National Natural Science Foundation of China(grant numbers:61671125); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324723","Light-weight network;attention mechanism;semantic segmentation","Semantics;Image segmentation;Task analysis;Training;Decoding;Feature extraction;Remote sensing","feature extraction;geophysical image processing;image colour analysis;image fusion;image resolution;image segmentation;learning (artificial intelligence);object detection;remote sensing;traffic engineering computing","weight attention semantic segmentation network;high-resolution remote sensing images;deep learning;powerful computing resources;light-weight network;attention modules;HRRS images;encoder-decoder architecture;multiscale feature fusion;global semantic information;attention mechanism","","5","","12","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Multimodal-Temporal Fusion: Blending Multimodal Remote Sensing Images to Generate Image Series With High Temporal Resolution","X. Liu; C. Deng; B. Zhao; J. Chanussot","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Univ. Grenoble Alpes, CNRS, Grenoble INP GIPSA-lab, Grenoble, France","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","10083","10086","This paper aims to tackle a general but interesting cross-modality problem in remote sensing community: can multimodal images help to generate synthetic images in time series and improve temporal resolution? To this end, we explore multimodal-temporal fusion, in which we attempt to leverage the availability of additional cross-modality images to simulate the missing images in time series. We propose a multimodal-temporal fusion framework, and mainly focus on two kinds of information for the simulation: intra-modal cross-modality information and inter-modal temporal information. To exploit the cross-modality information, we adopt available paired images and learn a mapping between different modality images using a deep neural network. Considering temporal dependency among time-series images, we formulate a temporal constraint in the learning to encourage temporal consistent results. Experiments are conducted on two cross-modality image simulation applications (SAR to visible and visible to SWIR), and both visual and quantitative results demonstrate that the proposed model can successfully simulate missing images with cross-modality data.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898453","Multimodal-Temporal Fusion;Cross-modality Image Translation;Image Time Series;Temporal Resolution;Deep Neural Networks","Time series analysis;Remote sensing;Image resolution;Neural networks;Data models;Synthetic aperture radar;Simulation","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing;time series","time-series images;cross-modality data;temporal resolution;cross-modality problem;multimodal-temporal fusion;intramodal cross-modality information;intermodal temporal information;multimodal remote sensing image blending;cross-modality image simulation;deep neural network","","3","","5","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Remote sensing image registration based on fusion of spatial transformation and dense convolution","G. Liu; Y. Chen; H. Chen","Department of computer Science and Information Engineering, Shanghai Institute of Technology, Shanghai, China; Department of computer Science and Information Engineering, Shanghai Institute of Technology, Shanghai, China; Department of computer Science and Information Engineering, Shanghai Institute of Technology, Shanghai, China","2020 International Conference on Computer Vision, Image and Deep Learning (CVIDL)","30 Nov 2020","2020","","","21","26","In order to achieve high-precision registration of remote sensing images, this paper proposes a remote sensing image registration method based on spatial transformation and dense convolution fusion. First, the dense convolutional network improved by the spatial transformation module is used for feature extraction to improve the robustness of feature points; then use improved Grid-based Motion Statistics (GMS) algorithm for feature matching, and use the homography matrix-based method to eliminate mismatched point pairs to improve matching accuracy; finally, the transformed model is used to solve the transformed model to achieve accurate registration of remote sensing images. Experiments show that the proposed method can effectively improve the correct matching rate of feature points, has higher registration accuracy and stronger robustness.","","978-1-7281-9481-3","10.1109/CVIDL51233.2020.00012","National Natural Science Foundation of China; Shanghai University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9270496","Dense neural networks;Improved Grid-based-Motion Statistics algorithm;Remote sensing image registration","Feature extraction;Remote sensing;Convolution;Robustness;Neural networks;Image registration;Training","convolutional neural nets;feature extraction;geophysical image processing;image fusion;image matching;image motion analysis;image registration;remote sensing","remote sensing image registration method;dense convolution fusion;dense convolutional network;spatial transformation module;feature extraction;grid-based motion statistics algorithm;feature matching;homography matrix-based method","","","","13","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Deep learning approach for large scale land cover mapping based on remote sensing data fusion","N. Kussul; A. Shelestov; M. Lavreniuk; I. Butko; S. Skakun","National Technical University of Ukraine, “Kyiv Polytechnic Institute”, Kyiv, Ukraine; National Technical University of Ukraine, “Kyiv Polytechnic Institute”, Kyiv, Ukraine; NASU, SSAU, Kyiv, Ukraine; NASU, SSAU, Kyiv, Ukraine; University of Maryland, College Park, MD, USA","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","198","201","In the paper we propose the methodology for solving the large scale classification and area estimation problems in the remote sensing domain on the basis of deep learning paradigm. It is based on a hierarchical model that includes self-organizing maps (SOM) for data preprocessing and segmentation (clustering), ensemble of multi-layer perceptrons (MLP) for data classification and heterogeneous data fusion and geospatial analysis for post-processing. The proposed methodology is applied for generation of high resolution land cover and land use maps for the territory of Ukraine from 1990 to 2010 and 2015.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729043","Deep learning;neural network;remote sensing data;big data;geospatial analysis;Landsat","Satellites;Agriculture;Remote sensing;Monitoring;Earth;Machine learning;Big data","geophysical image processing;image classification;image fusion;image segmentation;land cover;learning (artificial intelligence);terrain mapping","deep learning approach;large-scale land cover mapping;remote sensing data fusion;large-scale classification;area estimation;self-organizing maps;data preprocessing;data segmentation;multilayer perceptrons;data classification;heterogeneous data fusion;geospatial analysis;high-resolution land cover map;high-resolution land use map;Ukraine;AD 1990 to 2010;AD 2015","","29","","20","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"A framework of collaborative change detection with multiple operators and multi-source remote sensing images","X. Chen; J. Li; Y. Zhang; L. Tao; W. Shen","Academy of Disaster Reduction and Emergency Management, Beijing Normal University, Beijing, China; Academy of Disaster Reduction and Emergency Management, Beijing Normal University, Beijing, China; Academy of Disaster Reduction and Emergency Management, Beijing Normal University, Beijing, China; Academy of Disaster Reduction and Emergency Management, Beijing Normal University, Beijing, China; College of Marine Sciences, Shanghai Ocean University, Shanghai, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","5169","5172","This paper proposes a framework of change detection with multi-source remote sensing images through collaboration of multiple operators. Firstly, pre-processed images are distributed to different operators. Then the images are classified by the operators independently. Finally, with uploaded classification results, change detection result can be derived through evidential fusion based on PCR5 rule in the server. By making use of complementary and redundant information in the images, the framework can solve the problems of information loss, imprecision, inconformity or conflict in multi-source data. The framework is applied to detect a landslide barrier lake with multi-source images from Landsat7 and GF-1, results show that as the amount of operator and input image increases, the proposed framework performs better than commonly used major voting strategy for disaster mapping.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730347","Change detection;multi-source image;evidential fusion;collaborative framework;remote sensing","Collaboration;Remote sensing;Satellites;Earth;Indexes;Data integration;Servers","geomorphology;geophysical image processing;image classification;image fusion;lakes;terrain mapping","collaborative change detection;multisource remote sensing image;multiple operator;change detection framework;preprocessed image;classification result;information loss;multisource data inconformity;landslide barrier lake;GF-1;Landsat7;disaster mapping;information loss;PCR5 rule;evidential fusion;change detection result","","1","","7","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Daff-Net: Dual Attention Feature Fusion Network for Aircraft Detection in Remote Sensing Images","M. Liu; Q. Hu; C. Wang; T. Tian; W. Chen","School of Computer Science, China University of Geosciences, Wuhan, Hubei, China; School of Computer Science, China University of Geosciences, Wuhan, Hubei, China; School of Computer Science, China University of Geosciences, Wuhan, Hubei, China; NA; NA","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4196","4199","Aircraft detection in remote sensing images has always been a research hotspot which has great significance in both civil and military applications. Due to the variations of aircraft types, poses, sizes and complex backgrounds, it is still difficult to effectively and accurately detect aircrafts in remote sensing images. This paper proposes DAFF-Net (Dual Attention Feature Fusion Network), which makes full use of the semantic information of the high-level feature map and the location information of the shallow feature map, and integrates the local features with its global dependency adaptively. Experiments on RSOD aircraft dataset have been implemented, and the results have proved that the detection accuracy of aircraft objects with different scales and densities can all be improved.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554880","National Natural Science Foundation of China(grant numbers:42071339); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554880","aircraft detection;remote sensing;convolutional neural network;dual attention;feature fusion","Image resolution;Convolution;Semantics;Interference;Network architecture;Military aircraft;Feature extraction","aircraft;feature extraction;geophysical image processing;image fusion;object detection;radar imaging;remote sensing","civil applications;military applications;aircraft types;aircrafts;remote sensing images;DAFF-Net;Dual Attention Feature Fusion Network;high-level feature map;shallow feature map;local features;RSOD aircraft dataset;detection accuracy;aircraft objects;daff-Net;aircraft detection","","2","","6","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Faster and Better Instance Segmentation for Large Scene Remote Sensing Imagery","H. Su; P. Huang; J. Yin; X. Zhang","ZheJiang Dahua Technology CO., LTD; ZheJiang Dahua Technology CO., LTD; ZheJiang Dahua Technology CO., LTD; ZheJiang Dahua Technology CO., LTD","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2187","2190","Instance segmentation is difficult to apply into high-precision scenes with limited computational resources, such as unmanned aerial vehicles (UAV), compared to object detection. In this article, a faster and better instance segmentation network (FB-ISNet) based on CondInst is proposed in large scene remote sensing imagery. The FB-ISN et aims to improve efficiency without sacrificing accuracy. First, we adopt the deep layer aggregation as the backbone network to extract feature. Then, the BiFPN is used to overcome the limitation of one-way information flow in FPN and obtain effective multi-scale feature fusion. Next, the detection and instance segmentation heads are optimized to further reduce the amounts of parameters. Finally, the experimental comparisons on the SSDD, NWPU VHR-10 dataset and a large scene remote sensing imagery show that our FB-ISNet can strike a good balance between accuracy and speed, and the comparison with the existing algorithms also demonstrates the superiority of our approach.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883112","Instance segmentation;BiFPN;deep layer aggregation;large scene;remote sensing images","Image segmentation;Head;Semantics;Object detection;Feature extraction;Autonomous aerial vehicles;Sensors","autonomous aerial vehicles;feature extraction;geophysical image processing;image fusion;image segmentation;object detection;remote sensing;remotely operated vehicles","FB-ISNet;FB-ISN et;effective multiscale feature fusion;instance segmentation heads;scene remote sensing imagery show;high-precision scenes;unmanned aerial vehicles","","","","13","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"A Review of Image Fusion Techniques","K. C. Bhataria; B. K. Shah","Computer Engineering Department, Sarvajanik College of Engineering and Technology, Surat, Gujarat, India; Computer Engineering Department, Sarvajanik College of Engineering and Technology, Surat, Gujarat, India","2018 Second International Conference on Computing Methodologies and Communication (ICCMC)","11 Oct 2018","2018","","","114","123","The technique of blending two images or more than two images which produces outcome as the composite fused image. The obtained fused image is the upgraded version of original images because it has all the salient information. The present applications makes majority usage of this fused image to speed up their processing tasks in their respective fields. Recent real-time applications which require image fusion are remote sensing applications, medical applications, surveillance application, photography applications etc. The broad categorization of image fusion techniques are Non-transform domain or spatial domain and Transform domain or frequency domain. This paper initiates with the introduction of image fusion. In the second section it explains the levels of fusion. The third section explains generic image fusion strategy. Further sections elaborates the taxonomy of image fusion techniques and their comparative tables lists all the merits and demerits of the same.","","978-1-5386-3452-3","10.1109/ICCMC.2018.8487686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8487686","Image fusion;Non-transform domain;Transform domain;real-time applications","Image fusion;Transforms;Principal component analysis;Photography;Real-time systems;Remote sensing;Surveillance","image fusion;photography;transforms","image fusion techniques;composite fused image;original images;real-time applications;remote sensing applications;medical applications;surveillance application;photography applications;generic image fusion strategy;Non-transform domain;spatial domain;Transform domain;frequency domain;comparative tables lists","","3","","28","IEEE","11 Oct 2018","","","IEEE","IEEE Conferences"
"Image Fusion Based on Gradient Regularized Convolution Sparse Representation","J. Wang; P. Ren; K. Yang; C. Qin; X. Zhang","Institute of Electronic Information, Northwestern Polytechnic University, Xi’an, China; Institute of Electronic Information, Northwestern Polytechnic University, Xi’an, China; Institute of Electronic Information, Northwestern Polytechnic University, Xi’an, China; Institute of Electronic Information, Northwestern Polytechnic University, Xi’an, China; Institute of Electronic Information, Northwestern Polytechnic University, Xi’an, China","2018 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","27 Jun 2019","2018","","","1","4","An image fusion method based on gradient regularized convolution sparse representation is proposed, which makes up for the shortcoming of conventional method. Target image is composed of optimal high frequency and low frequency by two scale decomposition of source image with sparse optimization function. The high frequency components are obtained by convolution sparse representation model and alternative direction multiplier method, which could raise ability to maintain image details, and low sensitivity to image registration. Optimal low frequency components are obtained with the strategy of maximum or average. Experimental results demonstrate that proposed method has a great improvement in details preserve of image.","2158-6276","978-1-7281-1581-8","10.1109/WHISPERS.2018.8747137","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747137","Convolution sparse representation;Gradient regularized;Alternating direction multiplier","Image fusion;Convolution;High frequency;Image edge detection;Symmetric matrices;Optimization;Indexes","gradient methods;image fusion;image registration;image representation","convolution sparse representation model;alternative direction multiplier method;image registration;gradient regularized convolution sparse representation;image fusion method;source image;sparse optimization","","","","17","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"Multimodal image fusion framework for end-to-end remote sensing image registration","L. Li; L. Han; M. Ding; H. Cao","College of Geological Engineering and Geomatics, Chang’an University, Xian, Shaanxi, CN; School of Land Engineering, Chang’an University, Xi’an, China; College of Geological Engineering and Geomatics, Chang’an University, Xian, Shaanxi, CN; College of Geological Engineering and Geomatics, Chang’an University, Xian, Shaanxi, CN","IEEE Transactions on Geoscience and Remote Sensing","","2023","PP","99","1","1","We formulate the registration as a function that maps the input reference and sensed images to eight displacement parameters between prescribed matching points, as opposed to the usual techniques (feature extraction - description - matching - geometric restrictions). The projection transformation matrix (PTM) is then computed in the neural network and used to warp the sensed image, uniting all matching tasks under one framework. In this paper, we offer a multimodal image fusion network with self-attention to merge the feature representation of the reference and sensed images. The integration information is then utilized to regress the prescribed points’ displacement parameters to get PTM between the reference and sensed images. Finally, PTM is supplied into the spatial transformation network (STN), which warps the sensed image to the same coordinates as the reference image, achieving end-to-end matching. Additionally, a dual-supervised loss function is proposed to optimize the network from both the prescribed point displacement and the overall pixel matching perspectives. The effectiveness of our method is validated by qualitative and quantitative experimental results on multimodal remote sensing image matching tasks. The code is available at: https://github.com/liliangzhi110/E2EIR.","1558-0644","","10.1109/TGRS.2023.3247642","National Natural Science Foundation of China(grant numbers:211035210511); Science and Technology Department of Shaanxi Province(grant numbers:211435220242); China Center for Remote Sensing of Natural Resources Aerial Mapping(grant numbers:42171348); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049547","End-to-end registration;Multimodal fusion;Remote sensing image;Spatial transformer networks","Remote sensing;Feature extraction;Image matching;Image registration;Task analysis;Convolutional neural networks;Image fusion","","","","","","","IEEE","22 Feb 2023","","","IEEE","IEEE Early Access Articles"
"Hyperspectral And Multispectral Image Fusion Based On Deep Attention Network","Q. Yang; Y. Xu; Z. Wu; Z. Wei","Nanjing University of Science and Technology, Nanjing, China; Nanjing University of Science and Technology, Nanjing, China; Nanjing University of Science and Technology, Nanjing, China; Nanjing University of Science and Technology, Nanjing, China","2019 10th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","5 Dec 2019","2019","","","1","5","Hyperspectral (HS) images have rich spectral information and can provide attribute information. High spatial resolution images, such as multispectral (MS) images and panchromatic (PAN) images, can provide fine geometric features. Thus, the fusion of the two images can achieve information complementarity and increase the accuracy and reliability of information. In this paper, we propose a hyperspectral and multispectral image fusion method based on deep attention network. Our model consists of two parts. One is the fusion network, which is used to fuse images. The other part is the spatial attention network, which is used to extract tiny textures and enhance the spatial structure. Experimental results compared with some state-of-the-art methods illustrate that our method is outstanding in both visual and numerical results.","2158-6276","978-1-7281-5294-3","10.1109/WHISPERS.2019.8920825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8920825","Hyperspectral image;multi-spectral image;image fusion;spatial attention;deep learning","Feature extraction;Spatial resolution;Image reconstruction;Hyperspectral sensors;Machine learning","image fusion;image resolution","high spatial resolution images;panchromatic images;fine geometric features;information complementarity;hyperspectral image fusion method;multispectral image fusion method;deep attention network;fusion network;spatial attention network;spatial structure;hyperspectral images;rich spectral information;attribute information","","6","","14","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Remote Sensing Image Segmentation Based on U-shaped Network with Atrous Spatial Pyramid","Y. Hou; L. Zhu; Q. Chen","Wuhan University of Science and Technology, Wuhan, China; Wuhan University of Science and Technology, Wuhan, China; Wuhan University of Science and Technology, Wuhan, China","2020 39th Chinese Control Conference (CCC)","9 Sep 2020","2020","","","7335","7339","Semantic segmentation in remote sensing images has always been an important research direction of computer vision, and is widely used in land and resources related fields such as land mapping, hydrology and environmental testing, urban and rural construction. This paper focuses on remote sensing image segmentation problem, which is still challenging for current semantic segmentation networks. To address this problem, this paper takes the DeepLabv3+[1] network as the baseline structure, and attaches a multi-level feature fusion module to well utilize the low-level information of the network. The proposed network performs better on multiple evaluations such as Precision, Recall, and Mean Absolute Error (MAE) compared to several state-of-the-art methods, and is able to extract details of the segmentation more precisely.","1934-1768","978-9-8815-6390-3","10.23919/CCC50068.2020.9188853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9188853","Semantic Segmentation;Remote Sensing Image;DeepLabv3+;FAM;MAE","Convolution;Image segmentation;Remote sensing;Feature extraction;Semantics;Decoding;Spatial resolution","computer vision;geophysical image processing;image fusion;image segmentation;neural nets;remote sensing","urban construction;rural construction;remote sensing image segmentation problem;semantic segmentation networks;multilevel feature fusion module;low-level information;U-shaped network;atrous spatial pyramid;computer vision;land mapping;environmental testing;hydrology;DeepLabv3+ network","","1","","11","","9 Sep 2020","","","IEEE","IEEE Conferences"
"An Effective Attention-Guided Feature Fusion Network for Segmentation of Remote Sensing Imagery","M. Zhu; J. Lu","School of Computer Science and engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and engineering, Nanjing University of Science and Technology, Nanjing, China","2020 12th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA)","30 Mar 2020","2020","","","269","274","Recently, the features learned by FCN-based image segmentation methods are usually not efficient enough to the remote sensing imagery. One major reason is that the loss of detailed features high-level layers in these models. An another reason is that the low-level layers lack semantic information. So we propose feature fusion network for segmentation of remote sensing images. To tackle the relatively problems we introduce our network which contains two structures: enhance network and fusion network. The enhance network devotes to capture more effective low-level features, especially small object and boundary features. And the fusion network devotes to fuse semantic information into low-level features and fuse more detail information into high-level features. Based on our proposed feature fusion network, a large number of experiments show that our proposed method performs well on many datasets, especially in segmentation of small target and boundary.","2157-1481","978-1-7281-7081-7","10.1109/ICMTMA50254.2020.00067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050315","semantic segmentation;deep learning;remote sensing image;small object","Image segmentation;Mechatronics;Automation;Fuses;Semantics;Sensors;Remote sensing","feature extraction;geophysical image processing;image fusion;image segmentation;learning (artificial intelligence);remote sensing","high-level layer features;attention-guided feature fusion network;low-level layer features;remote sensing images;semantic information;FCN-based image segmentation methods","","1","","22","IEEE","30 Mar 2020","","","IEEE","IEEE Conferences"
"Cloud Detection of Remote Sensing Image Based on Multi Feature Fusion","Z. Ning; W. Wei; S. Qin; Y. Chengzong; Z. Xinzhong","Shanghai Aerospace Electronic Technology Institute, Shanghai, China; Shanghai Aerospace Electronic Technology Institute, Shanghai, China; Shanghai Aerospace Electronic Technology Institute, Shanghai, China; Shanghai Aerospace Electronic Technology Institute, Shanghai, China; Shanghai Aerospace Electronic Technology Institute, Shanghai, China","2020 5th IEEE International Conference on Big Data Analytics (ICBDA)","27 May 2020","2020","","","298","303","The existing cloud detection algorithms mainly include multi-channel threshold method and cloud detection method based on image features. The multi-channel threshold method is usually only for the reflectance of remote sensing image. The method based on image features is not enough for information mining of remote sensing image, and the feature expression ability is not strong enough, which results in the problems of low accuracy and poor robustness of the algorithm. In this paper, we propose to extract point features and block features from different forms of cloud to fuse multiple features, so as to achieve the features with good expression ability. Combined with active learning of support vector machine, more representative samples are fully selected for training, so as to achieve a higher classification accuracy rate in the case of fewer samples. The experimental results show that the cloud detection method proposed in this paper has high accuracy, good robustness and fast computing speed, so it can adapt to the application of satellite platform.","","978-1-7281-4111-4","10.1109/ICBDA49040.2020.9101218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9101218","Feature fusion;Remote sensing image;Cloud detection;Active learning","Feature extraction;Clouds;Cloud computing;Remote sensing;Reflectivity;Ice;Satellites","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);remote sensing;support vector machines","remote sensing image;multichannel threshold method;cloud detection method;image features;feature expression ability;point features;block features;multifeature fusion;information mining;active learning;support vector machine;satellite platform","","1","","12","IEEE","27 May 2020","","","IEEE","IEEE Conferences"
"The Kohonen map for credal fusion of heterogeneous data","I. Hammami; G. Mercies; A. Hamouda","Faculé des Sciences de Tunis, Université de Tunis El Manar, Tunis, Tunisie; IMT Atlantique Bretagne-Pays de Loire, Brest, Bretagne, FR; Faculé des Sciences de Tunis, Université de Tunis El Manar, Tunis, Tunisie","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","2947","2950","Fusion of optical and radar remote sensing data is an important and challenging task for many applications such as multisource classification, due to the highly heterogeneous nature of the information they contain. The aim of this paper is to propose a novel strategy for multisensor optical-radar data classification in the evidential framework, which is well known for its ability to deal with imperfect information of each sensor and for its ability to exploit the redundancies and the complementarities between fused data and between sensors. A major contribution of our work is the use of Kohonen's map to model the input evidences of radar sensor since the optical sensor was addressed in our previous works. Also a specific design is presented here for fusing those heterogeneous data. First experiments show the effectiveness of the proposed approach.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326433","Dempster-Shafer Theory;remote sensing;image fusion;radar and optical images","Laser radar;Optical imaging;Optical sensors;Radar imaging;Adaptive optics;Measurement","geophysical image processing;image classification;image fusion;inference mechanisms;optical sensors;remote sensing by laser beam;remote sensing by radar","optical remote sensing data fusion;radar remote sensing data fusion;multisource classification;multisensor optical-radar data classification;Kohonen map;radar sensor;optical sensor;heterogeneous data;credal fusion","","1","","8","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"LSNET: Extremely Light-Weight Siamese Network for Change Detection of Remote Sensing Image","B. Liu; H. Chen; Z. Wang; W. Xie; L. Shuai","School of Resources and Environment, University of Electronic Science and Technology of China; School of Resources and Environment, University of Electronic Science and Technology of China; Novel Product R&D Department, Truly Opto-Electronics Co., Ltd., Shanwei, China; School of Resources and Environment, University of Electronic Science and Technology of China; School of Resources and Environment, University of Electronic Science and Technology of China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2358","2361","The Siamese network is becoming the mainstream in change detection of remote sensing images (RSI). However, in recent years, the development of more complicated structure, module and training processe has resulted in the cumbersome model, which hampers their application in large-scale RSI processing. To this end, this paper proposes an extremely lightweight Siamese network (LSNet) for RSI change detection, which replaces standard convolution with depthwise separable atrous convolution, and removes redundant dense connections, retaining only valid feature flows while performing Siamese feature fusion, greatly compressing parameters and computation amount. Compared with the first-place model on the CCD dataset, the parameters and the computation amount of LSNet is greatly reduced by 90.35% and 91.34% respectively, with only a 1.5% drops in accuracy.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884446","remote sensing image;change detection;lightweight;Siamese network","Charge coupled devices;Training;Image coding;Convolution;Computational modeling;Feature extraction;Computational efficiency","feature extraction;geophysical image processing;image classification;image fusion;image representation;image retrieval;learning (artificial intelligence);remote sensing;visual databases","LSNET;extremely light-weight Siamese network;remote sensing image;training processe;large-scale RSI processing;extremely lightweight Siamese network;LSNet;RSI change detection;depthwise separable atrous convolution","","2","","15","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Decision Calibration Network for Semantic Labeling of High-Resolution Remote Sensing Images","H. Bai; J. Chen; Q. Wang; C. He","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; Sichuan Jiuzhou Eletric Group Co., Ltd, Mianyang, China","2022 3rd International Conference on Geology, Mapping and Remote Sensing (ICGMRS)","18 Aug 2022","2022","","","542","545","Semantic labeling of high-resolution remote sensing images is a challenging task, requiring the models to effectively distinguish different classes of ground objects while learning advanced feature representations. First of all, we propose a dual-decoder semantic labeling neural network based on the atrous spatial pyramid pooling module and attention mechanism to achieve the high-precision classification of different ground objects. The main idea is to enhance the high-level feature representation by using the complementary relationship that may exist between different decoders. Furthermore, based on this network structure, a decision calibration auxiliary loss is proposed to improve the models’s ability to classify examples of highly ambiguous output by different decoders. Finally, we conduct experimental verification on the ISPRS Vaihingen and Potsdam datasets, and the results show that the auxiliary loss can effectively improve the classification accuracy of the model.","","978-1-6654-8595-1","10.1109/ICGMRS55602.2022.9849228","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9849228","semantic labeling;high-resolution remote sensing images;dual-decoder;auxiliary loss","Geology;Semantics;Neural networks;Force;Feature extraction;Decoding;Calibration","feature extraction;geophysical image processing;image classification;image fusion;image representation;image segmentation;learning (artificial intelligence);neural nets;object detection;remote sensing","decision calibration network;high-resolution remote sensing images;feature representations;dual-decoder semantic labeling neural network;atrous spatial pyramid pooling module;attention mechanism;high-precision classification;different ground objects;high-level feature representation;different decoders;network structure;decision calibration auxiliary loss;highly ambiguous output","","","","12","IEEE","18 Aug 2022","","","IEEE","IEEE Conferences"
"Satellite Image Fusion for Obtaining High Resolution Images Using Deep Neural Network","A. N. Rahman; V. Tripathiy; A. D. Gupta; B. Paul; M. T. Kurian; V. P. Vijayan","Innovation and Research Society, Mavelipuram, Kerala, India; Department of Computer Science and Engineering, Graphic Era Deemed to be University, Dehradun, Uttarakhand, India; Department of Geography, Chandernagore Government College, Hugli, West Bengal, India; Department of Information Technology, Rajagiri School of Engineering and Technology, Cochin, Kerala, India; Department of Electronics and Communication Engineering, Baselios Mathews II College of Engineering, Sasthamkotta, Kerala, India; Mangalam College of Engineering, Kottayam, Kerala, India","2022 International Conference on Intelligent Innovations in Engineering and Technology (ICIIET)","5 Dec 2022","2022","","","301","306","Due to its critical function in a wide range of applications, scene categorization of high-resolution remote sensing (RS) photos has drawn increasing attention. A technique for spatiotemporal fusion using deep neural networks (DNNs) with a large amount of remote sensing data as the application background. An innovative multispectral image fusion architecture is proposed in this paper. The proposed method for fusing satellite images entails two phases, each using two neural networks. In the first stage, an adaptively weighted injection-based joints detailed approach to remotely sensed image fusion is discussed. Multispectral (MS) and panchromatic (PAN) images are used to extract spatial features using a wavelet transform. In contrast to the conventional detail injection technique, dictionary learning from the sub-images themselves is used to construct the primary joint details by sparsely representing the extracted features. To minimize spectrum distortions in the fused images while keeping spatial information, we implemented a unique loss function for this DNN. This network is known as the ’Spectral Reimbursement Network (SRN).’ Finally, using three datasets, full-reference, and limited-reference criterion, the proposed strategy is compared against several state-of-the-art methods. Experiment findings demonstrate that the suggested technique can compete in both spatial and spectral parameters.","","978-1-6654-5653-1","10.1109/ICIIET55458.2022.9967537","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9967537","Spectral reimbursement network;fused MS image;Deep neural network;Satellite image","Deep learning;Wavelet transforms;Technological innovation;Satellites;Neural networks;Feature extraction;Spatiotemporal phenomena","deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image fusion;image representation;image resolution;remote sensing;wavelet transforms","adaptively weighted injection-based joints detailed approach;critical function;deep neural Network;deep neural networks;high resolution images;high-resolution remote sensing photos;loss function;multispectral image fusion architecture;panchromatic images;primary joint details;remote sensing data;satellite image fusion;satellite images;scene categorization;spatial parameter;spatial features;spatiotemporal fusion;spectral parameter;spectral reimbursement network;spectrum distortions","","","","11","IEEE","5 Dec 2022","","","IEEE","IEEE Conferences"
"Improving the spatial resolution of hyperspectral image using panchromatic and multispectral images: An integrated method","X. Meng; H. Shen; H. Li; Q. Yuan; H. Zhang; L. Zhang","School of Resource and Environmental Sciences, Wuhan University, P. R. China; School of Resource and Environmental Sciences, Wuhan University, P. R. China; School of Resource and Environmental Sciences, Wuhan University, P. R. China; School of Geodesy and Geomatics, Wuhan University, P. R. China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, P. R. China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, P. R. China","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","23 Oct 2017","2015","","","1","4","Hyperspectral (HS) images have been extensively used in various fields; however, the low spatial resolution restricts its application. In this paper, an integrated HS image fusion method is proposed. An integrated model is established to express the relationships between the desired image and multi-source high-spatial-resolution observations. Maximum a posteriori (MAP) framework is employed to formulate the fusion model. On one hand, the proposed integrated HS fusion method can take full advantage of the more complementary spatial information from multi-source sensors to enhance the HS images. On the other hand, the complementary information of the high-spatial-resolution observations on spectral range is fully considered to maximize the spatial and spectral fidelity of all the HS bands. Furthermore, the proposed method is able to fuse the HS images for the challenging large-spatial-resolution-difference ratio. The HYDICE simulated and the ETM+ panchromatic (PAN), multispectral (MS), MODIS real datasets are used to verify the effectiveness of the proposed method.","2158-6276","978-1-4673-9015-6","10.1109/WHISPERS.2015.8075387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075387","Hyperspectral;integrated image fusion;multi-source observations;spectral range;large ratio","MODIS;Spatial resolution;Hyperspectral imaging;Image sensors;Sensors","hyperspectral imaging;image fusion;image resolution;remote sensing","multisource sensors;high-spatial-resolution observations;hyperspectral image;maximum a posteriori framework;hyperspectral fusion method;integrated hyperspectral image fusion method;panchromatic image;multispectral image;ETM+ panchromatic dataset;MODIS dataset;multispectral dataset","","7","","11","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Semantic Vehicle Segmentation in Very High Resolution Multispectral Aerial Images Using Deep Neural Networks","N. Merkle; S. M. Azimi; S. Pless; F. Kurz","Remote Sensing Technology Institute, German Aerospace Center (DLR), Oberpfaffenhofen; Remote Sensing Technology Institute, German Aerospace Center (DLR), Oberpfaffenhofen; Institute of Optical Sensor Systems, German Aerospace Center (DLR), Berlin-Adlershof; Remote Sensing Technology Institute, German Aerospace Center (DLR), Oberpfaffenhofen","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","5045","5048","The fusion of complementary information from co-registered multi-modal image data enables a more detailed and more robust understanding of an image scene or specific objects, and is important for several applications in the field of remote sensing. In this paper, the benefits of combining RGB, near infrared (NIR) and thermal infrared (TIR) aerial images for the task of semantic vehicle segmentation through deep neural networks are investigated. Therefore, RGB, NIR and TIR image triplets acquired by the Modular Aerial Camera System (MACS) are precisely co-registered through the application of a virtual camera system and subsequently used for the training of different neural network architectures. Various experiments were conducted to investigate the influence of the different sensor characteristics and an early or late fusion within the network on the quality of the segmentation results.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898513","Aerial Imagery;Data Fusion;Deep Learning;Multispectral Imagery;Vehicle Segmentation","Cameras;Image segmentation;Feature extraction;Training;Remote sensing;Decoding;Semantics","cameras;geophysical image processing;image classification;image colour analysis;image fusion;image registration;image resolution;image segmentation;infrared imaging;neural nets;remote sensing","semantic vehicle segmentation;deep neural networks;complementary information;co-registered multimodal image data;detailed understanding;image scene;remote sensing;RGB;virtual camera system;neural network architectures;early fusion;late fusion;modular aerial camera system;high resolution multispectral aerial images","","","","7","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Component Substitution Based Fusion of WorldView Imagery","V. R. Pandit; R. J. Bhiwani","Engineering & Technology, Sant Gadge Baba Amravati University, Amravati, Maharashtra, India; Dept. of Electronics & Telecomm. Engineering, Babasaheb Naik College of Engineering, Pusad, Maharashtra, India","2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT)","30 Dec 2019","2019","","","1","7","The fusion of multispectral and panchromatic images acquired on common location to form resultant image that features higher spectral as well as higher spatial resolutions is also referred to as ‘Pansharpening’. In the recent past, many such fusion algorithms belonging to various approaches have been proposed. In this paper, the authors compare performances of the most popular image fusion algorithms using Component Substitution approach. Selected algorithms are applied to two real datasets, acquired by WorldView-3 and WorldView-4 satellite sensors. The performances are evaluated based on popular image quality metrics. This performance analysis will surely help in evaluation of the best possible trade-off in between spectral resolution and spatial resolution for the fused image by selected algorithms.","","978-1-5386-5906-9","10.1109/ICCCNT45670.2019.8944532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8944532","Remote sensing;image fusion;pansharpening;panchromatic image;multispectral image;component substitution","Spatial resolution;Image fusion;Principal component analysis;Transforms;Satellites;Sensors","geophysical image processing;image fusion;image resolution;remote sensing","WorldView imagery;multispectral images;panchromatic images;higher spatial resolutions;image fusion algorithms;Component Substitution approach;WorldView-3;WorldView-4 satellite sensors;popular image quality metrics;spectral resolution;spatial resolution;fused image","","3","","23","IEEE","30 Dec 2019","","","IEEE","IEEE Conferences"
"Comparative analysis of different fusion rules for SAR and multi-spectral image fusion based on NSCT and IHS transform","X. J. Chong; C. Xuejiao","College of Earth Science and Engineering, Hohai University, Nanjing, China; College of Earth Science and Engineering, Hohai University, Nanjing, China","2015 International Conference on Computer and Computational Sciences (ICCCS)","21 Dec 2015","2015","","","271","274","In order to improve the fusion quality of SAR and multi-spectral image, this paper proposes an image fusion method based on nonsubsampled contourlet transform (NSCT) and IHS transform. Since the fusion rule plays a very important role during the fusion process, four fusion rules are analyzed and compared. Three fusion rules are commonly used in previous works and a new fusion rule is proposed in this paper. To evaluate the performance of different fusion rules, fusion experiments are carried on COSMO-SkyMed SAR and Landsat OLI image. The experimental results indicate that the proposed rule is more effective than the other three regular fusion rules.","","978-1-4799-1819-5","10.1109/ICCACS.2015.7361364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7361364","Iimage fusion;NSCT;IHS transform;fusion rule","Transforms;Synthetic aperture radar;Image fusion;Remote sensing;Satellites;Filter banks;Image resolution","image fusion;radar imaging;synthetic aperture radar;transforms","multispectral image fusion;nonsubsampled contourlet transform;NSCT;intensity hue saturation;IHS transform;fusion rule;COSMO-SkyMed SAR;Landsat OLI image","","1","","11","IEEE","21 Dec 2015","","","IEEE","IEEE Conferences"
"Panchromatic and multispectral images fusion using sparse representation","M. Ghamchili; H. Ghassemian","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2017 Artificial Intelligence and Signal Processing Conference (AISP)","26 Mar 2018","2017","","","80","84","In this paper, we propose a new pansharpening method based on sparse representation theory to fuse panchromatic and multispectral images. In the proposed method, the high-resolution multispectral image is reconstructed by adding some details to the multispectral image. The details are achieved directly by a proper dictionary which is constructed using a high pass version of the panchromatic image, so-called `detail dictionary', and proper sparse coefficients. The required atoms for generating the details are chosen by two objective functions. One of these functions chooses atoms having high spatial information and the other one selects atoms with high spectral information. Then, the details are made from a linear combination of these atoms. We use both sets of the atoms to increase the spatial details and decrease the spectral distortion. In order to investigate the efficiency of the proposed method, two datasets from Pleiades and WorldView-2 satellites are used. Based on the experimental results, it is found that the proposed method performs better than the state-of-the-art methods in maintaining of spectral information as well as increasing spatial details objectively and visually.","","978-1-5386-2585-9","10.1109/AISP.2017.8324113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8324113","Dictionary learning;image fusion;detail dictionary;approximation dictionary;sparse coefficient;remote sensing;pansharpening","Dictionaries;Spatial resolution;Distortion;Image fusion;Satellites;Remote sensing","geophysical image processing;image fusion;image representation;image resolution;remote sensing","sparse coefficients;WorldView-2 satellites;high-resolution multispectral image;sparse representation theory;pansharpening method;multispectral images fusion;panchromatic images fusion;state-of-the-art methods;spatial details;high spectral information;high spatial information;objective functions;detail dictionary;panchromatic image;high pass version;proper dictionary","","4","","28","IEEE","26 Mar 2018","","","IEEE","IEEE Conferences"
"Spatiotemporal scene interpretation of space videos via deep neural network and tracklet analysis","L. Mou; X. X. Zhu","Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Germany","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","1823","1826","Spaceborne remote sensing videos are becoming indispensable resources, opening up opportunities for new remote sensing applications. To exploit this new type of data, we need sophisticated algorithms for semantic scene interpretation. The main difficulties are: 1) Due to the relatively poor spatial resolution of the video acquired from space, moving objects, like cars, are very difficult to detect, not to mention track; 2) camera movement handicaps scene interpretation. To address these challenges, in this paper we propose a novel framework that fuses multispectral images and space videos for spatiotemporal analysis. Taking a multispectral image and a spaceborne video as input, an innovative deep neural network is proposed to fuse them in order to achieve a fine-resolution spatial scene labeling map. Moreover, a sophisticated approach is proposed to analyze activities and estimate traffic density from 150,000+ tracklets produced by a Kanade-Lucas-Tomasi keypoint tracker. The proposed framework is validated using data provided for the 2016 IEEE GRSS data fusion contest, including a video acquired from the International Space Station and a DEIMOS-2 multispectral image. Both visual and quantitative analysis of the experimental results demonstrates the effectiveness of our approach.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729468","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729468","space videos;scene labeling;deep learning;activity analysis;traffic density estimation","Videos;Labeling;Tracking;Estimation;Satellites;Remote sensing;Semantics","geophysical image processing;image fusion;neural nets;remote sensing;road traffic","spatiotemporal scene interpretation;deep neural network;tracklet analysis;spaceborne remote sensing video;remote sensing application;semantic scene interpretation;spatial video resolution;moving object;camera movement handicap;spatiotemporal analysis;spaceborne video;fine-resolution spatial scene labeling map;traffic density;traffic activity;Kanade-Lucas-Tomasi keypoint tracker;AD 2016;GRSS data fusion contest;international space station;DEIMOS-2 multispectral image","","32","","8","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Stepwise Refinement Of Low Resolution Labels For Earth Observation Data: Part 1","D. Cerra; N. Merkle; C. Henry; K. Alonso; P. d’Angelo; S. Auer; R. Bahmanyar; X. Yuan; K. Bittner; M. Langheinrich; G. Zhang; M. Pato; J. Tian; P. Reinartz","German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","27 Sep 2021","2020","","","7054","7057","This paper describes the contribution of the DLR team ranking 3rd in Track 1 of the 2020 IEEE GRSS Data Fusion Contest, with results ranking 2nd in Track 2 of the same contest being reported in a companion paper. The classifications are based on refinements of low-resolution MODIS labeling using available higher resolution Sentinel-1 and Sentinel-2 data. Results are initialized with a handcrafted decision tree integrating output from a random forest classifier, and subsequently boosted by detectors for specific classes.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9547213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547213","","Earth;Data integration;Geoscience and remote sensing;Detectors;Labeling;Decision trees;Spatial resolution","geophysical image processing;image classification;image fusion;image resolution;terrain mapping","stepwise refinement;low resolution labels;Earth observation data;DLR team;2020 IEEE GRSS Data Fusion Contest;Sentinel-1 data;Sentinel-2 data;handcrafted decision tree","","2","","8","IEEE","27 Sep 2021","","","IEEE","IEEE Conferences"
"Gabor Wavelet Based Feature Extraction and Fusion for Hyperspectral and Lidar Remote Sensing Data","S. Jia; M. Zhang; J. Zhu","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1","4","In recent years, it has been found that the fusion processing of remote sensing data produced by multiple sensors is often effective for material classification. Specifically, the joint use of hyperspectral image (HSI) and Light Detection And Ranging (LiDAR) data for classification has been an active topic of research in remote sensing field. Since hyperspectral and LiDAR data provide complementary information (spectral reflectance, and vertical structure, respectively), one promising and challenging approach is to fuse these data in the information extraction procedure. In this paper, we propose an efficient feature extraction and fusion method based on Gabor wavelet, leading to a fusion of the spectral, spatial and elevation data. The core idea of the proposed fusion approach is stacking elevation and intensity data of LiDAR as additional channels to spectral bands. Our strategies are based on the Gabor feature stack structure, which are natural and effective. The features extracted by Gabor wavelets have proved to be discriminant features when considered for thematic classification in remote sensing applications especially when dealing with hyperspectral images due to their ability to extract joint spatial and spectrum information from HSI. Experimental results on the real hyperspectral image data have shown the better discriminative power of our approach for classification.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518351","Hyperspectral image;LiDAR;Gabor wavelets;Feature extraction;Classification","Feature extraction;Laser radar;Hyperspectral imaging;Training;Sensors","feature extraction;geophysical image processing;image classification;image fusion;optical radar;remote sensing;remote sensing by laser beam;wavelet transforms","joint spatial;spectrum information;hyperspectral image data;remote sensing applications;thematic classification;discriminant features;Gabor feature stack structure;spectral bands;intensity data;fusion approach;spatial elevation data;spectral elevation data;fusion method;efficient feature extraction;information extraction procedure;vertical structure;spectral reflectance;LiDAR data;remote sensing field;Ranging data;Light Detection;HSI;material classification;multiple sensors;fusion processing;lidar remote sensing data;Gabor wavelet","","","","13","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Remote Sensing Object Detection Method Based on Attention Mechanism and Multi-scale Feature Fusion","Y. Liu; Y. Xiao","School of Automation and Electronic Information, Xiangtan University, Xiangtan, China; School of Automation and Electronic Information, Xiangtan University, Xiangtan, China","2022 41st Chinese Control Conference (CCC)","11 Oct 2022","2022","","","7155","7160","Aiming at the low target detection accuracy due to the complex imaging background, small size, large number and dense arrangement of targets in optical remote sensing images, this paper was proposed an improved Faster R-CNN algorithm. Firstly, the attention mechanism module is reasonably embedded in the backbone network to emphasize the target information and suppress the background information; Secondly, a feature fusion method is designed to fuse the information of each feature layer in the backbone network to improve the small target detection ability; In addition, the loss function and pooling function are improved and the Anchor parameters are optimized. It can be seen from the experimental data that the detection accuracy of the method proposed in this paper is significantly improved compared with the original algorithm, and the detection accuracy is more dominant than other mainstream algorithms.","1934-1768","978-988-75815-3-6","10.23919/CCC55666.2022.9902724","National Natural Science Foundation(grant numbers:61773330); National Natural Science Foundation(grant numbers:2017JJ2251); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9902724","remote sensing image;attention mechanism;feature fusion;the loss function","Fuses;Object detection;Feature extraction;Optical imaging;Optical sensors;Remote sensing","feature extraction;geophysical image processing;image fusion;image segmentation;object detection;remote sensing;target tracking","attention mechanism module;backbone network;background information;complex imaging background;feature fusion method;feature layer;improved Faster R-CNN algorithm;loss function;low target detection accuracy;mainstream algorithms;multiscale feature fusion;optical remote sensing images;pooling function;sensing object detection method;target detection ability;target information","","","","14","","11 Oct 2022","","","IEEE","IEEE Conferences"
"Fusion of multi-scale hyperspectral and lidar features for tree species mapping","W. Liao; F. Van Coillie; L. Li; B. Zhao; L. Gao; W. Philips; B. Zhang","Department of Telecommunications and Information Processing, Ghent University, Ghent, Belgium; Department of Forest and Water Management, Gent University, Ghent, Belgium; State Key Laboratory of Remote Sensing Science, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Remote Sensing Science, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Remote Sensing Science, Chinese Academy of Sciences, Beijing, China; Department of Telecommunications and Information Processing, Ghent University, Ghent, Belgium; State Key Laboratory of Remote Sensing Science, Chinese Academy of Sciences, Beijing, China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","2879","2882","The added value of multiple data sources on tree species mapping has been widely analyzed. In particular, fusion of hyperspectral (HS) and LiDAR sensors for forest applications is a very hot topic. In this paper, we exploit the use of multi-scale features to fuse HS and LiDAR data for tree species mapping. Hyperspectral data is obtained from the APEX sensor with 286 spectral bands. LiDAR data has been acquired with a TopoSys sensor Harrier 56 at full waveform. We generate multi-scale features on both HS and LiDAR data, by considering the diameter and the height layer of different tree species. Experimental results on a forested area in Belgium demonstrate the effectiveness of using multi-scale features for fusion of HS image and LiDAR data both visually and quantitatively.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127599","Data fusion;remote sensing;hyperspectral image;LiDAR data;graph-based","Laser radar;Vegetation;Hyperspectral imaging;Sensors;Fuses","geophysical image processing;image classification;image fusion;optical radar;remote sensing by laser beam;remote sensing by radar;vegetation;vegetation mapping","multiple data sources;tree species mapping;multiscale features;LiDAR data;hyperspectral data;TopoSys sensor Harrier 56;lidar features;tree species;APEX sensor;HS image","","","","11","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"A Dynamic End-to-End Fusion Filter for Local Climate Zone Classification Using SAR and Multi-Spectrum Remote Sensing Data","P. Feng; Y. Lin; G. He; J. Guan; J. Wang; H. Shi","State Key Laboratory of Space-Ground Integrated Information Technology, CAST, Beijing, China; Group of Intelligent Signal Processing, Harbin Engineering University, Harbin, China; State Key Laboratory of Space-Ground Integrated Information Technology, CAST, Beijing, China; Group of Intelligent Signal Processing, Harbin Engineering University, Harbin, China; State Key Laboratory of Space-Ground Integrated Information Technology, CAST, Beijing, China; State Key Laboratory of Space-Ground Integrated Information Technology, CAST, Beijing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","4231","4234","Local Climate Zone (LCZ) classification is potentially popular because of its extensive applications. Recently, data from different remote sensors including synthetic aperture radar (SAR) and multi-spectrum are employed for LCZ classification. However, different bands in SAR and multi-spectrum are difficult to fuse because of their various physical properties. In this paper, an dynamic end-to-end fusion filter is proposed. Firstly, a convolutional neural network (CNN) based dynamic filter network (DFN) is introduced to integrate different bands in SAR and multi-spectrum data, which enhances the fusion accuracy by a flexible dynamic operation. Then the filter is used for feature extraction, hence improve the performance of the classifier. The proposed method is evaluated using Sentinel-1 and Sentinel-2 dataset and the improvement of accuracy shows the superiority of the proposed dynamic data fusion approach.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324427","National Natural Science Foundation of China(grant numbers:61806018,41801291); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324427","Dynamic filter network;Local Climate Zone Classification;Multi-spectrum;End-to-end;Fusion","Feature extraction;Synthetic aperture radar;Meteorology;Remote sensing;Fuses;Convolution;Vegetation","atmospheric techniques;atmospheric temperature;convolutional neural nets;feature extraction;geophysical image processing;image classification;image filtering;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar","SAR;LCZ classification;dynamic end-to-end fusion filter;dynamic filter network;flexible dynamic operation;dynamic data fusion approach;multispectrum remote sensing data;Local Climate Zone classification;remote sensors;feature extraction;Sentinel-1 and Sentinel-2 dataset","","1","","13","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Multispectral and Panchromatic Image Fusion Via Convolution Sparse Coding with Joint Sparsity","F. Zhang; K. Zhang","State Key Laboratory of Geo-information Engineering, Xi'an, China; School of Information Science and Engineering, Shandong Normal University, Ji'nan, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","629","632","In this paper, a low spatial resolution multispectral (LR MS) and panchromatic (PAN) image fusion method based on convolution sparse coding (CSC) is proposed to model the global structures existing in source images. In the proposed method, CSC is adopted to decompose the high frequency (HF) component properly and joint sparse prior is also used to capture the correlation in the bands of MS images. By joint sparsity, the correlation is further inherited into their corresponding feature maps. Then, the spatial information in LR MS image is enhanced well after detailed fusion rule for spatial details. Finally, the fusion image is reconstructed by the fused low frequency and HF. The experimental results on real datasets from QuickBird and Geoeye-1 satellites verify that the proposed method can better preserve the spatial and spectral information in the fused images.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324250","Open Fund of State Key Laboratory of Geo-information Engineering(grant numbers:SKLGIE2018-M-3-2); Natural Science Foundation of China(grant numbers:61901246,U1736122); China Postdoctoral Science Foundation(grant numbers:2019TQ0190,2019M662432); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324250","Multispectral;panchromatic;image fusion;convolution sparse coding;joint sparse prior","Hafnium;Satellites;Spatial resolution;Correlation;Convolution;Image reconstruction;Image coding","geophysical image processing;image coding;image fusion;image resolution;remote sensing","spatial information;spectral information;convolution sparse coding;joint sparsity;CSC;LR MS image;detailed fusion rule;fusion image;QuickBird satellites;Geoeye-1 satellites","","","","14","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Evaluate the contribution of SAR data in improving the impervious surfaces extraction at varying scales","Ru Xu; Hongsheng Zhang; Hui Lin","Chinese University of Hong Kong, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China","2016 4th International Workshop on Earth Observation and Remote Sensing Applications (EORSA)","29 Aug 2016","2016","","","280","283","Multiple sources of remote sensing data have an increasingly wide use as an effective and comprehensive approach in many remote sensing applications, such as the estimation of impervious surface. Impervious surface is an important indicator of most urban-related environment, such as non-point source pollution, urban heat island and urban climate. However, accurate mapping of urban impervious surfaces is still challenging due the diversity of land cover materials. Among various studies, the synergistic use of optical and SAR data is one of the most challenging issues due to the differences of optical and SAR remote sensing techniques. Numerous researches have shown the good outcomes of ISE from fusion of optical and SAR data, while the potential of combining optical and SAR data was still not fully explored. In this study, the scale effects of combining optical and SAR data for ISE were investigated. Experiments will be carried out with satellite data of TM, TM+ASAR and TM + TerraSAR-X data obtained in November 2008 in the city of Shenzhen, China. The processing steps include: 1) Classifying the remote sensing image into 6 types, including water, soil, vegetation, shadow, dark impervious surface (DIS) and bright impervious surface (BIS) using TM, TM + ASAR and TM+TerraSAR-X data respectively; 2) Combining the DIS and BIS as impervious surfaces; 3) Calculating the impervious surfaces percentage (ISP) using grids with different sizes (30, 60, 90, 120….6000 m respectively), and 4) Evaluating the ISE with the indicator of correlation coefficient (CC) to compare the difference of results between different data type. The CC was employed to quantitatively evaluate the ISE accuracy based on different scales (grid sizes) from data combination of optical and SAR data. Preliminary experiments indicated that the ISE results derived from fusion data is better compared with single optical data and the ISP was different under different grid sizes.","","978-1-5090-1479-8","10.1109/EORSA.2016.7552813","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552813","classification;impervious surface;scale effect","Image resolution;Earth;Synthetic aperture radar;Google;Manuals;Satellites;Remote sensing","feature extraction;image fusion;radar imaging;remote sensing by laser beam;remote sensing by radar;synthetic aperture radar","impervious surfaces extraction;remote sensing data;urban-related environment;urban impervious surfaces;optical data;SAR data;optical remote sensing techniques;SAR remote sensing techniques;ISE;satellite data;remote sensing image;dark impervious surface;DIS;bright impervious surface;BIS;impervious surfaces percentage;ISP;correlation coefficient;fusion data","","","","7","IEEE","29 Aug 2016","","","IEEE","IEEE Conferences"
"A Remote Sensing Spatiotemporal Fusion Model of Landsat and Modis Data via Deep Learning","P. Dai; H. Zhang; L. Zhang; H. Shen","School of Resource and Environmental Science, Wuhan University, P.R. China; School of Resource and Environmental Science, Wuhan University, P.R. China; School of Resource and Environmental Science, Wuhan University, P.R. China; School of Resource and Environmental Science, Wuhan University, P.R. China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","7030","7033","In this paper, a novel spatiotemporal fusion model based on deep learning is proposed, which handles the huge spatial resolution gap and the nonlinear mapping between the high spatial resolution (HSR) image and the corresponding high temporal resolution (HTR) image at the same imaging time. Considering the huge spatial resolution gap, a two-layer fusion strategy is adopted. In each layer, the convolutional neural network (CNN) model is employed to exploit the non-linear mapping between the HSR and HTR image and reconstruct the high-spatial and high-temporal (HSHT) resolution images. In the experiment, Landsat data is the representation of the high spatial resolution images, MODIS data is used as the corresponding low spatial resolution images. The experimental results on two different datasets clearly illustrate the superiority of the proposed model.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518758","Spatiotemporal fusion;convolutional neural network;non-linear mapping","Spatial resolution;Remote sensing;Artificial satellites;Earth;Spatiotemporal phenomena;MODIS","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing;sensor fusion;spatiotemporal phenomena","corresponding low spatial resolution images;Landsat data;convolutional neural network model;two-layer fusion strategy;imaging time;corresponding high temporal resolution image;high spatial resolution image;nonlinear mapping;huge spatial resolution gap;novel spatiotemporal fusion model;deep learning;MODIS data;remote sensing spatiotemporal fusion model","","6","","8","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Hyperspectral and panchromatic image fusion based on CNMF","T. Mu; R. Nie; C. Ma; J. Liu","Information Science and Engineering, Yunnan University, Kunming, China; Information Science and Engineering, Yunnan University, Kunming, China; Information Science and Engineering, Yunnan University, Kunming, China; Information Science and Engineering, Yunnan University, Kunming, China","2021 3rd International Conference on Advances in Computer Technology, Information Science and Communication (CTISC)","6 Sep 2021","2021","","","293","297","Hyperspectral images contain rich spectral features, but the spatial resolution of hyperspectral images is low, so image fusion technology in the same scene plays an essential role in satellite imaging. The commonly used method now is to transfer the spectral information in the hyperspectral image to the panchromatic image, but many algorithms cannot avoid spectral distortion. Based on the coupled non-negative matrix decomposition (CNMF) algorithm, a spectral constraint regularization term is introduced to avoid spectral distortion and maintain spectral integrity. The experimental results were compared with the other four most advanced methods, this method has obvious advantages in terms of visual effects and evaluation indicators.","","978-1-6654-1868-3","10.1109/CTISC52352.2021.00060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9527635","image fusion;unmixing spectral decomposition;coupled non-negative matrix factorization;spectral constraints","Information science;Satellites;Imaging;Distortion;Visual effects;Matrix decomposition;Spatial resolution","geophysical image processing;image fusion;image resolution;matrix decomposition;remote sensing","hyperspectral image;rich spectral features;image fusion technology;satellite imaging;spectral information;spectral distortion;nonnegative matrix decomposition algorithm;CNMF;spectral constraint regularization term;spectral integrity;panchromatic image fusion","","1","","9","IEEE","6 Sep 2021","","","IEEE","IEEE Conferences"
"A self-learning approach for pan-sharpening of multispectral images","M. Khateri; H. Ghassemian","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2017 IEEE International Conference on Signal and Image Processing Applications (ICSIPA)","30 Nov 2017","2017","","","199","204","Due to the importance of high-resolution multi-spectral (HRM) images in many remote sensing applications, pan-sharpening techniques have been proposed to increase the spatial resolution of a low-resolution multi-spectral (LRM) image using a high-resolution panchromatic (HRP) image. In this paper, we propose a self-learning approach to pan-sharpen the LRM images. Many structures in a natural image redundantly tend to repeat in the same scale as well as different scales. These similar structures in different levels can be used to reconstruct the HRM bands with more details; in this perspective, we can construct the HRM data from the available HRP and LRM data by using self-similarity in a multi-scale procedure. The proposed method has been applied on GeoEye-1 data and DEIMOS-2 data, and then fused images compared with some popular and state-of-the-art methods in terms of several assessment indexes. The experimental results demonstrate that the proposed method can retain spectral and spatial information of the source images efficiently.","","978-1-5090-5559-3","10.1109/ICSIPA.2017.8120606","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8120606","Pan-sharpening;image fusion;superresolution;multi-scale;self-learning;panchromatic data;multi-spectral data","Image reconstruction;Spatial resolution;Remote sensing;Conferences;Image fusion","geophysical image processing;geophysical techniques;image fusion;image resolution;remote sensing","self-learning approach;high-resolution multispectral images;remote sensing applications;pan-sharpening techniques;spatial resolution;low-resolution multispectral image;high-resolution panchromatic image;LRM images;natural image;HRM bands;multiscale procedure;GeoEye-1 data;DEIMOS-2 data;spectral information;spatial information;source images","","2","","30","IEEE","30 Nov 2017","","","IEEE","IEEE Conferences"
"A method of target fusion detection based on the multi-fractal analysis","Weigang Zhu; Weiqing Tian; Chuangzhan Zeng","Academy of Equipment, Beijing, China; Academy of Equipment, Beijing, China; Academy of Equipment, Beijing, China","2016 15th International Conference on Optical Communications and Networks (ICOCN)","13 Mar 2017","2016","","","1","3","A target detection method is proposed which is based on the multi-fractal spectrum characteristics of the remote sensing image. The method uses multi-fractal spectrum to reflect the image global features and effectively suppress noise characteristics at the same time. The membership function is constructed by multi-fractal spectrum of visible and SAR images to indicate the uncertainty of their sensors information. The trade-off between the intersection operation and union operation of the feature level fusion detection is achieved by the membership function, in order to reduce the false alarm rate and the missed alarm rate. The experimental results show that the method can combine the respective advantages of SAR and visible images, and effectively improve the accuracy of detection.","","978-1-5090-3491-8","10.1109/ICOCN.2016.7875601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7875601","Multi-fractal spectrum;Remote sensing image;Fusion;Detection","Fractals;Remote sensing;Synthetic aperture radar;Image edge detection;Noise measurement;Sensors;Image fusion","feature extraction;fractals;image fusion;image sensors;object detection;radar imaging;synthetic aperture radar","target fusion detection method;multifractal spectrum characteristics;remote sensing imaging;image global feature;noise characteristics;SAR imaging;visible imaging;level fusion detection;false alarm rate reduction","","","","5","IEEE","13 Mar 2017","","","IEEE","IEEE Conferences"
"Comparative Analysis Between Optical and Fused Image with SAR","K. Aslam; R. M. Zahid Khalil; S. u. Haq; S. Ahmed","Dept of RS & GISc, Institute of Space Technology, Karachi, Pakistan; Dept of RS & GISc, Institute of Space Technology, Karachi, Pakistan; Dept of RS & GISc, Institute of Space Technology, Karachi, Pakistan; Dept of Physics, University of Karachi","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1528","1531","Image fusion is a technique that integrates complementary information from multiple remote sensing images such that the fused image is more suitable for processing task and information extraction. Passive sensors are capable of sensing the reflected electromagnetic energy in the visible and infrared region while active sensors provide additional information using microwave region. This broad spectrum provides more information of earth surface as compared to optical data alone. This study compares the land cover classification results of optical imagery (Landsat-8) and fused imagery (Landsat-8 and Sentinel-1 VV polarized data). The image fusion was then performed using wavelet transformation technique. The data were classified into four classes namely water bodies, built-up area, vegetation cover, and barren land. Google Earth and Landsat imagery were used as a reference image for accuracy assessment. The fused image showed higher accuracy than optical image i.e. Kappa coefficient increased from 0.78 to 0.9 and overall accuracy increased from 89.4% to 92.7%. This study indicates that multi-source information i.e., image fusion can significantly improve the interpretation and accuracy of classification","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324255","Land cover classification;Fusion;Synthetic Aperture Radar","Remote sensing;Earth;Artificial satellites;Optical imaging;Optical sensors;Adaptive optics;Biomedical optical imaging","geophysical image processing;image classification;image fusion;land cover;remote sensing by radar;synthetic aperture radar;vegetation mapping","land cover classification results;optical imagery;Landsat-8;Sentinel-1 VV polarized data;image fusion;reference image;multisource information;comparative analysis;complementary information;multiple remote sensing images;passive sensors;reflected electromagnetic energy;visible region;infrared region;optical data;SAR;wavelet transformation technique;water bodies;built-up area;barren land;vegetation cover;Google Earth","","","","13","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Remote sensing image fusion using machine learning and deep learning: a systematic review","A. Tawade; S. Virnodkar","Department of Computer Engineering, Mumbai University, Mumbai, India; Department of Computer Engineering, Mumbai University, Mumbai, India","7th International Conference on Computing in Engineering & Technology (ICCET 2022)","20 Jun 2022","2022","2022","","36","46","Modern Remote Sensing (RS) technologies have emerged and picked up a substantial momentum as a valuable source of data for environmental, economic, and social applications over the past few years. RS has given birth to few other beneficial processes like image fusion that extracts & registers relevant important knowledge from satellite images to make them more useful before subjecting to future applications. Image Fusion is typically performed with three broader techniques: Pixel level, Feature Level and Decision Level. With the help of proper image fusion methods, the concerned information is extracted from various images, which can then be utilized for everyday applications, ranging from weather forecasts to reports on natural disasters or climate change. This study talks about a practice of applying Artificial Intelligence (AI) to the great extent considering its recent advancement in the domain of image processing. In a nutshell, this paper explores and analyses the key remote sensing data fusion techniques based on Machine Learning and Deep Learning with respect to multi-satellite images and single satellite multi-spectral band images and reviews the theory, principles, applications, constraints, and benefits of each technique with a goal of discussing a potential future direction of this study.","","978-1-83953-704-2","10.1049/icp.2022.0589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9800303","","","artificial intelligence;geophysical image processing;image fusion;image processing;learning (artificial intelligence);remote sensing;sensor fusion","social applications;beneficial processes;relevant important knowledge;satellite images;Pixel level;Feature Level;Decision Level;proper image fusion methods;everyday applications;image processing;key remote sensing data fusion techniques;machine learning;deep learning;multisatellite images;single satellite multispectral band images;sensing image fusion;systematic review;Modern Remote Sensing technologies;substantial momentum;environmental applications;economic, applications","","","","","","20 Jun 2022","","","IET","IET Conferences"
"Multisource Remote Sensing Data Classification Using Fractional Fourier Transformer","X. Zhao; M. Zhang; R. Tao; W. Li; W. Liao; W. Phlips","IPI-imec, Ghent University, Belgium; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Flanders Make, Belgium; IPI-imec, Ghent University, Belgium","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","823","826","Focusing on joint classification of Hyperspectral image (HSI) and Light detection and ranging (LiDAR) data, a fractional Fourier image transformer (FrIT) is proposed as a backbone network in this paper. In the proposed FrIT, HSI and LiDAR data are firstly fused at pixel-level. Both multi-source and HSI feature extractors are utilized to capture local contexts. Then, a plug-and-play image transformer FrIT is explored for global contexts and sequential feature extraction. Unlike the attention-based representations in classic visual image transformer (VIT), FrIT is capable of speeding up the transformer architectures massively. To reduce the information loss from shallow to deep layers, FrIT is devised to connect contextual features in multiple fractional domains. At last, to evaluate the performance of FrIT, a new HSI and LiDAR benchmark is provided for extensive experiments, on which the proposed FrIT gains an improvement of 3% over state-of-the-art methods.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884573","National Key R&D Program of China(grant numbers:2021YFB3900502); National Natural Science Foundation of China(grant numbers:61922013); Beijing Natural Science Foundation(grant numbers:JQ20021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884573","Fractional Fourier image transformer (FrIT);hyperspectral image (HSI);light detection and ranging (LiDAR);multisource remote sensing","Visualization;Laser radar;Focusing;Benchmark testing;Transformers;Feature extraction;Distance measurement","feature extraction;Fourier transforms;geophysical image processing;geophysical signal processing;image classification;image colour analysis;image fusion;object detection;optical radar;remote sensing","fractional Fourier transformer;joint classification;LiDAR;fractional Fourier image transformer;backbone network;pixel-level;HSI feature extractors;local contexts;global contexts;sequential feature extraction;attention-based representations;classic visual image transformer;transformer architectures;contextual features;multiple fractional domains;FrIT gains;multisource remote sensing data classification","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Detection and discrimination of complex thrust and salt tectonics structures using field data and RASAT images around the Emirhan region (Sivas, Turkey)","S. B. Çiçekliyurt; K. Ş. Kavak; J. -P. Callot; J. -C. Ringenbach","Cumhuriyet Universitesi, Sivas, TR; Cumhuriyet Universitesi, Sivas, TR; Universite de Pau et des Pays de l'Adour, Pau, Nouvelle-Aquitaine, FR; Total SA, Paris-La Defense, ÃŽle-de-France, FR","2016 24th Signal Processing and Communication Application Conference (SIU)","23 Jun 2016","2016","","","2225","2228","RASAT L2 visible and panchromatic images were used to detect both thrust and salt tectonics structures as a reliable geologic mapping tool in this research. As a geographical transition plateau between Inner and Eastern Anatolian region, Sivas Basin doesn't show hampering vegetation on geologic outcroppings for remote sensing analyses. This feature also allow healthy interpretation possibilities. Salt structures in Sivas Basin can be counted in worldwide well-known examples (e.g. La Popa Mexico, Great Kavir Iran and Axel Heiberg Canada) conveniently. In this research, a detailed geological map was prepared in the field. The results show that both of visual interpretations and digital image processing methods spectrally could provide healthy discrimination power between different rock lithologies. Image fusion method was contributed to reveal these subtle tectonic and geomorphologic evidences that developed in and between salt and clastic lithologies. In addition, specifically chosen Landsat ETM+ bands such as 5 and 7 images together with RASAT visible bands were also used to create RGB combinations for geological studies.","","978-1-5090-1679-2","10.1109/SIU.2016.7496217","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7496217","geology;RASAT;remote sensing;salt tectonics;Sivas Basin;thrust tectonics","Remote sensing;Geology;Satellites;Earth;Petroleum;Image fusion;Reliability","image fusion;rocks;tectonics;terrain mapping","complex thrust discrimination;salt tectonics structure;field data;Emirhan region;Sivas basin;Turkey;RASAT L2 visible image;panchromatic image;thrust detection;geologic mapping tool;geographical transition plateau;Anatolian region;vegetation;geologic outcropping;remote sensing analysis;La Popa;Mexico;Great Kavir;Iran;Axel Heiberg;Canada;digital image processing method;rock lithologies;image fusion method;Landsat ETM+ band;RASAT visible band;RGB combination","","","","","IEEE","23 Jun 2016","","","IEEE","IEEE Conferences"
"Multi-class Object Detection in Fusion Remote Sensing Images Based on Deep Learning","T. Sun; K. Liu","School of Information Science and Engineering, University of Jinan, Jinan, China; School of Information Science and Engineering, University of Jinan, Jinan, China","2021 International Conference on Electronic Communications, Internet of Things and Big Data (ICEIB)","31 Jan 2022","2021","","","188","192","Deep learning has been applied successfully in the machine visual field, and it has already surpassed humans in object detection of images. Object detection is a significant content of remote sensing image analysis and is a key link to transform image data into application results. Based on a self-made high-resolution remote sensing image dataset, the Faster RCNN algorithm is used in the two-stage method and the RetinaNet algorithm in the one-stage method to carry out experiments. For the small and dense object in the remote sensing dataset, three improvements are proposed, and finally an object detection model is proposed with strong generalization ability. The results show the proposed model proposed can achieve better results in detection accuracy and computational overhead.","","978-1-6654-3755-4","10.1109/ICEIB53692.2021.9686449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9686449","remote sensing image analysis;high-resolution remote sensing image;one-stage method;two-stage method;strong generalization ability","Deep learning;Visualization;Computational modeling;Object detection;Transforms;Maintenance engineering;Internet of Things","geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);neural nets;object detection;remote sensing","image data;application results;high-resolution remote;image dataset;Faster RCNN algorithm;two-stage method;RetinaNet algorithm;one-stage method;small object;dense object;remote sensing dataset;object detection model;detection accuracy;multiclass object detection;fusion remote sensing images;deep learning;machine visual field;significant content;remote sensing image analysis;key link","","","","8","IEEE","31 Jan 2022","","","IEEE","IEEE Conferences"
"Sar and Optical Image Fusion for Coastal Surveillance","L. Zheng; J. Pei; Y. Zhang; Y. Huang; J. Wu; J. Yang","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, P.R. China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, P.R. China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, P.R. China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, P.R. China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, P.R. China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, P.R. China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","2802","2805","Coastal surveillance has long been paid a lot of attention for the threat of flooding due to some natural phenomena, such as global warming. Prompt and accurate reaction to the visualization of the flooded areas is the key. An image fusion rule is thus proposed in this paper to achieve image enhancement of the flooded areas. The rule, targeted at high-frequency parts of the synthetic aperture radar (SAR) and optical images, is able to exploit and combine the merits of both SAR and optical images to obtain the exact flooded areas with the clear boundaries. Experimental results validate the performance of the proposed fusion rule and show that not only the clarity of fusion images is improved, but also the texture and brightness contrast are greatly enhanced.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900420","image fusion;monitor flooded area;non-subsampled contourlet transform","Optical imaging;Radar polarimetry;Optical sensors;Synthetic aperture radar;Sea measurements;Standards;Adaptive optics","floods;image enhancement;image fusion;optical images;radar imaging;synthetic aperture radar","coastal surveillance;natural phenomena;global warming;accurate reaction;image fusion rule;image enhancement;high-frequency parts;SAR;optical images;exact flooded areas;fusion images","","","","12","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Application of CSGE-PSPnet Remote Sensing Image Semantic Segmentation Technology in Transmission and Transformation Engineering Design","B. Li; K. Sun; Y. Lang; L. Guan; F. Lu; Y. Zhu","Design and Consulting Ceter State Grid Economic and Technological Research Institute CO., LTD, Beijing, China; Engineering and Degital Certer Powerchina Sichuan Electric Power Engineering Co., LTD., Chengdu, China; Geographic Information Center Powerchina Sichuan Electric Power Engineering Co., LTD., Chengdu, China; Design and Consulting Ceter State Grid Economic and Technological Research Institute CO., LTD, Beijing, China; Design and Consulting Ceter State Grid Economic and Technological Research Institute CO., LTD, Beijing, China; Planning and Operations Department, Economic and Technological, Research Institute of State Grid Shandong Electric Power Company, Jinan, China","2022 Asian Conference on Frontiers of Power and Energy (ACFPE)","29 Nov 2022","2022","","","85","89","With the breakthrough development of remote sensing technology and deep learning, remote sensing technology also has a broad application prospect in the field of transmission and transformation engineering design. Semantic segmentation, as a key link in remote sensing image processing, is the basis for improving the intelligence of transmission and substation engineering, for siting, construction, maintenance, and monitoring of transmission and substation engineering by accurately identifying important facilities, sensitive locations and environmental changes. Therefore, this paper proposes a remote sensing image semantic segmentation model(CSGE-PSPnet), which is based on PSPNet(Pyramid Scene Parsing Network) and SGE(Spatial Group-Wise Enhance Attention). CSGE-PSPnet could enhance local significance features while extracting global features, realizes the refined extraction of key information, and designs CSGE Block(Spatial-Channel Group-Wise Enhance Attention) to extract channel and spatial local information at the same time to enhance local significance features. Multi-level features are fused through dense connection structures to retain feature information of small-scale targets. Subsequently, the deep global features are mined by PSPNet on the feature map after fusion, and the contextual information of different regions is fused to obtain the segmentation results. In addition, one auxiliary loss layer and one major loss layer further improve the model's generalization ability. The experimental results show that CSGE-PSPnet is superior to other methods and can accurately identify small targets, and CSGE Block significantly enhances the feature representation.","","978-1-6654-7084-1","10.1109/ACFPE56003.2022.9952233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9952233","power transmission and transformation engineering;site selection;semantic segmentation of remote sensing images;mix domain attention;PSPNet","Substations;Target recognition;Semantic segmentation;Image edge detection;Power transmission;Maintenance engineering;Feature extraction","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);power engineering computing;power transmission;remote sensing;substations","CSGE Block;CSGE-PSPnet remote sensing image semantic segmentation technology;deep global features;feature information;feature map;local significance features;multilevel features;remote sensing image processing;remote sensing image semantic segmentation model;remote sensing technology;spatial local information;spatial-channel group-wise enhance attention;substation engineering;transformation engineering design","","","","23","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"Hyperspectral and multispectral image fusion using collaborative representation with local adaptive dictionary pair","T. Zhao; Y. Zhang; X. Xue; M. He","Shaanxi Key Laboratory of Information Acquisition and Processing, Northwestern Polytechnical University, Xi'an, China; Shaanxi Key Laboratory of Information Acquisition and Processing, Northwestern Polytechnical University, Xi'an, China; Shaanxi Key Laboratory of Information Acquisition and Processing, Northwestern Polytechnical University, Xi'an, China; Shaanxi Key Laboratory of Information Acquisition and Processing, Northwestern Polytechnical University, Xi'an, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","7212","7215","In this paper, the spatial resolution of hyperspectral image (HSI) is enhanced by fusing it with multispectral image (MSI) of the same scene with a higher spatial resolution. The w-hole spectrum covered by HSI channels is divided into several regions according to MSI spectral channels. The HSI-MSI fusion problem is then simplified by fusing images of each spectral region one after another. Specifically, a fusion algorithm based on collaborative representation (CR) with local adaptive dictionary pair is proposed. Compared to the classic global dictionary, the scale of the local adaptive one is much smaller such that the related computational cost is also reduced. The employment of CR is capable of reducing the reconstruction error to guarantee an improved fusion performance. Simulative experiments are deployed for illustration and comparison.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730881","Collaborative representation;hyperspectral;image fusion;local adaptive dictionary pair;multispectral","Dictionaries;DH-HEMTs;Algorithm design and analysis","geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image representation;image resolution","reconstruction error;collaborative representation employment;computational cost;local adaptive scale;classic global dictionary;local adaptive dictionary pair;collaborative representation;spectral region;HSI-MSI fusion problem;spectral image spatial resolution;multispectral image fusion;hyperspectral image fusion","","6","","10","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Hyperspectral and Panchromatic Image Fusion Based on Weighted Tensor Matrix","J. Qu; Q. Du; Y. Li; W. Dong","State Key Lab. of Integrated Service Networks, Xidian University, Xi’an, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, USA; State Key Lab. of Integrated Service Networks, Xidian University, Xi’an, China; State Key Lab. of Integrated Service Networks, Xidian University, Xi’an, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","632","635","In this paper, a new hyperspectral image (HSI) and panchromatic image (PANI) fusion approach via weighted tensor matrix is proposed. In the proposed method, homomorphic filtering is use for obtaining spatial component of HSI, and a weighted root mean squared error (RMSE)-based algorithm is proposed to extract the total intensity details of HSI. In addition, an optimized weighted tensor matrix-based method is proposed to acquire the integrated intensity details from both HSI and PANI. Comparative analyses show the proposed approach performs better than other excellent approaches in visual inspection and objective assessment.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900283","hyperspectral;panchromatic;image fusion;homomorphic filter;weighted tensor","Spatial resolution;Hyperspectral imaging;Information filters;Bayes methods","feature extraction;hyperspectral imaging;image filtering;image fusion;matrix algebra;mean square error methods;tensors","weighted root mean squared error-based algorithm;optimized weighted tensor matrix;hyperspectral image fusion;panchromatic image fusion;homomorphic filtering","","1","","11","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Enhanced Residual Dense Network Joint with GRUS for Multispectral and Hyperspectral Image Fusion","J. Xiao; Q. Yuan; J. Li; H. Shen","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Resource and Environmental Science, Wuhan University, Wuhan, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2448","2451","It is very significant to enhance the spatial resolution of hyperspectral images for more accurate image interpretation. In this paper, we propose an innovative fusion method by the enhanced residual dense network to better extract the spatio-spectral features joint with GRU. The enhanced residual dense blocks (ERDB) contains a modified spatial and spectral attention module. All attention coefficients are calculated based on original images. Then, the idea of gate recurrent unit (GRU) is applied to integrate the useful information from results of all ERDBs followed by a convolution for reconstruction. Finally, a skip connection is added to further maintain the spectrum. The effectiveness of this method can be seen from the experimental results of simulation experiments.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554402","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554402","Hyperspectral image;Image fusion;Three-dimensional convolution;Residual network","Convolution;Geoscience and remote sensing;Color;Logic gates;Feature extraction;Spatial resolution;Image reconstruction","feature extraction;geophysical image processing;image classification;image fusion","original images;attention coefficients;spectral attention module;modified spatial;ERDB;enhanced residual dense blocks;GRU;spatio-spectral features;innovative fusion method;accurate image interpretation;spatial resolution;hyperspectral image;enhanced residual dense network joint","","","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Assessment of Predictive Ability of Starfm Based on Different Modis-Landsat Image Pair Date","D. Xie; F. Gao; L. Li","Faculty of Geographical Science, Beijing Normal University, Beijing, China; U. S. Department of Agriculture, Hydrology and Remote Sensing Laboratory, Beltsville, MD, USA; Faculty of Geographical Science, Beijing Normal University, Beijing, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","7247","7250","Accurate spatiotemporal information about crop progress during the growing season is critical for crop yield estimation. Crop progress monitoring at field scale requires high resolution remote sensing data in both time and space. Remote sensing data from a single sensor cannot satisfy the requirement at present. Data fusion approach has been developed to fuse remote sensing imagery from Landsat and MODIS instruments. The Spatial and Temporal Adaptive Reflectance Fusion Model (STARFM) is one of the most popular spatial and temporal data fusion algorithms and has been applied in many applications. The data fusion accuracy was evaluated for many sites. Previous studies found that the accuracy of data fusion results depended on the pair images used. In this study, several Landsat-8 reflectance images (path28/row31) in 2015 were selected as pair images to evaluate the data fusion accuracy. Results were assessed based on the observed Landsat data that have not been used as pair images due to partial cloud coverage or image gaps. Several statistic metrics, including average absolute difference, root mean square error, correlation coefficient, and the spectral angle mapper, were calculated to assess the data fusion results. The initial results show that the predictability of each images pair at different dates is different. Closer dates have better prediction accuracy as expected. Interestingly, the different crop type (corn and soybeans) shows different data fusion accuracies even using same image pair. This study suggests that data fusion results could be further improved if an appropriate image pair is selected. Accurate dense time-series data at Landsat resolution will enhance our ability in crop condition monitoring and crop yield estimation at field scale.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517833","STARFM;Landsat;MODIS;Crop;Data Fusion","Remote sensing;Earth;Artificial satellites;Reflectivity;Agriculture;Data integration;MODIS","crops;geophysical image processing;geophysical techniques;image fusion;image resolution;remote sensing;sensor fusion;spatiotemporal phenomena;time series","crop progress;crop yield estimation;high resolution remote sensing data;data fusion approach;remote sensing imagery;temporal data fusion algorithms;Landsat-8 reflectance images;partial cloud coverage;different data fusion accuracies;Landsat resolution;crop condition monitoring;Landsat instrument;MODIS instrument;modis-Landsat image pair date;spatial data fusion algorithms;dense time-series data;Landsat data","","1","","7","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Visible and thermal image fusion using curvelet transform and brain storm optimization","K. Madheswari; N. Venkateswaran; V. Sowmiya","Department of Computer Science and Engineering, SSNCollege of Engineering, Chennai, India; Department of Electronics and Communication Engineering, SSNCollege of Engineering, Chennai, India; Department of Electronics and Communication Engineering, SASTRA University, Thanjavur, India","2016 IEEE Region 10 Conference (TENCON)","9 Feb 2017","2016","","","2826","2829","In this paper, we propose a brain storm optimized image fusion framework in the curvelet transform domain that combines thermal image with the visual image to obtain a single informative fused image. The source images are decomposed using the curvelet transform and the high frequency sub-band coefficients are fused by maximum selection rule, whereas the low frequency sub-band coefficients are fused by weighted linear combination rule. The human intelligence based brain storm optimization (BSO) algorithm is employed to find the optimal weights in fusing low frequency sub-band coefficients. Simulation have been performed to compare our results with other multi resolution fusion methods such as gradient (GRAD) pyramid, shift invariant discrete wavelet transform (SIDWT) and non sub-sampled contourlet transform (NSCT). The quality of the fused image is assessed using five different quality metrics and the results indicate that the proposed method outperforms other multiresolution fusion methods in terms of both subjective and objective quality metrics.","2159-3450","978-1-5090-2597-8","10.1109/TENCON.2016.7848558","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848558","image fusion;visible image;thermal image;curvelet transform;brain storm optimization","Image resolution;Remote sensing;Image decomposition;Computed tomography;Transforms;Biomedical imaging;Filter banks","curvelet transforms;discrete wavelet transforms;image fusion;infrared imaging;optimisation","thermal image fusion;curvelet transform;brain storm optimization;single informative fused image;low frequency subband coefficients;weighted linear combination rule;human intelligence;BSO algorithm;multiresolution fusion methods;GRAD pyramid;gradient pyramid;shift invariant discrete wavelet transform;SIDWT;non subsampled contourlet transform;NSCT;quality metrics;objective quality metrics;subjective quality metrics","","2","","17","IEEE","9 Feb 2017","","","IEEE","IEEE Conferences"
"Satellite image registration using hybrid salient region detection method","C. Shanthini; J. Anitha","Department of Electronics and Communication, Karunya University, Tamil Nadu, India; Karunya Institute of Technology and Sciences, Coimbatore, Tamil Nadu, IN","2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT)","4 Aug 2016","2016","","","1","5","In remote sensing multisensor image fusion or registration is the process of combining relevant information from two or more images into a single image. The resulting image will be more informative than any of the input images. In order to transform the remote sensing images to retrieve more information, this paper proposes a hybrid method that consists of image segmentation, salient region detection and image fusion. First of all, the paper presents the superpixel segmentation method in order to divide the image into subareas and for the feature extraction we implemented the difference of Gaussian and local binary pattern from the salient regions. This proposed method is tested on remote sensing images. Software results shows that the method is fast and gives less error compared to other existing methods.","","978-1-5090-1277-0","10.1109/ICCPCT.2016.7530356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530356","Image segmentation;Salient region detection;Local binary pattern;Remote sensing images","Image segmentation;Image registration;Remote sensing;Histograms;Feature extraction;Image edge detection;Computers","feature extraction;geophysical image processing;image fusion;image registration;image retrieval;image segmentation;object detection;remote sensing","satellite image registration;hybrid salient region detection method;remote sensing multisensor image fusion;information retrieval;superpixel segmentation method;feature extraction;difference of Gaussian;local binary pattern","","","","11","IEEE","4 Aug 2016","","","IEEE","IEEE Conferences"
"POP Image Fusion -- Derivative Domain Image Fusion without Reintegration","G. D. Finlayson; A. E. Hayes","University of East Anglia, Norwich, UK; University of East Anglia, Norwich, UK","2015 IEEE International Conference on Computer Vision (ICCV)","18 Feb 2016","2015","","","334","342","There are many applications where multiple images are fused to form a single summary greyscale or colour output, including computational photography (e.g. RGB-NIR), diffusion tensor imaging (medical), and remote sensing. Often, and intuitively, image fusion is carried out in the derivative domain. Here, a new composite fused derivative is found that best accounts for the detail across all images and then the resulting gradient field is reintegrated. However, the reintegration step generally hallucinates new detail (not appearing in any of the input image bands) including halo and bending artifacts. In this paper we avoid these hallucinated details by avoiding the reintegration step. Our work builds directly on the work of Socolinsky and Wolff who derive their equivalent gradient field from the per-pixel Di Zenzo structure tensor which is defined as the inner product of the image Jacobian. We show that the x-and y-derivatives of the projection of the original image onto the Principal characteristic vector of the Outer Product (POP) of the Jacobian generates the same equivalent gradient field. In so doing, we have derived a fused image that has the derivative structure we seek. Of course, this projection will be meaningful only where the Jacobian has non-zero derivatives, so we diffuse the projection directions using a bilateral filter before we calculate the fused image. The resulting POP fused image has maximal fused detail but avoids hallucinated artifacts. Experiments demonstrate our method delivers state of the art image fusion performance.","2380-7504","978-1-4673-8391-2","10.1109/ICCV.2015.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410403","","Image fusion;Jacobian matrices;Tensile stress;Discrete wavelet transforms;Matrix decomposition;Eigenvalues and eigenfunctions;Periodic structures","gradient methods;image colour analysis;image fusion;Jacobian matrices;principal component analysis;tensors","POP image fusion;derivative domain image fusion;colour output;computational photography;diffusion tensor imaging;remote sensing;composite fused derivatives;gradient field;hallucinated details;equivalent gradient field;per-pixel Di Zenzo structure tensor;image Jacobian;y-derivatives;x-derivatives;principal characteristic vector of the outer product;POP;nonzero derivatives;bilateral filter","","6","2","38","IEEE","18 Feb 2016","","","IEEE","IEEE Conferences"
"Joint quality measure for accuracy assessment of pansharpening methods","G. Palubinskas","German Aerospace Center DLR, Remote Sensing Technology Institute, Oberpfaffenhofen, Wessling, Germany","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","601","604","A new joint quality measure JQM which is a sole measure is proposed for quality ranking of pansharpening methods. It is based on a newly proposed composite similarity measure CMSC which consists of means, standard deviations and correlation coefficient and is translation invariant with respect to all parameters. JQM itself consists of a weighted sum of two terms. First term is measured between a low pass filtered pansharpened image and original multispectral image in a reduced resolution scale. The second one - between weighted intensity calculated from pansharpened image and original panchromatic image in a high resolution scale. Experimental results show advantages of a new measure JQM for quality assessment of pansharpening methods on the one hand and drawbacks of already known measure QNR on the other hand.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325835","Multi-resolution;multi-sensor;image fusion;pansharpening;quality assessment measure","Filtering;Joints;Quality assessment;Remote sensing;Interpolation;Spatial resolution","geophysical image processing;hyperspectral imaging;image filtering;image fusion;remote sensing","pansharpening methods;joint quality measure;JQM;composite similarity measure;CMSC;standard deviations;correlation coefficient;translation invariant;filtered pansharpened image;multispectral image;weighted intensity;panchromatic image;quality assessment","","","","17","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Hyperspectral Image Fusion Using Fast High-Dimensional Denoising","P. Nair; V. S. Unni; K. N. Chaudhury","Department of Electrical Engineering, Indian Institute of Science, Bangalore, India; Department of Electrical Engineering, Indian Institute of Science, Bangalore, India; Department of Electrical Engineering, Indian Institute of Science, Bangalore, India","2019 IEEE International Conference on Image Processing (ICIP)","26 Aug 2019","2019","","","3123","3127","In hyperspectral image fusion, a high resolution multispectral (MS) image is combined with a low resolution hyperspectral (HS) image to obtain a high resolution HS image. In this work, we propose a ""plug-and-play"" framework for HS-MS fusion, where the inversion step at each iteration involves the solution of a linear system, and the regularization is performed using a high-dimensional kernel denoiser. The core contribution is the design of the denoiser, which can denoise an HS-image at low complexity using clustering and convolutions. In particular, it can exploit the inter-band correlations, which cannot be done using band-by-band denoising. An important technical aspect of our denoiser is that it can be expressed as the proximal map of a proper, closed, and convex regularizer, which guarantees the convergence of the plug-and-play iterations. Preliminary results suggest that we are competitive with state-of-the-art algorithms for HS-MS fusion in terms of speed and restoration accuracy.","2381-8549","978-1-5386-6249-6","10.1109/ICIP.2019.8803377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803377","hyperspectral image fusion;plug-and-play;regularization;high-dimensional denoiser","Kernel;Spatial resolution;Symmetric matrices;Hyperspectral imaging;Noise reduction","convolutional neural nets;geophysical image processing;hyperspectral imaging;image denoising;image fusion;image resolution;iterative methods;pattern clustering;remote sensing","high-dimensional denoising;hyperspectral image fusion;high resolution multispectral image;low resolution hyperspectral image;high resolution HS image;plug-and-play framework;HS-MS fusion;high-dimensional kernel denoiser;band-by-band denoising;plug-and-play iterations","","6","","30","IEEE","26 Aug 2019","","","IEEE","IEEE Conferences"
"Assessment of the spectral quality of fused images using the CIEDE2000 distance","D. Rodriguez-Esparragon; J. Marcello; F. Eugenio-Gonzalez; A. Garcia-Pedrero; C. Gonzalo-Martin","Instituto de Oceanografia y Cambio Global, ULPGC Las Palmas de Gran Canaria, Spain; Instituto de Oceanografia y Cambio Global, ULPGC Las Palmas de Gran Canaria, Spain; Instituto de Oceanografia y Cambio Global, ULPGC Las Palmas de Gran Canaria, Spain; Center for Biomedical Technology (CTB), Universidad Politécnica de Madrid (UPM), Madrid, Spain; Center for Biomedical Technology (CTB), Universidad Politécnica de Madrid (UPM), Madrid, Spain","2017 International Conference and Workshop on Bioinspired Intelligence (IWOBI)","24 Jul 2017","2017","","","1","4","Image fusion plays an important role in remote sensing applications. Because of this, the evaluation of the spectral quality of pan-sharpened images is a fundamental subject to optimize and compare the results of different algorithms. The aim of this paper is to explore the use of CIEDE2000 distance to evaluate the spectral quality of the fused images. To do this, a database containing remote sensing imagery and its fusion products was created. The spectral quality of the imagery on the database was evaluated using both common quantitative indices and CIEDE2000. The results were compared to the qualitative assessment provided by a mean opinion score test.","","978-1-5386-0850-0","10.1109/IWOBI.2017.7985536","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985536","image fusion;image quality;spectral quality;qualityindicators;CIEDE2000;pan-sharpening","Image color analysis;Indexes;Principal component analysis;Distortion;Remote sensing;Visual perception","image fusion;remote sensing","spectral quality;fused images;CIEDE2000 distance;image fusion;remote sensing imagery;fusion products;qualitative assessment","","1","","12","IEEE","24 Jul 2017","","","IEEE","IEEE Conferences"
"Improving Satellite-Aerial Image Matching Success Rate by Image Fusion","J. -I. Shin; T. Kim; W. -S. Yoon; H. -J. Park","Research Center of Geoinformatic Enineering, Inha University, Incheon, Republic of Korea; Department of Geoinformatic Engineering, Inha University, Incheon, Republic of Korea; Department of Geoinformatic Engineering, Inha University, Incheon, Republic of Korea; Department of Geoinformatic Engineering, Inha University, Incheon, Republic of Korea","2018 2nd European Conference on Electrical Engineering and Computer Science (EECS)","25 Nov 2019","2018","","","224","227","Image matching is an important method to collect ground control points (GCPs) by finding correspondence between incoming images and chips of reference image maps. It is an essential process for automated precise geo-registration of satellite imagery. To get higher georeferencing accuracy, reference chips must be matched precisely on the images. The importance of higher matching success rate is increased with limited number of chips. In this study, we aim to match incoming satellite images against reference chips generated from aerial color ortho-images. Matching the two dataset is difficult since they have different spectral responses as well as different textures. We try to improve matching success rate by using pansharpened satellite images. The results showed higher matching success rate with pansharpened images due to similar spectral range and higher spatial resolution. Therefore, pansharpened image is helpful to improve image matching success rate in automated precise georeferencing of high-resolution satellite imagery.","","978-1-7281-1929-8","10.1109/EECS.2018.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8910110","image matching;satellite image;pan-sharpening","Image matching;Satellites;Image color analysis;Color;Spatial resolution;Image fusion;Correlation","geographic information systems;geophysical image processing;image colour analysis;image fusion;image matching;image registration;image resolution;remote sensing","image fusion;ground control points;reference image maps;automated precise geo-registration;higher georeferencing accuracy;reference chips;incoming satellite images;aerial color ortho-images;pansharpened satellite images;automated precise georeferencing;high-resolution satellite imagery;satellite-aerial image matching success rate","","1","","8","IEEE","25 Nov 2019","","","IEEE","IEEE Conferences"
"Two-stage fusion of thermal hyperspectral and visible RGB image by PCA and guided filter","W. Liao; X. Huang; F. Van Coillie; G. Thoonen; A. Pižurica; P. Scheunders; W. Philips","IGhent University-TELIN-IPI-iMinds, Ghent, Belgium; The State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; FORSIT, Gent University, Ghent, Belgium; iMinds-Vision Lab, University of Antwerp, Antwerp, Belgium; IGhent University-TELIN-IPI-iMinds, Ghent, Belgium; IGhent University-TELIN-IPI-iMinds, Ghent, Belgium; IGhent University-TELIN-IPI-iMinds, Ghent, Belgium","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","23 Oct 2017","2015","","","1","4","Nowadays, advanced technology in remote sensing allows us to get multi-sensor and multi-resolution data from the same region. Fusion of these data sources for classification remains challenging problems. In this paper, we propose a novel algorithm for hyperspectral (HS) image pansharpening with two-stage guided filtering in PCA (principal component analysis) domain. In the first stage, we first downsample the highresolution RGB image to the same spatial resolution of original low-resolution HS image, and use guided filter to transfer the image details (e.g. edge) of the downsampled RGB image to the original HS image in the PCA domain. In the second stage, we perform upsampling on the resulting HS image from the first stage by using original high-resolution RGB image and guided filter in PCA domain. This yields a clear improvement over an older approach with one stage guided filtering in PCA domain. Experimental results on fusion of a low spatial-resolution Thermal Infrared HS image and a high spatial-resolution visible RGB image from the 2014 IEEE GRSS Data Fusion Contest, are very encouraging.","2158-6276","978-1-4673-9015-6","10.1109/WHISPERS.2015.8075405","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075405","Hyperspectral images;pansharpening;principal component analysis;guided filter","Principal component analysis;Spatial resolution;Hyperspectral imaging;Image edge detection","hyperspectral imaging;image filtering;image fusion;principal component analysis;remote sensing","hyperspectral image pansharpening;two-stage guided filtering;PCA domain;high-resolution RGB image;low-resolution HS image;multisensor;data sources;visible RGB image fusion;thermal infrared HS image;thermal hyperspectral RGB image fusion;remote sensing;remote sensing;principal component analysis","","12","1","10","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Dual-Branch Fusion Network for Residential Area Extraction from a Ziyuan-3 Multi-Spectral and Multi-View Data Set","D. Li; J. Li; X. Huang","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2722","2725","The accurate extraction of residential area is of great significance to disaster assessment, urban management, and climate change research. Deep learning-based methods are limited by annotations acquisition and single data source. In this paper, a dual-branch encoder is proposed to extract the features of different inputs, and a multi-attention fusion module is proposed to effectively fuse dual-branch features. Furthermore, we propose a novel encoder-decoder architecture, called the dual-branch fusion network (DBNet). In addition, we propose a large-scale residential area extraction data set (ZRA) containing 43 Ziyuan-3 (ZY3) multi-spectral (MS) and multi-view (MV) images. Experiments on ZRA, our DBNet achieve state-of-the-art performance with a F1 score of 84.6% and an IoU of 73.3%.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883483","Residential Area Extraction;High-Resolution Remote Sensing;Convolutional Neural Network;Ziyuan-3 Satellite Images","Training;Satellites;Fuses;Annotations;Image edge detection;Soft sensors;Urban areas","decoding;disasters;feature extraction;geophysical image processing;image fusion;learning (artificial intelligence);remote sensing;sensor fusion;town and country planning","multiview data set;disaster assessment;climate change research;deep learning-based methods;annotations acquisition;single data source;dual-branch encoder;multiattention fusion module;dual-branch features;novel encoder-decoder architecture;dual-branch fusion network;large-scale residential area extraction data;Ziyuan-3 multispectral;multiview images","","","","15","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Image Registration of Satellite Imagery with Deep Convolutional Neural Networks","M. Vakalopoulou; S. Christodoulidis; M. Sahasrabudhe; S. Mougiakakou; N. Paragios","CVN, CentraleSupélec, Université Paris-Saclay and INRIA Saclay, France; ARTORG Center, University of Bern, Bern, Switzerland; CVN, CentraleSupélec, Université Paris-Saclay and INRIA Saclay, France; ARTORG Center, University of Bern, Bern, Switzerland; TheraPanacea, Paris, France","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","4939","4942","Image registration in multimodal, multitemporal satellite imagery is one of the most important problems in remote sensing and essential for a number of other tasks such as change detection and image fusion. In this paper, inspired by the recent success of deep learning approaches we propose a novel convolutional neural network architecture that couples linear and deformable approaches for accurate alignment of remote sensing imagery. The proposed method is completely unsupervised, ensures smooth displacement fields and provides real time registration on a pair of images. We evaluate the performance of our method using a challenging multitemporal dataset of very high resolution satellite images and compare its performance with a state of the art elastic registration method based on graphical models. Both quantitative and qualitative results prove the high potentials of our method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898220","Deep Learning;Deformable and Linear Registration;Convolutional Neural Networks (CNN);Very High Resolution Satellite Images","Remote sensing;Strain;Satellites;Computer architecture;Image resolution;Image registration;Convolutional neural networks","convolutional neural nets;geophysical image processing;image fusion;image registration;image resolution;learning (artificial intelligence);remote sensing","time registration;challenging multitemporal dataset;high resolution satellite images;elastic registration method;image registration;deep convolutional neural networks;multimodal satellite imagery;multitemporal satellite imagery;change detection;image fusion;deep learning;novel convolutional neural network architecture;deformable approaches;remote sensing imagery;smooth displacement fields","","8","","13","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Image Fusion of RISAT-1 SAR Backscattered Image with AWiFS Optical Data","S. S. Mohd Naseem Akhter; P. P. Rege","Electronics and Telecommunication, College of Engineering, Pune (Affiliated to Savitribai Phule Pune University), Pune, India; Electronics and Telecommunication, College of Engineering, Pune (Affiliated to Savitribai Phule Pune University), Pune, India","2018 IEEE Punecon","27 Jun 2019","2018","","","1","5","Synthetic Aperture Radar (SAR) imaging is independent of solar illumination and cloud cover. The interpretability of optical images is improved by fusing with SAR images. The pixel intensity values of SAR image are often converted to physical quantity called backscattering coefficient measured in dB. It is an important property of SAR which is used to differentiate among the land surfaces. Hence, converting SAR image to backscattered image using backscattering coefficient provides more information compared to SAR image. SAR images are captured in four polarization modes. HH and VV polarization modes provide information of smooth surface and water bodies while HV and VH polarization modes provide information of vegetation and crop. In this paper, fusion of backscattered image in HH and HV polarization mode with optical image is carried out. Product from RISAT-1 and AWiFS are used for fusion.","","978-1-5386-7278-5","10.1109/PUNECON.2018.8745384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8745384","Image Fusion;Synthetic Aperture Radar;RISAT-1;AWiFS;Backscattered image","","backscatter;geophysical image processing;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar;vegetation;vegetation mapping","image fusion;RISAT-1 SAR;Synthetic Aperture Radar imaging;optical image;SAR image;solar illumination;cloud cover;backscattering coefficient;HH polarization mode;VV polarization mode;HV polarization mode;VH polarization mode;vegetation information;crop information;backscattered image fusion;pixel intensity values","","1","","16","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"An Enhanced Image Fusion Framework Using Morphological Operations Based Unsharp Masking","S. K. Panguluri; L. Mohan","Department of Electronics and Communication Engineering, Vignan's Foundation for Science, Technology, and Research, Guntur, Andhra Pradesh, India; Department of Electronics and Communication Engineering, Vignan's Foundation for Science, Technology, and Research, Guntur, Andhra Pradesh, India","2021 International Conference on Computer Communication and Informatics (ICCCI)","21 Apr 2021","2021","","","1","6","The idea of infrared (I-R) and Visible (V-I) image fusion is to integrate multiple source images and to produce a single useful informative image. Nowadays the image generated from I-R and V-I image fusion process has been used majorly in surveillance and remote sensing applications. It plays a crucial role in improving visibility and situation awareness especially in surveillance applications. This paper is introducing an enhanced I-R and V-I image fusion framework. A new Enhancement method is constructed using morphological operations based unsharp masking has been used in this algorithm for enhancing I-R and V-I source images. This enhancement method has produced high quality enhanced results which in return tremendously helped in improving the final fusion result. In this algorithm curve-let transform has been used to produce “detailed” and “approximation” coefficients. Integration of “approximation” coefficients is done through using “PCA fusion rule”. Combining of “detailed” coefficients is done with using “max fusion rule”. Fused image reconstruction is done with using inverse curve-let transform. The proposed fusion framework has produced superior results and outperformed than the similar existing fusion frameworks in terms of both visual quality and metrics values in comparison.","2329-7190","978-1-7281-5875-4","10.1109/ICCCI50826.2021.9402531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402531","infrared image;visible image;morphological operations based unsharp masking;curve-let transform","Measurement;Visualization;Surveillance;Transforms;Image fusion;Image reconstruction;Morphological operations","computational geometry;curvelet transforms;image enhancement;image fusion;image reconstruction;infrared imaging;inverse transforms;principal component analysis","surveillance applications;morphological operations;unsharp masking;approximation coefficients;PCA fusion rule;fused image reconstruction;visible image fusion;infrared image fusion;IR source image enhancement;VI source image enhancement;max fusion rule;inverse curvelet transform","","1","","15","IEEE","21 Apr 2021","","","IEEE","IEEE Conferences"
"Multi-focus image fusion with online sparse dictionary learning","J. Wang; L. Liu; X. Zhu; N. Ai; K. Yan","School of Information Science and Technology, Northwest University, Xi'an, Shaanxi Province, China; School of Information Science and Technology, Northwest University, Xi'an, Shaanxi Province, China; School of Information Science and Technology, Northwest University, Xi'an, Shaanxi Province, China; School of Information Science and Technology, Northwest University, Xi'an, Shaanxi Province, China; China Academy of Space Technology (Xi' an), Institute of Remote Sensing and Data Transmission, Xi'an, Shaanxi Province, China","2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS)","23 Apr 2018","2017","","","406","409","This paper presents an effective multi-focus image fusion method based on the online sparse dictionary learning with double sparsity model. First, we learn the dictionaries through the source images using the online sparse dictionary learning algorithm, which enables a multi-scale analysis and train an adaptive dictionary. Then, the sparse representation coefficients of the source images may be acquired by the learned dictionary. Finally, the fused image is formed by choosing the max fusion rule and the learned dictionary. Experimental results show that the proposed method is superior to the conventional fusion methods in terms of the visual and indicator evaluation.","2327-0594","978-1-5386-0497-7","10.1109/ICSESS.2017.8342942","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8342942","multi-focus image fusion;online sparse dictionary learning;double-sparsity model;K-SVD","","image fusion;image representation","sparse representation coefficients;source images;fused image;max fusion rule;multifocus image fusion method;online sparse dictionary learning algorithm;adaptive dictionary;double sparsity model","","1","","10","IEEE","23 Apr 2018","","","IEEE","IEEE Conferences"
"Better Fusion of Multi-scale Features for Remote Sensing Object Detection","X. Feng","Tiangong University, Tianjin, China","2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE)","21 Feb 2022","2022","","","271","274","In recent years, techniques for generating remotely sensed images have been widely developed and remote sensing object detection has received increasing attention. In order to detect as many objects at different scales as possible in high-resolution remote sensing images, existing remote sensing target detection methods tend to use feature pyramid networks to extract features. However, different levels of the feature map contain objects at different scales. As a result, while identifying one predictive result in one feature map as a positive sample, other scale feature maps may treat this one predictive result as background. This can reduce the accuracy of the object detection algorithm. To fuse multi-scale features more effectively, we propose a remote sensing object detection method (MSFF) based on multi-scale feature fusion, which classifies all targets into different scales for location and classification through adaptive pooling. Our method improves the network structure of the feature pyramid network and also recalculates the weights to reduce the semantic gap in feature fusion. Experimental results show that our method has good results. Our proposed method achieves a good result of 75.6% mAP on the DOTA dataset.","","978-1-6654-0886-8","10.1109/ICCECE54139.2022.9712836","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712836","object detection;deep learning;remote sensing;FPN;multi-scale feature fusion","Knowledge engineering;Fuses;Semantics;Object detection;Feature extraction;Prediction algorithms;Sensors","feature extraction;geophysical image processing;image classification;image fusion;object detection;remote sensing","high-resolution remote sensing images;remote sensing target detection methods;feature pyramid network;feature map;predictive result;scale feature maps;object detection algorithm;multiscale features;remote sensing object detection method;multiscale feature fusion;good result;remotely sensed images","","1","","10","IEEE","21 Feb 2022","","","IEEE","IEEE Conferences"
"Fusion detection of ship targets in low resolution multi-spectral images","Y. Liu; L. Yao; W. Xiong; Z. Zhou","School of Electronic Science and Engineering, National University of Defense Technology, Hunan, China; Research Institute of Information Fusion, Naval Aeronautical Engineering Institute, Yantai, Shandong, China; Research Institute of Information Fusion, Naval Aeronautical Engineering Institute, Yantai, Shandong, China; School of Electronic Science and Engineering, National University of Defense Technology, Hunan, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","6545","6548","Aiming at ship detection in multi-spectral images at low resolution, this paper proposes a new method for ship detection based on fusion detection which combines spectral feature with thermal feature. Firstly, it selects infrared band instead of visible band image to detect cloud according to size feature. With cloud available, it does segmentation work in thermal infrared image for the removal of cloud pixels. Then, the result is mapped to the IR images and cloud masking is completed. Next, the fusion of two kinds of images using wavelet transform is adopted. At last, the fused image is used to detect ships and morphological operations are used to discriminate ships. The experiment result on multi-spectral data of Landsat 8 shows that the proposed method which is robust against clutter can detect ships effectively.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730710","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730710","Ship detection;Landsat 8;Image fusion;Wavelet Transform","Marine vehicles;Clouds;Satellites;Remote sensing;Image resolution;Earth;Clutter","image fusion;image segmentation;object detection;remote sensing;ships;wavelet transforms","ship target fusion detection;multispectral images;ship detection;spectral feature;thermal feature;visible band image;cloud detection;image segmentation;thermal infrared images;cloud pixel removal;cloud masking;image fusion;wavelet transform;Landsat 8 spectral data","","9","","8","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Image Fusion for Hyperspectral Image Super-Resolution","H. Irmak; G. B. Akar; S. E. Y. uksel","Radar and Electronic Warfare Systems Business Sector, Aselsan Inc., Ankara, TURKEY; Dept. of Electrical and Electronics Eng., Middle East Techical University, Ankara, TURKEY; Dept. of Electrical and Electronics Eng., Hacettepe University, Ankara, TURKEY","2018 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","27 Jun 2019","2018","","","1","5","Hyperspectral sensors have high spectral resolution by capturing images in hundreds of bands. Despite the high spectral resolution, low spatial resolution of these sensors restricts the performance of the hyperspectral imaging applications such as target tracking and image classification. Fusing the hyper-spectral image (HSI) with higher spatial resolution RGB or multispectral image (MSI) data is a commonly used method in the resolution enhancement of the HSIs. In this paper, we propose a new fusion technique for the HSI super-resolution. The main contribution of this study is formulating the fusion problem in a quadratic manner and also regularizing the solution quadratically using smoothness prior. Moreover, another contribution of the proposed method is converting the fusion problem from spectral domain to the abundance map domain which gives more robust and spectrally consistent results. In the proposed method, first, abundance maps are obtained using linear spectral unmixing and then a quadratic energy function is obtained using these maps and high resolution (HR) RGB image. In addition, quadratic function is regularized using additional constraints. Solving the regularized quadratic function gives the HR abundance maps and these maps are used to reconstruct HR HSI. Experiments show that proposed method yields better performance as compared to state of the art methods in different performance metrics.","2158-6276","978-1-7281-1581-8","10.1109/WHISPERS.2018.8747231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747231","Image Fusion;Super-resolution;Hyperspectral;Abundance Maps;Quadratic optimization","Hyperspectral imaging;Spatial resolution;Measurement;Visualization;Image reconstruction","geophysical image processing;hyperspectral imaging;image colour analysis;image fusion;image reconstruction;image resolution","hyperspectral sensors;high spectral resolution;low spatial resolution;hyperspectral imaging applications;target tracking;image classification;higher spatial resolution RGB;multispectral image data;HSI super-resolution;spectral domain;linear spectral unmixing;quadratic energy function;regularized quadratic function;image fusion technique;high resolution RGB imaging;hyperspectral image superresolution enhancement;MSI;HR HSI reconstruction","","4","","17","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"Synthetic aperture radar image change detection based on an image fusion strategy","Z. Zhao; Z. Zhu; G. Chen; J. Zhao","North Automatic Control Technology Insititute, Taiyuan, China; North Automatic Control Technology Insititute, Taiyuan, China; Unit 32381, Beijing, China; College of Geoexploration Science and Technology, Jilin University, Changchun, China","2022 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)","12 Jan 2023","2022","","","151","155","Change detection is a hot topic and of great importance in remote sensing. The logarithm operation can be an effective means of eliminating the influence of multiplicative noise in the synthetic aperture radar (SAR) image. However, due to the nature of the logarithmic function, regions of change with high gray values are attenuated. In this study, we propose SAR to detect image changes based on an image fusion strategy by combining two different methods. First, we reduce speckle by non-subsampled shearlet transform in the log-domain, since this logarithmic function has the advantage of transforming multiplicative speckle noise into additive noise. As we know, the difference method always detects the areas with the largest changes. The difference method based on saliency extraction (SE) is applied to the exponent transformational SAR image to complement the disadvantage of WLR. WLR is used to obtain a change map of WLR to reduce the influence of SE when WLR does not detect the changed areas. Finally, two change maps can be added to obtain the final result. Experimental results for real SAR image pairs show the effectiveness of the proposed method in terms of detection rate, false alarm rate, and overall accuracy.","","978-1-6654-6468-0","10.1109/ICICML57342.2022.10009685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10009685","Change detection;image fusion strategy;non-subsampled shearlet transform;saliency extraction;synthetic aperture radar","Additive noise;Radar detection;Transforms;Speckle;Radar imaging;Radar polarimetry;Synthetic aperture radar","geophysical image processing;image denoising;image fusion;object detection;radar imaging;speckle;synthetic aperture radar;transforms","additive noise;change map;changed areas;detection rate;difference method;exponent transformational SAR image;high gray values;image changes;image fusion strategy;largest changes;logarithm operation;logarithmic function;multiplicative noise;multiplicative speckle noise;remote sensing;SAR image pairs;synthetic aperture radar image change detection;WLR","","","","11","IEEE","12 Jan 2023","","","IEEE","IEEE Conferences"
"Local variance based fusion method for multi-component images","A. B. Far; F. Flitti; J. Altiti","School of Engineering, American University in Dubai, Dubai, UAE; Faculty of Engineering, Higher Colleges of Technology, Dubai, UAE; Faculty of Engineering, Higher Colleges of Technology, Dubai, UAE","2020 Advances in Science and Engineering Technology International Conferences (ASET)","16 Jun 2020","2020","","","1","4","Image fusion has numerous applications in medicine, remote sensing, astronomy, military and security. This paper presents a new image fusion method based on local weights measuring the local image variability in each input image. The method was tested on medical images, remote sensing images and multi-focus images and compared to three existing methods visually and using objective measures. The results obtained with the proposed method are visually and objectively good in comparison with the other methods.","","978-1-7281-4640-9","10.1109/ASET48392.2020.9118205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9118205","image fusion;local variability;multicomponent images","Weight measurement;Visualization;Extraterrestrial measurements;Security;Remote sensing;Image fusion;Astronomy","image fusion;statistical analysis","multicomponent images;image fusion;local image variability;objective measures;local variance based fusion","","","","13","IEEE","16 Jun 2020","","","IEEE","IEEE Conferences"
"Hyperspectral and Multispectral Image Fusion with Dual-Source Spatial-Spectral Dictionary","J. Tian; Y. Zhang; Y. Lu; S. Mei","Shaanxi Key Laboratory of Information Acquisition and Processing, Northwestern Polytechnical University, Xi'an, China; Shaanxi Key Laboratory of Information Acquisition and Processing, Northwestern Polytechnical University, Xi'an, China; Shaanxi Key Laboratory of Information Acquisition and Processing, Northwestern Polytechnical University, Xi'an, China; Shaanxi Key Laboratory of Information Acquisition and Processing, Northwestern Polytechnical University, Xi'an, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","7034","7037","This paper presents a new dictionary-based method to fuse hyperspectral image (HSI) and multispectral image (MSI) of the same observed scene. To incorporate spatial as well as spectral information and features from both source HSI and MSI, dual-source spatial-spectral dictionary pair is constructed. Furthermore, collaborative representation based image representation and reconstruction is employed for its outstanding representation performance and efficiency. Simulative experiments illustrate that the newly proposed fusion method is capable of producing better or comparable fusion result when compared to some state-of-the-art dictionary-based HSI and MSI fusion methods, with much less computational cost.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518809","Collaborative representation;dictionary;hyperspectral;image fusion;multispectral","Dictionaries;Spatial resolution;Hyperspectral imaging;Collaboration","hyperspectral imaging;image fusion;image reconstruction;image representation","image reconstruction;multispectral image fusion;state-of-the-art dictionary-based HSI fusion methods;MSI fusion methods;spatial information;hyperspectral image;dictionary-based method;collaborative representation based image representation;dual-source spatial-spectral dictionary pair;spectral information;observed scene","","","","6","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"NPP Estimation of High Heterogeneous Region based on Spatiotemporal Fusion","W. Li; J. Wu; L. Zhao","School of Geographic and Biologic information, Nanjing University of Posts and Telecommunications, NanJing, China; NFGA, The Key Laboratory of Forestry Remote Sensing and Information System, Beijing, China; NFGA, The Key Laboratory of Forestry Remote Sensing and Information System, Beijing, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2841","2844","Net Primary Productivity (NPP) is an important part of the carbon cycle of terrestrial ecosystems. The technical advantages and huge potential of remote sensing technology in NPP estimation make it a hot spot in the research field. The vigorous development of many remote sensing fusion algorithms provides fine-resolution remote sensing data support for high-precision NPP dynamic monitoring. In recent years, the expansion of urban areas and climate change have had a great impact on the NPP of vegetation. In accordance with the requirements of large-scale and high-temporal-spatial resolution productivity assessment of urban area, we chose the northern Jiangsu area as the research area and uses three remote sensing data spatiotemporal fusion methods, STARFM, ESTARFM, and STDFA to blend Landsat and MODIS data. Three methods are compared in aspects of NDVI reconstruction ability in large-scale, highly heterogeneous regional application scenarios, the accuracy of NPP estimation through CASA model, and the ability of fine spatial description. The results of the study show that STDFA gets the highest correlation coefficient between its NDVI reconstruction results and MODIS products, which is 0.82. Correlation coefficient of STARFM and ESTARFM are 0.77 and 0.75, respectively. The STARFM is significantly lower in the NPP estimation results among the three methods, meanwhile STDFA performs best in our test site.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883185","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883185","Net primary productivity;Spatial-temporal fusion;NDVI time series;STDFA","Productivity;Correlation coefficient;Heuristic algorithms;Urban areas;Estimation;Vegetation mapping;Spatiotemporal phenomena;Climate change","geophysical image processing;image fusion;image resolution;remote sensing;sensor fusion;vegetation","high heterogeneous region;Net Primary Productivity;remote sensing technology;remote sensing fusion algorithms;fine-resolution remote sensing data support;high-precision NPP dynamic monitoring;urban area;high-temporal-spatial resolution productivity assessment;northern Jiangsu area;remote sensing data spatiotemporal fusion methods;STDFA;highly heterogeneous regional application scenarios;fine spatial description;NPP estimation results","","","","9","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Cloud removal by fusing multi-source and multi-temporal images","C. Zhang; Z. Li; Q. Cheng; X. Li; H. Shen","School of Resource and Environmental Science, Wuhan University, China; School of Resource and Environmental Science, Wuhan University, China; School of Urban Design of Wuhan University, Wuhan University, China; School of Remote Sensing and Information Engineering, Wuhan University, China; School of Resource and Environmental Science, Wuhan University, China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","2577","2580","Remote sensing images often suffer from cloud cover. Cloud removal is required in many applications of remote sensing images. Multitemporal-based methods are popular and effective to cope with thick clouds. This paper contributes to a summarization and experimental comparison of the existing multitemporal-based methods. Furthermore, we propose a spatiotemporal-fusion with poisson-adjustment method to fuse multi-sensor and multitemporal images for cloud removal. The experimental results show that the proposed method is able to obtain more accurate results than the current multitemporal-based methods, especially when the multi-temporal images suffer from significant changes.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127522","Cloud removal;multi-temporal;multi-sensor;data fusion","Remote sensing;Indexes","clouds;geophysical image processing;geophysical signal processing;image classification;image fusion;remote sensing;sensor fusion","multitemporal-based methods;multisource images;spatiotemporal-fusion;remote sensing images;cloud cover;cloud removal;multitemporal images;multisensor;poisson-adjustment method","","5","","9","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Object based fusion of multi-sensor imagery while preserving spectrally significant information","M. Goyal; K. S. Rajan","International Institute of Information Technology, Hyderabad, India; International Institute of Information Technology, Hyderabad, India","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","4028","4031","Data fusion is a prevalent method to extract the best combination of satellite images from different modalities - spectral, spatial, temporal. A new method of object-based fusion of high resolution multispectral (MS) and panchromatic (PAN) images is proposed in this paper, which emphasizes on spectral characteristics preservation. It is a hybrid approach where individual objects detected in the images are considered for mapping data and information transfer is done on a per-pixel basis. In addition, the paper proposes a quantitative assessment measure to assess the spectral distortion of the fused outcomes. The quantitative results demonstrate that the proposed method is better in terms of preserving spectral characteristics as compared to other widely used methods such as Principle Component (PC) based fusion, Intensity Hue Saturation (IHS) based fusion and Color Normalization (CN) or Brovey.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127884","multi sensor image fusion;object based image fusion;spectral distortion","Image fusion;Image segmentation;Spatial resolution;Distortion;Image color analysis;Data integration","distortion;image fusion;image resolution;object detection;radar imaging;remote sensing;spectral analysis","object based fusion;multisensor imagery;data fusion;MS;spectral characteristics preservation;hybrid approach;information transfer;per-pixel basis;quantitative assessment measure;spectral distortion;objects detection;data mapping;spectrally significant information preservation;multispectral images;panchromatic images;PAN;satellite images extraction","","","","11","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Data Fusion and Remote Sensing: An ever-growing relationship","M. Schmitt; X. X. Zhu",TUM; TUM,"IEEE Geoscience and Remote Sensing Magazine","16 Dec 2016","2016","4","4","6","23","Characterized by a certain focus on the heavily discussed topic of image fusion in its beginnings, sensor data fusion has played a significant role in the remote sensing research community for a long time. With this article, we aim to provide a short overview of established definitions, targeting a generalized understanding of the topic. In addition, a review of the state of the art of remote sensing data fusion research is given. By bringing together the conventional view expressed in the classical data fusion community and a review of current activities in the field of Earth observation, this article provides a holistic view of generic data fusion concepts and their applicability to the remote sensing domain.","2168-6831","","10.1109/MGRS.2016.2561021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7740215","","Data fusion;Remote sensing;Synthetic aperture radar;Data integration;Optical imaging;Laser radar","image fusion;remote sensing","sensor data fusion;image fusion;remote sensing research community;remote sensing data fusion research;classical data fusion community;Earth observation;generic data fusion concepts;remote sensing domain","","161","","185","IEEE","16 Dec 2016","","","IEEE","IEEE Magazines"
"Evaluation of Spatiotemporal Fusion Models in Land Surface Temperature Using Polar-Orbiting and Geostationary Satellite Data","Y. Li; H. Wu; Z. -L. Li; S. Duan; L. Ni","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Agricultural Remote Sensing, Institute of Agricultural Resources and Regional Planning, Chinese Academy of Agricultural Sciences, Ministry of Agriculture, Beijing, China; Key Laboratory of Agricultural Remote Sensing, Institute of Agricultural Resources and Regional Planning, Chinese Academy of Agricultural Sciences, Ministry of Agriculture, Beijing, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","236","239","The tradeoff between spatial and temporal resolution in satellite observations substantially restrains the potential applications of Land Surface Temperature (LST) products. So far, many spatiotemporal fusion models have been developed to address the issue and a unified comparison in LST data fusion is still required. In this paper, four popular spatiotemporal fusion algorithms including Spatial and Temporal Adaptive Reflectance Fusion Model (STARFM), Unmixing-based data fusion method, Flexible Spatiotemporal Data Fusion (FSDAF) and Spatio-Temporal Integrated Temperature Fusion Model (STITFM) were adopted to generate high spatial resolution LST using polar-orbiting and geostationary satellite data. The predicted LST was evaluated by the actual LST product and the result indicates that the overall accuracy of FSDAF is satisfied (about 2.87K) and the FSDAF algorithm is recommended to generate LSTs at high spatial and temporal resolution in heterogeneous area.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323319","National Key R&D Program of China(grant numbers:2016YFB0500304); National Natural Science Foundation of China(grant numbers:41871267); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323319","Land Surface Temperature;Polar-orbiting Satellite;Geostationary Satellite;Spatiotemporal Fusion;Comparison","Land surface temperature;Remote sensing;Spatial resolution;Temperature sensors;Spatiotemporal phenomena;Land surface;Prediction algorithms","geophysical image processing;geophysical techniques;image fusion;image resolution;land surface temperature;remote sensing;sensor fusion;spatiotemporal phenomena","Unmixing-based data fusion method;Flexible Spatiotemporal Data Fusion;Spatio-Temporal Integrated Temperature Fusion Model;high spatial resolution LST;geostationary satellite data;actual LST product;temporal resolution;spatiotemporal fusion models;satellite observations;Land Surface Temperature products;LST data fusion;popular spatiotemporal fusion algorithms;Temporal Adaptive Reflectance Fusion Model","","","","12","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Multi-Scale Bidirectional Feature Fusion for One-Stage Oriented Object Detection in Aerial Images","L. Pei; G. Cheng; X. Sun; Q. Li; M. Zhang; S. Miao","School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2592","2595","This paper aims to address the problem of oriented object detection under the complex background of remote sensing images. To this end, we propose a one-stage object detection method with feature fusion structure, and modify the loss function to enhance the detection of small objects. More specifically, on the basis of the end-to-end one-stage object detection model RetinaNet, the method of gliding the vertices of the horizontal bounding box is used to describe an oriented object. In order to obtain multi-scale context information, we design a feature fusion module. Besides, we propose a novel area-weighted loss function to pay more attention to small objects. Experimental results conducted on the DOTA dataset demonstrate that the proposed framework outperforms several state-of-the-art baselines.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555142","National Science Foundation of China(grant numbers:61772425); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555142","Deep Learning;Remote Sensing Images;Oriented Object Detection;Feature Fusion","Object detection;Feature extraction;Sensors;Data mining;Remote sensing","deep learning (artificial intelligence);geophysical image processing;image fusion;object detection;remote sensing","multiscale bidirectional feature fusion;one-stage oriented object detection;complex background;remote sensing images;feature fusion structure;horizontal bounding box;multiscale context information;area-weighted loss function;aerial images;RetinaNet;vertices;DOTA dataset;deep learning","","3","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Reproducibility of Spectral and Radiometric Normalized Similarity Indices for Multiband Images","A. Arienzo; L. Alparone; B. Aiazzi; S. Baronti; A. Garzelli","Department of Information Engineering (DINFO), University of Florence, Florence, Italy; Department of Information Engineering (DINFO), University of Florence, Florence, Italy; Research Area of Florence, Institute of Applied Physics (IFAC-CNR), Sesto F.no, FI, Italy; Research Area of Florence, Institute of Applied Physics (IFAC-CNR), Sesto F.no, FI, Italy; Department of Information Engineering and Mathematics, University of Siena, Siena, Italy","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","839","842","Several tasks of remote sensing entail the measurement of the similarity/dissimilarity of a test multiband image to a reference multiband image. To this purpose, several indices has been developed over the last two decades. The most widely used indices are normalized to avoid dependence on the data format. In this work, we will focus on such indices and provide a novel insight on their behaviors. Spectral indices are those performing crossed measurements between couple of different bands of the test and reference image. Wherever crossed measurements do not occur, the index is purely spatial, or better radiometric. Both theoretical insights and simulations performed on a GeoEye dataset, with the products of twelve pansharpening methods, show that their performance ranking does not depend on the data format for purely radiometric indices, while it significantly depends on the data format, either spectral radiance or digital numbers (DN), for a purely spectral index, like the spectral angle mapper (SAM). The dependence on the data format is weak for indices that balance the spectral and radiometric similarity, like the family of indices, Q2n, based on hypercomplex algebra.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898662","Multiband images;Pansharpening;Statistical quality indices;Remote sensing data formats","Indexes;Remote sensing;Spatial resolution;Radiometry;Atmospheric measurements;Distortion measurement","image fusion;image resolution;radiometry;remote sensing","spectral indices;data format;spectral radiance;spectral similarity;radiometric similarity;radiometric normalized similarity indices;remote sensing;reference multiband image;GeoEye dataset","","","","18","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Marine Sediment Mapping Using Multi-Source and Multi-Dimensional Acoustic Images Based on Evidential Fusion","X. Chen; J. Li; W. Shen; L. Tao; Y. Cui; Y. Hong","School of Earth and Space Sciences, Peking University, Beijing, China; Faculty of Geographical Science, Beijing Normal University, Beijing, China; College of Marine Sciences, Shanghai Ocean University, Shanghai, China; School of Geographic Sciences, Nanjing University of Information Science and Technology, Nanjing, China; School of Earth and Space Sciences, Peking University, Beijing, China; School of Earth and Space Sciences, Peking University, Beijing, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","4989","4992","This paper proposes a novel method to fuse multi-source acoustical remote sensing images for marine sediment mapping. Acoustic images from sidescan sonar, multibeam bathymetry, and sub-bottom profiler describe the 2-D, 2.5-D, and 3-D properties of the seabed sediment respectively. In attempt to make use of the multi-dimensional information from the multi-source data, the evidential fusion method, combined with object-based classification and spatial overlay analysis is proposed. Firstly, marine sediments are independently classified in the multi-source acoustic images with object based methods. Then, the spatial overlay analysis is conducted to group the classification results as evidence of different sediments. Finally, the evidential fusion method is employed to determine the exact distribution of sediments on the map. The proposed method introduces the classification results of sub-bottom profiler data that provide useful information, even though the data is limited in spatial coverage and is rarely used in automatic sediment mapping. The experiments show that the fused data from the three different sources of acoustic images significantly improve the mapping accuracy.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517470","Marine sediment;multi-source image;evidential fusion;classification;ocean;remote sensing","Sediments;Acoustics;Support vector machines;Remote sensing;Backscatter;Radio frequency;Fuses","acoustic imaging;acoustic signal processing;bathymetry;geophysical image processing;geophysical signal processing;image classification;image fusion;oceanographic techniques;remote sensing;sediments;sensor fusion;sonar;sonar imaging","automatic sediment mapping;fused data;marine sediment mapping;multidimensional acoustic images;multisource acoustical remote sensing images;seabed sediment;multidimensional information;multisource data;evidential fusion method;object-based classification;spatial overlay analysis;multisource acoustic images;object based methods;subbottom profiler data","","","","7","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"SD-DSAN: Saliency-Driven Dense Spatial Attention Network for Pan-Sharpening","W. Zhu; Y. Sun; S. Wang; L. Zhang","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1079","1082","The demands for spectral and spatial quality in remote sensing (RS) images vary from region to region. Saliency detection is an effective tool to distinguish different regions with different demands. In this paper, we introduce saliency detection to satisfy these demands and propose a novel saliency-driven pan-sharpening network to further improve the fusion quality. Firstly, we combine foreground distribution with background prior to generate the initial saliency map, and implement least-square optimization to improve the detection accuracy. Then, we construct a dense spatial attention network trained through a new spatial-spectral-based loss function designed by saliency to meet diverse spectral and spatial needs of different regions. Thus, accurate fused images can be predicted. Experiments on SPOT-5 dataset indicate that our proposal has excellent properties with respect to the unified spatial-spectral quality against state-of-the-art methods.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884763","Beijing Natural Science Foundation(grant numbers:4222046); National Natural Science Foundation of China(grant numbers:61571050,41771407); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884763","Pan-sharpening;spatial-spectral-based loss;remote sensing;residual network;saliency detection","Sensors;Proposals;Remote sensing;Optimization;Saliency detection","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);object detection;remote sensing","novel saliency-driven pan-sharpening network;fusion quality;initial saliency map;detection accuracy;spatial-spectral-based loss function;accurate fused images;spatial-spectral quality;SD-DSAN;saliency-driven dense spatial attention network;spatial quality;remote sensing images;saliency detection;different demands","","","","16","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Few-Shot Remote Sensing Scene Classification with Multi-Metric Fusion","Z. Wang; J. Qiao","College of Computer Science & Engineering, Northeastern University, Shenyang, China; College of Computer Science & Engineering, Northeastern University, Shenyang, China","2022 15th International Conference on Advanced Computer Theory and Engineering (ICACTE)","14 Nov 2022","2022","","","38","43","Few-shot remote sensing scene classification is one of the research topics in the field of computer vision and few-shot learning, aiming to classify remote sensing scene through few training samples. The current methods of few-shot remote sensing scene classification use single metric, thus the classification accuracy is affected for the features cannot be effectively extracted. Therefore, we propose multi-metric fusion networks (MMFN) to address the problem via assembling a feature map multi encoder (FMME) and relation attention networks (RAN) to extract the features effectively and improve the classification accuracy. The FMME is designed to further encode the feature map which is extracted in the embedding phase to get different meaningful features. The RAN is aiming to calculate the similarity between features via fusing results of multiple methods based on image attention mechanism. Experimental results on three remote sensing data sets show that the multi-metric fusion method can extract meaningful features and effectively improve the classification performance of few-shot remote sensing scene.","2154-7505","978-1-6654-8180-9","10.1109/ICACTE55855.2022.9943594","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9943594","few-shot learning;remote sensing scene;metric learning;attention mechanism","Training;Computer vision;Image analysis;Phase measurement;Feature extraction;Sensors;Data mining","computer vision;feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing","classification accuracy;classification performance;few-shot learning;few-shot remote sensing scene classification use;multimetric fusion method;multimetric fusion networks;relation attention networks;remote sensing data sets","","","","24","IEEE","14 Nov 2022","","","IEEE","IEEE Conferences"
"Quality studies for N-band image fusion","Q. Guo; Q. Wang; M. Ehlers; H. Zhang","Institute for Geoinformatics and Remote Sensing, University of Osnabrueck, Osnabrueck, Germany; Institute for Geoinformatics and Remote Sensing, University of Osnabrueck, Osnabrueck, Germany; School of Physics and Optoelectronic Engineering, Guangdong University of Technology, Guangzhou, China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China","2015 International Conference on Fuzzy Theory and Its Applications (iFUZZY)","28 Jan 2016","2015","","","6","11","The Ehlers fusion method, which combines a standard intensity-hue-saturation (IHS) transform with the fast Fourier transform (FFT) filtering, is a good color preservation method for multi-temporal and multi-sensor datasets. However, for the dataset with more than three bands, the fusion process is complicated. Because only every three bands are fused repeatedly for multiple times until all bands are fused. The hyperspherical color sharpening (HCS) fusion method can fuse the dataset with more than three bands. The HCS approach uses a transform between n-dimensional Cartesian space and n-dimensional hyperspherical space. Moreover, from a structural point of view, the hyperspherical color space is very similar to the IHS color space. Although the existing principal component analysis (PCA) and Gram Schmidt (GS) methods are also able to integrate the dataset with more than three bands, but they, especially the PCA, usually lead to the spectral distortion. Hence, we propose the Ehlers-HCS fusion method to fuse n band datasets, even to hyperspectral images. The original Ehlers fusion renamed as Ehlers-IHS fusion for easy distinction. The WorldView-2 dataset including a panchromatic and eight multispectral bands are used as the input experimental real data. The fused images are visually and quantitatively analyzed for the spectral preservation and spatial improvement to show the effectiveness of the Ehlers-HCS fusion.","2377-5831","978-1-4673-6570-3","10.1109/iFUZZY.2015.7391885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391885","","Image color analysis;Transforms;Principal component analysis;Remote sensing;Modulation;Image fusion;Spatial resolution","fast Fourier transforms;filtering theory;image fusion;principal component analysis","hyperspectral images;Gram Schmidt method;principal component analysis;n-dimensional hyperspherical space;n-dimensional Cartesian space;hyperspherical color sharpening fusion method;color preservation method;FFT filtering;fast Fourier transform filtering;standard intensity-hue-saturation transform;Ehlers fusion method;N-band image fusion","","","","26","IEEE","28 Jan 2016","","","IEEE","IEEE Conferences"
"On the Fusion Strategies of Sentinel-1 and Sentinel-2 Data for Local Climate Zone Classification","J. Gawlikowski; M. Schmitt; A. Kruspe; X. X. Zhu","Institute of Data Science, German Aerospace Center (DLR), Jena, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Institute of Data Science, German Aerospace Center (DLR), Jena, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Germany","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2081","2084","Local Climate Zone (LCZ) classification is the most commonly used scheme to analyze how local urban morphology affects the climate of local areas. Classification methods are often based on remote sensing data or on a fusion of several data sources. In this study, the effects of different fusion strategies of optical and synthetic aperture radar (SAR) data on the accuracy of LCZ classifications are investigated. The data processing is implemented with a convolutional neural network (CNN), where until a fusion layer, separate data sources are processed separately on branches. Strategies of splitting the data into branches and the effects of different fusion stages are compared, together with approaches based on sums of independent classifiers. For our setting, the stage of fusion does not seem to have a big influence on the accuracy. The results of this study contribute to a better understanding of cooperative usage of multispectral and SAR data.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324234","Local Climate Zone Classification;Data Fusion;Fusion Network","Training;Meteorology;Synthetic aperture radar;Optical imaging;Task analysis;Remote sensing;Vegetation mapping","convolutional neural nets;geophysical image processing;image classification;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar","multispectral SAR data;fusion stages;data sources;fusion layer;convolutional neural network;data processing;LCZ classifications;fusion strategies;remote sensing data;classification methods;local urban morphology;local climate zone classification;sentinel-2 data;sentinel-1","","7","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A Recurrent Refinement Network for Satellite Video Super-Resolution","Y. Xiao; X. Su; Q. Yuan","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","3865","3868","Deep learning-based methods have shown superior performance in VSR tasks. However, satellite video frames are characterized by large width, low resolution, and lack of features. Consequently, the conventional VSR method is not suitable for satellite video. In this paper, a recurrent refinement network is proposed. Considering that the vast majority of remote sensing images belong to the static background, a single-image SR (SISR) method is first used to obtain high-resolution features for a specific target frame. To further complement the missing details, the network learns the complementary information enhanced by an Encoder-Decoder structure from adjacent frames to refine the results of SISR. To measure the contribution of different adjacent frames to the recovery of the target frame, a temporal attention mechanism is introduced in the final fusion stage. The experiment on the video data of Jilin-1 demonstrates the effectiveness of our method.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553281","Satellite video;super-resolution;deep learning","Learning systems;Satellites;Aggregates;Superresolution;Task analysis;Remote sensing","image classification;image fusion;image resolution;learning (artificial intelligence);remote sensing;video signal processing","recurrent refinement network;satellite video super-resolution;deep learning-based methods;VSR tasks;satellite video frames;conventional VSR method;remote sensing images;single-image SR method;high-resolution features;specific target frame;different adjacent frames;video data","","","","7","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Geospatial techniques for flood inundation mapping","Kuldeep; P. K. Garg; R. D. Garg","Geomatics Engineering Group, Civil Engineering, Indian Institute of Technology, Roorkee, India; Uttarakhand Technical University, Dehradun, India; Geomatics Engineering Group, Civil Engineering, Indian Institute of Technology, Roorkee, India","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","4387","4390","In India, most of the rivers form big size natural islands due to change in its course. However, identification of suitable river island for construction of Eco-friendly parks/tourist destination is a very challenging task since these are exposed to river flooding. River islands which are least vulnerable to the impact of severe flooding can be a suitable place for construction of tourism destination such as eco-friendly Parks, Hotels etc. The study involves a two step approach viz. automatic extraction of river islands and model development for flood inundation mapping for extraction of eco-friendly tourism destinations. In this study, automatic extraction of the river islands has been carried out using texture based classification approach. The satellite data acquired by the Indian Remote Sensing Satellites sensors such as LISS-III and Cartosat-1 DEM have been used for analyses. In the first step, satellite imagery has been broadly categorized into 6 landuse/cover classes viz. Water, Sand, Islands, Settlements, Agriculture and Cropland. Extraction of such islands which remain unaffected during severe flooding has been accomplished with the flood inundation mapping which has been carried out in HEC-GeoRas with in GIS environment. The model utilizes the primary 4 inputs viz. geometry of the river (DEM, slope), time series data of water surface elevation, landuse/cover, and location of rain gauge station for flood inundation mapping. This paper also investigates the applicability of the eco-island concept to include protection of wetland, management of land-resources, sustainable use of natural resources and construction of ecological park/hotels. The output of the study will be very useful for Government authorities in stabilizing economy, and enhancing the tourism infrastructure in a better way.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730143","Remote sensing;Flooding;River islands;Texture descriptors;Image classification;Eco-tourism;Image fusion","Floods;Satellites;Rivers;Discharges (electric);Remote sensing;Spatial resolution","digital elevation models;feature extraction;floods;geographic information systems;geophysical image processing;image classification;image texture;land cover;land use;remote sensing;rivers;travel industry;wetlands","geospatial technique;flood inundation mapping;eco-friendly park construction;river flooding;severe flooding impact;tourism destination construction;automatic river island extraction;texture-based classification approach;satellite data;Indian remote sensing satellite;LISS-III DEM;Cartosat-1 DEM;satellite imagery;land use;land cover;sand;settlement;agriculture;cropland;HEC-GeoRas;GIS environment;river geometry;time series data;water surface elevation;rain gauge station location;wetland protection;land-resource management;natural resources;tourism infrastructure","","7","","8","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"A Spark-based Parallel NPTSR Algorithm for Hyperspectral Image fusion","X. Liu; Y. Zhu; Q. Zhu; J. Sun","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","2021 Ninth International Conference on Advanced Cloud and Big Data (CBD)","11 Jul 2022","2022","","","13","18","NPTSR is a super-resolution method for hyperspectral image fusion that uses tensor-tensor product to characterize nonlocal patch for the purpose of fusing hyperspectral images and multispectral images. Due to its high computational efficiency in single-machine environments, it is difficult to adapt NPTSR to large-scale remote sensing datasets and large numbers of iterations during the hyperspectral image super-resolution procedure. To address the above-mentioned limitations, this paper proposes a Spark-based parallel method for NPTSR algorithm. We develop a parallel method for nonlocal patches extraction relying on the characteristics of remote sensing data in tensor representation. In addition, we design the parallel implementation of the iterative procedure involved in NPTSR algorithm to accelerate its execution on Spark. Experimental results show that, compared with the serial version, the parallel NPTSR algorithm achieves significant speedup while guaranteeing similar image fusion accuracy and convergence rate. Moreover, the parallel implementation exhibits decent scalability to the increasing size of hyperspectral dataset.","","978-1-6654-0745-8","10.1109/CBD54617.2021.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9816259","hyperspectral image;image fusion;NPTSR;tensor computation;parallel algorithm","Tensors;File systems;Scalability;Superresolution;Big Data;Iterative algorithms;Sparks","geophysical image processing;hyperspectral imaging;image fusion;image representation;image resolution;iterative methods;parallel processing;remote sensing;tensors","tensor representation;similar image fusion accuracy;convergence rate;parallel implementation exhibits decent scalability;hyperspectral dataset;Spark-based parallel NPTSR algorithm;hyperspectral image fusion;super-resolution method;tensor-tensor product;nonlocal patch;hyperspectral images;multispectral images;high computational efficiency;single-machine environments;large-scale remote sensing datasets;hyperspectral image super-resolution procedure;Spark-based parallel method;nonlocal patches extraction;remote sensing data","","","","10","IEEE","11 Jul 2022","","","IEEE","IEEE Conferences"
"ROC curve analysis for validating objective image fusion metrics","N. Messer; S. Ezekiel; M. H. Ferris; E. Blasch; M. Alford; M. Cornacchia; A. Bubalo","Indiana University of Pennsylvania Indiana, PA; Indiana University of Pennsylvania Indiana, PA; Binghamton University, State University of New York, Binghamton, NY; Maria Cornacchia, Adnan Bubalo Air Force Research Laboratory, Rome, NY; Maria Cornacchia, Adnan Bubalo Air Force Research Laboratory, Rome, NY; Air Force Research Laboratory Information Directorate, Rome, NY, US; Air Force Research Laboratory Information Directorate, Rome, NY, US","2015 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)","31 Mar 2016","2015","","","1","6","Image fusion is a process that allows for the synthesis of information from multiple source images into a single image. There are many applications for image fusion including night vision, medical imaging, and remote sensing. Over the many applications, numerous image fusion algorithms have been explored from averaging pixel intensities to fusion through multi-resolution decomposition transforms such as the wavelet or contourlet. Objective evaluation of a given image fusion method is still a major challenge especially when there exists no reference image. Existing no-reference objective fusion metrics include information theory based metrics, image feature based metrics, and structural similarity metrics. However there has been very little work done in validating which objective metric best evaluates a given image fusion algorithm. Receiver operating characteristic (ROC) curves and the area under the curve (AUC) provide a viable validation method for metric selection. This study focuses on validating objective fusion metrics over mutual information, spatial frequency, and structural similarity Index Measure (SSIM) used to evaluate fusion algorithms for denoising applications.","2332-5615","978-1-4673-9558-8","10.1109/AIPR.2015.7444531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444531","Image Fusion;No-Reference Objective Fusion Metrics;Mutual Information;Spatial Frequency;Structural Similarity Index Measure (SSIM);Receiver Operating Characteristic (ROC) curves;Area Under the Curve (AUC)","Image fusion;Frequency measurement;Mutual information;Entropy;Receivers;Indexes","image denoising;image fusion;sensitivity analysis","ROC curve analysis;objective image fusion metrics validation;multiple image sources;night vision;medical imaging;remote sensing;pixel intensity;multiresolution decomposition transform;wavelet transform;contourlet transform;no-reference objective fusion metrics;information theory based metrics;image feature based metrics;structural similarity metrics;image fusion algorithm;receiver operating characteristic curve;area under the curve;AUC;metric selection;mutual information;spatial frequency;structural similarity index measure;SSIM;denoising application","","2","","20","IEEE","31 Mar 2016","","","IEEE","IEEE Conferences"
"A New Satellite Image Fusion Method Based on Distributed Compressed Sensing","F. Li; S. Hong; L. Wang","Department of Communication Engineering, Xiamen University, Fujian, China; Department of Communication Engineering, Xiamen University, Fujian, China; Department of Communication Engineering, Xiamen University, Fujian, China","2018 25th IEEE International Conference on Image Processing (ICIP)","6 Sep 2018","2018","","","1882","1886","In this paper, we propose a method for fusion of low-resolution multispectral (LRM) image and high-resolution panchromatic (HRP) image to obtain high-resolution multispectral (HRM) image based on distributed compressed sensing (DCS). In the proposed method, HRP image is firstly used to obtain approximation and detail dictionary. Then, joint-sparsity-model-1 (JSM-1) is applied directly to both LRM bands and HRM bands. Each band in LRM image is decomposed into common component and innovation component which can be sparsely represented over the approximation dictionary. Based on Orthogonal Matching Pursuit (OMP) algorithm, the sparse coefficients are calculated from JSM-1 of the LRM image. Lastly, each band in HRM image is modeled as the fusion of the corresponding LRM band and detail band over the detail dictionary. Two datasets are used in the experiments to validate the proposed method and the results show that the proposed method has better performance than the traditional methods.","2381-8549","978-1-4799-7061-2","10.1109/ICIP.2018.8451648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451648","Distributed compressed sensing;high-resolution panchromatic image;joint sparsity model;low-resolution multispectral image;satellite image fusion","Dictionaries;Compressed sensing;Sensors;Spatial resolution;Satellites;Matching pursuit algorithms","compressed sensing;geophysical image processing;image fusion;image resolution;iterative methods;remote sensing;sparse matrices","new satellite image fusion method;distributed compressed sensing;low-resolution multispectral image;high-resolution multispectral image;HRP image;joint-sparsity-model-1;LRM bands;HRM bands;LRM image;innovation component;approximation dictionary;JSM-1;HRM image;orthogonal matching pursuit algorithm;high-resolution panchromatic image;sparse coefficients","","1","","20","IEEE","6 Sep 2018","","","IEEE","IEEE Conferences"
"A Weakly Supervised Landslide Extraction Method from Remote Sensing Images Using Deep Attention and Multi-feature Fusion","X. Deng; L. Shen; X. Yan; D. Zheng","Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China","2022 3rd International Conference on Geology, Mapping and Remote Sensing (ICGMRS)","18 Aug 2022","2022","","","679","684","The semantic segmentation model significantly improves the accuracy of landslide extraction from very high resolution (VHR) remote sensing images, but it requires manual sketching of many pixel-level annotations. Pixel-level annotation needs can be solved by weakly supervised learning based on image-level annotations. We propose a weakly supervised strategy combining deep attention and multi-feature fusion for landslide extraction. By obtaining high quality class activation maps (CAMs), an accurate landslide extraction model can be trained. Many experiments on the VHR remote sensing images after the Jiuzhaigou earthquake show that the proposed strategy can obtains more complete and accurate CAMs, and the landslide extraction accuracy is better than mainstream weakly supervised methods and achieved results comparable to strong supervised method.","","978-1-6654-8595-1","10.1109/ICGMRS55602.2022.9849372","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9849372","landslide extraction;weakly supervised;class activation maps;deep attention;multi-feature fusion","Image segmentation;Image resolution;Annotations;Supervised learning;Semantics;Manuals;Terrain factors","deep learning (artificial intelligence);earthquakes;feature extraction;geomorphology;geophysical image processing;image fusion;image resolution;image segmentation;remote sensing;supervised learning","VHR remote sensing images;CAMs;deep attention;multifeature fusion;very high resolution remote sensing images;pixel-level annotations;weakly supervised learning;image-level annotations;class activation maps;weakly supervised landslide extraction;semantic segmentation;Jiuzhaigou earthquake","","","","18","IEEE","18 Aug 2022","","","IEEE","IEEE Conferences"
"Nonnegative Tensor Factorization Based Fusion for Changes Detection in Multiresolution Remote Sensing Images","Y. K. Benkouider; F. Z. Benhalouche; M. S. Karoui; M. Iftene","Agence Spatiale Algérienne, Centre des Techniques Spatiales, Arzew, Algérie; Agence Spatiale Algérienne, Centre des Techniques Spatiales, Arzew, Algérie; Agence Spatiale Algérienne, Centre des Techniques Spatiales, Arzew, Algérie; Agence Spatiale Algérienne, Alger, Algérie","2022 IEEE Mediterranean and Middle-East Geoscience and Remote Sensing Symposium (M2GARSS)","9 Aug 2022","2022","","","33","36","Changes detection in multitemporal remote sensing images with different spatial and spectral resolutions remains an important challenge. Most of the changes detection methods that exist in the literature do not take into account this multiresolution issue, and use spatial/spectral resampling process to avoid it. However, this type of procedure can lead to loss of spatial or spectral information. In this paper, a fusion-based approach to detect changes between optical images with different spatial and spectral resolutions is proposed. This approach uses nonnegative tensor decomposition to obtain two unobservable images having the same spatial and spectral resolutions from two observable images acquired by different sensors. Changes detection is done by a pixel-wise difference between these two latent images. Test results on realistic synthetic data demonstrate that the proposed approach have a good potential for changes detection in multiresolution remote sensing images.","","978-1-6654-2795-1","10.1109/M2GARSS52314.2022.9840208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9840208","changes detection;fusion;nonnegative tensor factorization;Tucker decomposition;nonnegative matrix factorization;multiplicative update rules","Optical losses;Image sensors;Tensors;Optical imaging;Optical sensors;Matrix decomposition;Spatial resolution","geophysical image processing;geophysical signal processing;geophysical techniques;image classification;image fusion;image resolution;matrix decomposition;optical images;remote sensing;tensors","changes detection methods;account this multiresolution issue;spatial information;spectral information;fusion-based approach;optical images;different spatial resolutions;spectral resolutions;nonnegative tensor decomposition;unobservable images;observable images;latent images;multiresolution remote sensing images;nonnegative tensor factorization;multitemporal remote sensing images","","","","12","IEEE","9 Aug 2022","","","IEEE","IEEE Conferences"
"Learning a Coupled Multilinear Network for Unsupervised Hyperspectral-Multispectral Image Fusion","J. Yang; L. Xiao","Nanjing Skyworth Institute of Information Technology Co., Ltd, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","4815","4818","Fusing low resolution (LR) HSI with high resolution (HR) multispectral image (MSI) is an important technology to obtain HR hypersepctral image (HSI), which is hard to directly acquire due to the hardware limitation. Deep learning (DL) has been applied in HSI-MSI fusion, but the representability of DL networks for multidimensional (i.e., spectral-spatial) features still need improvement. And most DL HSI-MSI fusion networks are in supervised fashion, HR ground truth HSI is required for training, which is unavailable in reality. In this work, we investigate tensor theory, and propose a coupled multilinear network (CMuNet) for unsupervised HSI-MSI fusion, where deep image prior and degradation model can be jointly learned. CMuNet consists of coupled multilinear filtering subnets, it jointly represents the LR HSI and HR MSI as a random code and multidimensional features on spatial and spectral modes. The HR HSI is inferred with the random code, features on spatial modes of HR MSI and features on spectral mode of LR HSI. Experiments on several HSIs demonstrate the effectiveness of the proposed method.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884309","National Natural Science Foundation of China(grant numbers:62001226); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200465); Fundamental Research Funds for the Central Universities(grant numbers:30920021134); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884309","Hyperspectral;fusion;tensor;deep","Training;Tensors;Image resolution;Correlation;Codes;Filtering;Image color analysis","geophysical image processing;hyperspectral imaging;image fusion;image resolution;image sensors;learning (artificial intelligence);remote sensing;sensor fusion;spectral analysis","coupled multilinear network;unsupervised hyperspectral-multispectral image fusion;low resolution HSI;high resolution multispectral image;HR hypersepctral image;hardware limitation;deep learning;DL networks;spectral-spatial;DL HSI-MSI fusion networks;truth HSI;CMuNet;unsupervised HSI-MSI fusion;deep image;coupled multilinear filtering subnets;LR HSI;HR MSI;random code;multidimensional features;spatial modes;spectral modes;HR HSI;spectral mode","","","","13","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Remote Sensing Image Scene Classification Based on Multi-level Feature Fusion","Y. Chen; T. Zheng; J. Han; M. Zheng; F. Zheng","School of Computer and Information Engineering, Henan University, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China; Xinxian Senior High School, Xinxian, China; Xinxian Transportation Bureau, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China","2021 7th International Conference on Computer and Communications (ICCC)","17 Jan 2022","2021","","","816","820","Remote sensing image scene classification is an important task in the fields of remote sensing image analysis and interpretation. Convolutional neural networks (CNNs) has been a representative image classification network owing to its capacity of feature learning and representation. Although the semantic information of feature map was enhanced along with the convolution layer increases, the spatial geometric detail information may be lost, which will greatly affect the accuracy of classification. To solve these problem, we proposed a multi-level convolutional feature fusion network (MLCF2N) for remote sensing image scene classification. Specifically, the original image is fed to the multi-level feature extraction network to extract features of different levels by increasing the depth of the convolution layer. And then, in order to highlight the key interest areas of all levels, we design a key information weight network to obtain feature map of the discriminative area in each level feature. Finally, we fuse the low-level features, middle- level features and high- level features to get more discriminative features for scene classification. Experiments are conducted on several public remote sensing image scene classification datasets: AID, NWPU-RESISC45 and OPTIMAL 31 to evaluate the performance of the method in this paper. The experimental results show that multi- level feature fusion classification is better than single- level feature classification and some of the state of art methods.","","978-1-6654-0950-6","10.1109/ICCC54389.2021.9674659","Natural Science Foundation of Henan Province; Key Scientific Research Project of Colleges and Universities in Henan Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9674659","remote sensing;scene classification;multi-level feature;CNN","Representation learning;Image analysis;Convolution;Fuses;Semantics;Feature extraction;Convolutional neural networks","feature extraction;image classification;image fusion;learning (artificial intelligence);neural nets;remote sensing","multilevel feature fusion;remote sensing image analysis;convolutional neural networks;representative image classification network;feature learning;feature map;multilevel convolutional feature fusion network;multilevel feature extraction network;key information weight network;low-level features;discriminative features;public remote sensing image scene classification datasets;multi level feature fusion classification;single- level feature classification","","","","26","IEEE","17 Jan 2022","","","IEEE","IEEE Conferences"
"Dynamic dictionary learning strategies for sparse representation based hyperspectral image enhancement","C. Grohnfeldt; T. M. Burns; X. X. Zhu","Remote Sensing Technology Institute, DLR German Aerospace Center, Wessling, Germany; School of Mechanical and Mining Engineering, The University of Queensland, Australia; Chair of Remote Sensing Technology, Technische Universitat Munchen (TUM), Germany","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","23 Oct 2017","2015","","","1","4","This paper introduces four new dynamic dictionary learning methods to sparse representation based hyperspectral resolution enhancement. The impact of the type and size of the dynamic dictionary on the reconstruction quality is investigated for the recently proposed sparse representation based multiresolution image fusion method J-SparseFI-HM. Low resolution hyperspectral and high resolution multispectral input images are simulated from recently acquired airborne HySpex data. Experiments reveal that fusion products can be substantially improved by changing the dictionary type from the currently used nearest neighbor selection to a modified dissimilarity based dynamic dictionary.","2158-6276","978-1-4673-9015-6","10.1109/WHISPERS.2015.8075407","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075407","dictionary learning;image fusion;hyperspectral resolution enhancement;sparse representation","Dictionaries;Image reconstruction;Machine learning;Spatial resolution;Hyperspectral imaging","compressed sensing;hyperspectral imaging;image enhancement;image fusion;image representation;image resolution","hyperspectral image enhancement;hyperspectral resolution enhancement;dictionary learning;sparse image representation;multiresolution image fusion","","","","12","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"A Convolutional Neural Network for Ship Targets Detection and Recognition in Remote Sensing Images","L. Jin; G. Liu","Harbin Institute of Technology, Harbin, China; Harbin Institute of Technology, Harbin, China","2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","3 Feb 2021","2020","9","","139","143","According to the target direction, target size, shooting angle and scene diversity in remote sensing image, the target detection and recognition accuracy of remote sensing image is not high. This paper adopts a convolutional neural network(CNN) for ship targets detection and recognition in Remote Sensing Images, which is based on Faster R-CNN model. In our model, the network is optimized for the characteristics of ship targets in remote sensing images, use the multi-level proposal region extraction and multi-level feature fusion technology to construct the CNN model for ship target detection and recognition. The experimental results show that the network model has strong robustness and has a high accuracy at ship targets detection and recognition in remote sensing image.","2693-2865","978-1-7281-5244-8","10.1109/ITAIC49862.2020.9339095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9339095","targets detection;targets recognition;convolutional neural network;multi-level proposal region;multi-level feature fusion","Image recognition;Target recognition;Object detection;Feature extraction;Proposals;Marine vehicles;Remote sensing","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image segmentation;object detection;remote sensing;ships","ship target detection;remote sensing image;Faster R-CNN model;convolutional neural network","","","","5","IEEE","3 Feb 2021","","","IEEE","IEEE Conferences"
"Feature Fusion with Deep Supervision for Remote-Sensing Image Scene Classification","U. Muhammad; W. Wang; A. Hadid","Sch. of Comput. & Control Eng., University of the Chinese Academy of Sciences, Beijing, Beijing, CN; School of Computer and Control Engineering, University of Chinese Academy of Sciences, Beijing, China; Center for Machine Vision and Signal Analysis (CMVS), University of Oulu, Finland","2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI)","16 Dec 2018","2018","","","249","253","The convolutional neural networks (CNNs) have shown an intrinsic ability to automatically extract high level representations for image classification, but there is a major hurdle to their deployment in the remote-sensing domain because of a relative lack of training data. Moreover, traditional fusion methods use either low-level features or score-based fusion to fuse the features. In order to address the aforementioned issues, we employed a deep supervision (DS) strategy to enhance the generalization performance in the intermediate layers of the AlexNet model for remote-sensing image scene classification. The proposed DS strategy not only prevents from overfitting, but also extracts the features more transparently. Secondly, the canonical correlation analysis (CCA) is adopted as a feature fusion strategy to further refine the features with more discriminative power. The fused AlexNet features achieved by the proposed framework have much higher discrimination than the pure features. Extensive experiments on two challenging datasets: 1) UC MERCED data set and 2) WHU-RS dataset demonstrate that the two proposed approaches both enhance the performance of the original AlexNet architecture, and also outperform several state-of-the-art methods currently in use.","2375-0197","978-1-5386-7449-9","10.1109/ICTAI.2018.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8576044","Pre-trained AlexNet, Canonical Correlation Analysis(CCA), Deep Supervision (DS), Scene Classification","Correlation;Feature extraction;Remote sensing;Mathematical model;Training;Fuses;Covariance matrices","feature extraction;feedforward neural nets;geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing","remote-sensing domain;traditional fusion methods;low-level features;score-based fusion;deep supervision strategy;remote-sensing image scene classification;DS strategy;feature fusion strategy;fused AlexNet features;pure features;convolutional neural networks;high-level representations;CCA;canonical correlation analysis;UC MERCED data set;WHU-RS dataset;CNN","","","","23","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"On the possibility of conditional adversarial networks for multi-sensor image matching","N. Merkle; P. Fischer; S. Auer; R. Müller","German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","2633","2636","A major research area in remote sensing is the problem of multi-sensor data fusion. Especially the combination of images acquired by different sensor types, e.g. active and passive, is a difficult task. Over the last years deep learning methods have proven their high potential for remote sensing applications. In this paper we will show how a deep learning method can be valuable for the problem of optical and SAR image matching. We investigate the possible of conditional generative adversarial networks (cGANs) for the generation of artificial templates. Contrary to common template generation approaches for image matching, the generation of templates using cGANs does not require the extraction of features. Our results show the possibility of realistic SAR-like template generation from optical images through cGANs and the potential of these templates for enhancing the matching of optical and SAR images by means of reliability and accuracy.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127535","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127535","conditional GANs;deep learning;image matching;multi-sensor;artificial template generation","Optical imaging;Adaptive optics;Optical sensors;Feature extraction;Synthetic aperture radar;Image matching;Gallium nitride","feature extraction;image fusion;image matching;learning (artificial intelligence);radar imaging;remote sensing;sensor fusion;synthetic aperture radar","artificial templates;optical images;multisensor image matching;research area;multisensor data fusion;remote sensing applications;deep learning method;optical SAR image matching;conditional generative adversarial networks;template generation;cGAN","","21","","10","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Multi-resolution Object Detection and Data Fusion for Large-scale Remote Sensing Images based on Deep Learning Method","Y. Wu; H. Hu; Y. Zhang","School of Instrumentation and Opto-electronic Engineering, Beihang University; The Troops, Beijing, China; School of Instrumentation and Opto-electronic Engineering, Beihang University","2019 IEEE 3rd Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)","6 Feb 2020","2019","","","933","937","With the development of aircraft and telemetry satellites, the acquisition of high-resolution remote sensing images becomes easier. Object detection in large-scale remote sensing images has become a valuable problem. At present, the object detector trained in the natural scene has achieved quite good performance, but in the remote sensing scene, the detection result is not unacceptable because the ground object size is too small, dense, and the background is complicated. In order to make the object detector based on deep learning method operating more accurately in large-scale remote sensing images, this paper proposes an algorithm for multi-resolution detection and fusion of data on remote sensing images. The algorithm divides the original large-scale remote sensing image into a plurality of sub-images according to certain parameter settings, and fuses the results after detecting the objects on each sub-image. The process will be adaptively performed multiple times and adopt different resolutions. Finally, all the obtained objects bounding boxes are subjected to non-maximum suppression processing to obtain the final detection result.","","978-1-7281-0513-0","10.1109/IMCEC46724.2019.8983902","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8983902","remote sensing image;multi-resolution;data fusion;deep learning;object detection","","image fusion;image resolution;learning (artificial intelligence);neural nets;object detection;remote sensing","deep learning method;remote sensing images;multiresolution object detection;data fusion;objects bounding boxes","","1","","7","IEEE","6 Feb 2020","","","IEEE","IEEE Conferences"
"Feature Fusion with Multi-Task Attention Networks for Semantic Segmentation from VHR Remote Sensing Images","X. Da; J. Zhang; L. Chen; M. Ma","Network and Information Technology Center, Xinjiang Agricultural University XJAU, Urumqi, China; School of Computer and Information Engineering, Xinjiang Agricultural University XJAU, Urumqi, China; School of Computer and Information Engineering, Xinjiang Agricultural University XJAU, Urumqi, China; College of Computer Science and Technology, Zhejiang University ZJU, Hangzhou, China","2021 3rd International Academic Exchange Conference on Science and Technology Innovation (IAECST)","7 Feb 2022","2021","","","193","198","Semantic segmentation from very high-resolution (VHR) remote sensing images is of great significance in a broad spectrum of applications. In recent years, deep learning techniques have been used to accomplish relevant tasks. Many of such attempts start with selecting only three image channels for fusion, followed by establishing a segmentation model, making it tough to exploit the information of all individual image channels. Consequently, fusing multi-modal data for improving segmentation performance has received considerable research attention. In this article, we propose a novel approach enabling effective feature fusion for better semantic segmentation from VHR remote sensing images. Based on the encoder-decoder model, our approach’s encoder comprises two branch networks simultaneously extracting features from heterogeneous image data. In particular, we introduce multi-task attention networks (MTANs) in the encoder to fuse feature maps, enabling both branch networks to complement each other through the task. Experiments demonstrated that our approach outperformed several benchmark models on two image datasets, proving its promise in semantic segmentation from VHR remote sensing images.s given in this document.","","978-1-6654-0267-5","10.1109/IAECST54258.2021.9695830","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9695830","Semantic Segmentation;High-Resolution Remote Sensing Images;Feature Fusion;Multi-task Attention Mechanism;Deep Learning","Deep learning;Image segmentation;Technological innovation;Fuses;Semantics;Multitasking;Feature extraction","feature extraction;geophysical image processing;image classification;image fusion;image resolution;image segmentation;learning (artificial intelligence);remote sensing","multitask attention networks;semantic segmentation;VHR remote sensing images;high-resolution remote sensing images;segmentation model;individual image channels;multimodal data;segmentation performance;effective feature fusion;branch networks;heterogeneous image data;image datasets","","","","22","IEEE","7 Feb 2022","","","IEEE","IEEE Conferences"
"Remote Sensing Image Aircraft Detection Based on Feature Fusion across Deep Learning Framework","W. Wei; J. Zhang; C. Xu","School of Information Science and Engineering Lanzhou University, Lanzhou, China; School of Information Science and Engineering Lanzhou University, Lanzhou, China; School of Information Science and Engineering Lanzhou University, Lanzhou, China","2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS)","19 Mar 2020","2019","","","1","5","The detection of remote sensing image aircraft targets based on deep learning has practical and important significance in the fields of military reconnaissance and disaster rescue. As a typical representative of the two mainstream detection algorithms, YOLOv3 and Faster_R_CNN have good detection effects on remote sensing image aircraft targets. However, for low quality remote sensing images, the two detection algorithms also have the phenomenons of omission and false detection. To deal with this, this paper proposes a target detection algorithm (YF_R_CNN) for ""Separate training, joint detection"", which realizes the cross-platform detection feature fusion of YOLOv3 based on darknet framework and Faster_R_CNN based on tensorflow framework, effectively alleviating the problems of existing algorithms. The experimental results show that the detection accuracy of YF_R_CNN algorithm reaches 94.8%, which is 3.7% and 3.1% higher than YOLOv3 and Faster_R_CNN respectively. The detection accuracy is obviously improved, and the algorithm has better flexibility and robustness.","2327-0594","978-1-7281-0945-9","10.1109/ICSESS47205.2019.9040808","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040808","component;remote sensing image;aircraft detection;feature fusion","","aerospace computing;aircraft;convolutional neural nets;geophysical image processing;image fusion;learning (artificial intelligence);object detection;remote sensing","target detection;YF_R_CNN;remote sensing image aircraft detection;tensorflow;cross-platform detection feature fusion;low quality remote sensing images;Faster_R_CNN;YOLOv3;deep learning","","","","11","IEEE","19 Mar 2020","","","IEEE","IEEE Conferences"
"A Remote Sensing Image Registration Method Based on Multi-features","K. Xie; J. Chen; M. Yang","Guangxi Key Laboratory of Trusted Software of Guilin, University of Electronic Technology, Guilin, China; Guangxi Key Laboratory of Cryptography and Information Security of Guilin, University of Electronic Technology, Guilin, China; Key Laboratory of Pattern Recognition, Chinese academy of sciences, Beijing, China","2019 IEEE 4th International Conference on Image, Vision and Computing (ICIVC)","6 Feb 2020","2019","","","134","138","Remote sensing image registration technology has important significance in the field of image processing. The registration technology of SAR images has been a hot research in this field, and it is also a challenge. Aiming at this difficulty, this paper proposes a fusion feature algorithm, which combines gradient, grayscale, texture, geometry and other features in heterogeneous remote sensing images for comprehensive matching. Compared with the traditional single feature, the improved detector has good performance and registration accuracy in SAR image and visible light image registration.","","978-1-7281-2325-7","10.1109/ICIVC47709.2019.8981069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8981069","remote sensing image registration;fusion features;SAR images;visible light images","","feature extraction;geophysical image processing;image fusion;image registration;remote sensing;synthetic aperture radar","remote sensing image registration method;multifeatures;sensing image registration technology;important significance;image processing;SAR images;hot research;fusion feature algorithm;geometry;heterogeneous remote sensing images;traditional single feature;registration accuracy;visible light image registration","","","","9","IEEE","6 Feb 2020","","","IEEE","IEEE Conferences"
"A decision fusion framework for high-resolution remote-sensing image classification","A. Jafari; M. Heidarpour","Malek-Ashtar University of Technology, Tehran, Iran; Malek-Ashtar University of Technology, Tehran, Iran","2015 9th Iranian Conference on Machine Vision and Image Processing (MVIP)","4 Feb 2016","2015","","","219","222","Classification of high-resolution remote-sensing images is a challenging research area. In this paper we proposed a novel decision fusion framework to combine bag of features (BOF) based classifiers. The proposed framework, can also be used in multi category image classification applications. A single voting algorithm is used for decision fusion and an ambiguity detection module is used to determine the ambiguous situations. An ambiguous situation will occur during multi-category voting, where more than one class got the maximum votes, and also when the number of the same votes doesn't exceeds a desired threshold. To resolve this situation we proposed to use the earth mover's distance (EMD) which is a metric for histogram matching. Indeed, we used the EMD to compare the BOF based histogram of images with the centroid classes. Finally, to evaluate the proposed framework, we used a multi-category remote-sensing image dataset and compared the proposed approach with several other similar approaches with BOF based classifiers. The experimental results demonstrate the effectiveness of the proposed framework.","2166-6784","978-1-4673-8539-8","10.1109/IranianMVIP.2015.7397540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7397540","Image Classification;Remote Sensing;EMD;Decision Fusion;Bag of Features","Image resolution;Visualization;Frequency conversion;Histograms;Kernel;Lead","geophysical image processing;image classification;image fusion;image matching;remote sensing","decision fusion framework;high-resolution remote-sensing image classification;bag of features based classifiers;multicategory image classification applications;single voting algorithm;decision fusion;ambiguity detection module;ambiguous situations;multicategory voting;earth mover distance;EMD;histogram matching;BOF based histogram;multicategory remote-sensing image dataset","","","","20","IEEE","4 Feb 2016","","","IEEE","IEEE Conferences"
"Hyperspectral Anomaly Detection Via Band Fusion","F. Li; M. Song; C. -I. Chang","Center for Hyperspectral Imaging in Remote Sensing, Information and Technology College, Dalian Maritime University, Dalian, China; Center for Hyperspectral Imaging in Remote Sensing, Information and Technology College, Dalian Maritime University, Dalian, China; Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, Baltimore, MD, USA","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2324","2327","This paper develops band fusion techniques to fuse hyperspectral data from a data communication and transmission perspective. It can provide progressive profiles of fusing different bands and improve the efficiency by data processing. Anomaly detection is investigated for its application to demonstrate its utility. The experimental results prove that the proposed band fusion methods can not only ensure the accuracy of the detection results, but also can improve the data processing efficiency.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324272","National Nature Science Foundation of China(grant numbers:61971082,61890964,61601077); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324272","Band fusion (BF);Hyperspectral image (HSI);Anomaly detection (AD)","Hyperspectral imaging;Image fusion;Fuses;Detectors;Anomaly detection;Visualization;Target recognition","geophysical image processing;hyperspectral imaging;image fusion;remote sensing;sensor fusion","hyperspectral anomaly detection;band fusion techniques;hyperspectral data;data communication;transmission perspective;progressive profiles;band fusion methods;data processing efficiency","","","","20","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Class Attention Network for Semantic Segmentation of Remote Sensing Images","Z. Rao; M. He; Y. Dai","Northwestern Polytechnical University, Xian, China; Northwestern Polytechnical University, Xian, China; Northwestern Polytechnical University, Xian, China","2020 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","31 Dec 2020","2020","","","150","155","Semantic segmentation in remote sensing images is beneficial to detect objects and understand the scene in earth observation. However, classical networks always failed to obtain an accuracy segmentation map in remote sensing images due to the imbalanced labels. In this paper, we proposed a novel class attention module and decomposition-fusion strategy to cope with imbalanced labels. Based on this motivation, we investigate related architecture and strategy by follows. (1) we build a class attention module to generate multi-class attention maps, which forces the network to keep attention to small sample categories instead of being flooded by large sample data. (2) we introduce salient detection, which decomposes semantic segmentation into multi-class salient detection and then fuses them to produce a segmentation map. Extensive experiments on popular benchmarks (e.g., US3D dataset) show that our approach can serve as an efficient plug-and-play module or strategy in the previous scene parsing networks to help them cope with the problem of imbalance labels in remote sensing images.","2640-0103","978-988-14768-8-3","","Johns Hopkins University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9306448","","Semantics;Image segmentation;Remote sensing;Fuses;Feature extraction;Task analysis;Buildings","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);object detection;remote sensing","class attention network;semantic segmentation;remote sensing images;classical networks;accuracy segmentation map;imbalanced labels;class attention module;multiclass attention maps;multiclass salient detection;previous scene parsing networks","","","","34","IEEE","31 Dec 2020","","","IEEE","IEEE Conferences"
"ISTA-Net Model-driven Deep Unfolding Network for Hyperspectral and Multispectral Image Fusion","X. Wang; Q. Zhu; N. Qi","Beijing University of Technology, China; Beijing University of Technology, China; Beijing University of Technology, China","2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","3 Aug 2022","2022","10","","1208","1212","Hyperspectral images (HSI) with high spatial and spectral resolutions have many applications in astronautics, re-mote sensing, and so on. However, it is challenging to obtain HSI with existing imaging techniques due to hardware limitations. In most cases, high-resolution multispectral (HrMS) images or low-resolution hyperspectral (LrHS) images are obtained. Therefore, the fusion of HrMS images and LrHS images for HSI super-resolution has attracted widespread attention. In this paper, we propose a network denoted as a model-based deep unfolding net-work(DuFNet) for hyperspectral image super-resolution (HSSR) task with clear interpretability. Specifically, we integrate the ISTA-Net into a well-established fusion network that is MHF-Net to fully take advantage of the generalization of the ISTA-Net. Experimental results demonstrate that the proposed NAM-DuFNet outperforms existing state-of-the-art methods in terms of subjective and objective results.","2693-2865","978-1-6654-2207-9","10.1109/ITAIC54216.2022.9836742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9836742","Hyperspectral;super-resolution;interpretability;fusion network","Superresolution;Optimization methods;Imaging;Iterative algorithms;Sensors;Task analysis;Spatial resolution","geophysical image processing;hyperspectral imaging;image fusion;image resolution;remote sensing","hyperspectral image fusion;multispectral image fusion;high-spatial resolutions;spectral resolutions;remote sensing;hardware limitations;high-resolution multispectral images;low-resolution hyperspectral images;HrMS images;LrHS images;HSI super-resolution;hyperspectral image super-resolution task;MHF-Net;HSSR task;NAM-DuFNet;ISTA-net model-driven deep unfolding network","","","","13","IEEE","3 Aug 2022","","","IEEE","IEEE Conferences"
"Building Change Detection Based on Deep Learning and Belief Function","X. Yuan; J. Tian; P. Reinartz","German Aerospace Center (DLR), Remote Sensing Technology Institute (IMF), Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute (IMF), Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute (IMF), Germany","2019 Joint Urban Remote Sensing Event (JURSE)","22 Aug 2019","2019","","","1","4","This paper proposes a new approach for building change detection using multi-temporal satellite stereo data. This approach is composed of three main steps. Firstly the building probability map can be derived by a state-of-the-art deep learning approach. In the second step, a decision fusion based fusion model refines and fuses the building changes from satellite stereo imagery and the digital surface models (DSMs). In the last step, the building probability maps are further fused with the building change indicators to generate an improved change detection result. Experiments on the multi-temporal data acquired over 5 years confirms the effectiveness of the proposed approach.","2642-9535","978-1-7281-0009-8","10.1109/JURSE.2019.8808968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8808968","Change detection;CNN;belief function;building probability map","Buildings;Remote sensing;Deep learning;Satellites;Time series analysis;Semantics;Image segmentation","geophysical image processing;image fusion;probability;remote sensing;sensor fusion;stereo image processing","building change detection;belief function;multitemporal satellite stereo data;building probability map;state-of-the-art deep learning approach;satellite stereo imagery;decision fusion based fusion model;digital surface models","","3","","18","IEEE","22 Aug 2019","","","IEEE","IEEE Conferences"
"Fusion of TanDEM-X and Cartosat-1 DEMS using TV-norm regularization and ANN-predicted weights","H. Bagheri; M. Schmitt; X. X. Zhu","Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","3369","3372","This paper deals with TanDEM-X and Cartosat-1 DEM fusion over urban areas with support of weight maps predicted by an artificial neural network (ANN). Although the TanDEM-X DEM is a global elevation dataset of unprecedented accuracy (following HRTI-3 standard), its quality decreases over urban areas because of artifacts intrinsic to the SAR imaging geometry. DEM fusion techniques can be used to improve the TanDEM-X DEM in problematic areas. In this investigation, Cartosat-1 elevation data were fused with the TanDEM-X DEM by weighted averaging and total variation (TV)-based regularization, resorting to weight maps derived by a specifically trained ANN. The results show that the proposed fusion strategy can significantly improve the final DEM quality.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127720","Data fusion;L1 norm total variation;weight map;Artificial Neural Network;TanDEM-X DEM;Cartosat-1 DEM","Urban areas;Training;Measurement;Neural networks;Buildings;Laser radar;Remote sensing","digital elevation models;geophysical image processing;image fusion;neural nets;remote sensing by radar;synthetic aperture radar","Cartosat-1 DEM fusion;urban areas;artificial neural network;fusion techniques;Cartosat-1 elevation data;DEM quality;TanDEM-X DEM fusion;TV-norm regularization;ANN-predicted weights;SAR imaging geometry;total variation-based regularization","","3","","12","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Fusion of very high resolution SAR and optical images for the monitoring of urban areas","C. V. Lopez; H. Anglberger; U. Stilla","Microwaves and Radar Institute, German Aerospace Center (DLR), Germany; Microwaves and Radar Institute, German Aerospace Center (DLR), Germany; Photogrammetry and Remote Sensing, Technical University of Munich (TUM), Munich, Germany","2017 Joint Urban Remote Sensing Event (JURSE)","11 May 2017","2017","","","1","4","Remote sensing data from SAR and optical sensors provide complementary information, with each type of data being better suited for certain tasks. In this paper, the potential of using both types of data together for the monitoring of urban areas will be shown. This is illustrated using a dataset of very high resolution SAR and optical images of the city of Oslo (Norway), containing over three years of TerraSAR-X images acquired with different orbits and incidence angles, and aerial optical images that were photogrammetrically processed to obtain a DSM and a true ortho mosaic.","","978-1-5090-5808-2","10.1109/JURSE.2017.7924551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7924551","","Optical sensors;Optical imaging;Synthetic aperture radar;Urban areas;Buildings;Monitoring;Time series analysis","digital elevation models;geophysical image processing;image fusion;image resolution;image segmentation;photogrammetry;remote sensing by radar;synthetic aperture radar;terrain mapping","very high resolution SAR-optical image fusion;urban area monitoring;remote sensing data;optical sensors;Oslo city;Norway;TerraSAR-X images;incidence angles;aerial optical images;DSM","","1","","12","IEEE","11 May 2017","","","IEEE","IEEE Conferences"
"FRN-YOLO: A Feature Re-fusion Network for Remote Sensing Target Detection","Y. Sun; W. Liu; X. Hou; F. Bi","North China University of Technology, Beijing, China; North China University of Technology, Beijing, China; North China University of Technology, Beijing, China; North China University of Technology, Beijing, China","2021 2nd International Conference on Computer Science and Management Technology (ICCSMT)","8 Jun 2022","2021","","","372","375","With the development of artificial intelligence technology, remote sensing target detection has gradually become a hot issue in the field of computer vision, which can be widely used in navigation, exploration, disaster warning, etc., and it has important research significance and application value for remote sensing target detection. However, the scale difference of remote sensing targets makes detection very difficult. Therefore, we propose a feature re-fusion network based on YOLO-FRN-YOLO. Based on the original three detection layers of YOLO, by re-fusing the features of the three output layers of the backbone, each feature layer can be deeply combined with The semantic information before sampling or after sampling, and the depth of the detection layer after feature re-fusion retains the semantic information of targets of different scales, and improves the detection ability of targets of different scales. The results show that on the RSOD datasets, the average precision of our method exceeds YOLOv3, and it is also better than other advanced networks.","","978-1-6654-2063-1","10.1109/ICCSMT54525.2021.00074","National Natural Science Foundation of China(grant numbers:61971006); Beijing Natural Science Foundation(grant numbers:4192021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9786959","remote sensing target detection;YOLO;feature re-fusion;muti-scale","Computer science;Computer vision;Navigation;Semantics;Object detection;Feature extraction;Detection algorithms","artificial intelligence;computer vision;feature extraction;geophysical image processing;image fusion;object detection;remote sensing","YOLO-FRN-YOLO;detection layer;feature re-fusion network;remote sensing target detection;artificial intelligence technology;computer vision;RSOD datasets","","","","12","IEEE","8 Jun 2022","","","IEEE","IEEE Conferences"
"Multi-Scale HARRIS-PIIFD Features for Registration of Visible and Infrared Images","C. Gao; W. Li","Beijing Key Lab of Fractional Signals and Systems, Beijing, China; Beijing Key Lab of Fractional Signals and Systems, Beijing, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","5437","5440","This paper aims at providing visible and infrared images registered in geometric space for image fusion. Focusing on the characteristics and differences of visible and infrared images, a feature-based registration algorithm is implemented. The key technologies include image scale-space for implementing multi -scale properties, Harris corner detection for keypoints extraction, and partial intensity invariant feature descriptor (PIIFD) for keypoints description. Eventually, a multi-scale Harris- Piifd image registration algorithm framework is proposed. The experimental results of four sets of representative real data show that the algorithm has excellent, stable performance in visible and infrared image registration, and can achieve accurate spatial alignment, which has strong practical application value and certain generalization ability.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555105","Beijing Natural Science Foundation(grant numbers:JQ20021); National Natural Science Foundation of China(grant numbers:61922013,61421001,U1833203); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555105","Image registration;infrared image;scale-space;Harris corner;PIIFD","Image registration;Geoscience and remote sensing;Focusing;Feature extraction;Robustness;Task analysis;Image fusion","feature extraction;image fusion;image matching;image registration;infrared imaging;medical image processing","visible images;infrared images;image fusion;feature-based registration algorithm;image scale-space;multi-scale properties;Harris corner detection;partial intensity invariant feature descriptor;multiscale Harris- Piifd image registration algorithm;visible image registration;infrared image registration;multiscale HARRIS-PIIFD features","","1","","7","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A Topological Data Analysis Guided Fusion Algorithm: Mapper-Regularized Manifold Alignment","J. Hu; D. Hong; Y. Wang; X. X. Zhu","Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Germany; Signal Processing in Earth Observation (SiPEO), Technische Universität München (TUM), Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Germany","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","2822","2825","Hyperspectral images and polarimetric synthetic aperture radar (PolSAR) data are two important data sources, yet they barely appear under the same scope, even though multi-modal data fusion is attracting more and more attention. To our best knowledge, this paper investigates for the first time semi-supervised manifold alignment (SSMA) for the fusion of the hyperspectral image and PolSAR data. The SSMA searches a latent space where different data sources are aligned, which is accomplished by using the label information and the topological structure of the data. This paper is the first attempt to apply topological data analysis (TDA), a recent mathematic sub-field of data analysis, in remote sensing. It aims to reveal relevant information from the shape of a data in its feature space, and has been proven powerful in medicine. The paper also proposes a novel algorithm, MAPPER-regularized manifold alignment, which embeds the TDA into a semi-supervised manifold alignment for the fusion of the hyper-spectral image and PolSAR data. The proposed algorithm exhibits superior performance in fusing a simulated EnMAP data set and a Sentinel-1 data set for an image of Berlin.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898471","Classification;data fusion;EnMAP;hyperspectral image;land cover;land use;manifold alignment;MAPPER;PolSAR;semi-supervised learning;Sentinel-1;topological data analysis (TDA)","Manifolds;Hyperspectral imaging;Data integration;Data analysis;Feature extraction","data analysis;hyperspectral imaging;image fusion;mathematical analysis;radar polarimetry;remote sensing by radar;synthetic aperture radar","Sentinel-1 data;topological data analysis guided fusion algorithm;mapper-regularized manifold alignment;polarimetric synthetic aperture radar data;PolSAR;multimodal data fusion;semisupervised manifold alignment;SSMA;hyperspectral image fusion;EnMAP data simulation;TDA;MAPPER-regularized manifold alignment","","1","","17","EU","14 Nov 2019","","","IEEE","IEEE Conferences"
"Anchor-Free Network for Multi-class Object Detection in Remote Sensing Images","G. Zhao; J. Pang; H. Zhang; J. Zhou; L. Li","Robot Technology Used for Special Environment Key Laboratory of Sichuan Province, Mianyang; Robot Technology Used for Special Environment Key Laboratory of Sichuan Province, Mianyang; Robot Technology Used for Special Environment Key Laboratory of Sichuan Province, Mianyang; Robot Technology Used for Special Environment Key Laboratory of Sichuan Province, Mianyang; Robot Technology Used for Special Environment Key Laboratory of Sichuan Province, Mianyang","2020 39th Chinese Control Conference (CCC)","9 Sep 2020","2020","","","7510","7515","The anchor-based object detection algorithms need many hyper-parameters artificial such as the threshold of intersection over union and the size of anchors, which may limit the detection performance to some extent. In order to better solve the problem of multi-class object detection in remote sensing images, this paper proposed an anchor-free object detection network that does not require any hyper-parameters artificial. The network improves the detection effect of small objects by using the improved feature pyramid network to fuse multi-scale feature maps more effectively, and Focal loss and loss of intersection over union (IoU loss) are used as loss functions to optimize the network. The experimental results on the DOTA dataset show that the mean average precision (mAP) of this network is 71.02%, which is at least 10.5% higher than other existing networks. The anchor-free network proposed in this paper achieves superior detection performance in remote sensing images.","1934-1768","978-9-8815-6390-3","10.23919/CCC50068.2020.9188903","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9188903","anchor-free;object detection;sensing image","Object detection;Remote sensing;Feature extraction;Training;Detectors;Task analysis","convolutional neural nets;feature extraction;image fusion;network theory (graphs);object detection;remote sensing","multiclass object detection;remote sensing images;anchor-free object detection network;detection effect;improved feature pyramid network;multiscale feature maps;anchor-free network;superior detection performance;anchor-based object detection algorithms;DOTA dataset;mean average precision","","1","","16","","9 Sep 2020","","","IEEE","IEEE Conferences"
"Retrieval of Remote Sensing Images Using Fused Color and Texture Features with K-Means Clustering","M. N. Vharkate; V. B. Musande","Department of Computer Engineering, MIT Academy of Engineering Alandi(D), Pune, India; Department of Computer Science and Engineering, Jawaharal Nehu Engineering College, Aurangabad, India","2018 Fourth International Conference on Computing Communication Control and Automation (ICCUBEA)","25 Apr 2019","2018","","","1","6","In the advancement of remote sensing satellite sensors, a large number of high-resolution satellite images are captured every day. To retrieve the required images from a large database has become a challenge. Here, we have used fused color and texture feature for retrieving remote sensing image. Here, we used HSV Histogram, Color moment and color autocorrelogram for color feature extraction. A wavelet transform is used for texture feature extraction. These combined color and texture are used for indexing using k-means clustering. Manhattan distance is also used for similarity matching. UC Merced Land use Land Cover Dataset has been used for the experiment. The k-means clustering with combined color and texture features has shown better retrieval performance than only color features. Indexing has been done using Manhattan distance and k-means clustering. K-means clustering gives better retrieval performance than Manhattan Distance.","","978-1-5386-5257-2","10.1109/ICCUBEA.2018.8697388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8697388","color moment;color autocorrelogram;wavelet transform;K-means clustering","Image color analysis;Feature extraction;Histograms;Wavelet transforms;Image retrieval;Remote sensing","feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;image retrieval;image texture;pattern clustering;remote sensing;wavelet transforms","Manhattan distance;UC Merced Land use Land Cover Dataset;retrieval performance;color features;remote sensing satellite sensors;high-resolution satellite images;color autocorrelogram;color feature extraction;texture feature extraction;fused color feature;fused texture feature;HSV histogram;color moment;wavelet transform;k-means clustering;similarity matching","","","","22","IEEE","25 Apr 2019","","","IEEE","IEEE Conferences"
"A Noise Proof Strategy for Spatio-Temporal Fusion of Remote Sensing Imagery","Y. Li; J. Li; A. Plaza","Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, School of Computer Science, China University of Geosciences, Wuhan, P. R. China; Department of Technology of Computers and Communications, Escuela Politécnica, Hyperspectral Computing Laboratory, University of Extremadura, Cáceres, Spain","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","895","898","Spatio-temporal fusion is a feasible way to generating the synthetic remote sensing data with high spatial resolution and high temporal resolution simultaneously by blending the fine and coarse resolution satellite images. To date, dozens of spatio-temporal fusion approaches have been developed. A basic rule of these approaches is the bands of coarse and fine images must be corresponding, which means the quality of fused images depends on that of both fine and coarse images. In the literature, the MODIS images are the most wildly used coarse images in spatio-temporal fusion. However, the MODIS images may suffer from serious stripe noises in the short-wave infrared-1 and short-wave infrared-2 bands, which will lead to undesired results of spatio-temporal fusion. To address this problem, we develop a noise proof strategy in this paper, which takes advantage of the spectral correlation of base fine image to remove the stripe noises of the base MODIS image, then the spatial correlation of base MODIS image is exploited to restore the MODIS image of the predicted time. Finally, the reconstructed MODIS images are fused with the base fine image to predict the missing fine images. The strategy is tested via real Landsat and MODIS images, and the experimental result demonstrates it is not only effective in removing the stripe noises of MDOIS short-wave infrared-1 and short-wave infrared-2 bands, but also able to improve the fusion accuracy.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884821","National Natural Science Foundation of China(grant numbers:42030111); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884821","Spatio-temporal fusion;MODIS;noise proof","Earth;Measurement;Strips;Satellites;Correlation;Artificial satellites;Image restoration","geophysical image processing;geophysical signal processing;image classification;image fusion;image resolution;remote sensing","coarse resolution satellite images;spatio-temporal fusion approaches;wildly used coarse images;short-wave infrared-2 bands;noise proof strategy;base fine image;stripe noises;base MODIS image;reconstructed MODIS images;missing fine images;synthetic remote sensing data;high spatial resolution;high temporal resolution;fine resolution satellite images","","","","9","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Multi-Scale Fusion Maximum Entropy Subspace Clustering for Hyperspectral Band Selection","H. Ma; Y. Wang; L. Jiang; M. Song; C. Yu; E. Zhao","Department of ground-air navigation, Air Force Communication Officer School, Dalian, China; Department of ground-air navigation, Air Force Communication Officer School, Dalian, China; Department of ground-air navigation, Air Force Communication Officer School, Dalian, China; Department of ground-air navigation, Air Force Communication Officer School, Dalian, China; Department of ground-air navigation, Air Force Communication Officer School, Dalian, China; Department of ground-air navigation, Air Force Communication Officer School, Dalian, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","779","782","A novel multi-scale fusion maximum entropy subspace clustering (MFMESC) for hyperspectral image (HSI) band selection is proposed in this paper. Subspace clustering is combined as a self-expression layer with stacked convolutional autoencoder, so that subspace clustering working in linear subspaces can deal with complicated HSI data with nonlinear characteristics. Multiple fully-connected linear layers are inserted between the encoder layers and their corresponding decoder layers to promote learning more favorable representations for subspace clustering. A multi-scale fusion module is designed to guide the fusion of multi-scale information extracted from different layers to learn a more discriminative self-expression coefficient matrix. Furthermore, the maximum entropy regularization is introduced in the subspace clustering to promote the connectivity within each subspace. Experimental results demonstrate the superiority of the proposed model against state of-the-art methods.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884654","China Postdoctoral Science Foundation(grant numbers:2020M670723); Fundamental Research Funds for the Central Universities(grant numbers:3132022232); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884654","hyperspectral band selection;maximum entropy regularization;subspace clustering;multi-scale fusion;stacked convolutional autoencoder","Geoscience and remote sensing;Benchmark testing;Entropy;Decoding;Data mining;Hyperspectral imaging","entropy;face recognition;feature extraction;geophysical image processing;image fusion;image representation;learning (artificial intelligence);neural nets;pattern clustering","hyperspectral band selection;novel multiscale fusion maximum entropy subspace clustering;hyperspectral image band selection;self-expression layer;linear subspaces;fully-connected linear layers;corresponding decoder layers;multiscale fusion module;multiscale information;maximum entropy regularization","","","","7","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"SAR image segmentation for land cover change detection","A. Das; A. Sahi; U. Nandini","Department of Computer Science, Sathyabama university, Chennai, India; Department of Computer Science, Sathyabama university, Chennai, India; Faculty of computing, Sathyabama University, Chennai, India","2016 Online International Conference on Green Engineering and Technologies (IC-GET)","4 May 2017","2016","","","1","6","Synthetic aperture radar is used for land cover change detection which can either be mounted on a drone or an aircraft, spacecraft. It can be used for land cover change detection by comparing two images which are taken at different intervals of time. For this we are using differencing methods. The operators used in differencing methods are log ratio and mean ratio. For obtaining a better difference in image. The image fusion technique is applied using complementary information obtained from log ratio and mean ratio differenced image then the image is segmented using k means clustering algorithms in which k centroids are placed as one for each cluster such that they are at maximum distance away from each other. The image obtained is compared with the ground truth, which has been already implemented. Now if the image is normal there is no change, and if the image is abnormal then the change has occurred during that time interval.","","978-1-5090-4556-3","10.1109/GET.2016.7916810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916810","Change detection;Image fusion;Non sub sampled contoured transform (NSCT);Synthetic Aperture Radar (SAR)","Synthetic aperture radar;Image fusion;Transforms;Image segmentation;Clustering algorithms;Image edge detection;Radar imaging","geophysical image processing;image fusion;image segmentation;land cover;remote sensing by radar;synthetic aperture radar","image fusion technique;synthetic aperture radar image segmentation;land cover change detection","","6","","24","IEEE","4 May 2017","","","IEEE","IEEE Conferences"
"Remote Sensing Image Registration Based on Fuzzy Shape Context Feature and Local Space Vector Similarity Constraint","X. Ma; Y. Yang; K. Yang","Ministry of Education, The Engineering Research Center of GIS Technology in Western China, China; Ministry of Education, The Engineering Research Center of GIS Technology in Western China, China; Ministry of Education, The Engineering Research Center of GIS Technology in Western China, China","2018 26th International Conference on Geoinformatics","6 Dec 2018","2018","","","1","5","Remote sensing image registration is a fundamental and challenging problem, and it is a critical prerequisite in a wide range of applications including environment monitoring, change detection, image fusion, image mosaic, and map updating. Remote sensing image registration is a key technology for dynamic monitoring of a city. In order to promote remote sensing image registration precision, we propose a remote sensing image registration method based on fuzzy shape context feature and local space vector similarity constraint. There are two main contributions: (1) Fuzzy shape context feature (2) Local spatial vector similarity constraints based on local vector feature. Meanwhile, we evaluated the performances of the proposed method and compared with four state-of-the-art methods (SIFTCPDR-SOCGLMDTPS) where our method shows the best alignments in most cases.","2161-0258","978-1-5386-7619-6","10.1109/GEOINFORMATICS.2018.8557108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8557108","fuzzy shape context feature(FSC);local vector feature;local spatial vector similarity constraint;Gaussian mixture model;Remote Sensing Image Registration","","fuzzy set theory;geophysical image processing;image registration;image sensors;remote sensing;vectors","image fusion;remote sensing image registration method;local vector feature;image mosaicking;fuzzy shape context feature;local spatial vector similarity constraints;environment monitoring;change detection;SIFTCPDR-SOCGLMDTPS","","","","20","IEEE","6 Dec 2018","","","IEEE","IEEE Conferences"
"Optimal Fusion Technique for Multi-Scale Remote Sensing Images Based on DWT and CNN","P. I. Basheer; K. P. Prasad; A. D. Gupta; B. Pant; V. P. Vijavan; D. Kapila","Department of Electronics and Communication Engineering, SSM Polytechnic College Tirur, Malappuram, Kerala; Department of Electronics and Communication Engineering, Siddartha Educational Academy Group of Institutions, Tirupati, India; Department of Geography, Chandernagore Government College, Hugli, West Bengal, India; Department of Computer Science and Engineering, Graphic Era Deemed to be University, Dehradun, Uttarakhand, India; Mangalam College of Engineering, Kottayam, Kerala, India; Department of Computer Science and Engineering, Lovely Professional University, Phagwara, India","2022 8th International Conference on Smart Structures and Systems (ICSSS)","1 Jun 2022","2022","","","1","6","The practise of fusing multiple photographs of the same scene captured at different focal lengths into a single all-focus image is known as multifocal image fusion. Local filters are utilised in most well-known fusion algorithms to capture high-frequency data before applying various fusion rules to create fused images. By decomposing the source and fusion images into numerous states, this work uses a discrete wavelet to create high-frequency and low-frequency images. The core CNN architecture in this study includes multistate extraction features & learning in residual, resulting in a multi scale & depth pan sharpening CNN data through remote sensing. Features from the images are extracted using D W T algorithms which is pre-trained. MATLAB is used to implement the suggested DWT -based picture fusion algorithm.","","978-1-6654-9761-9","10.1109/ICSSS54381.2022.9782239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9782239","principal component analysis;singular-value decomposition;fusion images;deep convolutional neural network","Wavelet domain;Frequency-domain analysis;Transforms;Feature extraction;Discrete wavelet transforms;Convolutional neural networks;Intelligent structures","convolutional neural nets;discrete wavelet transforms;feature extraction;image capture;image filtering;image fusion;optimisation;remote sensing","multiscale remote sensing images;focal lengths;all-focus image;multifocal image fusion;local filters;fusion rules;fusion images;CNN architecture;DWT;multistate extraction;discrete wavelet","","","","10","IEEE","1 Jun 2022","","","IEEE","IEEE Conferences"
"Fusion of Hyperspectral and Panchromatic Images using Structure Tensor","J. Qu; Y. Li; W. Dong","State Key Lab. of Integrated Service Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Service Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Service Networks, Xidian University, Xi'an, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","7216","7219","In this paper, a new hyperspectral image fusion method with structure tensor is proposed. The proposed method utilizes PCA transformation to obtain the spatial details of HS image. Then, an image enhancement approach is applied to the PAN image to sharpen spatial information. Since structure tensor represents structure and spatial information, structure tensor is introduced to extract spatial details of the enhanced PAN image. Unlike traditional methods which extract details only from PAN image, the proposed method considers spatial details of the HS and PAN images simultaneously, and a weighted fusion method is presented to integrate spatial details of the two images to obtain complete spatial details. Finally, an injection gains matrix is constructed to reduce spectral and spatial distortion, and the fused image is generated by injecting the complete spatial information. Experimental results demonstrate that the proposed method obtains the excellent performance in both objective and subjective evaluations.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518264","Structure tensor;hyperspectral image;panchromatic image;weighted fusion","Tensile stress;Principal component analysis;Hyperspectral imaging;Image fusion;Spatial resolution;Distortion","geophysical image processing;image enhancement;image fusion;image resolution;matrix algebra;principal component analysis;remote sensing;tensors","panchromatic images;structure tensor;hyperspectral image fusion method;HS image;image enhancement approach;enhanced PAN image;weighted fusion method;spectral distortion;spatial distortion;PCA transformation;injection gains matrix","","1","","7","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Change Detection in Heterogeneous Remote Sensing Images Based on an Imaging Modality-Invariant MDS Representation","R. Touati; M. Mignotte; M. Dahmane","DIRO, University of Montréal, Canada; Universite de Montreal, Montreal, QC, CA; R&D vision Department, Centre de Recherche Informatique de Montréal (CRIM)","2018 25th IEEE International Conference on Image Processing (ICIP)","6 Sep 2018","2018","","","3998","4002","In this paper, we propose a new multimodal change detection in remote sensing. The proposed method is based on a projection of the two multisensor satellite images to a common feature space, in which the two heterogeneous images share the same statistical properties and on which any classical monomodal change detection methods can be applied. This transformation of the before and after images is mainly based on a Multidimensional Scaling(MDS) representation which can be also viewed as a de-texturing approach of the two multisource images. Experimental results involving different types of imaging techniques confirm the reliability of the proposed approach.","2381-8549","978-1-4799-7061-2","10.1109/ICIP.2018.8451184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451184","","Satellites;Optical imaging;Histograms;Remote sensing;Feature extraction;Optical sensors","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;image texture;remote sensing","Multidimensional Scaling representation;multisource images;classical monomodal change detection methods;heterogeneous images;common feature space;multisensor satellite images;multimodal change detection;imaging modality-invariant MDS representation;heterogeneous remote sensing images;imaging techniques","","22","","26","IEEE","6 Sep 2018","","","IEEE","IEEE Conferences"
"Change detection in heterogeneous remote sensing images based on the fusion of pixel transformation","Z. -g. Liu; L. Zhang; G. Li; Y. He","School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","2017 20th International Conference on Information Fusion (Fusion)","14 Aug 2017","2017","","","1","6","A new change detection method for heterogeneous remote sensing images (i.e. SAR & optics) has been proposed via pixel transformation. It is difficult to directly compare the pixels from heterogeneous images for detecting changes. We propose to transfer the pixels in different images to a common feature space for convenience of comparison. For each pixel in the 1st image, it will be transferred to the 2nd feature space associated with the 2nd image according to the given unchanged pixel pairs. In fact, this transformation is done assuming that the pixel is not affected by the events. Then the difference value between the estimation of transferred pixel and the actual one in the same location of the 2nd image can be calculated. The bigger difference value, the higher possibility of change happening. We can similarly do the opposite transformation from the 2nd image to the 1st image, and one more difference value is obtained in the 1st feature space. Change occurrences will be detected using Fuzzy C-means clustering method based on the sum of two difference values. The flood detection in the SAR and optical images is given in the experiments, and it shows that the proposed method is able to efficiently detect changes.","","978-0-9964-5270-0","10.23919/ICIF.2017.8009656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009656","","Remote sensing;Synthetic aperture radar;Feature extraction;Clustering algorithms;Training;Neurons;Self-organizing feature maps","feature extraction;fuzzy set theory;geophysical image processing;image fusion;image resolution;pattern clustering;remote sensing;synthetic aperture radar","change detection;heterogeneous remote sensing images;pixel transformation fusion;heterogeneous images;feature space;unchanged pixel pairs;difference value;fuzzy c-means clustering method;flood detection;SAR;optical images","","10","","20","","14 Aug 2017","","","IEEE","IEEE Conferences"
"Removing Haze Influence from Remote Sensing Images Captured with Airborne Visible/ Infrared imaging Spectrometer by Cascaded Fusion of DCP, GF, LCC with AHE","R. S. Gound; S. D. Thepade","Computer Engineering Department, Pimpri Chinchwad College of Engineering, Pune, India; Computer Engineering Department, Pimpri Chinchwad College of Engineering, Pune, India","2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)","12 Apr 2021","2021","","","658","664","Haze is one of the factors which deteriorate the quality of AVIRIS (Airborne Visible/ Infrared imaging Spectrometer) images [1]. It reduces clarity and interpretability of images. Removal or suppression of haze becomes essential to enhance quality of images for further applicability and uses. Present article exemplifies haze detection and removal of AVIRIS images by using Proposed Method of Fusion of Dark Channel Prior (DCP), Guided Filter (GF) and Local Color Correction (LCC) with Adaptive Histogram Equalization (AHE). Proposed method of removing haze influence from remote sensing images captured by Airborne Visible/ Infrared imaging Spectrometer produces output with better quality. Images which contain, shadows, flat and highly reflective surfaces results in limitations of proposed method. Proposed fusion based haze removal method gives highest entropy among the algorithms compared, as reflected from experimentation.","","978-1-7281-8529-3","10.1109/ICCCIS51004.2021.9397060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9397060","AVIRIS;Dark Channel Prior;Guided Image Filtering;Local Color Correction;Adaptive Histogram Equalization","Histograms;Satellites;Image color analysis;Clouds;Infrared imaging;Entropy;Remote sensing","entropy;filtering theory;geophysical image processing;geophysical signal processing;image colour analysis;image enhancement;image fusion;image restoration;infrared imaging;remote sensing;visibility","LCC;AHE;AVIRIS images;haze detection;DCP;haze influence;remote sensing images captured;fusion based haze removal method;cascaded Fusion","","","","17","IEEE","12 Apr 2021","","","IEEE","IEEE Conferences"
"Dual-Tasks Siamese Transformer Framework for Building Damage Assessment","H. Chen; E. Nemni; S. Vallecorsa; X. Li; C. Wu; L. Bromley","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University; United Nations Satellite Centre (UNOSAT), United Nations Institute for Training and Research (UNITAR); European Organization for Nuclear Research (CERN); State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University; United Nations Satellite Centre (UNOSAT), United Nations Institute for Training and Research (UNITAR)","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1600","1603","Accurate and fine-grained information about the extent of damage to buildings is essential for humanitarian relief and disaster response. However, as the most commonly used architecture in remote sensing interpretation tasks, Convolutional Neural Networks (CNNs) have limited ability to model the non-local relationship between pixels. Recently, Transformer architecture first proposed for modeling long-range dependency in natural language processing has shown promising results in computer vision tasks. Considering the frontier advances of Transformer architecture in the computer vision field, in this paper, we present a Transformer-based damage assessment architecture (DamFormer). In DamFormer, a siamese Transformer encoder is first constructed to extract non-local and representative deep features from input multitemporal image-pairs. Then, a multitemporal fusion module is designed to fuse information for downstream tasks. Finally, a lightweight dual-tasks decoder aggregates multi-level features for final prediction. To the best of our knowledge, it is the first time that such a deep Transformer-based network is proposed for multitemporal remote sensing interpretation tasks. The experimental results on the large-scale damage assessment dataset xBD demonstrate the potential of the Transformer-based architecture.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883139","Transformer;building damage assessment;multi-task learning;deep learning;multitemporal images","Computer vision;Architecture;Buildings;Computer architecture;Transformers;Feature extraction;Decoding","computer vision;disasters;feature extraction;geophysical image processing;image classification;image fusion;image resolution;image segmentation;natural language processing;neural nets;radar imaging;remote sensing","Transformer-based damage assessment architecture;siamese Transformer encoder;representative deep features;input multitemporal image-pairs;multitemporal fusion module;downstream tasks;lightweight dual-tasks;aggregates multilevel features;deep Transformer-based network;multitemporal remote sensing interpretation tasks;large-scale damage assessment dataset;Transformer-based architecture;tasks siamese Transformer framework;building damage assessment;fine-grained information;buildings;humanitarian relief;disaster response;commonly used architecture;Convolutional Neural Networks;nonlocal relationship;Transformer architecture;long-range dependency;natural language processing;computer vision tasks;computer vision field","","5","","13","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"A Highly Efficient Method for Training Sample Selection in Remote Sensing Classification","C. Yang; Q. Li; G. Wu; J. Chen","College of Information Engineering, Shenzhen University, Shenzhen, China; Mapping and Geolnformation &Shenzhen, Key Laboratory of Spatial Smart Sensing and services Shenzhen University, Shenzhen, China; College of Life Sciences and Oceanography, Shenzhen University, Shenzhen, China; Ministry of Education, Nanjing Normal University, Nanjing, China","2018 26th International Conference on Geoinformatics","6 Dec 2018","2018","","","1","5","Remote sensing classification is an important way to obtain land cover information, and the selection of classification training samples for most of the classification method is an expensive and time-consuming task. However, the traditional training samples selection method is a direct selection based on two-dimensional (2D) images, therefore, training sample selection efficiency is always low in the regions with complex terrain and landscape fragmentation, and the ROI (region of interest) separability is unsatisfactory for classification. This study aims at the low efficiency and low ROI separability for traditional training sample selection method put forward a new training sample selection method using a three-dimensional (3D) terrain model that was created by OLI image fusion digital elevation model (DEM) to select ROIs, which departs from the traditional method based on a two-dimensional image. A Landsat-8 OLI image of the Yunlong Reservoir Basin in Kunming was used to test this proposed method. Study results showed that the proposed method obtained ROI separability that was greater than 1.9, and with most reaching 2.0; while the ROI separability of traditional method still had unqualified situation, which showed the new method was more effective.","2161-0258","978-1-5386-7619-6","10.1109/GEOINFORMATICS.2018.8557085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8557085","Remote sensing classification;Training samples;3D terrain;ROI separability;ImageFusion","","digital elevation models;geophysical image processing;image classification;image fusion;land cover;reservoirs;terrain mapping","Yunlong Reservoir Basin;Kunming;landscape fragmentation;region of interest;classification training samples;remote sensing classification;OLI image fusion digital elevation model;traditional training sample selection method;low ROI separability;sample selection efficiency;traditional training samples selection method;classification method","","1","","17","IEEE","6 Dec 2018","","","IEEE","IEEE Conferences"
"Dual Attention D-LinkNet for Road Segmentation in Remote Sensing Images","K. Wu; F. Cai","Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China","2022 IEEE 14th International Conference on Advanced Infocomm Technology (ICAIT)","26 Aug 2022","2022","","","304","307","Road segmentation from high-resolution remote sensing images is a hot research topic in recent years. The roads in high-resolution remote sensing images are irregular with large scale variations. In addition, the scenes of such images are complicated with interference factors that have negative impacts on the segmentation results. On this account, we propose a dual attention network (DA-LinkNet), which combines the advantages of D-LinkNet and the dual attention mechanism. The network is built with D-LinkNet architecture and the Coordinate Attention block is added in its center part, which aims to enhance feature representation. To better combine features from different branches, we adopt the Attentional Feature Fusion block to replace the linear feature fusion operation of skip connection. The comparative experiments on DeepGlobe Road Extraction dataset demonstrate the effectiveness and superiority of the proposed method, which exceed state-of-the-art methods by at least 2 % for mean Intersection over Union (mIoU).","2770-1603","978-1-6654-7156-5","10.1109/ICAIT56197.2022.9862683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9862683","remote sensing images;road segmentation;coordinate attention;attentional feature fusion","Image segmentation;Visualization;Roads;Semantics;Interference;Feature extraction;Task analysis","feature extraction;geophysical image processing;geophysical signal processing;image fusion;image segmentation;remote sensing;roads","high-resolution remote sensing images;roads;dual attention network;DA-LinkNet;dual attention mechanism;D-LinkNet architecture;Coordinate Attention block;Attentional Feature Fusion block;DeepGlobe Road Extraction dataset;dual Attention D-LinkNet;Road segmentation","","","","8","IEEE","26 Aug 2022","","","IEEE","IEEE Conferences"
"Super-resolution Reconstruction of Airborne Remote Sensing Images based on Multi-scale Fusion","F. Chu; H. Liu; Z. Wang; Z. Cao","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China","2022 3rd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)","23 Dec 2022","2022","","","648","651","To extract more detailed features of airborne remote sensing images to obtain more information, super-resolution reconstruction is performed on them. However, the existing super-resolution reconstruction algorithms of airborne remote sensing images have poor feature extraction capabilities, and smooth image edges, and are difficult to restore high-frequency information effectively. In this paper, the residual features of different residual modules are densely connected to form a dense group (DG), which combines different residual features to reduce the redundancy of features and ensure the effective transmission of high-frequency residual features. Further, the residual features of DG are densely connected to realize the reuse of information, and combined with multi-scale fusion, a two-branch lightweight multi-scale fusion super-resolution reconstruction network is proposed. The experimental results show that the algorithm has good performance and is lightweight, and can obtain a better reconstruction effect.","","978-1-6654-5160-4","10.1109/ICBAIE56435.2022.9985886","Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985886","component;multi-scale fusion;super-resolution reconstruction;airborne remote sensing image;residual dense network;lightweight","Image edge detection;Superresolution;Redundancy;Reconstruction algorithms;Feature extraction;Image restoration;Internet of Things","feature extraction;geophysical image processing;image fusion;image reconstruction;image resolution;remote sensing","airborne remote sensing images;detailed features;different residual features;different residual modules;existing super-resolution reconstruction algorithms;high-frequency information;high-frequency residual features;multiscale fusion super-resolution reconstruction network;poor feature extraction capabilities;reconstruction effect;smooth image edges","","","","11","IEEE","23 Dec 2022","","","IEEE","IEEE Conferences"
"Remote Sensing Image Scene Classification Method Combined Attention Mechanism and Multiscale Feature","Y. Chen; L. Zheng; X. Chen; F. Zheng","School of Computer and Information Engineering, Henan University, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China","2021 6th International Conference on Image, Vision and Computing (ICIVC)","14 Sep 2021","2021","","","186","190","Remote sensing image scene classification is a hot topic in computer vision field. However, in remote sensing image, the background information is complex and the scale of objects varies greatly. To address these limitations, in this paper, we propose a method combined attention mechanism and multi-scale features fusion. First, ResNet50 is employed as the feature extraction backbone network to extract features, and multiple attention networks are used to extract attention maps from channels and spaces simultaneously. Second, a sampling branch is utilized to further amplify important features. In addition, a context information extraction network is designed to pool the features into three-scale and fuse them with the convolutional layer to provide richer feature information. In order to avoid network overfitting and improve the generalization ability of the model. Extensive experiments were evaluated on AID and NWPU-RESISC45 dataset. Our method outperforms the baseline by 2.71% on AID and 2.75% on NWPU benchmark respectively. The experimental results demonstrate that comparing with the state-of-the-art methods and baseline, the accuracy of our method is significantly improved.","","978-1-6654-4368-5","10.1109/ICIVC52351.2021.9526992","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9526992","scene classification;attention mechanism;multiscale;CNN;remote sensing","Computer vision;Image analysis;Fuses;Semantics;Benchmark testing;Feature extraction;Information retrieval","computer vision;feature extraction;geophysical image processing;image classification;image fusion;remote sensing","sensing image scene classification method combined attention mechanism;multiscale feature;computer vision field;remote sensing image;background information;multiscale features fusion;feature extraction backbone network;multiple attention networks;attention maps;amplify important features;context information extraction network;richer feature information;network overfitting","","","","19","IEEE","14 Sep 2021","","","IEEE","IEEE Conferences"
"Multi-Band Image Fusion Using Gaussian Process Regression with Sparse Rational Quadratic Kernel","F. S. Longman; L. Mihaylova; L. Yang; K. N. Topouzelis","Department of Automatic Control and System Engineering, University of Sheffield, Mappin Street, UK; Department of Automatic Control and System Engineering, University of Sheffield, Mappin Street, UK; Department of Electrical and Computer Engineering, University of Canterbury, Chrischurch, New Zealand; Department of Marine Sciences, University of the Aegean, University Hill, Mytilene, Greece","2019 22th International Conference on Information Fusion (FUSION)","27 Feb 2020","2019","","","1","8","This paper proposes an approach for multi-band image fusion using a multiple output variable Gaussian Process (GP) model. The considered model uses a new covariance function, which is a product of an intrinsically sparse kernel and a Rational Quadratic Kernel (RQK) to model the pixel coordinates and intensity of the high spatial resolution image. The new kernel serves as a stochastic prior for each band of the estimated image. The developed approach allows the exchange of information between the different modalities enabling local structure of the high spatial resolution image on which the model is trained. The accuracy performance and image quality assessment show that the proposed approach achieves compelling enhancement when compared with other fusion methods.","","978-0-9964527-8-6","10.23919/FUSION43075.2019.9011352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9011352","Image Fusion;Remote Sensing;Gaussian Processes;Multi-output Variable Gaussian Processes","Kernel;Spatial resolution;Image fusion;Gaussian processes;Training","approximation theory;Gaussian processes;image fusion;image resolution;regression analysis","high spatial resolution image;image quality assessment;multiband image fusion;Gaussian process regression;multiple output variable Gaussian Process model;intrinsically sparse kernel;multiband image fusion methods;sparse rational quadratic kernel","","1","","20","","27 Feb 2020","","","IEEE","IEEE Conferences"
"MTF-deblurring preprocessing for CS and MRA pansharpening methods","F. Palsson; J. R. Sveinsson; M. O. Ulfarsson; J. A. Benediktsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","1104","1107","The fusion of low resolution multispectral (MS) images and high resolution panchromatic (PAN) images, i.e., pansharpening, is an important technique in remote sensing and has many applications where high resolution imagery is important. Component substitution (CS) and multiresolution analysis (MRA) are two large families of pansharpening methods that are fast and computationally efficient. They can be described using a general framework, where details from the PAN image are added to the upsampled and interpolated MS image. However, these methods often suffer from spectral and spatial distortions. We propose a pre-processing step, where instead of just interpolating the MS image to the resolution scale of the PAN image, we do a deconvolution of the interpolated MS image based on the sensor's modulation transfer function (MTF). This results in large improvement gains in the spectral and spatial quality of the fused image. We demonstrate our method using a real WorldView-2 dataset and show that our approach significantly improves the tested methods in both the CS and MRA families of pansharpening methods.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325963","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325963","Image fusion;pansharpening;WorldView-2;component substitution;multiresolution analysis","Yttrium;Spatial resolution;Measurement;Remote sensing;Distortion;Interpolation","geophysical image processing;geophysical techniques;image fusion;image resolution;interpolation;optical transfer function;remote sensing","MRA pansharpening method;MTF-deblurring preprocessing;CS pansharpening method;low resolution multispectral image;high resolution panchromatic image;component substitution;multiresolution analysis;modulation transfer function;spatial fused image quality;spectral fused image quality;WorldView-2 dataset;CS family;MRA family","","","","9","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Towards a combined sparse representation and unmixing based hybrid hyperspectral resolution enhancement method","C. Grohnfeldt; X. X. Zhu","DLR German Aerospace Center, Remote Sensing Technology Institute, Oberpfaffenhofen, Wessling, Germany; Technische Univcrsität München, Lehrstuhl für Methodik der Femerkundung, Munich, Germany","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","2872","2875","The fusion of hyperspectral data with a corresponding higher resolution multispectral image has become an increasingly active research field. The goal is to create a hyperspectral image that has the spatial resolution of the multispectral image. This work aims at combining two established fusion algorithms, namely J-SparseFI-HM and CNMF, to a new method which features their individual advantages. The sparse representation based J-SparseFI-HM algorithm is used to pre-process those hyperspectral channels that have a strong spectral overlap with the multispectral instrument. Then, three modified versions of the matrix factorization and unmixing based CNMF method are used for post-processing. The results are assessed and compared to the individual products of J-SparseFI-HM and CNMF, revealing a great potential for performance improvement.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326414","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326414","image fusion;J-SparseFI-HM;CNMF;resolution enhancement;sparse representation;spectral unmixing","Hyperspectral imaging;Spatial resolution;Image fusion;Image reconstruction","geophysical image processing;hyperspectral imaging;image enhancement;image representation;image resolution;matrix decomposition;terrain mapping","sparse representation;unmixing based hybrid hyperspectral resolution enhancement method;hyperspectral data fusion;high resolution multispectral image;fusion algorithms;J-SparseFI-HM fusion algorithm;CNMF;hyperspectral channel preprocess;multispectral instrument;matrix factorization;unmixing based CNMF method;J-SparseFI-HM products","","5","","15","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Fusing Landsat and MODIS Data for Vegetation Monitoring","F. Gao; T. Hilker; X. Zhu; M. Anderson; J. Masek; P. Wang; Y. Yang","USDA-ARS, Hydrology and Remote Sensing Laboratory, Beltsville, MD, USA; College of Forestry, Oregon State University, Corvallis, OR, USA; Department of Ecosystem Science and Sustainability, Colorado State University, Fort Collins, CO, USA; USDA-ARS, Hydrology and Remote Sensing Laboratory, Beltsville, MD, USA; NASA, Goddard Space Flight Center, Greenbelt, MD, USA; Institute of Eco-environment and Agro-meteorology, Chinese Academy of Meteorological Sciences, Beijing, China; USDA-ARS, Hydrology and Remote Sensing Laboratory, Beltsville, MD, USA","IEEE Geoscience and Remote Sensing Magazine","30 Sep 2015","2015","3","3","47","60","Crop condition and natural vegetation monitoring require high resolution remote sensing imagery in both time and space - a requirement that cannot currently be satisfied by any single Earth observing sensor in isolation. The suite of available remote sensing instruments varies widely in terms of sensor characteristics, spatial resolution and acquisition frequency. For example, the Moderate-resolution Imaging Spectroradiometer (MODIS) provides daily global observations at 250m to 1km spatial resolution. While imagery from coarse resolution sensors such as MODIS are typically superior to finer resolution data in terms of their revisit frequency, they lack spatial detail to capture surface features for many applications. The Landsat satellite series provides medium spatial resolution (30m) imagery which is well suited to capturing surface details, but a long revisit cycle (16-day) has limited its use in describing daily surface changes. Data fusion approaches provide an alternative way to utilize observations from multiple sensors so that the fused results can provide higher value than can an individual sensor alone. In this paper, we review the Spatial and Temporal Adaptive Reflectance Fusion Model (STARFM) and two extended data fusion models (STAARCH and ESTARFM) that have been used to fuse MODIS and Landsat data. The fused MODISLandsat results inherit the spatial details of Landsat (30 m) and the temporal revisit frequency of MODIS (daily). The theoretical basis of the fusion approach is described and recent applications are presented. While these approaches can produce imagery with high spatiotemporal resolution, they still rely on the availability of actual satellite images and the quality of ingested remote sensing products. As a result, data fusion is useful for bridging gaps between medium resolution image acquisitions, but cannot replace actual satellite missions.","2168-6831","","10.1109/MGRS.2015.2434351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284777","","Satellites;Remote sensing;Vegetation mapping;MODIS;Spatial resolution;Reflectivity","geophysical image processing;image fusion;remote sensing;vegetation","Landsat data fusion;MODIS data fusion;vegetation monitoring;crop condition;high resolution remote sensing imagery;Earth observing sensor;Moderate-resolution Imaging Spectroradiometer;coarse resolution sensor;Landsat satellite series;daily surface change;Spatial and Temporal Adaptive Reflectance Fusion Model;STARFM;data fusion model;satellite images;remote sensing product;image acquisition;satellite mission","","185","","39","IEEE","30 Sep 2015","","","IEEE","IEEE Magazines"
"Combining Deep and Shallow Neural Networks with Ad Hoc Detectors for the Classification of Complex Multi-Modal Urban Scenes","D. Cerra; M. Pato; E. Carmona; S. M. Azimi; J. Tian; R. Bahmanyar; F. Kurz; E. Vig; K. Bittner; C. Henry; P. d'Angelo; R. Müller; K. Alonso; P. Fischer; P. Reinartz","Remote Sensing Technology Institute (MF-PBA), German Aerospace Center (DLR), Weßling, Germany; Remote Sensing Technology Institute (MF-PBA), German Aerospace Center (DLR), Weßling, Germany; Remote Sensing Technology Institute (MF-PBA), German Aerospace Center (DLR), Weßling, Germany; Remote Sensing Technology Institute (MF-PBA), German Aerospace Center (DLR), Weßling, Germany; Remote Sensing Technology Institute (MF-PBA), German Aerospace Center (DLR), Weßling, Germany; Remote Sensing Technology Institute (MF-PBA), German Aerospace Center (DLR), Weßling, Germany; Remote Sensing Technology Institute (MF-PBA), German Aerospace Center (DLR), Weßling, Germany; Remote Sensing Technology Institute (MF-PBA), German Aerospace Center (DLR), Weßling, Germany; Remote Sensing Technology Institute (MF-PBA), German Aerospace Center (DLR), Weßling, Germany; Remote Sensing Technology Institute (MF-PBA), German Aerospace Center (DLR), Weßling, Germany; Remote Sensing Technology Institute (MF-PBA), German Aerospace Center (DLR), Weßling, Germany; Remote Sensing Technology Institute (MF-PBA), German Aerospace Center (DLR), Weßling, Germany; Remote Sensing Technology Institute (MF-PBA), German Aerospace Center (DLR), Weßling, Germany; Remote Sensing Technology Institute (MF-PBA), German Aerospace Center (DLR), Weßling, Germany; Remote Sensing Technology Institute (MF-PBA), German Aerospace Center (DLR), Weßling, Germany","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","3856","3859","This article describes the workflow of the classification algorithm which ranked at 2nd place in the 2018 GRSS Data Fusion Contest. The objective of the contest was to provide a classification map with 20 classes on a complex urban scenario. The available multi-modal data were acquired from hyperspectral, LiDAR and very high-resolution RGB sensors flown on the same platform over the city of Houston, TX, USA. The classification was obtained by merging deep convolutional and shallow fully-connected neural networks on a simplified set of classes, complemented by a series of specific detectors and ad hoc classifiers.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517699","data fusion;classification;LiDAR;hyperspectral;very high-resolution","Artificial neural networks;Laser radar;Buildings;Automobiles;Detectors;Feature extraction","geophysical image processing;geophysical signal processing;image classification;image fusion;neural nets;optical radar;pattern classification;sensor fusion;terrain mapping","hyperspectral LiDAR;high-resolution RGB sensors;ad hoc classifiers;shallow neural networks;ad hoc detectors;complex multimodal urban scenes;classification algorithm;classification map;complex urban scenario;GRSS Data Fusion Contest","","8","","3","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Single Remote Sensing Image Dehazing Using a Dual-Step Cascaded Residual Dense Network","Y. Huang; X. Chen","College of Electronic and Information Engineering, Shenyang Aerospace University, Shenyang, China; College of Electronic and Information Engineering, Shenyang Aerospace University, Shenyang, China","2021 IEEE International Conference on Image Processing (ICIP)","23 Aug 2021","2021","","","3852","3856","Remote sensing (RS) dehazing is an extremely challenging task since the non-uniform distribution of haze and fog severely degrade the images and difficult to extract features. To address these issues, we propose an end to end Dual-step Cascaded Residual Dense Network called DCRD-Net, which can exactly remove haze from the hazy RS image and precisely restoring the details. The architecture of network contains two cascaded task-driven subnetworks, in order to deal with the coarse and fine haze-relevant features separately. Besides, the Residual Dense Enhancement Block (RDEB) is involved to guide the feature extraction, so that multi-scale information can be used to estimate the local and global features. Further, the Squeeze and Excitation (SE) Block is employed to optimize the RDEB, for the purpose to get the contextual feature and reduce computation complex. Quantitative and qualitative results illustrate that the designed framework outperforms the recent outstanding dehazing methods on promoting haze remove and restoring detailed in the synthetic and real-world RS images under various scenes. To encourage more comparisons, we release our codes on GitHub https://github.com/cxtalk/DCRD-Net.","2381-8549","978-1-6654-4115-5","10.1109/ICIP42928.2021.9506603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506603","Residual Dense Enhancement Block;image dehazing;remote sensing image;cascade network","Image color analysis;Fuses;Conferences;Computer architecture;Feature extraction;Image restoration;Sensors","computational complexity;feature extraction;fog;geophysical image processing;image denoising;image enhancement;image fusion;image restoration;image segmentation;remote sensing;visual databases","single remote sensing image dehazing;hazy RS image;cascaded task-driven subnetworks;coarse haze-relevant features;fine haze-relevant features;RDEB;feature extraction;local features;global features;contextual feature;dual-step cascaded residual dense network;DCRD-Net;residual dense enhancement block;multiscale information fusion;squeeze and excitation block;synthetic RS image restoration;real-world RS image restoration","","1","","22","IEEE","23 Aug 2021","","","IEEE","IEEE Conferences"
"Double-Triplet-Pseudo-Siamese Architecture For Remote Sensing Aircraft Target Recognition","X. Cao; H. Zou; X. Ying; R. Li; S. He; F. Cheng","College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China","2021 International Conference on Computer, Blockchain and Financial Development (CBFD)","20 Apr 2022","2021","","","140","146","The key challenge of remote sensing aircraft target (RSAT) recognition is that features generated by similar target are difficult to distinguish. To solve the problem, we present a double-triplet-pseudo-siamese (DTPS) architecture to learn to distinguish the subtle discriminative features between similar targets. Specifically, we first construct image triplet and mask triplet, which are then sent to the convolutional neural networks, fully connected layers and softmax sequentially for classification. Besides the classification predictions, we utilize standard templates for contrastive prediction in the test process and introduce a discriminative fusion method to fuse the multiple prediction. In addition, we utilize classification loss, contrast loss and triplet loss during training, which help the network to distinguish similar targets by metric learning. We conduct extensive experiments on benchmark RSAT datasets to demonstrate the effectiveness of our network and the experimental results show that the performance of the proposed method surpasses other existing methods.","","978-1-6654-1227-8","10.1109/CBFD52659.2021.00034","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9759163","Fine-grained recognition;Remote sensing of aircraft targets;Pseudo-Siamese;Triplet;Discriminative fusion","Training;Measurement;Target recognition;Fuses;Computer architecture;Feature extraction;Sensors","aircraft navigation;convolutional neural nets;feature extraction;image classification;image fusion;learning (artificial intelligence);object recognition;remote sensing","double-triplet-pseudosiamese architecture;remote sensing aircraft target recognition;subtle discriminative features;image triplet;mask triplet;convolutional neural networks;fully connected layers;classification predictions;contrastive prediction;discriminative fusion method;classification loss;contrast loss;triplet loss;DTPS architecture;metric learning;benchmark RSAT datasets","","","","12","IEEE","20 Apr 2022","","","IEEE","IEEE Conferences"
"Enhanced spatial resolution for VNREDSat-1 multispectral images using IHS fusion technique based on sensor spectral response function","L. N. H. Hoa; L. D. Cuong; L. C. Ke","Ministry of Public Security, Vietnam; Ministry of Public Security, Vietnam; Vietnam Association of Surveying Mapping and Remote Sensing","2016 Eighth International Conference on Knowledge and Systems Engineering (KSE)","1 Dec 2016","2016","","","304","308","Fusion technique of panchromatic image Pan in high spatial resolution with multispectral images MS (R, G, B, NIR) in low spatial resolution of Vietnam VNREDSat-1 satellite images to create multispectral images in high spatial resolution is described in this paper. The fusion method is based on the sensor spectral characteristics of VNREDSat-1, so called as sensor spectral response function. The proposed algorithm for image fusion will be evaluated through quantitative quality of fused images, based on the image statistical parameters such as the threshold error (bias), standard deviation (SD), root mean square error (RMSE) and ERGAS index. Surveying results of five studied show that the solution using the distances weighting between four multispectral image channels (R, G, B, NIR) and Pan image of Vietnam VNREDSat-1 data for Intensity-Hue-Saturation (IHS) method was very effective in fast calculation time, low cost, good quality of fused images.","","978-1-4673-8929-7","10.1109/KSE.2016.7758071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7758071","","Image fusion;Image color analysis;Spatial resolution;Remote sensing;Image quality;Satellites;Optical sensors","geophysical image processing;image fusion;image resolution;spectral analysis;statistical analysis","spatial resolution enhancement;VNREDSat-1 multispectral images;IHS fusion technique;intensity-hue-saturation method;sensor spectral response function;panchromatic image;high spatial resolution;Vietnam VNREDSat-1 satellite images;image fusion;fused image quantitative quality;image statistical parameters;threshold error;standard deviation;SD;root mean square error;RMSE;ERGAS index;distances weighting;multispectral image channels;Pan image;Vietnam VNREDSat-1 data","","1","","10","IEEE","1 Dec 2016","","","IEEE","IEEE Conferences"
"Pansharpening based on an improved ratio enhancement","X. Li; Q. Xu; F. Gao; L. Hu","School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; Computer and Information Engineering School, Jiangxi Normal University, Nanchang, China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","1100","1103","Pansharpening technique is very important for many remote sensing applications. Many fusion algorithms have been proposed to pan-sharpen multispectral (MS) images. However, there are still some spatial or spectral distortion problems in fusion result. There are two major reasons: First, panchromatic (PAN) image contains some interference spectrum information similar to MS image which may cause color distortion in the fusion result. Second, MS image has some interference spatial information approximates PAN image which may cause spatial artifacts. It is difficult to simultaneous eliminate the interference information from PAN and MS images. To solve the above problems, the paper presents an improved pan-sharpen algorithm which integrates the advantages of the ratio enhancement method and Gaussian-fitting. The high-frequency information of each ith band of MS image and the low-frequency information of PAN image are extracted by Gaussian-fitting, and the information is synthesized into a group of low-resolution PAN images. Finally, each ith band of MS image is pan-sharpened by a ratio enhancement, in which the ratio is obtained by image division between the PAN image and the ith synthesized low-resolution PAN image. Extensive experiments have been implemented on WorldView-2 images. Visual comparison and quantitative analysis demonstrated that the proposed method can achieve good performance in spatial and spectral fidelity.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325962","Pan-sharpening;ratio enhancement;fusion technique;Gaussian-fitting","Distortion;Remote sensing;Image color analysis;Spatial resolution;Image fusion;Indexes","geophysical image processing;geophysical techniques;image fusion","WorldView-2 images;PAN image low-frequency information;Gaussian-fitting;improved pan-sharpen algorithm;panchromatic image;spectral distortion problems;pan-sharpen multispectral images;remote sensing applications;pansharpening technique;improved ratio enhancement","","1","","12","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"A Deep Learning-Based Heterogeneous Spatio-Temporal-Spectral Fusion: SAR and Optical Images","M. Jiang; J. Li; H. Shen","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","1252","1255","Image fusion is a powerful means to integrate complementary spatio-temporal-spectral information among multi-source remote sensing images. The existing remote sensing image fusion is mostly limited to the fusion between optical images, and most of them are limited to the fusion between two sensors. Based on this, this paper proposes a heterogeneous spatio-temporal-spectral fusion method based on deep learning. Specifically, it combines the low-spatial-resolution (LR) cloudy image with the high-spatial-resolution (HR) SAR images and the HR cloud-free optical image to remove the clouds and improve the spatial resolution of the LR cloudy image. The SAR image is acquired at the same date as the LR cloudy image, while the HR cloud-free image is acquired at another date. Experiments are performed on the images of Landsat 8, Sentinel-1, and Sentinel-2. The experimental results show that the proposed method can effectively achieve the joint goal of spatial resolution improvement and cloud removal of the Landsat image.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554031","National Natural Science Foundation of China(grant numbers:62071341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554031","Heterogeneous;SAR;optical;cloud-removal;spatial resolution improvement","Earth;Artificial satellites;Clouds;Optical imaging;Radar polarimetry;Optical sensors;Spatial resolution","deep learning (artificial intelligence);geophysical image processing;geophysical techniques;image fusion;image resolution;optical images;remote sensing;synthetic aperture radar","Sentinel-1;Sentinel-2;Landsat image;spatial resolution improvement;HR cloud-free image;SAR image;LR cloudy image;HR cloud-free optical image;high-spatial-resolution SAR images;low-spatial-resolution cloudy image;heterogeneous spatio-temporal-spectral fusion method;remote sensing image fusion;multisource remote sensing images;complementary spatio-temporal-spectral information;optical images;deep learning-based heterogeneous spatio-temporal-spectral fusion","","3","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Impulse noise reduction using image fusion approach","N. U. Khan; S. U. Khan; W. Y. Chai","CECOS University, Peshawar, Pakistan; CECOS University, Peshawar, Pakistan; FCSIT. Universiti Malaysia Sarawak, Kuching, Malaysia","2016 Sixth International Conference on Innovative Computing Technology (INTECH)","9 Feb 2017","2016","","","261","264","Digital image enhancement has been a hot topic during the past decades. In this paper, we have established a new fusion based approach for impulse noise reduction from multi-sensors images. Image fusion is an important technique used in remote sensing, military and medical applications. In the proposed approach, firstly, only the uncorrupted pixels from multi-sensor images are grouped and then all those grouped pixels are filter with another state of the art approach for optimal results. The proposed approach is tested with several noisy images by finding the Mean Square Error, Peak Signal to Noise Ratio and Mutual Information values between the original and restored images. The experimental results show that the proposed algorithm outclasses all methods that individually de-noised and fused images.","","978-1-5090-2000-3","10.1109/INTECH.2016.7845011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7845011","image de-noising;image fusion;impulse noise;median filter","Wiener filters;Mathematical model;Image restoration;Optical filters;Filtering algorithms;Noise measurement","image denoising;image enhancement;image filtering;image fusion;image restoration;mean square error methods","image fusion;impulse noise reduction;digital image enhancement;multisensor images;uncorrupted pixels;noisy images;mean square error;peak signal-to-noise ratio;mutual information values;original images;restored images;fused images","","2","","14","IEEE","9 Feb 2017","","","IEEE","IEEE Conferences"
"Analysis and Detection of Mucilage Bloom from Multispectral Satellite Images","S. Küçük; B. Abacı; M. Dede; S. E. Yüksel; M. Yılmaz","Elektrik ve Elektronik Mühendisliği Bölümü, Erzurum Teknik Üniversitesi, Erzurum, Türkiye; Elektrik ve Elektronik Mühendisliği Bölümü, Hacettepe Üniversitesi, Ankara, Türkiye; Elektrik ve Elektronik Mühendisliği Bölümü, Hacettepe Üniversitesi, Ankara, Türkiye; Elektrik ve Elektronik Mühendisliği Bölümü, Hacettepe Üniversitesi, Ankara, Türkiye; Biyomühendislik Bölümü, Bursa Teknik Üniversitesi, Bursa, Türkiye","2022 30th Signal Processing and Communications Applications Conference (SIU)","29 Aug 2022","2022","","","1","4","In this paper, we aim to detect and observe the mucilage formations in the Sea of Marmara by means of Sentinel-2A satellite data. For this purpose, we produce mucilage index maps by utilizing the relationship between the spectral bands of Sentinel-2A. Sentinel-2A has four 10m fine bands and six 20m coarse bands. To compute the mucilage index, the spectral bands must have the same spatial resolution. Although the Sentinel-2A does not have a panchromatic band, the spatial resolution of the 20m bands can be increased to 10m thanks to its four fine bands. Based on the results of this analysis, we utilize seven of the existing image fusion approaches to enhance the spatial resolution of 20m bands to 10m. We monitor changes in mucilage formations over time with the mucilage maps acquired by using fused images through the mucilage index.","2165-0608","978-1-6654-5092-8","10.1109/SIU55565.2022.9864988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864988","Fusion;mucilage;Sentinel-2A;remote sensing","Satellites;Signal processing;Sensors;Indexes;Spatial resolution;Monitoring;Image fusion","geophysical image processing;image fusion;image resolution;ocean composition;oceanographic techniques;remote sensing","mucilage maps;mucilage bloom;multispectral satellite images;mucilage formations;satellite data;mucilage index maps;spectral bands;fine bands;coarse bands;panchromatic band;image fusion;Sentinel-2A","","","","0","IEEE","29 Aug 2022","","","IEEE","IEEE Conferences"
"Multi-Modal Fusion Architecture Search for Land Cover Classification Using Heterogeneous Remote Sensing Images","X. Li; L. Lei; G. Kuang","State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","5997","6000","Optical and SAR modalities can provide the complementary information on land properties for better land cover classification. Most of existing multi-modal land cover classification methods based on two-streams convolutional neural networks (CNNs), which obtained fusion features by merging optical and SAR features that come from manually selective layer of different streams. However, they ignored different semantic between manually selective optical and SAR features, which might result in suboptimal fusion features. We tackle the problem of finding good fusion architectures for multimodal land cover classification inspired by the network architecture search (NAS), and introduces the multi-modal fusion architecture search network (M2PASNet). Extensive experimental results show superior performances of our work on a broad co-registered optical and SAR dataset.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555029","Land cover classification;convolutional neural networks (CNNs);network architecture search (NAS);multi-modal fusion","Semantics;Neural networks;Optical computing;Optical fiber networks;Network architecture;Optical imaging;Search problems","feature extraction;geophysical image processing;image classification;image fusion;neural nets;radar imaging;remote sensing;sensor fusion;synthetic aperture radar","different streams;manually selective optical SAR features;suboptimal fusion features;good fusion architectures;network architecture search;multimodal fusion architecture search network;heterogeneous remote sensing;land properties;existing multimodal land cover classification methods;two-streams convolutional neural networks;manually selective layer","","3","","14","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A Novel Composite Kernel Approach for Multisensor Remote Sensing Data Fusion","P. Ghamisi; B. Rasti; R. Gloaguen","Helmholtz-Zentrum Dresden-Rossendorf (HZDR), Helmholtz Institute Freiberg for Resource Technology (HIF), Freiberg, Germany; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Helmholtz-Zentrum Dresden-Rossendorf (HZDR), Helmholtz Institute Freiberg for Resource Technology (HIF), Freiberg, Germany","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","2507","2510","The increased availability of active and passive data captured over the same scene of interest makes it desirable to jointly utilize multisensor data to perform accurate classification. This paper proposes a novel fusion approach to integrate hyperspectral and LiDAR-derived digital surface model for land-cover classification. In this context, we propose a novel multisensor composite kernel technique based on extreme learning machines (named as multisensor composite kernels (MCKs)), which is capable of combining different methods in the feature fusion level in an effective way. In the proposed approach, we use extinction profiles to extract spatial and elevation features of hyperspectral and LiDAR data. Then, hyperspectral Stein's unbiased risk estimator (HySURE) is applied to identify the subspace (informative features) of spectral, spatial, and elevation features. Finally, MCK is applied to the extracted spectral, spatial, and elevation features to produce the final classification map. Results obtained by the proposed approach reveal the fact that this approach can effectively fuse and classify hyperspectral and LiDAR images and improve the classification accuracy of each data source significantly. In addition, the proposed method is fully automatic.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900136","Classification;Hyperspectral;LiDAR;Extreme Learning Machne;Multisensor Data Fusion;Extinction Profiles","Hyperspectral imaging;Laser radar;Feature extraction;Kernel;Data integration","feature extraction;geophysical image processing;geophysical signal processing;image classification;image fusion;learning (artificial intelligence);optical radar;pattern classification;remote sensing;sensor fusion;terrain mapping","extracted spectral elevation features;spatial, elevation features;final classification map;hyperspectral LiDAR images;classification accuracy;data source;novel composite kernel approach;multisensor remote sensing data fusion;active data;passive data;multisensor data;fusion approach;hyperspectral LiDAR-derived digital surface model;land-cover classification;novel multisensor composite kernel technique;extreme learning machines;multisensor composite kernels;feature fusion level;hyperspectral LiDAR data;hyperspectral Stein's unbiased risk estimator;subspace;informative features","","1","","14","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Band Independent Residual Networks for Optical Remote Sensing Images Fusion","M. El Amin Larabi; M. Iftene; M. I. Tchenar; K. Bakhti; H. Kamel","The State key laboratory of Virtual Reality Technology and Systems, Beihang University; Agence Spatiale Algérienne, Centre des Techniques Spatiales, Algérie; The State key laboratory of Virtual Reality Technology and Systems, Beihang University; Agence Spatiale Algérienne, Centre des Techniques Spatiales, Algérie; Agence Spatiale Algérienne, Centre des Techniques Spatiales, Algérie","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4480","4483","With the development of deep learning (DL) techniques, recent research on pansharpening has improved the reconstruction accuracy in both spectral and spatial domains. Pansharpening is defined as the task of restoring high-frequencies details of high-resolution multispectral (MSHR) images from its low-resolution counterpart (MSLR) by exploiting panchromatic (PAN) high spatial resolution information. The proposed method exploits band independent deep neural network (BIN) for pansharpening, which uses a single band as input at once in combination with its PAN counterpart to minimize the residual error; this last has obtained significant improvement in accuracy with more stability and robustness against state-of-the-art methods. This perfection is due to the ability to train a very deep network through indirect and free data augmentation technique by considering each band as a separate training input, which makes it possible to duplicate each MSLR image into $s$ inputs ($s$ is number of bands).","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553740","Deep Neural Network;CNN;bands;pansharpening","Deep learning;Training;Satellites;Pansharpening;Streaming media;Stability analysis;Robustness","deep learning (artificial intelligence);geophysical image processing;image fusion;image reconstruction;image resolution;optical sensors;remote sensing","data augmentation technique;separate training input;MSLR image;band independent residual networks;optical remote sensing images;deep learning techniques;pansharpening;spectral domains;high-resolution multispectral images;low-resolution counterpart;panchromatic high spatial resolution information;band independent deep neural network;PAN counterpart","","","","14","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Fusion of multispectral and panchromatic images based on a novel inter-band structure model","S. Zhong; Y. Zhang","Dept. of Information Engineering, Harbin Institute of Technology, Harbin, China; Dept. of Information Engineering, Harbin Institute of Technology, Harbin, China","2015 IEEE International Conference on Image Processing (ICIP)","10 Dec 2015","2015","","","457","461","Image fusion is one of the most important image processing methods in the field of remote sensing. Multispectral (MS) or hyperspectral (HS) images are often fused with panchromatic (PAN) images to enhance their spatial resolution while preserving the spectral information, which will lead to a better interpretation in subsequent applications. In order to achieve this goal, an improved method of image fusion, which is based on the amélioration de la résolution spatiale par injection de structures (ARSIS) Concept, is proposed in this paper. In the new method, the degree of diversity between each pixel and its surroundings is measured utilizing Center-Surround Spectral Angle (CSSA) model, and the more different a pixel is, the more detailed information is injected into the MS image. In order to verify the advantages of our proposed fusion method, experiments are processed on two data sets. Several assessment criteria are used to demonstrate the effectiveness of our proposed algorithm. Moreover, from the view of application, accuracy of classification on fused images is also evaluated to further show the superiority of our method.","","978-1-4799-8339-1","10.1109/ICIP.2015.7350840","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350840","Image fusion;ARSIS Concept;inter-band structures model;Center-Surround Spectral Angle model","Spatial resolution;Signal to noise ratio;Approximation methods;Correlation;Transforms;Standards;Image edge detection","geophysical image processing;hyperspectral imaging;image fusion;image resolution;remote sensing;spectral analysis","multispectral image fusion;panchromatic image fusion;interband structure model;image processing methods;remote sensing;MS images;HS images;PAN images;spatial resolution;spectral information;ARSIS concept;center-surround spectral angle model;CSSA model","","","","9","IEEE","10 Dec 2015","","","IEEE","IEEE Conferences"
"A Model Distillation Approach for Explaining Black-Box Models for Hyperspectral Image Classification","G. Taskin","Istanbul Technical University, Institute of Disaster Management, Istanbul, Turkey","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","3592","3595","Recent studies in remote sensing reveal that complex nonlinear learning models such as deep learning or ensemble-based learning are mostly preferred compared to shallow machine learning methods in solving many problems such as classification, image fusion, change detection, unmixing, and object recognition. The fact that much remote sensing data can be obtained quickly, abundantly, and free of charge, and the increasing computing power of computers with developing technology, are why such methods are preferred. With the emergence of big data, these methods provide more effective solutions than in past years, and they can outperform shallow machine learning methods in many remote sensing applications. Despite their high accuracy, such learning models have several limitations due to their black-box structure. Because of the high nonlinearity in predictive models, these models cannot explain why and how decisions are made. This paper presents a global model distillation approach to replace a black-box model with a fully explainable surrogate model utilizing polynomial chaos expansion. Preliminary results show that the proposed method can accurately replace a complex nonlinear model with a simpler one in hyperspectral image classification.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884727","Istanbul Technical University(grant numbers:MGA-2021-42793); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884727","Explainable AI;Hyperspectral image classification;surrogate modeling;model distillation","Chaos;Deep learning;Merging;Predictive models;Object recognition;Random forests;Monitoring","chaos;image classification;image fusion;learning (artificial intelligence);object recognition;polynomials;remote sensing","black-box model;hyperspectral image classification;complex nonlinear learning models;deep learning;shallow machine learning;image fusion;change detection;remote sensing data;big data;remote sensing applications;black-box structure;predictive models;global model distillation approach;fully explainable surrogate model;complex nonlinear model","","","","19","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Hyperspectral-Multispectral Image Fusion with Rank Estimation by using a Joint-sparse Regularizer","I. Ortiz; S. Rivera; T. Gelvez; F. Rojas; H. Arguello","Department of Computer Science, Universidad Industrial de Santander, Bucaramanga, Colombia; Department of Computer Science, Universidad Industrial de Santander, Bucaramanga, Colombia; Department of Electrical Engineering, Universidad Industrial de Santander, Bucaramanga, Colombia; Department of Computer Science, Universidad Industrial de Santander, Bucaramanga, Colombia; Department of Computer Science, Universidad Industrial de Santander, Bucaramanga, Colombia","2021 XXIII Symposium on Image, Signal Processing and Artificial Vision (STSIVA)","11 Nov 2021","2021","","","1","6","Combining information of a low-spatial resolution hyperspectral image (HSI) and a low-spectral resolution multispectral image (MSI) to obtain a high spatial-spectral resolution image (HRI) has become an important framework for some applications such as remote sensing. The state-of-the-art approaches use the prior information of the HRI, as sparsity, smoothness, or low-rankness, to solve the resultant fusion inverse problem. However, these methods using a global low-rank commonly require to know a priori the rank of the HRI, which is hardly ever known in the acquisition process. Therefore, this work introduces a joint-sparse regularizer in order to estimate the rank of the HRI intrinsically. Specifically, the proposed method follows an alternating direction method of multipliers based on the linear mixture decomposition. Simulations showed that the proposal allows fusing the HSI and MSI source while estimating the rank of the HRI during the iterative process.","2329-6259","978-1-6654-1668-9","10.1109/STSIVA53688.2021.9591999","Vicerectoria de Investigación y Extensión of Universidad Industrial de Santander; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9591999","Spectral imaging;Hyperspectral-multispectral image fusion;Linear mixture model;Joint-sparse regularizer","Inverse problems;Estimation;Mixture models;Signal processing;Proposals;Spatial resolution;Image fusion","hyperspectral imaging;image enhancement;image fusion;image resolution;image sensors;inverse problems;iterative methods;remote sensing","resultant fusion inverse problem;global low-rank;HRI;joint-sparse regularizer;HSI;hyperspectral-multispectral image fusion;rank estimation;combining information;low-spatial resolution hyperspectral image;low-spectral resolution multispectral image;spatial-spectral resolution image;remote sensing","","1","","18","IEEE","11 Nov 2021","","","IEEE","IEEE Conferences"
"Performance Analysis of Wavelet Functions in Fusion of MRI and CT Images","R. V. Ravi; M. V. Sujith; K. M. Shafeen; T. A. Asharaf U; C. T. Sajidh; M. T. S. Mohan","Department of Electronics and Communication Engineering, MEA Engineering College, Malappuram, Kerala, India; Department of Electronics and Communication Engineering, MEA Engineering College, Malappuram, Kerala, India; Department of Electronics and Communication Engineering, MEA Engineering College, Malappuram, Kerala, India; Department of Electronics and Communication Engineering, MEA Engineering College, Malappuram, Kerala, India; Department of Electronics and Communication Engineering, MEA Engineering College, Malappuram, Kerala, India; Department of Electronics and Communication Engineering, MEA Engineering College, Malappuram, Kerala, India","2020 7th International Conference on Smart Structures and Systems (ICSSS)","22 Sep 2020","2020","","","1","6","The fusion of images is the mechanism by which two or more images are merged into one image with important features. Fusion is an important technology in many different areas, including remote sensing, robotics and medical applications. The image fusion results in a composite image that is ideally suited for human and machine perception or external image processing tasks. In medical imaging technology, the Magnetic Resonance Image (MRI) highlights the soft tissue of the body and Computed Tomography (CT) provides a better view on hard tissue highlighting bones so their fusion will lead to better information content. Highlight of this particular image fusion is, one of the most useful diagnoses of tumor it provides the identification of gross tumor volume and clinical target volume by 80% more comparing to the MRI and CT images can provide by itself. In this paper, we compared the efficiency of different fusion techniques. The wavelet based image fusion techniques comprises of two steps among which the first step is Discrete Wavelet Transform (DWT) based decomposition of two input images into four coefficients each such as approximation, vertical, horizontal and diagonal and fusion of each respective coefficients is performed based on some particular fusion rules. the fusion rules can be used for this particular application are Maximum, Minimum and Mean. Various parameters like Entropy, Mutual Information and Standard Deviations were used to evaluate the fused image.","","978-1-7281-7223-1","10.1109/ICSSS49621.2020.9202310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202310","Image Fusion;wavelet based image fusion;Fusion Rules;Fusion Techniques;Fusion Performance","Biomedical imaging;Magnetic resonance imaging;Computed tomography;Discrete wavelet transforms;Image fusion;Wavelet analysis","biomedical MRI;bone;computerised tomography;discrete wavelet transforms;image fusion;medical image processing;tumours","MRI images;CT images;medical imaging technology;magnetic resonance image;tumor volume;wavelet based image fusion;discrete wavelet transform;soft tissue;computed tomography;bones","","1","","20","IEEE","22 Sep 2020","","","IEEE","IEEE Conferences"
"Hybrid Transformer Networks for Urban Land Use Classification from Optical and SAR Images","R. Liu; H. Zhang; J. Ling","The University of Hong Kong Shenzhen Institute of Research and Innovation, Shenzhen, China; The University of Hong Kong Shenzhen Institute of Research and Innovation, Shenzhen, China; The University of Hong Kong Shenzhen Institute of Research and Innovation, Shenzhen, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","707","710","Mapping the land cover/use type of urban area surface plays a vital role in many remote sensing applications. The performance of classification is inevitably limited by the finite amount of information available from a single data source, the restricted atmosphere condition and the complex landscape of the urban areas. Even when multiple sources of data are used, the fusion strategy is relatively homogeneous. In this paper, we aim to explore the potential of transformer based fusion method in mapping the urban regions with optical and synthetic aperture radar images. Specifically, we propose a hybrid fusion transformer network that simultaneously implements multi-source data fusion at both the feature and the decision levels. The experiments are conducted on the high resolution multiple remote sensing images, and the results show that the hybrid fusion based on transformer can achieve 82.17% in overall accuracy (OA) and 76.91 % in kappa coefficient. Moreover, compared with convolution neural network based methods, the transformer based methods are on average 2% higher in OA and 3.6% higher in kappa coefficient.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883122","urban land cover/use classification;trans-former;image fusion","Soft sensors;Urban areas;Optical computing;Transformers;Optical imaging;Adaptive optics;Radar polarimetry","geophysical image processing;image classification;image fusion;image resolution;neural nets;radar imaging;remote sensing;sensor fusion;synthetic aperture radar","hybrid transformer networks;urban land use classification;urban area surface;remote sensing applications;single data source;restricted atmosphere condition;complex landscape;urban areas;fusion strategy;transformer based fusion method;urban regions;optical aperture radar images;synthetic aperture radar images;hybrid fusion transformer network;multisource data fusion;high resolution multiple remote sensing images;convolution neural network;transformer based methods","","","","8","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Remote Sensing Data Fusion With Generative Adversarial Networks: State-of-the-art methods and future research directions","P. Liu; J. Li; L. Wang; G. He","Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Magazine","13 Jul 2022","2022","10","2","295","328","In the past decades, remote sensing (RS) data fusion has always been an active research community. A large number of algorithms and models have been developed. Generative adversarial networks (GANs), as an important branch of deep learning, show promising performances in a variety of RS image fusions. This review provides an introduction to GANs for RS data fusion. We briefly review the frequently used architecture and characteristics of GANs in data fusion and comprehensively discuss how to use GANs to realize fusion for homogeneous RS, heterogeneous RS, and RS and ground observation (GO) data. We also analyze some typical applications with GAN-based RS image fusion. This review provides insight into how to make GANs adapt to different types of fusion tasks and summarizes the advantages and disadvantages of GAN-based RS data fusion. Finally, we discuss promising future research directions and make a prediction on their trends.","2168-6831","","10.1109/MGRS.2022.3165967","National Natural Science Foundation of China(grant numbers:61731022,41971397); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779854","","Data integration;Generators;Generative adversarial networks;Computer architecture;Degradation;Data models;Image fusion","data fusion;deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;remote sensing;reviews","GO data;deep learning;state-of-the-art methods;active research community;remote sensing;generative adversarial networks;GAN-based RS data fusion;GAN-based RS image fusion;ground observation data;heterogeneous RS;homogeneous RS","","4","","238","IEEE","23 May 2022","","","IEEE","IEEE Magazines"
"Single Hyperspectral Image Super-Resolution Using Admm-Adam Theory","T. -H. Lin; C. -H. Lin","Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan; School of Computing, National Cheng Kung University, Tainan, Taiwan","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1756","1759","In the remote sensing field, the spatial resolution of hyperspectral images (HSIs) is poor compared to RGB and multispectral images. Hence, hyperspectral image super-resolution (HISR) has become a popular topic recently. A branch of HISR methods is based on image fusion, but these methods rely on high-spatial-resolution counterpart image (e.g., multispectral image of the same scene) that is, however, not always available. Therefore, developing single hyperspectral image super-resolution (SHISR) method is highly desired. Due to the lack of abundant high-quality HSIs (i.e., big data) in satellite remote sensing, deep learning itself would be insufficient to well solve SHISR. We solve SHISR based on the recently invented ADMM-Adam learning theory, which blends the advantages from deep learning and convex optimization, thereby allowing software engineers to solve various challenging inverse problems without big data and sophisticated regularizer. For the first time, ADMM-Adam is adopted to solve SHISR in this paper, and experimental evidences well support its superiority even just with small data.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883334","Ministry of Science and Technology(grant numbers:MOST 110-2636-E-006-026); MOE; National Cheng Kung University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883334","Hyperspectral image;single image superresolution;convex optimization;deep learning;alternating direction method of multipliers (ADMM);adaptive moment estimation (ADAM)","Deep learning;Satellites;Inverse problems;Superresolution;Big Data;Feature extraction;Convex functions","geophysical image processing;hyperspectral imaging;image fusion;image resolution;inverse problems;learning (artificial intelligence);remote sensing","high-quality HSIs;big data;satellite remote sensing;SHISR;recently invented ADMM-Adam learning theory;deep learning;convex optimization;admm-Adam theory;remote sensing field;spatial resolution;hyperspectral images;multispectral image;HISR methods;image fusion;high-spatial-resolution counterpart image;single hyperspectral image super-resolution method","","","","20","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"An Airborne Remote Sensing Image Mosaic Algorithm Based on Feature Points","Y. Zhen; Z. Sun; J. Li; Y. Peng","Harbin Institute of Technology, Harbin, China; Harbin Institute of Technology, Harbin, China; Harbin Institute of Technology, Harbin, China; Harbin Institute of Technology, Harbin, China","2016 Sixth International Conference on Instrumentation & Measurement, Computer, Communication and Control (IMCCC)","8 Dec 2016","2016","","","202","205","This paper focuses on a image mosaic method based on feature points. Image registration and stitching are the key technology of image mosaic. Based on analysis of research, this paper selected a mosaic algorithm based on SIFT feature points which have a good scale and rotation invariant feature for further research. For the wrong matches because of the algorithm restriction, occlusion, sensor moves, or the symmetric structure of the scene itself, this paper introduces the RANSAC algorithm for precise matching. During the process of image stitching, this paper reaches the goal of image mosaic by using the Homography Matrix solved by RANSAC.","","978-1-5090-1195-7","10.1109/IMCCC.2016.145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774766","image mosaic;image match;SIFT;Scale-space;image fusion;feature points","Feature extraction;Remote sensing;Transforms;Algorithm design and analysis;Error probability;Image registration;Lighting","feature extraction;geophysical image processing;image matching;image registration;image segmentation;iterative methods;remote sensing;transforms","homography matrix;image matching;RANSAC algorithm;rotation invariant feature;scale invariant feature;SIFT feature points;image stitching;image registration;airborne remote sensing image mosaic algorithm","","5","","7","IEEE","8 Dec 2016","","","IEEE","IEEE Conferences"
"Joint Feature Extraction for Multispectral and Panchromatic Images Based on Convolutional Neural Network","Y. Chen; M. Zhang; W. Li; Q. Du","College of Information Science & Technology, Beijing University of Chemical Technology; College of Information Science & Technology, Beijing University of Chemical Technology; College of Information Science & Technology, Beijing University of Chemical Technology; Department of Electrical and Computer Engineering, Mississippi State University","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5005","5008","Along with very high-resolution satellites were launched frequently, such as the satellite WorldView-3, panchromatic and multispectral remote-sensing images can be acquired easily. However, it is still an interesting and challenging task to fuse and classify these images. In general, panchromatic image has a high spatial resolution, but with only one spectral band. Multispectral image usually has four or eight bands, but the spatial resolution is four times smaller than panchromatic image. In this paper, an unsupervised feature extraction framework is proposed, which combines multispectral (MS) image and panchromatic (PAN) image into convolution neural network (CNN). There is an image-to-image mapping, learning from the input source (i.e., MS) to the output source (i.e., PAN). Then, by integrating the hidden layer of deep CNN, the extracted features represent MS and PAN data. The experimental results of two practical remote sensing data sets show the validity of the framework.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518885","Multispectral Image;Panchromatic Image;Convolutional Neural Network;Joint Feature Extraction","Feature extraction;Training;Support vector machines;Remote sensing;Spatial resolution;Satellites;Data mining","feature extraction;geophysical image processing;geophysical signal processing;image classification;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing;terrain mapping","practical remote sensing data sets;joint feature extraction;multispectral images;panchromatic images;convolutional neural network;high-resolution satellites;satellite WorldView-3;remote-sensing images;interesting task;general image;panchromatic image;high spatial resolution;spectral band;multispectral image;unsupervised feature extraction framework;PAN;convolution neural network;image-to-image mapping","","2","","13","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Multisource Data Fusion for the Detection of Settlements Without Electricity","Y. Ma; Y. Li; K. Feng; X. Geng; L. Jiao; F. Liu; Y. Yang","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","1839","1842","The international charity SolarAid aims to provide access to lights in areas without electricity, and it is a challenge to accurately and efficiently transmit the lights to the areas in need. Multisource, multitemporal, and multimodal remote sensing images can provide rich information about the target area, so using multisource remote sensing images for accurate detection of human settlements without electricity is a feasible solution. In this paper two separate detection tasks are formulated: building two attention SENet for settlements detection and light detection using the Sentinel-2 dataset and the Suomi Visible Infrared Imaging Radiometer Suite (VIIRS) night time dataset, respectively. In addition, we study a new outlier removal method based on the pixel distribution characteristics of the VIIRS dataset for data pre-processing, and propose a post-processing method based on region continuity for further correction of the results. Experiments show that our method can maximize the use of multisource data information and rank first in the detection of settlements without electricity challenge track (Track DSE) of the 2021 IEEE GRSS Data Fusion Contest.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553860","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553860","multisource data fusion;attention mechanism;outlier removal;Sentinel-2;VIIRS","Buildings;Data integration;Infrared imaging;Radiometry;Task analysis;Remote sensing","geophysical image processing;image classification;image fusion;infrared imaging;remote sensing;terrain mapping","IEEE GRSS Data Fusion Contest;electricity challenge track;multisource data information;post-processing method;data pre-processing;VIIRS dataset;outlier removal method;Suomi Visible Infrared Imaging Radiometer Suite night time dataset;Sentinel-2 dataset;light detection;settlements detection;detection tasks;human settlements;multisource remote sensing images;multimodal remote sensing images;international charity SolarAid;multisource data fusion","","2","","6","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Lightweight Fine-Grained Recognition Method Based on Multilevel Feature Weighted Fusion","Y. Pan; L. Tang; B. Zhao","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4767","4770","Fine-grained recognition in remote sensing images has played a critical role in military and civil fields. Recently, with the rapid growth of convolutional neural networks (CNNs), many fine-grained recognition methods have been proposed. However, due to the large amount of parameters and computational complexity, it is difficult to apply these methods in practical applications. To this end, we propose a novel lightweight fine-grained recognition method based on multilevel feature weighted fusion. First, we design a lightweight CNN (LCNN) framework. Second, we propose a multilevel feature weighted fusion method to improve the recognition accuracy. Third, we adopt a feature channel based loss function to train the proposed model end-to-end. Experiments are conducted on the challenging remote sensing dataset MTARSI to evaluate our proposed method. The results show that the proposed method can achieve state-of-the-art performance.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553338","Fine-grained recognition;LCNN;multilevel feature weighted fusion","Image recognition;Military computing;Convolutional neural networks;Computational complexity;Remote sensing","convolutional neural nets;image fusion;image recognition;learning (artificial intelligence);remote sensing","lightweight fine-grained recognition method;lightweight CNN framework;multilevel feature weighted fusion method;recognition accuracy;feature channel based loss function;convolutional neural networks;LCNN framework;computational complexity;remote sensing images;MTARSI remote sensing dataset","","2","","12","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Nonlinear separation source and parameterized feature fusion for satelite image patch exemplars","H. Elmannai; M. A. Loghmari; M. S. Naceur","Laboratoire de Télédétection et Système d'Informations à Référence Spatiale Ecole Nationale d'Ingénieurs de Tunis, Université de Tunis El Manar (UTM), Tunisia; Laboratoire de Télédétection et Système d'Informations à Référence Spatiale Ecole Nationale d'Ingénieurs de Tunis, Université de Tunis El Manar (UTM), Tunisia; Laboratoire de Télédétection et Système d'Informations à Référence Spatiale Ecole Nationale d'Ingénieurs de Tunis, Université de Tunis El Manar (UTM), Tunisia","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","405","408","We present a new approach for remote sensing image classification. The methodology combines many related tasks namely non linear source separation, feature extraction, feature fusion and learning classification. Nonlinear source separation is a pre-processing stage that aims to compensate the nonlinear mixing natural phenomenon. Latent signals, called sources are transformed to the feature presentation in the feature extraction stage. Feature information presentation is preliminary in machine learning or machine vision projects and provides an efficient and reliable data presentation than original data. Fusing feature aims to enrich the information characteristics about the land cover namely textural information, contours and multi-resolution information. Parameterized fusion model aim to determine the best feature weights in terms of data classification. Finally, a machine learning classification method is used for remote sensing data base. Experimental results show that the proposed fusion method enhances the classification accuracy and provide powerful tool for image exemplars classification.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325786","Source separation;Support Vector Machine;feature extraction;feature fusion;classification","Feature extraction;Source separation;Classification algorithms;Support vector machines;Accuracy;Remote sensing;Reliability","feature extraction;geophysical image processing;image classification;image fusion;land cover;learning (artificial intelligence);remote sensing","remote sensing data base;machine learning classification method;parameterized fusion model;multiresolution information;textural information;land cover;latent signal;nonlinear mixing natural phenomenon;feature extraction;remote sensing image classification;satelite image patch exemplars;parameterized feature fusion;nonlinear source separation","","1","","12","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Improved Flood Mapping Based on the Fusion of Multiple Satellite Data Sources and In-Situ Data","Y. -J. Kwak; R. Pelich; J. Park; W. Takeuchi","International Centre for Water Hazard and Risk Management (ICHARM-UNESCO), Public Works Research Institute, Tsukuba, Japan; Department of Environmental Research and Innovation (ERIN), Luxembourg Institute of Science and Technology (LIST), Belvaux, Luxembourg; Dept. of Environmental Information, Tokyo University of Information Sciences, Chiba, Japan; Institute of Industrial Science, The University of Tokyo, Tokyo, Japan","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","3521","3523","For high accuracy flood mapping, an algorithm that integrates multiple satellite data sources is essential to maximize the sensor ability and compensate the limitations of optical and SAR data. The main objective of this study is to propose an algorithm of dynamic flood detection using optical and Synthetic Aperture Radar (SAR) images that compares and combines two different statistical thresholding approaches. To improve the flood detection accuracy, image fusion technique was investigated to maximize the utilization of calibrated and optimized flood maps as the integrated flood detection approach. To showcase the advantages of the proposed methodology, we employ MODIS, Landsat-8 and Sentinel-IA images acquired over a challenging area along the Brahmaputra River where flood events often occur.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517336","Dynamic flood mapping;multiple satellite data;MODIS;Sentinel-I;image fusion","Floods;MODIS;Synthetic aperture radar;Rivers;Optical imaging;Optical sensors;Earth","floods;geophysical image processing;hydrological techniques;image fusion;radar imaging;remote sensing by radar;rivers;synthetic aperture radar","multiple satellite data sources;high accuracy flood mapping;optical SAR data;dynamic flood detection;flood detection accuracy;image fusion technique;calibrated flood maps;optimized flood maps;integrated flood detection approach;flood events;statistical thresholding approaches;synthetic aperture radar;SAR images;MODIS images;Landsat-8 images;Sentinel-IA images","","3","","7","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"A New Spatio-Temporal Fusion Method for Remotely Sensed Data Based on Convolutional Neural Networks","Y. Li; C. Liu; L. Yan; J. Li; A. Plaza; B. Li","Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Hyperspectral Computing Laboratory, Avenida de la Universidad s/n, Caceres, Spain; School of Computer Science and Engineering, Beihang University, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","835","838","In some remote sensing applications such as change detection, satellite images with both high spatial and high temporal resolution are required. However, no single satellite sensor can currently provide such images due to technical specifications. To solve this problem, spatio-temporal fusion provides a cost-effective solution. In this paper, we propose a new spatio-temporal fusion approach, based on convolutional neural networks (CNNs), for Landsat and MODIS image fusion. Specifically, the proposed approach utilizes CNNs to model the heterogeneity of fine pixels from the coarse MODIS images. Here, the heterogeneity of fine pixels is defined as the difference between the reflectance changes obtained from the two types of images. After that, two transition-predicted images can be obtained using the trained CNNs, which are then fused in order to obtain a fi-nal prediction. In our newly proposed approach, CNNs are only used to learn the heterogeneity of fine pixels rather than the whole images, thus providing a more stable and less time-consuming strategy as compared to other available approaches. We evaluated the proposed approach on a public spatio-temporal fusion dataset and the obtained results suggest that our newly developed method achieves state-of-the-art performance.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898524","Spatio-temporal fusion;convolutional neural networks (CNNs);heterogeneity","Remote sensing;Spatial resolution;Artificial satellites;Earth;MODIS;Predictive models","convolutional neural nets;geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);remote sensing;terrain mapping","trained CNNs;fine pixels;public spatio-temporal fusion dataset;new spatio-temporal fusion method;remotely sensed data;convolutional neural networks;remote sensing applications;change detection;satellite images;high temporal resolution;single satellite sensor;technical specifications;spatio-temporal fusion approach;coarse MODIS images;reflectance changes;transition-predicted images","","1","","12","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Sentinel-1 and Sentinel-2 Data Fusion for Urban Change Detection","A. Benedetti; M. Picchiani; F. Del Frate","University of Tor Vergata, Rome, Italy; University of Tor Vergata, Rome, Italy; University of Tor Vergata, Rome, Italy","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1962","1965","In this paper a new approach based on the fusion of Sentinel-1 and Sentinel-2 products to map urban change detection and to observe suburb's development is presented. The algorithm developed can process data in a fast, automatic and accurate way. To reach this goal, the processing chain uses an iterative multitemporal approach based, for each iteration, on three procedures. The first and second ones are based on Pulse Coupled Neural Network (PCNN) applied to SAR and optical images, respectively, while the third processing is an optical multiband filter, implementing the spectral difference computation. The three outputs of each iteration are fused together by means of a weighted average formulation. The algorithm may deal with multitemporal acquisitions to improve the overall accuracy in the detection of urban changes by the integration of the outputs at different time intervals.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517586","Sentinel-1;Sentinel-2;image fusion;change detection;global monitoring urbanization","Optical filters;Optical imaging;Synthetic aperture radar;Change detection algorithms;Remote sensing;Data integration;Adaptive optics","geophysical image processing;geophysical techniques;image fusion;neural nets;remote sensing by radar;synthetic aperture radar;terrain mapping","Sentinel-1;Sentinel-2 data fusion;Sentinel-2 products;map urban change detection;suburb;processing chain;iterative multitemporal approach;Pulse Coupled Neural Network;optical images;optical multiband filter;urban changes","","13","","13","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Based on Multi-Feature Information Attention Fusion for Multi-Modal Remote Sensing Image Semantic Segmentation","C. Zhang","Department of School of Information Science and Technology, University of Jinan, Jinan, Shandong, China","2021 IEEE International Conference on Mechatronics and Automation (ICMA)","27 Aug 2021","2021","","","71","76","Semantic segmentation of remote sensing images are widely used in land census and agriculture. The scenes in remote sensing images are complex, easily affected by season, such as farmland. Besides, the size of the target in remote sensing image is different, the shape is irregular, and there is often the problem of missing detection, so the multi-source data information is directly fed into the neural network, resulting in fuzzy segmentation boundary, which is difficult to achieve fine segmentation. To solve this problem, we propose a Dual-way Feature attention Fusion Network (DFFNet), which consists of two branches, optical remote sensing image branch and elevation feature branch. The optical remote sensing image branch uses the spatial relationship module to learn and infer the global relationship between any two spatial positions or feature maps and then extracts the multi-level features of the image by capturing more context information and pyramid attention mechanism. The elevation feature branch strengthens the classification. Based on the boundary information, the remote sensing image segmentation is realized. Experiment results on ISPRS Vaihingen image dataset demonstrate the effectiveness of the proposed method.","2152-744X","978-1-6654-4101-8","10.1109/ICMA52036.2021.9512594","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9512594","Semantic segmentation;spatial attention;channel attention;feature fusion","Integrated optics;Image segmentation;Image coding;Semantics;Optical computing;Feature extraction;Optical imaging","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);remote sensing","multisource data information;fuzzy segmentation boundary;Dual-way Feature attention Fusion Network;optical remote sensing image branch;multilevel features;elevation feature branch;remote sensing image segmentation;ISPRS Vaihingen image dataset;multiFeature information attention Fusion;multimodal remote sensing image semantic segmentation","","2","","23","IEEE","27 Aug 2021","","","IEEE","IEEE Conferences"
"Fusion of Panchromatic and Hyperspectral Images in the Reflective Domain by a Combinatorial Approach and Application to Urban Landscape","Y. Constans; S. Fabre; H. Carfantan; M. Seymour; V. Crombez; X. Briottet; Y. Deville","UPS-CNRS-OMP-CNES, IRAP, Université de Toulouse, Toulouse, France; ONERA, DOTA, Toulouse, France; UPS-CNRS-OMP-CNES, IRAP, Université de Toulouse, Toulouse, France; Airbus Defence and Space, Toulouse, France; Airbus Defence and Space, Toulouse, France; ONERA, DOTA, Toulouse, France; UPS-CNRS-OMP-CNES, IRAP, Université de Toulouse, Toulouse, France","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2648","2651","Hyperspectral pansharpening methods, which aim to combine hyperspectral and panchromatic images, yield limited performance for scenes whose strong spatial heterogeneity induces mixed pixels. The SOSU method has been designed to handle this limitation and provided good results on agricultural and peri-urban landscapes. However, its performance was reduced on more complex urban scenes, which contain a higher proportion of mixed pixels. This article presents a new version of this method, called SOSU-2021, adapted to better process urban scenes. SOSU-2021 is tested on an urban dataset at a 1.6 m spatial resolution. We obtain better numerical results than with the previous SOSU version, and in the worst case, 56 % of the mixed pixels are better or equally processed by SOSU-2021 than by the method used as a reference.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554444","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554444","Image fusion;panchromatic;hyperspectral;reflective domain;urban;combinatorial analysis","Transfer functions;Modulation;Geoscience and remote sensing;Pansharpening;Mixed integer linear programming;Spatial resolution;Optimization","filtering theory;geophysical image processing;geophysical signal processing;image fusion;image reconstruction;image representation;image resolution;remote sensing","hyperspectral images;reflective domain;combinatorial approach;urban landscape;hyperspectral pansharpening methods;panchromatic images;scenes whose strong spatial heterogeneity;mixed pixels;SOSU method;peri-urban landscapes;complex urban scenes;higher proportion;called SOSU-2021;process urban scenes;urban dataset;m spatial resolution;previous SOSU version;size 1.6 m","","","","11","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Multispectral Pansharpening Based on High-Pass Modulation Regression","H. -Y. Yao; P. Wang; X. Shen; L. Shi; C. Zhao","Key Laboratory of Meteorology and Ecological Environment of Hebei Province, Meteorological Institute of Hebei Province, Shijiazhuang, China; Shanghai Key Lab of Intelligent Information Processing, Fudan University, Shanghai, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Key Laboratory of Meteorology and Ecological Environment of Hebei Province, Meteorological Institute of Hebei Province, Shijiazhuang, China; Key Laboratory of Meteorology and Ecological Environment of Hebei Province, Meteorological Institute of Hebei Province, Shijiazhuang, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1075","1078","In this paper, a multispectral pansharpening based on high-pass modulation regression (HPMR) is proposed. Firstly, full-scale estimation is applied to improving the quality of the injection coefficient estimation. Then the injection coefficient is performed in the HPM which accomplishes the fusion by building a detailed relationship between the panchromatic (PAN) image and the multispectral (MS) image. Experiments on two data sets assessed both at reduced resolution and at full resolution show that the proposed method can acquire better performance than the state-of-the-art pansharpening methods.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883745","Fundamental Research Funds for the Central Universities; Nanjing University of Aeronautics and Astronautics(grant numbers:NZ2020009,xcxjh20210405,2021YJXGG11); National Natural Science Foundation of China(grant numbers:61801211); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883745","Remote image fusion;pansharpening;high-pass modulation;full-scale regression","Q measurement;Image resolution;Buildings;Modulation;Estimation;Geoscience and remote sensing;Pansharpening","geophysical image processing;image fusion;image resolution;image sampling;remote sensing","injection coefficient estimation;panchromatic image;multispectral image;state-of-the-art pansharpening methods;multispectral pansharpening;high-pass modulation regression;full-scale estimation","","","","18","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Sentinel-3 Image Super-Resolution Using Data Fusion and Convolutional Neural Networks","R. Fernandez; R. Fernandez-Beltran; F. Pla","Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2867","2870","With the increasing availability of Sentinel-2 (S2) and Sentinel-3 (S3) data, developing higher-level data products becomes a very attractive option to relieve the spatial limitations of the Ocean and Land Colour Instrument (OLCI) of S3. In this context, this paper investigates the suitability of super-resolving operational OLCI products using the Multi-Spectral Instrument (MSI) of S2 as an offline spatial reference. Specifically, the proposed approach assembles a multi -spectral data fusion scheme together with a convolutinal neural network (CNN) mapping function to project the OLCI sensor onto its corresponding spatial reference which is synthetically generated by the OLCI/MSI fusion. In this way, the trained model is able to super-resolve operational OLCI products under demand without the need of using MSI data. The experimental part of the work shows the suitability of the proposed approach in the context of the Copernicus programme.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554826","Sentinel-2 (S2);Sentinel-3 (S3);image fusion;super-resolution (SR)","Image color analysis;Instruments;Oceans;Superresolution;Neural networks;Data integration;Geoscience and remote sensing","geophysical image processing;image fusion;image resolution;image sensors;neural nets;remote sensing;sensor fusion","corresponding spatial reference;super-resolve operational OLCI products;MSI data;Sentinel-3 image super-resolution;convolutional neural networks;Sentinel-2;Sentinel-3 data;higher-level data products;spatial limitations;MultiSpectral Instrument;offline spatial reference;multi-spectral data fusion scheme;convolutinal neural network mapping function;OLCI sensor","","","","16","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Evaluation of Informativeness and Effectiveness of Known Image Fusion Methods","A. Hryvachevskyi; S. Fabirovskyy; I. Prudyus; R. Holyaka","SQUAD, Kyiv, Ukraine; Department of Radioelectronic Devices and Systems, Lviv Polytechnic National University, Lviv, Ukraine; Department of Radioelectronic Devices and Systems, Lviv Polytechnic National University, Lviv, Ukraine; Department of Electronics and Information Technology, Lviv Polytechnic National University, Lviv, Ukraine","2022 IEEE 16th International Conference on Advanced Trends in Radioelectronics, Telecommunications and Computer Engineering (TCSET)","9 May 2022","2022","","","539","543","The analysis of fusion methods at the pixel level is carried out in the paper. Their mathematical models in the MATLAB package are implemented. The most effective methods of visible and infrared ranges image fusion are determined by evaluating the effectiveness of methods using the authors' proposed improved method of evaluating the informativeness of images and known metrics. These studies were conducted on a set of test images consisting of ten pairs of spatially synchronized images obtained in the visible and thermal ranges of electromagnetic waves.","","978-1-6654-6861-9","10.1109/TCSET55632.2022.9767053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9767053","fusion;informativeness;thermal image;remote sensing","Measurement;Transforms;Thermal conductivity;Mathematical models;Discrete wavelet transforms;Telecommunications;Synchronization","image fusion;Matlab","pixel level;mathematical models;MATLAB package;visible ranges image fusion;infrared ranges image fusion;test images;spatially synchronized images;thermal ranges;electromagnetic waves","","","","30","IEEE","9 May 2022","","","IEEE","IEEE Conferences"
"An Overview of Multimodal Remote Sensing Data Fusion: From Image to Feature, From Shallow to Deep","D. Hong; J. Chanussot; X. X. Zhu","Remote Sensing Technology Institute, German Aerospace Center, Wessling, Germany; Univ. Grenoble Alpes, INRIA, CNRS, Grenoble INP, LJK, Grenoble, France; Data Science in Earth Observation, Technical University of Munich, Munich, Germany","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","1245","1248","With the ever-growing availability of different remote sensing (RS) products from both satellite and airborne platforms, simultaneous processing and interpretation of multimodal RS data have shown increasing significance in the RS field. Different resolutions, contexts, and sensors of multimodal RS data enable the identification and recognition of the materials lying on the earth's surface at a more accurate level by describing the same object from different points of the view. As a result, the topic on multimodal RS data fusion has gradually emerged as a hotspot research direction in recent years. This paper aims at presenting an overview of multimodal RS data fusion in several mainstream applications, which can be roughly categorized by 1) image pansharpening, 2) hyperspectral and multispectral image fusion, 3) multimodal feature learning, and (4) crossmodal feature learning. For each topic, we will briefly describe what is the to-be-addressed research problem related to multimodal RS data fusion and give the representative and state-of-the-art models from shallow to deep perspectives.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554255","German Research Foundation (DFG)(grant numbers:ZH 498/7-2); AXA Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554255","Classification;crossmodal;data fusion;deep learning;feature learning;multimodal;pansharpening;remote sensing;shallow models","Earth;Satellites;Data integration;Pansharpening;Sensor phenomena and characterization;Data models;Object recognition","","","","2","","64","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Multi-scale-and-depth convolutional neural network for remote sensed imagery pan-sharpening","Y. Wei; Q. Yuan; X. Meng; H. Shen; L. Zhang; M. Ng","State Key Laboratory of Information Engineering, Survey Mapping and Remote Sensing, Wuhan University, P. R. China; School of Geodesy and Geomatics, Wuhan University, P. R. China; School of Resource and Environmental Science, Wuhan University, P. R. China; School of Resource and Environmental Science, Wuhan University, P. R. China; State Key Laboratory of Information Engineering, Survey Mapping and Remote Sensing, Wuhan University, P. R. China; Department of Mathematics, Hong Kong Baptist University","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","3413","3416","Pan-sharpening is a fundamental and significant task in the field of remote sensed imagery fusion, which demands fusion of panchromatic and multi-spectral images with the rich information accurately preserved in both spatial and spectral domains. In this paper, to overcome the drawbacks of traditional pan-sharpening methodologies, we employed the advanced concept of deep learning to propose a Multi-Scale-and-Depth Convolutional Neural Network (MSDCNN) as an end-to-end pan-sharpening model. By the results of a large number of quantitative and visual assessments, the qualities of images fused by the proposed network have been confirmed superior to compared state-of-the-art methods.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127731","Remote Sensing;Pan-sharpening;Deep learning;Convolutional neural network;Residual Learning","","image fusion;learning (artificial intelligence);neural nets;remote sensing","multiscale-and-depth convolutional neural network;remote sensed imagery pan-sharpening;remote sensed imagery fusion;panchromatic image;multispectral image;deep learning;MSDCNN model;end-to-end pan-sharpening model;image quality","","1","","17","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Machine Learning Methods for Road Edge Detection on Fused Airborne Hyperspectral and LIDAR Data","R. Senchuri; A. Kuras; I. Burud","Faculty of Science and Technology, Norwegian University of Life Sciences, Aas, Norway; Faculty of Science and Technology, Norwegian University of Life Sciences, Aas, Norway; Faculty of Science and Technology, Norwegian University of Life Sciences, Aas, Norway","2021 11th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Jul 2021","2021","","","1","5","In the last decades, remote sensing sensors, such as hyperspectral systems or LiDAR scanners, have been used for urban mapping. However, an analysis in the urban environment is very complex in applications, e.g., road detection, city management, and urban planning. One of the important urban features is the detection of the road edges. In this study, an approach on multisensory hyperspectral and LiDAR data fusion (HL-Fusion) is introduced for road edge detection using different machine learning algorithms, such as Support Vector Machines, Random Forests, and Convolutional Neural Networks. The first results show that the Random Forest algorithm outperformed in the experiments on the study area at Oslo's surroundings in Norway. This study opens a window for further investigation on machine learning algorithms and a better understanding of HL-Fusion capabilities.","2158-6276","978-1-6654-3601-4","10.1109/WHISPERS52202.2021.9484007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9484007","Hyperspectral;road edge detection;LiDAR;machine learning;data fusion;remote sensing","Support vector machines;Machine learning algorithms;Laser radar;Roads;Image edge detection;Urban planning;Signal processing algorithms","edge detection;geophysical image processing;image classification;image fusion;learning (artificial intelligence);neural nets;optical radar;remote sensing;remote sensing by laser beam;sensor fusion;support vector machines","road edge detection;fused airborne hyperspectral;LIDAR data;remote sensing sensors;hyperspectral systems;urban mapping;urban environment;road detection;urban planning;important urban features;road edges;multisensory hyperspectral;LiDAR data;Support Vector Machines;Random Forest algorithm;HL-Fusion capabilities","","4","","22","IEEE","19 Jul 2021","","","IEEE","IEEE Conferences"
"A short survey of hyperspectral remote sensing and hyperspectral remote sensing research at tübıtak Uzay","U. Sakarya; M. Teke; C. Demirkesen; O. Haliloğlu; A. Ö. Kozal; H. S. Deveci; A. F. Öztoprak; B. U. Töreyin; S. Z. Gürbüz","TÜBİTAK UZAY (The Scientific and Technological Research Council of Turkey, Space Technologies Research Institute), Ankara, Turkey; TÜBİTAK UZAY (The Scientific and Technological Research Council of Turkey, Space Technologies Research Institute), Ankara, Turkey; TÜBİTAK UZAY (The Scientific and Technological Research Council of Turkey, Space Technologies Research Institute), Ankara, Turkey; TÜBİTAK UZAY (The Scientific and Technological Research Council of Turkey, Space Technologies Research Institute), Ankara, Turkey; TÜBİTAK UZAY (The Scientific and Technological Research Council of Turkey, Space Technologies Research Institute), Ankara, Turkey; TÜBİTAK UZAY (The Scientific and Technological Research Council of Turkey, Space Technologies Research Institute), Ankara, Turkey; TÜBİTAK UZAY (The Scientific and Technological Research Council of Turkey, Space Technologies Research Institute), Ankara, Turkey; TUBITAK Uzay Teknolojileri Arastirma Enstitusu, Ankara, Ankara, TR; TÜBİTAK UZAY (The Scientific and Technological Research Council of Turkey, Space Technologies Research Institute), Ankara, Turkey","2015 7th International Conference on Recent Advances in Space Technologies (RAST)","20 Aug 2015","2015","","","187","192","Hyperspectral remote sensing (HSRS) is becoming more and more attractive. Recent advances in sensor technologies enabled numerous applications of this imaging modality. HSRS research has been conducted at TÜBİTAK UZAY since 2012. This paper provides a short survey of these research and development activities ranging from hyperspectral remote sensing applications, radiometric correction, geometric correction and denoising to classification and fusion with other modalities of HS data using most recent algorithms.","","978-1-4799-7697-3","10.1109/RAST.2015.7208339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208339","survey;hyperspectral remote sensing","Hyperspectral imaging;Noise;Satellites;Agriculture;Feature extraction","geophysical image processing;hyperspectral imaging;image classification;image denoising;image fusion;remote sensing","TUBITAK UZAY;Turkey;sensor technology;hyperspectral remote sensing activity;hyperspectral remote sensing application;radiometric correction;geometric correction;hyperspectral data denoising;hyperspectral data classification;hyperspectral data fusion;hyperspectral data modality","","3","","61","IEEE","20 Aug 2015","","","IEEE","IEEE Conferences"
"High spatiotemporal resolution PM2.5 concentration estimation with satellite and ground observations: A case study in New York City","Y. Zhao; B. Huang; A. Marinoni; P. Gamba","Dept. GRM; Dept. GRM; Universita degli Studi di Pavia, Pavia, Lombardia, IT; Dept. GRM","2018 IEEE International Conference on Environmental Engineering (EE)","14 Jun 2018","2018","","","1","5","High spatiotemporal resolution concentration of fine particulate matter (PM2.5) enables accurate and detailed air quality monitoring, especially for metropolitan cities with high levels of population density. Although ground air quality monitoring stations can provide timely and accurate observations, they are usually very sparsely distributed, and cannot provide PM2.5 concentration data with continuous spatial coverage. Instead, satellite observations, e.g., Landsat 8/Thermal Infrared Sensor (TIRS) and Terra/Moderate Resolution Imaging Spectroradiometer (MODIS), can both obtain data with continuous coverage. However, there is a trade-off between satellite sensors' spatial and temporal resolution. Hence, this study presents an estimation model for PM2.5 concentrations that combines these multi-source data to produce high spatiotemporal resolution concentration maps in urban area. The approach is tested on New York City, NY, USA. Specifically, we first use cloud-free MODIS thermal band images and the corresponding ground-station PM2.5 records to build a local PM2.5 prediction model. Then, we exploit a spatiotemporal image fusion technique to obtain Landsat-like thermal band image series from Landsat 8/TIRS (100 m spatial resolution) and Terra/MODIS (1 km spatial resolution) sensors. Finally, we convert the fused high spatiotemporal resolution thermal band images to PM2.5 concentration maps by the prediction model from step 1. The validation between the estimated and the real PM2.5 values shows that the detailed Landsat-like high spatial resolution PM2.5 estimations are more accurate than the original blurred MODIS one.","","978-1-5386-4182-8","10.1109/EE1.2018.8385255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8385255","air quality;Landsat;MODIS;PM2.5 concentration;spatiotemporal image fusion;thermal band","Remote sensing;Earth;Artificial satellites;MODIS;Spatial resolution;Estimation;Spatiotemporal phenomena","air quality;geophysical image processing;image fusion;image resolution;land surface temperature;radiometry;remote sensing;spatiotemporal phenomena","ground observations;New York City;fine particulate matter;metropolitan cities;population density;ground air quality monitoring stations;continuous spatial coverage;Landsat 8/Thermal Infrared Sensor;Terra/Moderate Resolution Imaging Spectroradiometer;trade-off between satellite sensors;estimation model;multisource data;high spatiotemporal resolution concentration maps;cloud-free MODIS thermal band images;spatiotemporal image fusion technique;Landsat-like thermal band image series;Landsat 8/TIRS;Terra/MODIS sensors;fused high spatiotemporal resolution thermal band images;high spatiotemporal resolution PM2.5 concentration estimation;corresponding ground-station PM2.5 prediction model;Landsat-like high spatial resolution;USA","","2","","11","IEEE","14 Jun 2018","","","IEEE","IEEE Conferences"
"Correlated probabilities based decision fusion method for multi-sensor data","A. Mazher; P. Li","Institute of Remote Sensing and GIS, Peking University, Beijing, China; Institute of Remote Sensing and GIS, Peking University, Beijing, China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","4032","4035","Most of the decision fusion techniques developed for the remote sensing applications have the drawback of assuming the conditional independence between the classification results, whereas, usually the correlation exists due to the same measuring instrument or same area under study. Fusion of Correlated Probabilities (FCP) method has a potential to deal with conditional dependence only for two data sets. The proposed Multi-sensor Fusion of Correlated Probabilities (MFCP) algorithm is the extended version of FCP by modifying the cross conditional dependence between three or more multi-sensor information sources. The proposed MFCP method is assessed in the multi-sensor land cover classification over the Beijing area for three sensor data sets. We evaluated and validated our proposed methodology by comparing it with four existing fusion methods. The experimental results demonstrated that the proposed MFCP method outperformed all the compared fusion methods in terms of overall accuracy, kappa and class wise accuracies. Therefore, the MFCP method is adaptable to any type and any number of sensor data sets.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127885","Cross Conditional Dependence;Multi-sensor Fusion of Correlated Probabilities;Decision Fusion","Remote sensing;Conferences;Indexes;Geology;Stochastic processes;Estimation;Earth","geophysical image processing;image classification;image fusion;land cover;probability","multisensor data;conditional independence;cross conditional dependence;multisensor information sources;multisensor land cover classification;MFCP;decision fusion;remote sensing;multisensor fusion of correlated probabilities;fusion of correlated probabilities;FCP","","","","12","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Interpolation and Gap Filling of Landsat Reflectance Time Series","Á. Moreno–Martínez; M. Moneta; G. C. Valls; L. Martino; N. Robinson; B. Allred; S. W. Running","Numerical Terradynamic Simulation Group (NTSG), University of Montana, USA; Department of Geosciences, University of Montana, USA; Image Processing Laboratory (IPL), Universitat de València, Spain; Image Processing Laboratory (IPL), Universitat de València, Spain; Numerical Terradynamic Simulation Group (NTSG), University of Montana, USA; Numerical Terradynamic Simulation Group (NTSG), University of Montana, USA; Numerical Terradynamic Simulation Group (NTSG), University of Montana, USA","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","349","352","Products derived from a single multispectral sensor are hampered by a limited spatial, spectral or temporal resolutions. Image fusion in general and downscaling/blending in particular allow to combine different multiresolution datasets. We present here an optimal interpolation approach to generate smoothed and gap-free time series of Landsat reflectance data. We fuse MODIS (moderate-resolution imaging spectroradiometer) and Landsat data globally using the Google Earth Engine (GEE) platform. The optimal interpolator exploits GEE ability to ingest large amounts of data (Landsat climatologies) and uses simple linear operations that scale easily in the cloud. The approach shows very good results in practice, as tested over five sites with different vegetation types and climatic characteristics in the contiguous US.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517503","MODIS;Landsat;data fusion;downscaling;blending;Kalman filter;optimal interpolator","Remote sensing;Earth;Artificial satellites;MODIS;Image resolution;Cloud computing;Time series analysis","geophysical image processing;image fusion;image resolution;interpolation;remote sensing;time series;vegetation;vegetation mapping","different vegetation types;simple linear operations;Landsat climatologies;GEE ability;optimal interpolator;Google Earth Engine platform;imaging spectroradiometer;Landsat reflectance data;gap-free time series;optimal interpolation approach;different multiresolution datasets;image fusion;temporal resolutions;spectral resolutions;spatial resolutions;single multispectral sensor;landsat reflectance time series;gap filling","","1","","20","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Deep Learning for SAR-Optical Image Matching","L. H. Hughes; N. Merkle; T. Bürgmann; S. Auer; M. Schmitt","Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; Airbus Defence and Space GmbH, Immenstaad, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","4877","4880","The automatic matching of corresponding regions in remote sensing imagery acquired by synthetic aperture radar (SAR) and optical sensors is a crucial pre-requesite for many data fusion endeavours such as target recognition, image registration, or 3D-reconstruction by stereogrammetry. Driven by the success of deep learning in conventional optical image matching, we have carried out extensive research with regard to deep matching for SAR-optical multi-sensor image pairs in the recent past. In this paper, we summarize the achieved findings, including different concepts based on (pseudo-)siamese convolutional neural network architectures, hard negative mining, alternative formulations of the underlying loss function, and creation of artificial images by generative adversarial networks. Based on data from state-of-the-art remote sensing missions such as TerraSAR-X, Prism, Worldview-2, and Sentinel-1/2, we show what is already possible today, while highlighting challenges to be tackled by future research endeavors.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898635","Deep Learning;Image Matching;Optical Images;SAR Images;Data Fusion","Optical imaging;Optical sensors;Image matching;Optical fiber networks;Training data;Training;Deep learning","convolutional neural nets;image fusion;image matching;learning (artificial intelligence);optical images;optical sensors;radar imaging;remote sensing;synthetic aperture radar","remote sensing imagery;optical sensors;data fusion;deep learning;SAR-optical multisensor image pairs;artificial images;SAR-optical image matching;synthetic aperture radar;Siamese convolutional neural network architectures","","23","","12","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"3D Expansion of SRCNN for Spatial Enhancement of Hyperspectral Remote Sensing Images","N. Aburaed; M. Q. Alkhatib; S. Marshall; J. Zabalza; H. Al Ahmad","Department of Electronic and Electrical Engineering, University of Strathclyde, UK; College of Engineering and IT, University of Dubai, UAE; Department of Electronic and Electrical Engineering, University of Strathclyde, UK; Department of Electronic and Electrical Engineering, University of Strathclyde, UK; College of Engineering and IT, University of Dubai, UAE","2021 4th International Conference on Signal Processing and Information Security (ICSPIS)","27 Dec 2021","2021","","","9","12","Hyperspectral Imagery (HSI) have high spectral resolution but suffer from low spatial resolution due to sensor tradeoffs. This limitation hinders utilizing the full potential of HSI. Single Image Super Resolution (SISR) techniques can be used to enhance the spatial resolution of HSI. Since these techniques rely on estimating missing information from one Low Resolution (LR) HSI, they are considered ill-posed. Furthermore, most spatial enhancement techniques cause spectral distortions in the estimated High Resolution (HR) HSI. This paper deals with the extension and modification of Convolutional Neural Networks (CNNs) to enhance HSI while preserving their spectral fidelity. The proposed method is tested, evaluated, and compared against other methodologies quantitatively using Peak Signal-to-noise Ratio (PSNR), Structural Similarity Index Measurement (SSIM), and Spectral Angle Mapper (SAM).","","978-1-6654-3796-7","10.1109/ICSPIS53734.2021.9652420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9652420","Hyperspectral;remote sensing;single image super resolution;3D convolution","Visualization;Interpolation;Three-dimensional displays;PSNR;Information security;Distortion;Sensors","convolutional neural nets;geophysical image processing;image fusion;image resolution;remote sensing","high spectral resolution;low spatial resolution;single image super resolution techniques;low resolution HSI;spatial enhancement techniques;spectral distortions;spectral fidelity;spectral angle mapper;hyperspectral remote sensing;hyperspectral imagery;estimated high resolution HSI;SISR techniques;structural similarity index measurement;SSIM;SAM","","4","","11","IEEE","27 Dec 2021","","","IEEE","IEEE Conferences"
"New hierarchical joint classification method for SAR-optical multiresolution remote sensing data","I. Hedhli; G. Moser; S. B. Serpico; J. Zerubia","DITEN, University of Genoa, Italy; DITEN, University of Genoa, Italy; DITEN, University of Genoa, Italy; INRIA, AYIN team, France","2015 23rd European Signal Processing Conference (EUSIPCO)","28 Dec 2015","2015","","","759","763","In this paper, we develop a novel classification approach for multiresolution, multisensor (optical and synthetic aperture radar), and/or multiband images. Accurate and time-efficient classification methods are particularly important tools to support rapid and reliable assessment of the ground changes. Given the huge amount and variety of data available currently from last-generation satellite missions, the main difficulty is to develop a classifier that can take benefit of multiband, multiresolution, and multisensor input imagery. The proposed method addresses the problem of multisensor fusion of SAR with optical data for classification purposes, and allows input data collected at multiple resolutions and additional multiscale features derived through wavelets to be fused.","2076-1465","978-0-9928-6263-3","10.1109/EUSIPCO.2015.7362485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362485","Multisensor;multiresolution remote sensing images;supervised classification;hierarchical Markov random fields","Image resolution;Optical sensors;Optical imaging;Signal resolution;Optical signal processing;Synthetic aperture radar","feature extraction;geophysical image processing;image classification;image fusion;image resolution;optical information processing;radar imaging;remote sensing by radar;synthetic aperture radar;wavelet transforms","hierarchical joint classification method;SAR-optical multiresolution remote sensing data;multisensor images;multiband images;synthetic aperture radar;ground changes assessment;last-generation satellite missions;multisensor fusion;optical data;multiscale features","","4","","20","","28 Dec 2015","","","IEEE","IEEE Conferences"
"A comparison on multiple level features for fusion of hyperspectral and LiDAR data","W. Liao; A. Pižurica; Renbo Luo; W. Philips","Ghent University-TELIN-IPI-iMinds, Belgium; Ghent University-TELIN-IPI-iMinds, Belgium; Ghent University-TELIN-IPI-iMinds, Belgium; Ghent University-TELIN-IPI-iMinds, Belgium","2017 Joint Urban Remote Sensing Event (JURSE)","11 May 2017","2017","","","1","4","Remote sensed images contain a wealth of information. Next to diverse sensor technologies that allow us to measure different aspects of objects on the Earth (spectral characteristics in hyperspectral (HS) images, height in Light Detection And Ranging (LiDAR) data), we also have advanced image processing algorithms that have been developed to mine relevant information from multisensor remote sensing data for Earth observation. However, automatic interpretation of remote sensed images is still very difficult. In this paper, we compare multiple level features for fusion of HS and LiDAR data for urban area classification. Experimental results on fusion of HS and LiDAR data from the 2013 IEEE GRSS Data Fusion Contest demonstrate that middle-level morphological attribute features outperform high-level deep learning features. Compared to the methods using raw data fusion and deep learning fusion, with the graph-based fusion method [4], overall classification accuracies were improved by 8%.","","978-1-5090-5808-2","10.1109/JURSE.2017.7924601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7924601","Urban remote sensing;graph fusion;deep learning;hyperspectral;LiDAR","Laser radar;Machine learning;Data integration;Feature extraction;Hyperspectral imaging","geophysical image processing;hyperspectral imaging;image classification;image fusion;remote sensing by laser beam;remote sensing by radar","raw data fusion;graph-based fusion method;deep learning fusion;high-level deep learning features;middle-level morphological attribute features;IEEE GRSS Data Fusion Contest;urban area classification;Earth observation;multisensor remote sensing data;advanced image processing algorithms;Light Detection And Ranging data;hyperspectral images;spectral characteristics;sensor technologies;remote sensed images;hyperspectral fusion","","1","","10","IEEE","11 May 2017","","","IEEE","IEEE Conferences"
"Image Quality Assessment Methods for Near-Infrared Wildfire Imagery","Z. Faniso-Mnyaka; V. Skosana; M. Nana; E. Magidimisha","Optronic Sensor System, Defence and Security, Council for Scientific and Industrial Research, Pretoria, South Africa; Optronic Sensor System, Defence and Security, Council for Scientific and Industrial Research, Pretoria, South Africa; Optronic Sensor System, Defence and Security, Council for Scientific and Industrial Research, Pretoria, South Africa; Optronic Sensor System, Defence and Security, Council for Scientific and Industrial Research, Pretoria, South Africa","2022 International Conference on Engineering and Emerging Technologies (ICEET)","11 Jan 2023","2022","","","1","6","Over the past two decades, there has been a surge of interest in the study of image quality assessment due to its broad applicability in many fields. Satellites and other remote sensing applications have been collecting vital data that is utilised to monitor targets or events in varying environmental conditions all over the world. Some of these collections include images of natural disasters and anthropogenic events such as wildfires, floods, and drought, among others. However, appropriate image quality assessment techniques have been lacking for image fusion and other remote sensing applications where the information is not targeting the human visual system. Currently, there are several perceptual image quality assessment methods that can be applied depending on the image sensor type. In this paper, we focus on various no-reference general and specific image quality methods that can be used to evaluate remote sensing images for fire detection. Further, we evaluate the effectiveness of the non-referential image quality techniques applied in the processing of airborne sensor images, notably those for fire detection, and correlate the effectiveness of these techniques to the accuracy of detection. In this paper Image quality assessment (IQA) methods such as entropy, BRISQUE, MUSIQ, exposure, and CPBD are analyzed along with methods for image distortion, i.e., Gaussian blur, and image enhancement such as HE, AHE, and CLAHE. Therefore, the no-reference image quality assessment investigation will contribute to the detection and correction of image quality processing issues in wildfires.","2831-3682","978-1-6654-9106-8","10.1109/ICEET56468.2022.10007146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10007146","no-reference method;Image quality;remote sensing;image distortions;wildfires;image enhancement","Image quality;Optical filters;Integrated optics;Fires;Optical distortion;Optical imaging;Calibration","disasters;entropy;geophysical image processing;image enhancement;image fusion;remote sensing;wildfires","airborne sensor images;anthropogenic events;BRISQUE;CPBD;entropy;exposure;fire detection;Gaussian blur;image distortion;image enhancement;image fusion;image quality processing issues;image sensor type;MUSIQ;nearinfrared wildfire imagery;no-reference image quality assessment investigation;nonreferential image quality techniques;perceptual image quality assessment methods;remote sensing applications;remote sensing images;specific image quality methods","","","","18","IEEE","11 Jan 2023","","","IEEE","IEEE Conferences"
"Change Detection from Molti-Band Remote Sensing Image Based on Spectral Product Fusion and MRF Model","Y. Zang; L. Xie; G. Xu; H. Li; X. Bai","State Grid Shandong Electric Power, Research Institute, Jinan, China; State Grid Shandong Electric Power, Research Institute, Jinan, China; Hubei Huazhong Electric Power Technology Development Co., Ltd, Wuhan, China; Hubei Huazhong Electric Power Technology Development Co., Ltd, Wuhan, China; State Grid Shanxi Electric Power Research Institute, Xi'an, China","2019 International Conference on Intelligent Computing, Automation and Systems (ICICAS)","2 Apr 2020","2019","","","835","838","Traditional remote sensing image change detection methods are difficult to detect complete change information by incomplete bands information, and change detection accuracy is not guaranteed. To solve this problem, the paper propose multi-band remote sensing image change detection method by spectral fusion and Markov Random Field. In the method, the difference image and ratio image are fused by spectral product fusion model, and get fusion image, all bands change information are used by iterative calculation, and obtain optimal change information. Experimental results show that the detection accuracy of the proposed method is superior to the current change detection methods, and good stability.","","978-1-7281-6106-8","10.1109/ICICAS48597.2019.00179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9051267","change detection, spectral fusion, MRF model, iterative calculation","","geophysical image processing;image fusion;iterative methods;Markov processes;remote sensing","Molti-band remote sensing image;Markov Random Field;optimal change information;fusion image;spectral product fusion model;ratio image;difference image;multiband remote sensing image change detection method;change detection accuracy;incomplete bands information;traditional remote sensing image change detection methods","","","","11","IEEE","2 Apr 2020","","","IEEE","IEEE Conferences"
"Hyperspectral and Multispectral Image Fusion Via Nonnegative Matrix Factorization and Deep Prior Regularization","B. Lin; Y. Zhang; Z. Lin; X. Wang; H. Huang","School of Electronics and Communication Engineering, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China; School of Electronics and Communication Engineering, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China; School of Electronics and Communication Engineering, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China; School of Electronics and Communication Engineering, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China; School of Electronics and Communication Engineering, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1440","1443","High-resolution hyperspectral image(HSI) is usually obtained by fusing a low-resolution HSI and a high-resolution RGB/multispectral image(MSI) due to physical imaging technique limitation. This paper proposes a novel fusion model, which factorizes the fusion image into two non-negative ma-trices based on linear spectral mixing model, and introduces deep prior, an implicit regularization function related to a deep denoising neural network, to model image texture laid in the factor matrix. Using the internal proximal altern ating linearized minimization algorithm, the fusion model is solved by a low-resolution and a high-resolution part: the low-resolution part updates one factor matrix based on the low-resolution HSI, whereas the high-resolution part updates the other factor matrix using a denoising neural network based on the high-resolution RGB/MSI. Experiments show that the proposed method delivers improved fusion performance on different datasets without a repeatedly training process, compared with other state-of-the-art methods.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883179","Hyperspectral image;non-negative ma-trix factorization(NMF);convolutional neural network(CNN);internal proximal alternating linearized minimization (iPALM)","Training;Measurement;Image texture;Noise reduction;Neural networks;Imaging;Geoscience and remote sensing","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image colour analysis;image denoising;image fusion;image reconstruction;image representation;image resolution;image sensors;image texture;learning (artificial intelligence);matrix algebra;matrix decomposition;minimisation;neural nets;sensor fusion;spectral analysis","high-resolution hyperspectral image;low-resolution HSI;physical imaging technique limitation;fusion model;fusion image;linear spectral mixing model;implicit regularization function;deep denoising neural network;image texture;factor matrix;high-resolution part;low-resolution part;fusion performance","","","","15","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Discriminative Feature Extraction and Fusion for Classification of Hyperspectral and Lidar Data","W. Song; Z. Gao; Y. Zhang","Department of Mathematics and Theories, Peng Cheng Laboratory, Shenzhen, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2271","2274","Multisource remote sensing data provide the abundant and complementary information for land cover classification. In this paper, we propose a deep hashing-based feature extraction and fusion framework for joint classification of hyper-spectral and LiDAR data. Firstly, HSIs and LiDAR data are fed into a two-stream network to extract deep features after data preprocessing. Then, we adopt hashing technique to constrain single-source and cross-source similarities, i.e., samples with same classes should have small feature distance and samples with different classes should have large feature distance. Furthermore, a feature-level fusion strategy is exploited to fuse the two kind of multisource information. Finally, we design an object function to consider the similarity information between sample pairs and semantic information of each sample, which can deliver the discriminative features for classification. The experiments on Houston data demonstrate the effectiveness of the proposed method over some competitive approaches.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883136","Hyperspectral images (HSIs);Light de-tection and ranging (LiDAR);classification;deep learning;hashing;feature extraction","Laser radar;Fuses;Semantics;Data preprocessing;Feature extraction;Distance measurement;Data mining","feature extraction;geophysical image processing;image classification;image fusion;neural nets;optical radar;remote sensing;sensor fusion","cross-source similarities;feature distance;feature-level fusion strategy;multisource information;similarity information;discriminative features;Houston data;lidar data;remote sensing data;abundant information;complementary information;land cover classification;deep hashing-based feature extraction;fusion framework;joint classification;deep features;data preprocessing","","","","17","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Automatic alignment of high resolution optical and SAR images for urban areas","S. Auer; M. Schmitt; P. Reinartz","Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR); Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR)","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","5466","5469","This paper presents the basics and functionality of SimGeoI, a simulation-based framework for the automated interpretation and alignment of optical and SAR remote sensing data. SimGeoI has been developed in order to align optical and SAR data based on given geometric information about objects represented by digital surface models. Thereby, the analysis of urban scenes is possible with independence of sensor type and perspective. After a brief introduction of the processor environment, possible applications of the framework are indicated with results of a case study for Istanbul (WorldView-2 and TerraSAR-X data). In this context, opportunities in the context of a joint analysis of high resolution optical and SAR data are addressed, i.e. concerning data fusion, change detection, and machine learning tasks.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128241","Optical Data;SAR Data;Data Fusion;Simulation;Interpretation;Urban Areas;Ray Tracing","Buildings;Optical imaging;Optical sensors;Synthetic aperture radar;Optical distortion;Sun;Satellites","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);radar imaging;remote sensing by radar;synthetic aperture radar","urban scenes;sensor type;TerraSAR-X data;data fusion;automatic alignment;urban areas;functionality;SimGeoI;automated interpretation;optical SAR remote sensing data;align optical SAR data;digital surface models;geometric information;WorldView-2","","","","6","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Superpixel Based Spatial and Temporal Adaptive Reflectance Fusion Model","W. Wang; G. Sun; Y. Yao; A. Zhang","College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, Shandong, China; Laboratory for Marine Mineral Resources, Qingdao National Laboratory for Marine Science and Technology, Qingdao, China; Ministry of Environmental Protection of China, Satellite Environment Center, Beijing, China; College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, Shandong, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2308","2311","At present, remote sensing images are mutually restricted in temporal and spatial resolution. A single satellite sensor cannot obtain remote sensing images with both high spatial resolution and high temporal resolution. Spatiotemporal fusion of remote sensing images is a promising method to solve this issue. The spatial and temporal adaptive reflectance fusion model (STARFM) is a widely-accepted method for spatiotemporal fusion. However, STARFM selects similar pixels in a regular rectangular window. This neighborhood window has many different land cover types, which leads to wrong selection of similar pixels. Therefore, we develop a novel spatial and temporal adaptive reflectance fusion model based on superpixel, denote by S-STARFM. In the proposed method, the target pixels to be predicted are divided into two categories, including changed pixels and unchanged pixels. Then the superpixels are used to improve the selection of similar pixels. To verify the effectiveness of S-STARFM, the moderate resolution imaging spectrometer (MODIS) and Landsat Enhanced Thematic Mapper Plus (ETM+) data are used to generate high spatiotemporal resolution images. The prediction image accuracy shows that the proposed method outperforms the STARFM.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323449","Spatiotemporal fusion;Superpixel;Landsat;MODIS;Surface reflectance","Remote sensing;Earth;Artificial satellites;Reflectivity;MODIS;Spatial resolution;Windows","geophysical image processing;image fusion;image resolution;remote sensing","temporal adaptive reflectance fusion model;STARFM;spatiotemporal fusion;moderate resolution imaging spectrometer;high spatiotemporal resolution images;superpixel based spatial;remote sensing images;high spatial resolution;high temporal resolution;spatial reflectance fusion model;image accuracy prediction;MODIS;Landsat ETM+ data;Enhanced Thematic Mapper Plus","","1","","8","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Coupling Model- and Data-Driven Methods for Remote Sensing Image Restoration and Fusion: Improving physical interpretability","H. Shen; M. Jiang; J. Li; C. Zhou; Q. Yuan; L. Zhang","School of Resource and Environmental Sciences (SRES), Wuhan University, Wuhan, China; School of Resource and Environmental Sciences (SRES), Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences (SRES), Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Magazine","13 Jul 2022","2022","10","2","231","249","In the fields of image restoration and image fusion, model- and data-driven methods are the two representative frameworks. However, both approaches have their respective advantages and disadvantages. Model-driven techniques consider the imaging mechanism, which is deterministic and theoretically reasonable; however, they cannot easily model complicated nonlinear problems. Data-driven schemes have a stronger prior-knowledge learning capability for huge data, especially for nonlinear statistical features; however, the interpretability of the networks is poor, and they are overdependent on training data. In this article, we systematically investigate the coupling of model- and data-driven methods, which has rarely been considered in the remote sensing image restoration and fusion communities. We are the first to summarize the coupling approaches into the following three categories: 1) data- and model-driven cascading methods, 2) variational models with embedded learning, and 3) model-constrained network learning methods. The typical existing and potential coupling techniques for remote sensing image restoration and fusion are introduced with application examples. This article also gives some new insights into potential future directions, in terms of both methods and applications.","2168-6831","","10.1109/MGRS.2021.3135954","National Natural Science Foundation of China(grant numbers:62071341,41971303); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829013","","Couplings;Training;Tensors;Transfer learning;Training data;Image restoration;Task analysis","geophysical image processing;image fusion;image restoration;inference mechanisms;learning (artificial intelligence);remote sensing;variational techniques","complicated nonlinear problem modeling;physical interpretability improvement;model-constrained network learning;embedded learning;variational models;nonlinear statistical features;imaging mechanism;image fusion;potential coupling techniques;model-driven cascading methods;remote sensing image restoration;data-driven methods","","4","","71","IEEE","13 Jul 2022","","","IEEE","IEEE Magazines"
"Using Image Segmentation for Fusion of Multispectral to Panchromatic Imagery","V. R. Pandit; R. J. Bhiwani","Engineering & Technology, Sant Gadge Baba Amravati University, Maharashtra State, India; BNCOE, Pusad, Sant Gadge Baba Amravati University, Maharashtra State, India","2019 Fifth International Conference on Image Information Processing (ICIIP)","10 Feb 2020","2019","","","23","28","Fusion of multispectral to panchromatic image obtained for the same location, that is ultimately producing new multispectral image with added details of spatial resolution is best known as `Pansharpening'. Developing such algorithms for image fusion and assessing quality by objective metrics are the most debated topics of research from the last few decades. In this paper, the authors attempt to validate use of image segmentation in pansharpening and compare performance of fusion process with other most recent algorithms. Fusion algorithms are tested on two datasets, acquired by QuickBird-2 and WorldView-2 respectively. Evaluations of corresponding results are based on computations of popular image quality metrics. The analysis will surely help to know the best possible trade-off between spatial and spectral resolutions for desired fused image.","2640-074X","978-1-7281-0899-5","10.1109/ICIIP47207.2019.8985910","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8985910","Remote sensing;image fusion;pansharpening;panchromatic image;multispectral image;image segmentation","","geophysical image processing;image classification;image fusion;image resolution;image segmentation;image sensors;remote sensing","image segmentation;panchromatic imagery;multispectral image;spatial resolution;pansharpening;image fusion;fusion algorithms;image quality metrics;spectral resolutions;QuickBird-2 dataset;WorldView-2 dataset","","3","","23","IEEE","10 Feb 2020","","","IEEE","IEEE Conferences"
"A New Pansharpening Algorithm using Morphological Lifting Transform","Y. Shi","Beijing Key Laboratory of Fractional Signals and Systems, Beijing Institute of Technology, Beijing, China","2018 IEEE 3rd International Conference on Signal and Image Processing (ICSIP)","3 Jan 2019","2018","","","250","254","Pansharpening is a fusion technique that merges panchromatic (PAN) and multispectral (MS) images in order to enhance the spatial resolution of MS. Over the last two decades, a variety of approaches have been proposed either based on component substitution or multiresolution analysis. This paper proposes a new pansharpening algorithm using the morphological lifting transform (MLT) with region-based injection. The lifting structure with morphological operators provides a flexible tool to construct new morphological wavelet transforms. The morphological operators compared to the linear operators can extract the spatial details more effectively. In addition, an adaptive injection gain based on minimum mean squared error (MMSE) with k-means clustering is adopted. The proposed algorithm is validated in two datasets: Pléiades and Landsat 7. Experimental results show that the proposed method has advantages on enhancing the spatial details while retaining the spectral information. And it outperforms other state-of-the-art methods.","","978-1-5386-6396-7","10.1109/SIPROCESS.2018.8600445","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8600445","pansharpening;image fusion;morphological lifting transform;morphological operator;remote sensing","Wavelet transforms;Spatial resolution;Remote sensing;Principal component analysis;Distortion;Multiresolution analysis","geophysical image processing;image fusion;image resolution;least mean squares methods;mathematical morphology;remote sensing;wavelet transforms","linear operators;spatial details;adaptive injection gain;minimum mean squared error;new pansharpening algorithm;morphological lifting transform;fusion technique;component substitution;region-based injection;lifting structure;morphological operators;morphological wavelet transforms","","2","","19","IEEE","3 Jan 2019","","","IEEE","IEEE Conferences"
"A high resolution burned area detector for Sentinel-2 and Landsat-8","M. Zanetti; D. Marinelli; M. Bertoluzza; S. Saha; F. Bovolo; L. Bruzzone; M. L. Magliozzi; M. Zavagli; M. Costantini","Department of Information Engineering and Computer Science, University of Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Italy; Fondazione Bruno Kessler, Center for Information and Communication Technology, Trento, Italy; Fondazione Bruno Kessler, Center for Information and Communication Technology, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Italy; e-GEOS, Rome, Italy; e-GEOS, Rome, Italy; e-GEOS, Rome, Italy","2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp)","14 Oct 2019","2019","","","1","4","Fires are disruptive events that should be carefully studied. To this date, the monitoring at large scale of fire events is mainly performed using low spatial resolution sensors such as MODIS and VIIRS and ancillary data like active fire maps. However, the data produced by the new generation of spaceborne multispectral missions can be used for burned area detection at a finer spatial and temporal scale with respect to existing methods. In this paper we present a method that analyzes time-series of optical images acquired by Landsat-8 and Sentinel-2 to detect burned areas in a fully automatic and unsupervised way. The method analyzes each image in the time-series to extract Candidate Burned Areas (CBAs). CBAs are analyzed accounting for the temporal correlation to reduce the number of false alarms. In particular, only pixels that have a temporal profile consistent with the physical event of a fire are selected. Moreover, the time variable is used to compute the confidence of the detection. The method has been tested on a time-series of the Attica region, Greece, which was affected by large fires in 2018. Preliminary experimental results showed that the method correctly identified the burned areas of the region.","","978-1-7281-4615-7","10.1109/Multi-Temp.2019.8866958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8866958","Burned Area Detection;Time-series;Multitemporal;Sentinel-2;Remote Sensing","Indexes;Vegetation mapping;Remote sensing;Spatial resolution;Artificial satellites;Earth;MODIS","fires;geophysical image processing;geophysical techniques;image classification;image fusion;image resolution;remote sensing;time series;vegetation mapping;wildfires","Landsat-8;Sentinel-2;Candidate Burned Areas;CBAs;temporal correlation;temporal profile;physical event;high resolution burned area detector;disruptive events;fire events;low spatial resolution sensors;ancillary data;active fire maps;spaceborne multispectral missions;burned area detection;temporal scale;time-series;optical images","","4","","12","IEEE","14 Oct 2019","","","IEEE","IEEE Conferences"
"Multi-Branch Deep Learning Model for Detection of Settlements Without Electricity","T. Di Martino; M. Lenormand; E. C. Koeniguer","ONERA, Traitement de l'Information et Systèmes, Université Paris-Saclay, Palaiseau, France; SONDRA, ONERA, CentraleSupélec, Université Paris-Saclay, Gif-sur-Yvette, France; ONERA, Traitement de l'Information et Systèmes, Université Paris-Saclay, Palaiseau, France","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","1847","1850","We introduce a multi-branch Deep Learning architecture that allows for the extraction of multi-scale features. Exploiting the data multi-modality structure through the combined use of various feature extractors provides high performance on data fusion tasks. Furthermore, the representation of the multi-temporality of the data using sensor-specific 3D convolutions with custom kernel size extracts temporal features at an early computation stage. Our methodology allows reaching performance up to 0.8876 F1 Score on the development phase dataset and around 0.8798 on the test phase dataset. Finally, we demonstrate the contribution of each sensor to the prediction task with the design of data-focused experiments.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554286","Deep Learning;Multi Temporal;Remote Sensing;Multi Sensor;Classification;Data Fusion Contest","Training;Deep learning;Earth;Three-dimensional displays;Geoscience and remote sensing;Feature extraction;Sensors","convolutional neural nets;data fusion;deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;neural net architecture;object detection;remote sensing","multiscale feature extraction;data multimodality structure;feature extractors;data fusion;sensor-specific 3D convolutions;kernel size;temporal feature extraction;data-focused experiments;settlements detection;multibranch deep learning architecture;data multitemporality;F1 score;development phase dataset;test phase dataset","","3","","11","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"On the Effect of Misregistration on Spatio-temporal Fusion","Y. Tang; Q. Wang","College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; College of Surveying and Geo-Informatics, Tongji University, Shanghai, China","2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp)","14 Oct 2019","2019","","","1","4","It is always challenging to acquire satellite sensor data with both fine spatial and fine temporal resolution, especially for monitoring at global scales. Amongst the widely used global monitoring satellites, Landsat sensor data have a coarse temporal resolution, but fine spatial resolution, while MODIS sensor data have fine temporal resolution, but coarse spatial resolution. One solution to this problem is to blend the two types of data using spatio-temporal fusion, creating images with both fine temporal and fine spatial resolution. Reliable geometric registration of images acquired by different sensors is a prerequisite of spatiotemporal fusion. Due to the potentially large differences between the spatial resolutions of images to be fused, the geometric registration process always contains some degree of uncertainty. This paper analyzes quantitatively the influence of geometric registration error on spatio-temporal fusion. The relationship between registration error and the accuracy of fusion was investigated under the influence of different temporal distances between images, different spatial patterns within the images and using different methods.","","978-1-7281-4615-7","10.1109/Multi-Temp.2019.8866847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8866847","Remote sensing data;Landsat;MODIS;spatiotemporal fusion;registration error","Earth;Remote sensing;Artificial satellites;Spatial resolution;MODIS;Monitoring;Reflectivity","geophysical image processing;image fusion;image registration;image resolution;remote sensing;spatiotemporal phenomena","satellite sensor data;fine temporal resolution;widely used global monitoring satellites;Landsat sensor data;coarse temporal resolution;fine spatial resolution;MODIS sensor data;coarse spatial resolution;spatio-temporal fusion;spatiotemporal fusion;spatial resolutions;different temporal distances;different spatial patterns","","","","12","IEEE","14 Oct 2019","","","IEEE","IEEE Conferences"
"A Fast Multidimensional Data Fusion Algorithm For Hyperspectral Spatiotemporal Super-Resolution","P. -C. Chang; J. -T. Lin; C. -H. Lin; P. -W. Tang; Y. Liu","The Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan; The Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan; Miin Wu School of Computing, National Cheng Kung University, Tainan, Taiwan; The Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan; The Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan","2022 12th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","22 Nov 2022","2022","","","1","5","In hyperspectral remote sensing, obtaining fine spatial-temporal and spatial-spectral resolution images are two critical fusion issues due to inherent optical sensor trade-offs. However, simultaneous realization of spatial, spectral, and temporal super-resolution is highly challenging. This paper formulates a new fusion framework incorporating all the three spatial/spectral/temporal dimensions to achieve hyperspectral spatiotemporal (HST) super-resolution. Additionally, unlike many fusion methods assuming availability of the spatial blurring matrix (SBM) in the forward model, we go a step further to blindly achieve HST super-resolution by automatically estimating the SBM. Subsequently, final results can be obtained through the fast iterative shrinkage-thresholding algorithm (FISTA) and the convex optimization-based coupled nonnegative matrix factorization (CO-CNMF) algorithm. We compare results of the proposed HST super-resolution method, termed GFCSR, with other extended spatiotemporal fusion frameworks designed for multispectral images, and, as it turns out, quantitative evaluations demonstrate the superiority of our algorithm.","2158-6276","978-1-6654-7069-8","10.1109/WHISPERS56178.2022.9955073","Ministry of Science and Technology; National Cheng Kung University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955073","Convex optimization;hyperspectral image;image fusion;multispectral image;nonnegative matrix factorization;spatiotemporal super-resolution","Superresolution;Signal processing algorithms;Data integration;Signal processing;Iterative algorithms;Spatiotemporal phenomena;Optical sensors","geophysical image processing;hyperspectral imaging;image fusion;image resolution;image sampling;iterative methods;matrix decomposition;remote sensing;spatiotemporal phenomena;spectral analysis","CO-CNMF;convex optimization-based coupled nonnegative matrix factorization algorithm;critical fusion issues;extended spatiotemporal fusion frameworks;fast iterative shrinkage-thresholding algorithm;FISTA;fusion methods;HST superresolution method;hyperspectral remote sensing;hyperspectral spatiotemporal superresolution;inherent optical sensor trade-offs;multidimensional data fusion algorithm;SBM;spatial blurring matrix;spatial-spectral resolution images;spatial-spectral-temporal dimensions","","","","20","IEEE","22 Nov 2022","","","IEEE","IEEE Conferences"
"Land Cover Classification of Huixian Wetland Based on SAR and Optical Image Fusion","J. Xiao; Y. Xiao; X. Sun; J. Huang; H. Wang","Lijiang School, Guilin Normal University, Guilin, China; Guanxi Key Laboratory of precision Navigation Technology and Application, Guilin University of Electronic Technology, Guilin, China; Guanxi Key Laboratory of precision Navigation Technology and Application, Guilin University of Electronic Technology, Guilin, China; Guanxi Key Laboratory of precision Navigation Technology and Application, Guilin University of Electronic Technology, Guilin, China; Guanxi Key Laboratory of precision Navigation Technology and Application, Guilin University of Electronic Technology, Guilin, China","2020 IEEE 3rd International Conference on Information Communication and Signal Processing (ICICSP)","20 Oct 2020","2020","","","316","320","In this paper, GF-1 WVF image and Sentinel-1 SAR image covering Huixian wetland area are used as data sources. The Gram Schmidt (GS) algorithm is first used to fuse GF-1 images and SAR images with different polarization modes, and then the Random Forest (RF) algorithm is used for supervised classification. Finally, the accuracy of classification results and the ability to extract information are compared. The experimental results show that the fusion image has obvious texture features and prominent karst landform features, compared with the GF-1 WVF image. Compared with the Sentinel-1 SAR image, the fusion image has obvious spectral features. Spectral differences between typical features are large; The overall classification accuracy of GF-1 images, GF-1 and Sentinel-1 VV polarization fusion images, and GF-1 and Sentinel-1 VH polarization fusion images have reached over 80%. The classification accuracy of GF-1 and Sentinel-1 VV polarization fusion images reaches 85.15%, which is better than GF-1 and Sentinel-1 VH polarization fusion images. The classification accuracy of water bodies in the VV polarization fusion image is better than that of GF-1. Bare ground has the highest classification accuracy among all fused images.","","978-1-7281-8823-2","10.1109/ICICSP50920.2020.9232103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9232103","GF-1 image;sar image;image fusion;random forest;huixian wetland","Wetlands;Radar polarimetry;Classification algorithms;Feature extraction;Radio frequency;Optical polarization;Optical imaging","feature extraction;geophysical image processing;image classification;image fusion;image texture;land cover;optical images;radar imaging;remote sensing by radar;synthetic aperture radar","land cover classification;optical image;GF-1 WVF image;Sentinel-1 SAR image;GF-1 images;Sentinel-1 VV polarization fusion images;Sentinel-1 VH polarization fusion images;VV polarization fusion image","","1","","16","IEEE","20 Oct 2020","","","IEEE","IEEE Conferences"
"Hyperspectral and Multispectral Image Fusion Based on Spectral Low Rank and Non-Local Spatial Similarities","R. Dian; S. Li","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3137","3140","Fusing a hyperspectral image (HSI) with a multispectral image (MSI) of the same scene has become a popular way to increase the spatial resolution of HSI. In this paper, we propose a novel HSI and MSI fusion method (termed as the SSS), which is based on spectral low rank and non-local spatial similarities. Firstly, to exploit the high spectral correlations of the desired high spatial resolution HSI, we formulate the fusion problem as the estimation of low-dimensional spectral subspace and coefficients. Since the HSI preserves most of spectral information, the spectral subspace is estimated from HSI via singular value decomposition. With the spectral subspace known, we plug a state-of-the-art denoising algorithm, weighted nuclear norm minimization, into the alternating direction method of multipliers to estimate the coefficients, which can effectively promote the non-local similarities of desired high spatial resolution HSI. Experiments demonstrate that our method is competitive to the state-of-the-art approaches.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899108","Hyperspectral image super-resolution;low rank;superpixels","Spatial resolution;Hyperspectral imaging;Plugs;Noise reduction;Measurement;Estimation","geophysical image processing;hyperspectral imaging;image denoising;image fusion;image representation;image resolution;minimisation;remote sensing;singular value decomposition","hyperspectral image;multispectral image;MSI fusion method;high spectral correlations;desired high spatial resolution HSI;fusion problem;low-dimensional spectral subspace;spectral information;nonlocal similarities","","2","","11","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"An Improved Fully Convolutional Network for Learning Rich Building Features","S. Wang; L. Zhou; P. He; D. Quan; Q. Zhao; X. Liang; B. Hou","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, P. R. China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, P. R. China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, P. R. China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, P. R. China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, P. R. China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, P. R. China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, P. R. China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","6444","6447","Many efficient approaches are proposed to detect building in remote sensing images. In this paper, in order to learning rich building features better, we propose a full convolutional network with dense connection. There contributions are made: 1) To strengthen feature propagation, an improved dense network is introduced to the full convolution network. 2) We have designed top-down short connections to facilitate the fusion of high and low feature information. 3) In addition, we add the weighted cross entropy edge loss function to make the network pay more attention to building edge in detail. Experiments show that the proposed method achieves excellent performance on the remote sensing image data taken by the QuickBird satellite.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898460","DenseNet;FCN;Feature Fusion;Edge Loss;Building Detection","Buildings;Image edge detection;Feature extraction;Remote sensing;Semantics;Image resolution;Convolution","buildings (structures);convolutional neural nets;edge detection;feature extraction;geophysical image processing;image fusion;image texture;learning (artificial intelligence);object detection;remote sensing","fully convolutional network;remote sensing images;feature propagation;weighted cross entropy edge loss function;building features learning;dense network;feature information fusion;QuickBird satellite;building texture details;building edge detection","","","","8","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Fusion of POLSAR and Multispectral Satellite Images: A New Insight for Image Fusion","J. Wang; J. Chen; Q. Wang","College of Computer and Information Engineering, Hohai University, Nanjing, China; College of Computer and Information Engineering, Hohai University, Nanjing, China; College of Computer and Information Engineering, Hohai University, Nanjing, China","2020 IEEE International Conference on Computational Electromagnetics (ICCEM)","12 Oct 2020","2020","","","83","84","Polarized synthetic aperture radar (POLSAR) and multispectral images have great complementarity in information volume. That is to say, POLSAR have high resolution but poor color information. Multispectral images have rich spectral channel information, but the resolution is low. Therefore, this work has explored the fusion problem of the two data source. A framework was proposed, which merged polarized channel fusion data and multispectral images based on the Sentinel-2 and GF-3 data. The experimental results showed that the fusion results greatly integrated the characteristics of each channel of POLSAR and optical image. Therefore, our work has great application potential in improving the accuracy of feature recognition.","","978-1-7281-3448-2","10.1109/ICCEM47450.2020.9219457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9219457","fusion;polarized synthetic aperture radar (POL-SAR);GF-3;Sentinel-2;feature recognition","Integrated optics;Visualization;Optical polarization;Satellites;Soft sensors;Optical imaging;Adaptive optics","geophysical image processing;image colour analysis;image fusion;radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","optical image;fusion results;channel fusion data;fusion problem;rich spectral channel information;poor color information;information volume;multispectral images;polarized synthetic aperture radar;image fusion;multispectral satellite images;POLSAR","","1","","7","IEEE","12 Oct 2020","","","IEEE","IEEE Conferences"
"Automate Lithological Classification of the Amotape Tahuin Metamorphic Complex in Ecuador using Random Forest and a Multi-Sensor Satellite Imagery Approach","M. Erith; E. Jhonatan; T. Daniel; B. Marielisa; B. Franz; S. Carmen; P. Victor; L. Aracely; Z. Alfonso","The Food and Agriculture Organization of United Nations (FAO), Quito-Ecuador; Instituto de Investigación Geológico y Energético (IIGE), Quito-Ecuador; Instituto de Investigación Geológico y Energético (IIGE), Quito-Ecuador; Instituto de Investigación Geológico y Energético (IIGE), Quito-Ecuador; Instituto de Investigación Geológico y Energético (IIGE), Quito-Ecuador; Instituto de Investigación Geológico y Energético (IIGE), Quito-Ecuador; Instituto de Investigación Geológico y Energético (IIGE), Quito-Ecuador; Instituto de Investigación Geológico y Energético (IIGE), Quito-Ecuador; Universidad Tecnológica Metropolitana (UTEM), Santiago de Chile-Chile","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2093","2096","In this paper, a methodological approach to classify lithology features using random forest and multisensor satellite imagery is presented. The incorporation of remote sensing techniques into regional geology mapping is important in order to make possible the implementation of monitoring strategies aimed to enhance national capabilities to provide information for natural disasters management, sustainable exploitation of natural resources, etc. In this context, the use of both optical/infrared and multi-wavelength synthetic aperture radar satellite data was evaluated in order to train a random forest algorithm, taking as reference field data, to generate a lithological map. Preliminary results have shown the capability of this approach to lithological classification with accuracies values higher than 80% in some conditions to map sedimentary, igneous, and metamorphic lithological classes.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323942","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323942","Landsat;Random Forest;Sentinel 1;Alos Palsar;Geology","Earth;Remote sensing;Artificial satellites;Satellites;Rocks;Radio frequency;Training","disasters;geology;geophysical image processing;geophysical prospecting;geophysical techniques;image fusion;random forests;remote sensing by radar;rocks;synthetic aperture radar;terrain mapping","multiwavelength synthetic aperture radar satellite data;random forest algorithm;reference field data;lithological map;map sedimentary;metamorphic lithological classes;automate lithological classification;amotape tahuin metamorphic complex;ecuador;multisensor satellite imagery approach;methodological approach;lithology features;remote sensing techniques;regional geology mapping;monitoring strategies;national capabilities;natural disasters management;sustainable exploitation;natural resources","","","","8","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Pan-Sharpen Multispectral Images Using Sparse Representation","P. -H. Hsu; H. -L. Kuo","Department of Civil Engineering, National Taiwan University; Department of Civil Engineering, National Taiwan University","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5143","5146","Pan-sharpening is an image fusion technique of synthesizing a high-resolution multispectral image from a low-resolution multispectral image and a high-resolution panchromatic image. In this paper, a novel pan-sharpening method for remote sensing images has been proposed with sparse representation over learned dictionaries. In the proposed method, the dictionaries are learned only from the high-resolution panchromatic image via the joint learning algorithm, instead of learning from the high-resolution multispectral image which are not available in practice. The sparse coefficients of the panchromatic image and low-resolution panchromatic image are calculated by the orthogonal matching pursuit algorithm. Then, the fused high-resolution multispectral image can be constructed by combining the obtained sparse coefficients and the high-resolution dictionary. The experiment results indicate that the proposed method not only preserve spectral and spatial details of the source images but overcoming the drawbacks of fusion distortion.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519099","Pan-sharpening;super resolution;sparse representation;dictionary learning;regularization","Dictionaries;Machine learning;Image reconstruction;Matching pursuit algorithms;Spatial resolution;Principal component analysis","geophysical image processing;geophysical techniques;image fusion;image representation;image resolution;remote sensing","sparse representation;image fusion technique;high-resolution multispectral image;low-resolution multispectral image;high-resolution panchromatic image;pan-sharpening method;remote sensing images;sparse coefficients;low-resolution panchromatic image;high-resolution dictionary;source images;pan-sharpen multispectral images;learned dictionaries;joint learning algorithm;orthogonal matching pursuit algorithm","","2","","7","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Triplet Attention Feature Fusion Network for SAR and Optical Image Land Cover Classification","Z. Xu; J. Zhu; J. Geng; X. Deng; W. Jiang","School of Electronics And Information, Northwestern Polytechnical University, Xi'an, China; Chinese Academy of Sciences, Aerospace Information Research Institute, Beijing, China; School of Electronics And Information, Northwestern Polytechnical University, Xi'an, China; School of Electronics And Information, Northwestern Polytechnical University, Xi'an, China; School of Electronics And Information, Northwestern Polytechnical University, Xi'an, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4256","4259","With recent advances in remote sensing, abundant multimodal data are available for applications. However, considering the redundancy and the huge domain differences among multimodal data, how to effectively integrate these data is becoming important and challenging. In this paper, we proposed a triplet attention feature fusion network (TAFFN) for SAR and optical image fusion classification. Specifically, spatial attention module and spectral attention module based on self-attention mechanism are developed to extract spatial and spectral long-range information from the SAR image and optical image respectively, at the same time, cross-attention mechanism is proposed to capture the long-range interactive representation. Triplet attentions are concatenated to further integrate the complementary information of SAR and optical images. Experiments on a SAR and optical multimodal dataset demonstrate that the proposed method can achieve the state-of-the-arts performance.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555126","National Natural Science Foundation of China(grant numbers:61901376); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555126","Feature fusion;attention mechanism;land cover classification;SAR image","Integrated optics;Redundancy;Optical fiber networks;Optical imaging;Feature extraction;Radar polarimetry;Optical sensors","feature extraction;geophysical image processing;geophysical signal processing;image classification;image fusion;optical images;radar imaging;remote sensing;sensor fusion;synthetic aperture radar;terrain mapping;visual perception","fusion network;SAR;optical image fusion classification;spatial attention module;spectral attention module;self-attention mechanism;long-range information;cross-attention mechanism;long-range interactive representation;triplet attention;optical images;optical multimodal dataset;sar;optical image land cover classification;remote sensing;abundant multimodal data;huge domain differences","","1","","8","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"The recent advances of data imaging and fusion processing for airborne X-SAR with high resolution","Ting Shen; Jun Li; Zhirui Wang; Lei Huang; Liwei Li; Ping Zhang","Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Beijing Institute of Radio Measurement, Beijing, China; Beijing Institute of Radio Measurement, Beijing, China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China","2016 Progress in Electromagnetic Research Symposium (PIERS)","10 Nov 2016","2016","","","2843","2848","X-SAR system is the airborne imaging radar with multi-mode synthetic aperture radar (SAR) at high-resolution, interferometer and full-polarization, which has been developed by the Institute of Remote Sensing and Digital Earth (RADI), Chinese Academy of Sciences (CAS), funded by the CAS Large Research Infrastructures. The first-stage form 2009 to 2015, X-SAR was successfully implemented to an operational SAR in X-band with high resolution (up to 0.5 m).The system performances and data imaging quality have verified by the flight tests. Many valuable results of the visual interpretation in typical images, particularly SAR image fusion processing have emphasized the X-SAR's target recognition capabilities. This paper presents the core characteristics of X-SAR images, having achieved by the spatial resolution optimized by low side-lobe, exact geographical precision and radiometric accuracy. The visual inspection of typical targets in example images is described such as the surface of desert hill, the vehicle discrimination and aircraft recognition. Meanwhile, the image fusion processing for target recognition has been implemented. The recent advances of SAR-optical image fusion used to target classification and SAR-SAR image fusion processing for change detection are also presented.","","978-1-5090-6093-1","10.1109/PIERS.2016.7735138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7735138","","Spatial resolution;Synthetic aperture radar;Radiometry;Imaging;Radar imaging;Calibration","airborne radar;image fusion;radar imaging;radar resolution;synthetic aperture radar","data imaging;airborne X-SAR system;multimode synthetic aperture radar;Institute of Remote Sensing and Digital Earth;Chinese Academy of Sciences;CAS;SAR image fusion processing;target recognition;change detection","","","","5","IEEE","10 Nov 2016","","","IEEE","IEEE Conferences"
"Optimal Component Substitution and Multi-Resolution Analysis Pansharpening Methods Using a Convolutional Neural Network","F. Palsson; J. R. Sveinsson; M. O. Ulfarsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3177","3180","The fusion of a low spatial resolution multispectral image and a high spatial resolution panchromatic image, i.e., pan-sharpening is an important technique in remote sensing where high resolution imagery is needed. Two of the largest families of such methods are the component substitution (CS) and multi-resolution analysis (MRA) methods. These families of methods can be described by general detail injection schemes which are closely related. In this paper, we propose pansharpening methods which are based on directly implementing these schemes using a convolutional neural network (CNN) such that the mean squared error between the down-sampled fused image and the observed multispectral image is minimized. Using a simulated Pleiades dataset we demonstrate that the proposed approach gives excellent results when compared to other state-of-the-art CS, MRA and CNN methods.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899299","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899299","Pansharpening;component substitution;multi-resolution analysis;convolutional neural network","Spatial resolution;Convolutional neural networks;Visualization;Training;Remote sensing;Protocols","geophysical image processing;geophysical techniques;image fusion;image resolution;neural nets;remote sensing","general detail injection schemes;convolutional neural network;fused image;CNN methods;optimal component substitution;low spatial resolution multispectral image;high spatial resolution panchromatic image;pan-sharpening;remote sensing;high resolution imagery;multiresolution analysis methods","","4","","10","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Improvement of MRA-Based Pansharpening Methods Through the Considerasion of Mixed Pixels","H. Li; L. Jing","Key Laboratory of Digital Earth Science, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Chinese Academy of Sciences, Beijing, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5112","5115","Fusion of high-spatial-resolution (HSR) multispectral (MS) and Panchromatic (PAN) images has become a research focus with the development of HSR remote sensing technology. Previous research demonstrated that improving the fused versions of mixed pixels (MPs) is effective for improving the quality of fused products generated by some fusion methods based on component substitution. In this work, two pansharpening methods based on multiresolution analysis were improved through considering the fusion of MPs based on image segmentation. The improved methods were compared with several other state-of-the-art fusion methods using a fusion experiment using two datasets recorded by WorldView-2 and GeoEye-1, respectively. Experimental results showed that the proposed method offers the lowest spectral distortions and more sharpened boundaries between different images objects than other methods, especially for boundaries between vegetation and other non-vegetation objects.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518081","pansharpening;mixed pixels;multiresolution analysis","Image segmentation;Remote sensing;Distortion;Earth;Vegetation mapping;Image resolution;Multiresolution analysis","geophysical image processing;image fusion;image resolution;image segmentation;remote sensing;vegetation","MRA-based pansharpening methods;HSR remote sensing technology;mixed pixels;multiresolution analysis;image segmentation;state-of-the-art fusion methods;fusion experiment;vegetation;panchromatic image;high-spatial-resolution multispectral image;WorldView-2;GeoEye-1","","4","","16","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"The IEEE GRSS data and algorithm standard evaluation (DASE) website: Incrementally building a standardized assessment for algorithm performance","F. Dell'Acqua; G. C. Iannelli; J. Kerekes; G. Moser; L. Pierce; E. Goldoni","University of Pavia, Pavia, Italy; Ticinum Aerospace srl, Pavia, Italy; Rochester Institute of Technology, Rochester, NY, USA; University of Genoa, Genoa, Italy; University of Michigan, Ann Arbor, MI, USA; IT consultant, Mantova, Italy","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","2601","2608","In order to ensure homogeneity in performance assessment of proposed algorithms for information extraction in the Earth Observation (EO) domain, standardized remotely sensed datasets are particularly useful and welcome. Fully aware of this principle, the IEEE Geoscience and Remote Sensing Society (GRSS) and especially its Image Analysis and Data Fusion Technical Committee (IADF), has been organizing for some years now the Data Fusion Contest (DFC). In the DFC, one specific dataset is made available to the scientific community, which can download it and use it to test its newly developed algorithms. The consistence of the starting dataset across participating groups ensures the significance of assessing and ranking results, to finally proclaim the winner who scored the highest. More recently, the IEEE GRSS has provided one more contribution to the standardization effort by building the Data and Algorithm Standard Evaluation (DASE) website. DASE can distribute to registered users a limited set of possible “standard” open datasets, together with some ground truth info, and automatically assess the processing results provided by the users. In this paper we report on the birth of this initiative and present some recently introduced features.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127528","","Algorithm design and analysis;Standards;Remote sensing;Object detection;Electronic mail;Web sites;Data integration","geophysical image processing;image fusion;remote sensing","DASE website;Data and Algorithm Standard Evaluation;IEEE Geoscience and Remote Sensing Society;IADF;Image Analysis and Data Fusion Technical Committee;IEEE GRSS data;DFC;Data Fusion Contest;Earth Observation domain","","2","","6","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"A Extremely Fast Spatio-Temporal Fusion Method for Remotely Sensed Images","Y. Li; J. Li; Z. Shaoquan","Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Jiangxi Province Key Laboratory of Water Information Cooperative Sensing and Intelligent Processing, Nanchang Institute of Technology, Nanchang, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4452","4455","Spatio-temporal fusion has been developed to generate the synthetic remote sensing data with high spatial resolution and high temporal resolution simultaneously. To date, a number of spatio-temporal fusion methods have been developed and most of them are remarkable. However, most the methods are designed to achieve better fusion performance and higher fusion accuracy, while the fusion speed is always ignored. As a matter of fact, most current spatio-temporal fusion methods are time-consuming. To address this defect of spatiotemporal fusion, in this paper we propose a extremely fast spatiotemporal fusion method. The core idea of this method is extracting the spatial information from the prior high-spatial-resolution images and embedding that into the low-spatial-resolution images by local normalization to predict the missing high-spatial-resolution images. In the experiments, two dataset, including a Landsat-MODIS dataset and a Sentienl-MODIS dataset, are adopted to testing this method. The experimental results demonstrate this method can achieve great performance with extremely fast speed.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554794","National Natural Science Foundation of China(grant numbers:61901208); China Postdoctoral Science Foundation(grant numbers:2020M672483); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554794","Spatio-temporal fusion;fast;Landsat;MODIS;Sentinel","Earth;Artificial satellites;Spatiotemporal phenomena;Data mining;Spatial resolution;Remote sensing;Image reconstruction","environmental monitoring (geophysics);geophysical image processing;geophysical techniques;image fusion;image resolution;remote sensing;sensor fusion;spatiotemporal phenomena","extremely fast spatio-temporal fusion method;remotely sensed images;synthetic remote sensing data;high spatial resolution;high temporal resolution;fusion performance;higher fusion accuracy;fusion speed;current spatio-temporal fusion methods;extremely fast spatiotemporal fusion method;low-spatial-resolution images;missing high-spatial-resolution images;extremely fast speed","","1","","14","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A new Pansharpening Method with Multi-Scale Structure Perception","Y. Pan; X. Li; A. Gao; L. Li; S. Mei; S. Yue","School of Electronics and Information, Northwestern Polytechnical University, P. R. China; School of Electronics and Information, Northwestern Polytechnical University, P. R. China; School of Electronics and Information, Northwestern Polytechnical University, P. R. China; School of Electronics and Information, Northwestern Polytechnical University, P. R. China; School of Electronics and Information, Northwestern Polytechnical University, P. R. China; School of Computer Science, University of Lincoln, Lincoln, UK","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","8046","8049","The remote sensing images provided by satellites usually contain complex earth objects with different scales. In the fusion of such images, most of the existing filtering-based pansharpening methods often suffer from spectral and/or spatial information distortions due to the inaccuracy of the detail extraction. Motivated by this, we propose an effective and straightforward multi-scale structure perception pansharpening method, which uses the structure-preserving filter with great structure-aware ability to progressively perceive the structures and accurately extract the details. The experiment is carried out on GeoEye-1 satellite images. Visual and objective analysis show that our method can produce high-quality pansharpened results and outperform some existing methods.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518250","Pansharpening;structure-preserving filter;panchromatic;multispectral","Information filters;Satellites;Filtering algorithms;Feature extraction;Remote sensing;Indexes","geophysical image processing;image fusion;image resolution;remote sensing","remote sensing images;complex earth;spectral information distortions;spatial information distortions;multiscale structure perception pansharpening method;structure-preserving filter;GeoEye-1 satellite images;visual analysis","","1","","12","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Landsat and MODIS Data Fusion products based phenology analysis of dryland in Shan Dong province","X. Zhuang; S. Zhao; X. Li; D. Cong","Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, Nanjing University, Nanjing, China; Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, Nanjing University, Nanjing, China; Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, Nanjing University, Nanjing, China; Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, Nanjing University, Nanjing, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","2590","2593","Current remote sensing sensors can only provide data either with high spatial resolution or with high temporal resolutions. It's a challenge to fully characterize the patchy phenology change in the dryland region. The Flexible Spatiotemporal Data Fusion (FSDAF) method, can produce synthesized frequent high spatial resolution images through blending Landsat 30 m data with MODIS 500 m data to produce synthetic imagery at Landsat spatial resolution and MODIS time steps. In this study, we evaluated the feasibility of using FSDAF to produce the synthetic imagery over a dryland vegetation study area in Shan Dong province, in order to track the phenological changes. In this study we assembled subsets of six Landsat-5 TM scenes and temporally-coincident MODIS datasets(MOD09Q1) spanning the 2001 January-December including the growing season in Shan Dong, which is noted as a crop producing province in china. In order to investigate the effects of temporal compositing and explore the goodness of the algorithm, we also adopt the spatial and temporal adaptive reflectance fusion model (STARFM). The STARFM and FSDAF algorithm both were applied to each MODIS data series to produce up to twelve synthetic images corresponding to each Landsat image. The accuracy of the synthetic images were evaluated by comparing the reflectance values with the corresponding pixel values of the reference Landsat image on a band-by-band basis. Our results indicate the effect of the FSDA algorithm in improving spatial and temporal resolution, although the results is slightly better, when compared with the STARFM model. This study demonstrates the feasibility of using FSDAF algorithm to assemble an imagery time series at MODIS temporal resolution and Landsat spatial resolution in crop-populated dryland ecosystems.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729669","FSDAF;Shan Dong;phenology analysis","Remote sensing;Satellites;Earth;MODIS;Spatial resolution;Reflectivity","ecology;geophysical image processing;image fusion;phenology;remote sensing;vegetation","Landsat data;MODIS data;data fusion product;phenology analysis;Shan Dong province;China;remote sensing sensor;phenology change;dryland region;flexible spatiotemporal data fusion;FSDAF method;synthetic imagery;Landsat spatial resolution;dryland vegetation study area;Landsat-5 TM scene;AD 2001 01 to 12;growing season;crop producing province;spatiotemporal adaptive reflectance fusion model;Landsat image;synthetic image accuracy;imagery time series;MODIS temporal resolution;crop-populated dryland ecosystem","","1","","3","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"How to combine lidar and very high resolution multispectral images for forest stand segmentation?","C. Dechesne; C. Mallet; A. Le Bris; V. Gouet-Brunet","Univ. Paris-Est, LASTIG MATIS, IGN, ENSG, Saint-Mande, France; Univ. Paris-Est, LASTIG MATIS, IGN, ENSG, Saint-Mande, France; Univ. Paris-Est, LASTIG MATIS, IGN, ENSG, Saint-Mande, France; Univ. Paris-Est, LASTIG MATIS, IGN, ENSG, Saint-Mande, France","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","2772","2775","Forest stands are a basic unit of analysis for forest inventory and mapping. Stands are defined as large forested areas of homogeneous tree species composition and age. Their accurate delineation is usually performed by human operators through visual analysis of very high resolution (VHR) infra-red and visible images. This task is tedious, highly time consuming, and needs to be automated for scalability and efficient updating purposes. The most appropriate fusion of two remote sensing modalities (lidar and multispectral images) is investigated here. The multispectral images give information about the tree species while 3D lidar point clouds provide geometric information. The fusion is operated at three different levels within a semantic segmentation workflow: over-segmentation, classification, and regularization. Results show that over-segmentation can be performed either on lidar or optical images without performance loss or gain, whereas fusion is mandatory for efficient semantic segmentation. Eventually, the fusion strategy dictates the composition and nature of the forest stands, assessing the high versatility of our approach.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127572","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127572","Lidar;multispectral imagery;fusion;forest stands;classification;segmentation","Laser radar;Image segmentation;Vegetation;Remote sensing;Feature extraction;Databases;Three-dimensional displays","forestry;geophysical image processing;image classification;image fusion;image segmentation;optical radar;remote sensing by laser beam;vegetation;vegetation mapping","forest mapping;infrared images;3D lidar point clouds;tree species;lidar image;very-high-resolution multispectral images;fusion strategy;visual analysis;homogeneous tree species composition;forested areas;forest inventory;forest stands;optical images;semantic segmentation workflow;remote sensing modalities","","1","","14","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"A New Active Image Captioning Fusion Strategy","G. Hoxha; A. Munari; F. Melgani","Department of Information Engineering and Computer science, University of Trento, Trento, Italy; Department of Information Engineering and Computer science, University of Trento, Trento, Italy; Department of Information Engineering and Computer science, University of Trento, Trento, Italy","2022 IEEE Mediterranean and Middle-East Geoscience and Remote Sensing Symposium (M2GARSS)","9 Aug 2022","2022","","","1","4","Image captioning aims to describe the content of an image through a textual description including attributes and relationships of detected objects. In remote sensing (RS) community IC is becoming an interesting solution for the study of very high spatial resolution (VHR) images that are characterized by high-level information detail. RSIC systems are developed in a supervised way where annotated samples are needed to train the system. However, obtaining a large amount of annotated samples is time-consuming and costly. To address this issue, in this work we propose an active learning solution to select the most important samples to label and include in the training set with the aim of maintaining the system's accuracy as high as possible while using a few amount of training samples. The most important samples are selected based on decision uncertainty and diversity criteria. Experimental results show that the proposed active learning solution represents a good trade-off between the number of training samples and the accuracy of the system.","","978-1-6654-2795-1","10.1109/M2GARSS52314.2022.9840136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9840136","Active learning;convolutional neural networks;image captioning;recurrent neural networks;very–high resolution (VHR) images","Training;Integrated circuits;Uncertainty;Labeling;Spatial resolution;Remote sensing","geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing","textual description;remote sensing community;very high spatial resolution images;RSIC systems;active learning solution;training samples;active image captioning fusion strategy","","","","17","IEEE","9 Aug 2022","","","IEEE","IEEE Conferences"
"Ghost-Free Fusion of Multi-Exposure Images in the Global Gradient Region Under Patch Alignment","Y. Wang; M. Liu; X. Chen; E. Zhao","State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xian, China; Center for Hyperspectral Imaging in Remote Sensing (CHIRS), Information Science and Technology College, Dalian Maritime University, Dalian, China; Center for Hyperspectral Imaging in Remote Sensing (CHIRS), Information Science and Technology College, Dalian Maritime University, Dalian, China; Center for Hyperspectral Imaging in Remote Sensing (CHIRS), Information Science and Technology College, Dalian Maritime University, Dalian, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","5441","5444","High dynamic range (HDR) technology is one of the most widely used ways to improve image quality, and fusion of a series of low dynamic range (LDR) images is the main measure to obtain a HDR image. However, because moving objects are often found in a series of LDR images, the fused HDR images produce ghostly shapes. In order to eliminate ghosts, this paper proposes a ghost-free multi-exposure fusion method. Firstly, aligning the moving object in the input multi-exposure sequence images with the moving object in the reference image, and the aligned sequence images are obtained. In order to consider assigning more weight to pixels in the better exposure area, two weighting functions are defined. One is to measure pixel values relative to the overall brightness and adjacent exposure images, and the other one is to reflect pixel values within a range that has a larger global gradient relative to other exposures. Based on these two weighting functions, the low exposure sequence images aligned in the Laplacian pyramid are finally fused. Through experimental comparison, the obtained image has no ghost, good visual effect, and rich details.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553602","National Nature Science Foundation of China(grant numbers:61801075,41801231); China Postdoctoral Science Foundation(grant numbers:2020M670723); State Key Laboratory of Integrated Services Networks, Xidian University(grant numbers:ISN20-15); Fundamental Research Funds for the Central Universities(grant numbers:3132019341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553602","dynamic range image;ghost free;patch alignment;global gradient;Laplace pyramid","Image quality;Laplace equations;Shape;Image color analysis;Heuristic algorithms;Dynamics;Geoscience and remote sensing","image fusion;image sequences","global gradient region;patch alignment;high dynamic range technology;image quality;low dynamic range images;HDR image;object moving;LDR images;ghost-free multiexposure fusion method;input multiexposure sequence images;reference image;larger global gradient;low exposure sequence images;global gradient relative;Laplacian pyramid;reflect pixel values;sequence images aligned;visual effect","","","","8","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A Pan-Sharpening Method based Latent Low-Rank Decomposition Model","H. Hallabia; A. B. Hamida","Digital Research Center of Sfax; University of Sfax, Tunisia","2019 IEEE 19th Mediterranean Microwave Symposium (MMS)","4 Aug 2020","2019","","","1","4","In this paper, we propose a novel method based on latent low-rank representation theory (LatLRD) for pan-sharpening, which aims to synthesize a high resolution multispectral (MS) image from a high resolution panchromatic (PAN) image and a low resolution MS image. Exploiting the property of the low-rank of the MS data, the LatLRD is first performed on the up-sampled MS image and the PAN image to reconstruct a composite image in order to preserve the spectral fidelity of MS images, while transferring spatial structures. Second, a multi-scale procedure is applied to the generated composite image from the LatLRD decomposition for extracting the spatial information. Finally, the details are injected to the up-sampled MS bands to obtain the corresponding MS image at fine resolution. Experimental results demonstrate that the proposed approach performs better than several state-of-the-art methods in enhancing the spatial quality and preserving the spectral fidelity.","2157-9830","978-1-7281-4064-3","10.1109/MMS48040.2019.9157255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9157255","Image fusion;Latent low-rank property;Detail extraction;Spectral fidelity","Spatial resolution;Remote sensing;Indexes;Signal resolution;Multiresolution analysis;Mathematical model","geophysical image processing;image fusion;image resolution;remote sensing","image fusion;spatial quality;spatial information extraction;up-sampled MS image;pan-sharpening method based latent low-rank decomposition model;MS bands;composite image reconstruction;LatLRD decomposition;spectral fidelity;PAN image;MS data;low resolution MS image;high resolution panchromatic image;high resolution multispectral image;low-rank representation theory","","","","18","IEEE","4 Aug 2020","","","IEEE","IEEE Conferences"
"A New Image and Video Fusion Method Based on Cross Bilateral Filter","D. P. Bavirisetti; G. Xiao; J. Zhao; X. Zhang; P. Wang","School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, P.R, China; School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, P.R, China; School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, P.R, China; School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, P.R, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, P.R, China","2018 21st International Conference on Information Fusion (FUSION)","6 Sep 2018","2018","","","1","8","Image fusion is quite common in applications such as digital photography, medical imaging, surveillance and remote sensing. Designing a common fusion algorithm for all image fusion applications is a challenging task. The cross bilateral filter based image fusion (CBFF) is one such general-purpose method which can be applied to both mono and multimodal image fusion applications. However, CBFF has some drawbacks. 1) It introduces artifacts or extra information into the fused image. 2) The runtime of the CBFF is high. To solve these problems and improve the performance further, we propose a new image fusion algorithm based on the cross bilateral filter by designing a simple and an efficient image fusion rule. Experiments are conducted on both images and videos. Results are analyzed using recent fusion metrics in addition to the qualitative and run time analysis. Results demonstrated that the proposed algorithm can be used as an alternative method to the CBFF.","","978-0-9964527-6-2","10.23919/ICIF.2018.8455767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8455767","Cross bilateral filter;multi-sensor;image fusion;video fusion","","filtering theory;image fusion;video signal processing","CBFF;video fusion method;medical imaging;common fusion algorithm;cross bilateral filter based image fusion;multimodal image fusion applications;fusion metrics;new image fusion algorithm;mono image fusion applications;efficient image fusion rule","","4","","17","","6 Sep 2018","","","IEEE","IEEE Conferences"
"Registration of High Resolution Sar and Optical Satellite Imagery Using Fully Convolutional Networks","S. Hoffmann; C. -A. Brust; M. Shadaydeh; J. Denzler","Computer Vision Group, Friedrich Schiller University Jena, Jena, Germany; Computer Vision Group, Friedrich Schiller University Jena, Jena, Germany; Computer Vision Group, Friedrich Schiller University Jena, Jena, Germany; Computer Vision Group, Friedrich Schiller University Jena, Jena, Germany","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","5152","5155","Multi-modal image registration is a crucial step when fusing images which show different physical/chemical properties of an object. Depending on the compared modalities and the used registration metric, this process exhibits varying reliability. We propose a deep metric based on a fully convo-lutional neural network (FCN). It is trained from scratch on SAR-optical image pairs to predict whether certain image areas are aligned or not. Tests on the affine registration of SAR and optical images showing suburban areas verify an enormous improvement of the registration accuracy in comparison to registration metrics that are based on mutual information (MI).","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898714","Remote Sensing;SAR-optical Image Registration;Fully Convolutional Network","Measurement;Optical imaging;Optical sensors;Training;Synthetic aperture radar;Image registration;Remote sensing","convolutional neural nets;geophysical image processing;image fusion;image registration;learning (artificial intelligence);optical images;synthetic aperture radar","optical satellite imagery;fully convolutional networks;multimodal image registration;image fusion;compared modalities;registration metric;convolutional neural network;SAR-optical image pairs;image areas;affine registration;optical images;registration accuracy","","8","","17","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"A Semantic Segmentation Method for High-resolution Remote Sensing Images Based on Encoder-Decoder","J. Yang; L. Zhao; J. Dang; Y. Wang; B. Yue; Z. Gu","School of Electronic and Information, Lanzhou Jiaotong University, Lanzhou, China; School of Electronic and Information, Lanzhou Jiaotong University, Lanzhou, China; School of Electronic and Information, Lanzhou Jiaotong University, Lanzhou, China; School of Electronic and Information, Lanzhou Jiaotong University, Lanzhou, China; School of Electronic and Information, Lanzhou Jiaotong University, Lanzhou, China; School of Electronic and Information, Lanzhou Jiaotong University, Lanzhou, China","2022 Tenth International Conference on Advanced Cloud and Big Data (CBD)","25 Jan 2023","2022","","","98","103","Image segmentation is a key technology in remote sensing image interpretation, and it is widely used in many fields. Aiming at the common problems of low segmentation accuracy and blurred target boundary in the semantic segmentation of high-resolution remote sensing images, a semantic segmentation method of high-resolution remote sensing images based on encoder-decoder structure is proposed, in which an attention mechanism is introduced to highlight important features, and an optimized Pyramid pooling module is used to extract multi-scale features from different layers. Finally, a multi-level and multi-scale feature fusion strategy is adopted to achieve fine-grained segmentation of high-resolution remote sensing images. The method is also compared and tested on the ISPRS Vaihingen dataset to verify the effectiveness.","","979-8-3503-0971-3","10.1109/CBD58033.2022.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024538","semantic segmentation;attention mechanism;multi-scale feature fusion;label smoothing","Smoothing methods;Semantic segmentation;Semantics;Big Data;Feature extraction;Decoding;Data mining","feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;image resolution;image segmentation;object detection;remote sensing","high-resolution remote sensing images;image segmentation;remote sensing image interpretation;semantic segmentation method","","","","18","IEEE","25 Jan 2023","","","IEEE","IEEE Conferences"
"Improved fuzzy c-means algorithm and its application to classification of remote sensing image in Chengdu city, China","D. Han; J. Ji; Y. Dai; G. Li; W. Fan; H. Chen","Shanghai Financial Information Technology Key Research Laboratory, Shanghai, China; School of Information Management and Engineering, Shanghai University of Finance and Economics, Shanghai, China; Management School, Shanghai University of International Business and Economics, Shanghai, China; School of Information Management and Engineering, Shanghai University of Finance and Economics, Shanghai, China; Pamplin Coll Business, Virginia Polytechnic Institute & State University, USA; School of Ocean and Earth Science, Tongji University, Shanghai, China","2016 International Conference on Progress in Informatics and Computing (PIC)","19 Jun 2017","2016","","","437","443","The classification recognition performance is a hot study in the field of remote sensing image. In this paper, texture feature, shape feature, radiation intensity of remote sensing image information were used to initial terrain classification. Then an improved fuzzy c-means algorithm was applied on classification, and it included optimization of determine clustering center, got the number of clustering automatically and removed the noise of image after classification. Meanwhile, as an alternative to expert knowledge, data fusion method was used, which included the fusion of aeromagnetic data, gravity data and elevation data. The empirical results showed that this method can avoid the highly dependent on domain knowledge experts in image recognition and got a better classification effect in remote sensing image.","","978-1-5090-3484-0","10.1109/PIC.2016.7949541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7949541","Fuzzy c-means algorithm;remote sensing image;image classification;data fusion","Remote sensing;Clustering algorithms;Classification algorithms;Feature extraction;Algorithm design and analysis;Correlation;Image recognition","fuzzy set theory;geophysical image processing;image classification;image fusion;optimisation;pattern clustering;remote sensing","improved fuzzy c-means algorithm;remote sensing image classification;Chengdu city;China;texture feature;shape feature;radiation intensity;initial terrain classification;remote sensing image information;determine clustering center optimization;aeromagnetic data fusion method;gravity data fusion method;elevation data fusion method;image recognition","","2","","18","IEEE","19 Jun 2017","","","IEEE","IEEE Conferences"
"Method of Remote Sensing Image Enhancement in NSST Domain Based on Multi-stages Particle Swarm Optimization","D. Sheng; Y. Wu","College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Ministry of Land and Resources, Chinese Academy of Geological Sciences, Beijina, China","2017 2nd International Conference on Multimedia and Image Processing (ICMIP)","25 Dec 2017","2017","","","161","166","To further improve the definition and contrast of remote sensing images, a method of remote sensing image enhancement in non-subsampled shearlet transform (NSST) domain is proposed based on multi-stages particle swarm optimization (MSPSO) algorithm and fuzzy sets. Firstly, the image to be enhanced is decomposed into a low-frequency sub-band and several high-frequency sub-bands through NSST. Secondly, the coefficients of high-frequency sub-bands are enhanced according to adaptive Bayesian threshold method and nonlinear gain function, while that of the low-frequency sub-band is processed by using the fuzzy enhancement method with its fuzzy parameters optimized by MSPSO algorithm. A comparison is made among the proposed method, bidirectional histogram equalization method, stationary wavelet transform method, non-subsampled contourlet transform (NSCT) adaptive threshold method and artificial bee colony (ABC) optimization method in NSCT domain in terms of the subjective visual effect and objective quantitative evaluation indices such as contrast gain, definition gain and information entropy. Experimental results show that the method proposed in this paper can effectively improve the contrast and definition of remote sensing images and enhance edges details with better visual effect.","","978-1-5090-5954-6","10.1109/ICMIP.2017.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8221091","remote sensing image enhancement; non-subsampled shearlet transform (NSST); fuzzy sets; multi-stages particle swarm optimization (MSPSO); Bayesian threshold method","Transforms;Image enhancement;Remote sensing;Particle swarm optimization;Fuzzy sets;Bayes methods;Image edge detection","artificial bee colony algorithm;edge detection;entropy;fuzzy set theory;geophysical image processing;image denoising;image enhancement;image fusion;particle swarm optimisation;remote sensing;transforms;wavelet transforms","remote sensing image enhancement;NSST domain;multistages particle swarm optimization;remote sensing images;fuzzy sets;adaptive Bayesian threshold method;fuzzy enhancement method;NSCT domain;nonsubsampled shearlet transform;NSST;artificial bee colony optimization;nonsubsampled contourlet transform adaptive threshold;NSCT adaptive threshold;bidirectional histogram equalization;ABC optimization;MPSO","","","","14","IEEE","25 Dec 2017","","","IEEE","IEEE Conferences"
"Research on fusion strategy of ascending and descending multi-baseline multi-frequency insar results to generate high quality DEM","Z. Qiming; Z. Xiaojie; J. Jian","Institute of Remote Sensing and Geographic Information System, Peking University, Beijing, China; Institute of Remote Sensing and Geographic Information System, Peking University, Beijing, China; Institute of Remote Sensing and Geographic Information System, Peking University, Beijing, China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","5685","5688","Compared with DEM generated from single frequency or single baseline repeat pass inteferometric SAR data, the quality of DEM fused with multi-baseline multi-frequency InSAR has been improved effectively. However, the quality of fused DEM by using same orbital and illuminating direction has still been constrained by some factors, among of which the layover and shadow effects are the most important ones, especially in areas where topography reliefs change rapidly. In this paper, we proposed an innovation method to fuse ascending and descending multi-baseline multi-frequency InSAR results to generate higher quality DEMs, which included how to access the effective information in the DEMs generated by using ascending and descending interferometric data, and to complement each other to eliminate potential deficient in layover and shadow areas. The key point of the fusion strategy was to set fusion weight for each resolution element according to the reliabilities, effective mean coherences and resolution ratios of ascending and descending DEMs to be fused. Experiments with real SAR data proved that the proposed method could improve the DEM quality effectively.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128298","Fusion;Ascending;Descending;DEM;InSAR","Indexes;5G mobile communication;Azimuth;Economic indicators","digital elevation models;geophysical image processing;image fusion;radar interferometry;remote sensing by radar;synthetic aperture radar;topography (Earth)","fusion strategy;fused DEM;DEM quality;inteferometric SAR data;multibaseline multifrequency InSAR","","1","","9","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Patch Based Pansharpening Using Weighted Nuclear Norm Minimization","K. Zhang; F. Zhang","School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","620","623","This paper proposed a multispectral (MS) and panchromatic (PAN) image fusion method based on low-rank assumption captured by weighted nuclear norm minimization (WNNM). In this method, low-rank matrix factorization is considered to model the relationship between low spatial resolution (LR) and high spatial resolution (HR) MS images. In the formulation, MS and PAN images are partitioned into patches and then clustered to further guarantee the low-rank property. Besides, WNNM is used to capture the prior about singular values, in which larger singular values are shrunked with smaller weights. By WNNM, the spatial details in MS images can be well enhanced. Finally, the fusion model is established by combining the low-rank matrix factorization with the fidelity term about PAN image. The experimental results on degraded and real datasets demonstrate the effectiveness of the proposed method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899226","Weighted nuclear norm minimization;multispectral;panchromatic;image fusion","Spatial resolution;Degradation;Minimization;Image fusion;Clustering algorithms;Satellites;Optimization","geophysical image processing;image denoising;image fusion;image resolution;matrix algebra;matrix decomposition;minimisation;singular value decomposition","weighted nuclear norm minimization;low-rank assumption;WNNM;low-rank matrix factorization;low spatial resolution;high spatial resolution;MS images;PAN image;low-rank property;larger singular values;smaller weights;spatial details;fusion model","","","","10","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Evaluation of various pansharpening methods using image quality metrics","A. D. Dhore; C. S. Veena","Department of Electronics & Communication Engineering, Technocrats Institute of Technology, Bhopal, India; Department of Electronics & Communication Engineering, Technocrats Institute of Technology, Bhopal, India","2015 2nd International Conference on Electronics and Communication Systems (ICECS)","18 Jun 2015","2015","","","871","877","The pansharpening has been a wide area of interest during these days because of its applications in remote sensing, geoscience. Day by day it is going deep, vast and interesting as well. The concept arises due to the fact that data provided by most earth observation satellites such as Ikonos, geoeye, quickbird and wordview2 are composed of several channels of multispectral image and single channel of panchromatic image. The pansharpening is derived from the fusion of these two images, which actually combines the characteristics of these two images. The image quality metrics provide the information about the special and spectral quality of the image. These special and spectral qualities are provided by the panchromatic and multispectral image. Most of the existing pan-sharpening quality assessment methods consider only the spectral quality and there are just few inventions which concentrate on these spatial characteristics. This paper presents a novel approach for spatial quality evaluation of pan-sharpening in high resolution satellite imagery. Obtained results clearly show the wide spatial discrepancy in quality of Pan-sharpened images, resulting from various pan-sharpening fusion methods which confirm the need for spatial quality valuation of fused products.","","978-1-4799-7225-8","10.1109/ECS.2015.7125039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7125039","image fusion;multispectral (MS) image;panchromatic (PAN) image;remote sensing;special resolution;spectral resolution;pansharpening;etc","Spatial resolution;Image color analysis;Distortion;Satellites;Wavelet transforms;Principal component analysis","geophysical image processing;image fusion;image resolution;remote sensing","pansharpening fusion methods;image quality metrics;remote sensing;geoscience;Earth observation satellites;Ikonos;geoeye;quickbird;wordview2;panchromatic image;multispectral image;special quality;spectral quality;pansharpening quality assessment methods;spatial quality evaluation;high resolution satellite imagery;special resolution;spectral resolution","","3","","14","IEEE","18 Jun 2015","","","IEEE","IEEE Conferences"
"Hyperspectral change detection by using IR-MAD and synthetic image fusion","J. Choi; B. Wang; G. Kim; Y. Han","School of Civil Engineering, Chungbuk National University, Cheongju, Korea; School of Civil Engineering, Chungbuk National University, Cheongju, Korea; School of Civil Engineering, Chungbuk National University, Cheongju, Korea; Center for Information and Communication Technology, Trento, Italy","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","1658","1661","We propose a modified IR-MAD based on the generation of synthetically fused images in order to minimize the effect of change detection results corresponding to noise and feature reduction. Synthetically fused hyperspectral images were first generated using a cross-sharpening algorithm. MAD variates according to each pair of synthetically fused images were then calculated to reduce the influence of data noise in the hyperspectral image. In particular, we applied the integration of MAD variates in this study. To evaluate the performance of our algorithm, we constructed a hyperspectral dataset using the Hyperion sensor and analyzed the data noise and bands of principal components.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326104","Change detection;data noise;IR-MAD;synthetically fused hyperspectral images","Hyperspectral imaging;Change detection algorithms;Algorithm design and analysis;Indexes;Satellites;Satellite broadcasting","geophysical image processing;hyperspectral imaging;image fusion","hyperspectral change detection;modified IR-MAD;synthetic image fusion;synthetically fused hyperspectral images;cross-sharpening algorithm;data noise;hyperspectral dataset;Hyperion sensor;principal components","","2","","6","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Hyperspectral and Multispectral Image Fusion Based on Spectral Matching in the Shearlet Domain","H. Rezaei; A. Karami; P. Scheunders","Faculty of Physics, Shahid Bahonar University of Kerman, Kerman, Iran; Visionlab, University of Antwerp, Belgium; Visionlab, University of Antwerp, Belgium","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","8070","8073","In this paper, a new method for spatial resolution enhancement of hyperspectral images (HSI), based on the non-subsampled shearlet transform (NSST) is introduced. The proposed method integrates a high spectral resolution HSI with a high spatial resolution multispectral image (MSI) of the same scene. First, the HSI is spatially upsampled by means of a bicubic interpolation. Second, a 2D NSST is applied to each spectral band of the upsampled HSI and the MSI respectively. Third, the spectral coverage regions of HSI and MSI are matched and the detail shearlet coefficients of the HSI bands are replaced by detail shearlet information of the MSI, based on the spectral matching of both sensors. The proposed method is applied to real datasets and compared with some state-of-the-art fusion algorithms. The obtained results show that the proposed method significantly increases the spatial resolution while preserving the spectral content of the HSI.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518922","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518922","Hyperspectral Images;Multispectral Images;Shearlet Transform;Fusion","Spatial resolution;Sensors;Transforms;Two dimensional displays;Hyperspectral imaging","geophysical image processing;hyperspectral imaging;image fusion;image resolution;interpolation;transforms","MSI;spectral matching;multispectral image fusion;shearlet domain;spatial resolution enhancement;hyperspectral images;spectral band;upsampled HSI;spectral coverage regions;fusion algorithms;nonsubsampled shearlet transform;shearlet coefficients;spectral resolution HSI;bicubic interpolation;2D NSST;shearlet information","","2","","6","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Super Resolution Detection Method of Moving Object based on Optical Image Fusion with MMW Radar","Z. Deng; Z. Cui; Z. Cao","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1900","1903","Due to the defects of millimeter wave (MMW) radar in angle resolution, with the increase of detection distance, the ability of radar to distinguish adjacent objects in azimuth direction will be weakened, resulting in the loss and misestimation of objects information. To solve this problem, radar and optical image object detection methods are fused. The information obtained from the optical image data is used to provide a prior information for radar signal processing algorithm, so as to improve the performance of radar object detection system. Based on the simulations and experiments in a variety of multi-object moving scenes, it shows that the fusion method can improve the detection accuracy of the system and restore the spatial location of the objects more accurately than traditional ways.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883395","millimeter wave radar;computer vision;sensor fusion;DOA;object detection","Laser radar;Radar detection;Signal processing algorithms;Object detection;Radar imaging;Millimeter wave radar;Optical imaging","image fusion;image resolution;object detection;optical images;radar signal processing","super resolution detection method;optical image fusion;MMW radar;millimeter wave radar;angle resolution;detection distance;adjacent objects;azimuth direction;misestimation;objects information;optical image object detection methods;optical image data;radar signal processing algorithm;radar object detection system;multiobject moving scenes;fusion method;detection accuracy","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"SEN23E: A Cloudless Geo-Referenced Multi-Spectral Sentinel-2/Sentinel-3 Dataset for Data Fusion Analysis","D. Ibañez; R. Fernandez-Beltran; F. Pla","Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1448","1451","The availability of geo-referenced coupled data of dif-ferent platforms is essential to train remote sensing (RS) multi-modal classification and bio-phyiscal parameter esti-mation learning methods. To properly develop a general-izing model different scenes and topographies are required. For this purpose, different multi-modal datasets have been published for the last years. Nevertheless, to our knowl-edge there is not any dataset composed of Sentinel-2 (S2) and Sentinel-3 (S3) geo-referenced images. In this paper we present SEN23, a dataset composed of 100 complete multi-spectral S2 and S3 paired images of different locations along Europe from the 2021 summer. The coupled images were obtained with a time difference of three or less days, containing less than a 1 % of cloud coverage and have a resolution difference of × 15. SEN23E is expected to help with the development of new multi-spectral, multi-resolution and multi-modal models for complex tasks which need con-text and complete images. SEN23E will be available at https://github.com/ibanezdf/SEN23E.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883867","Dataset;Data Fusion;Remote Sensing;Multi-spectral;Sentinel-2 (S2);Sentinel-3 (S3)","Learning systems;Image resolution;Biological system modeling;Europe;Data integration;Surfaces;Sensors","geophysical image processing;hydrological techniques;image classification;image fusion;learning (artificial intelligence);remote sensing;sensor fusion","data fusion analysis;geo-referenced coupled data;dif-ferent platforms;remote sensing multimodal classification;bio-phyiscal parameter esti-mation;model different scenes;topographies;different multimodal datasets;Sentinel-3 geo-referenced images;SEN23;100 complete multispectral;coupled images;time difference;resolution difference;multiresolution;multimodal models","","","","12","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Classication Of Land-Cover Through Machine Learning Algorithms For Fusion Of Sentinel-2a And Planetscope Imagery","M. A. Z. Aguilera","Master in Information and Communications Sciences - Engineering Faculty-, University Distrital Francisco Jose de Caldas, Bogota, Colombia","2020 IEEE Latin American GRSS & ISPRS Remote Sensing Conference (LAGIRS)","12 Aug 2020","2020","","","246","253","To monitor and manage the changes in the land use and land cover, is vital the process of classification; machine learning offers the potential for effective and efficient classification of remotely sensed imagery. However, not many articles have explicitly dealt with the effects of image fusion on land-cover classification accuracy. Although some studies have compared thematic mapping accuracy produced using different classification algorithms, there are no currently many studies that utilize image fusion for assessing different machine learning algorithms for classification purposes. The main aim of this study is to compare different machine learning algorithm for pixel classification of imagery fused with sensors Sentinel-2A and PlanetScope. The method used for image fusion is a variational model, the high spectral resolution of Sentinel-2A imagery and the high spatial resolution of PlanetScope imagery was fused; the machine learning algorithms evaluated are six that have been widely used in the remote sensing community: DT (Decision Tree), Boosted DT, RF (Random Forest), SVM radial base (Support Vector Machine), ANN (Artificial Neural Networks), KNN (k-Nearest Neighbors), for the classification four spectral indices (NDVI, NDMI, NDBI, MSAVI) were included, derived of the image fusion. The results show that the highest accuracy was produced by SVM radial base (OA: 87.8%, Kappa: 87%) respect to the other methods, nevertheless the methods RF, Boosted DT and KNN shown to be very powerful methods for classification of the study area.","","978-1-7281-4350-7","10.1109/LAGIRS48042.2020.9165632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165632","Land-cover classification;Machine Learning;Sentinel-2A;PlanetScope;Fusion Images","Image fusion;Support vector machines;Spatial resolution;Machine learning algorithms;Earth;Machine learning;Remote sensing","decision trees;geophysical image processing;image classification;image fusion;learning (artificial intelligence);neural nets;random forests;remote sensing;support vector machines;terrain mapping","pixel classification;high spectral resolution;high spatial resolution;PlanetScope imagery;boosted DT;SVM radial base;support vector machine;land use;remotely sensed imagery;land-cover classification accuracy;mapping accuracy;image fusion;machine learning algorithm;Sentinel-2A;PlanetScope;decision tree;random forest;RF;ANN;artificial neural networks;KNN;k-nearest neighbors;NDVI spectral index;NDMI spectral index;NDBI spectral index;NDBI spectral index;MSAVI spectral index","","3","","38","IEEE","12 Aug 2020","","","IEEE","IEEE Conferences"
"A novel and simple method for panchromatic sharpening","D. E. Canbay","Istanbul Teknik Universitesi, Istanbul, TR","2018 26th Signal Processing and Communications Applications Conference (SIU)","9 Jul 2018","2018","","","1","3","Panchromatic sharpening methods are frequently used in remote sensing technologies where satellite images are processed to obtain more useful forms. Obtaining high resolution images, by combining the images collected via different sensors with various wave lengths and resolutions, is very useful in imaging water resources, agricultural areas, forests, etc. In this study, application of a simple and very fast panchromatic sharpening method is presented including a comparative analysis (in terms of quality metrics) with the existing methods in the literature.","","978-1-5386-1501-0","10.1109/SIU.2018.8404640","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8404640","Panchromatic sharpening;IHS;Brovey;YUV420;remote sensing;satellite image;LANDSAT","Remote sensing;Artificial satellites;Earth;Principal component analysis;Satellites;Image resolution;Measurement","geophysical image processing;geophysical signal processing;hydrological techniques;image fusion;image resolution;principal component analysis;remote sensing;water resources","simple method;panchromatic sharpening methods;remote sensing technologies;satellite images;high resolution images;wave lengths;resolutions;simple sharpening method;very fast panchromatic sharpening method;novel method;imaging water resources;agricultural areas;forests;comparative analysis","","2","","","IEEE","9 Jul 2018","","","IEEE","IEEE Conferences"
"Radar and Optical Image Fusion using Airborne Sensor Data from the Heligoland Island","H. Anglberger; J. Fischer; D. Frommholz","DLR, Microwaves and Radar Institute, Oberpfaffenhofen, Wessling, Germany; DLR, Microwaves and Radar Institute, Oberpfaffenhofen, Wessling, Germany; DLR, Institute of Optical Sensor Systems, Berlin, Germany","2018 19th International Radar Symposium (IRS)","30 Aug 2018","2018","","","1","7","An accurate geometrical alignment of remote sensing data is the basis for higher-level image processing techniques used to extract information. Fusing radar image data with other sensor data sources states a special case because the coordinate system is based on the measured range which causes ambiguous regions due to layover effects. An accurate 3D representation of the scene is essential to find a fitting geometrical transformation between the respective sensor image spaces. This paper applies a method that accurately maps detailed 3D information of the German island of Heligoland to the slant-range-based coordinate system of radar images imaged by DLR's airborne F-SAR sensor. The highly accurate 3D information along with optical imagery has been acquired by DLR's airborne optical sensor system MACS.","2155-5753","978-3-7369-9545-1","10.23919/IRS.2018.8448211","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8448211","","Optical sensors;Laser radar;Radar imaging;Optical imaging;Optical scattering;Three-dimensional displays","geophysical image processing;image fusion;image representation;optical images;radar imaging;radar receivers;remote sensing by radar;synthetic aperture radar","coordinate system;ambiguous regions;fitting geometrical transformation;radar images;optical imagery;airborne sensor data;Heligoland island;remote sensing data;higher-level image processing techniques;radar image data;3D representation;sensor image spaces;DLR airborne F-SAR sensor;DLR airborne optical sensor system MACS;geometrical alignment","","1","","6","","30 Aug 2018","","","IEEE","IEEE Conferences"
"Pixel-level Visual and Thermal Images Fusion Using Maximum and Minimum Value Selection Strategy","M. K. Panda; B. N. Subudhi; T. Veerakumar; M. S. Gaur","Department of EE, IIT, Jammu, India; Department of EE, IIT, Jammu, India; Department of ECE, NIT, Ponda, Goa, India; Department of CSE, IIT, Jammu, India","2020 IEEE International Symposium on Sustainable Energy, Signal Processing and Cyber Security (iSSSC)","25 Feb 2021","2020","","","1","6","In this paper, an effective pixel-level fusion algorithm is presented for visible and thermal images obtained from different sensors. The fused image may contain more essential information for human as well as machine intuition. Because of the said merits, pixel-level image fusion plays an essential role in military, remote sensing, and night vision applications. Initially, a center sliding window is used in the infrared image. The initial feature map is obtained by utilizing the maximum selection strategy between the pixel-intensity of visible image and block-based average of center sliding window. This initial feature map is not able to preserve the useful information, which is applicable for human visible intuition. So in the next stage, the source images are compared by using the minimum selection strategy to obtain the final feature map. Eventually, the fused image is achieved by utilizing the weighted-average technique among the feature maps. To estimate the efficiency of the proposed algorithm quantitatively, and qualitatively tests were carried out on a publicly available “TNO-database”. The experimental results of the proposed algorithm attained higher accuracy as compared to seven existing techniques in visual quality and objective assessment.","","978-1-7281-8880-5","10.1109/iSSSC50941.2020.9358864","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9358864","Infrared image;Visible image;Pixel-level image fusion","Visualization;Signal processing algorithms;Thermal sensors;Signal processing;Feature extraction;Image fusion;Visual perception","image fusion;infrared imaging","fused image;feature maps;minimum value selection strategy;effective pixel-level fusion algorithm;visible images;thermal images;machine intuition;pixel-level image fusion;night vision applications;center sliding window;infrared image;maximum selection strategy;pixel-intensity;visible image;human visible intuition;source images;minimum selection strategy;TNO-database","","2","","31","IEEE","25 Feb 2021","","","IEEE","IEEE Conferences"
"Euclidian Norm Based Fusion Strategy for Multi Focus Images","H. Shihabudeen; J. Rajeesh","Department of Electronics & Communication, College of Engineering Kidangoor, Kerala, India; Department of Electronics & Communication, College of Engineering Kidangoor, Kerala, India","2021 2nd International Conference on Advances in Computing, Communication, Embedded and Secure Systems (ACCESS)","20 Oct 2021","2021","","","222","227","Collecting salient and relevant information from many images and merging this to generate a quality image is the main goal of image fusion technique. Because of the camera's characteristics while photographing a scene, multi focus images will be produced. Each image of the scene has a different set of features and the merging leads to a good capture of the scene. Activity level measurement and fusion strategy are the critical areas of study in multi focus fusion. To find various focused information in transformed and spatial domains, there have been a lot of algorithms developed. Convolutional neural networks are excellent at representing deep features in an easier format and this property is used to represent multi focus images. Each pixel's activity map is used as a parameter in the fusion strategy. Euclidian norm are a good tool to find the similarities between a set of values. ℓ2 Euclidian norm along with activity map performs the fusion of feature maps collected by residual network. When compared to other fusion algorithms, the presented technique is efficient and improves the image quality. The merged images correlate with human visual perception. The algorithm is suitable for applications like remote sensing, surveillance, and medical diagnosis, etc.","","978-1-7281-7136-4","10.1109/ACCESS51619.2021.9563338","Centre for Engineering Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9563338","image fusion;euclidian norm;activity map;visual perception;multi focus","Photography;Surveillance;Merging;Tools;Level measurement;Convolutional neural networks;Image fusion","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image capture;image fusion;image representation","image fusion;multifocus fusion;focused information;ℓ2 Euclidian norm;image quality;merged images;relevant information;image features;scene capture;convolutional neural networks;deep features representation;multifocus image representation;pixel activity map","","1","","30","IEEE","20 Oct 2021","","","IEEE","IEEE Conferences"
"Time-Series 3D Building Change Detection Based on Belief Functions","J. Tian; J. Dezert; R. Qin","Remote Sensing Technology Institute, German Aerospace Center, Oberpfaffenhofen, Germany; ONERA - The French Aerospace Lab, Chemin de la Hunière, Palaiseau, France; The Ohio State University, Columbus, OH, USA","2018 21st International Conference on Information Fusion (FUSION)","6 Sep 2018","2018","","","1","5","One of the challenges of remote sensing image based building change detection is distinguishing building changes from other types of land cover alterations. Height information can be a great assistance for this task but its performance is limited to the quality of the height. Yet, the standard automatic methods for this task are still lacking. We propose a very high resolution stereo series data based building change detection approach that focuses on the use of time series information. In the first step, belief functions are explored to fuse the change features from the 2D and height maps to obtain an initial change detection result. In the second step, the building probability maps (BPMs) from the series data are adopted to refine the change detection results based on Dempster-Shafer theory. The final step is to fuse the series building change detection results in order to obtain a final change map. The advantages of the proposed approach are demonstrated by testing it on a set of time series data captured in North Korea.","","978-0-9964527-6-2","10.23919/ICIF.2018.8455206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8455206","Change detection;belief functions;DST;DSM","Buildings;Three-dimensional displays;Feature extraction;Indexes;Iron;Remote sensing;Time series analysis","geophysical image processing;image fusion;inference mechanisms;object detection;probability;remote sensing;time series;uncertainty handling","remote sensing image;land cover alterations;height information;standard automatic methods;high resolution stereo series data;change detection approach;time series information;building probability maps;series building change detection results;final change map;time series data;time-series 3D building change detection;North Korea","","5","","16","","6 Sep 2018","","","IEEE","IEEE Conferences"
"Panchromatic Sharpening of Multispectral Satellite Imagery Via an Explicitly Defined Convex Self-Similarity Regularization","C. -H. Wang; C. -H. Lin; J. M. Bioucas Dias; W. -C. Zheng; K. -H. Tseng","Center for Space and Remote Sensing Research, National Central University, Taiwan; Department of Electrical Engineering, National Cheng Kung University, Taiwan; Instituto de Telecomunicações, Instituto Superior Técnico, Universidade de Lisboa, Portugal; Center for Space and Remote Sensing Research, National Central University, Taiwan; Center for Space and Remote Sensing Research, National Central University, Taiwan","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3129","3132","In satellite imaging remote sensing, injecting spatial details extracted from a panchromatic image into a multispectral image is referred to as pansharpening, which is ill-posed and requires regularization. Self-similarity, a critical prior knowledge yielding great success in regularizing various imaging inverse problems, has been widely observed in natural images; its formalization is not, however, straightforward. Very recently, we mathematically described the self-similarity pattern as a weighted graph, which can then be transformed into an explicit convex regularizer, that is adopted in our pansharpening criterion design. Most importantly, such convexity allows the adoption of convex optimization theory in solving self-similarity regularized inverse problems with convergence guarantee. One step of our pansharpening algorithm is exactly the proximal operator induced by our new self-similarity regularizer, which is solved by another customized algorithm that is interesting in its own right as could be used as a denoiser. Experiments show promising performance of the proposed method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900610","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900610","Panchromatic sharpening;self-similarity regularization;convex optimization;proximal operator;denoiser","Inverse problems;Satellites;Imaging;Convergence;Remote sensing;Optimization;Noise reduction","convex programming;geophysical image processing;geophysical techniques;image denoising;image fusion;image resolution;inverse problems;remote sensing","panchromatic sharpening;multispectral satellite imagery via;explicitly defined convex self-similarity regularization;satellite imaging remote sensing;spatial details;panchromatic image;critical prior knowledge;imaging inverse problems;natural images;self-similarity pattern;explicit convex regularizer;pansharpening criterion design;convex optimization theory;self-similarity regularized inverse problems;pansharpening algorithm;self-similarity regularizer","","4","","17","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Texture variable analysis for landscape patches represented using super-resolution mapping","A. M. Muad","Department of Electrical, Universiti Kebangsaan Malaysia, Selangor, Malaysia","2017 IEEE 8th Control and System Graduate Research Colloquium (ICSGRC)","19 Oct 2017","2017","","","51","56","This paper presents the analyses of texture variables from the image enhanced using super-resolution mapping. Two widely known super-resolution mapping techniques, pixel swapping and Hopfield neural network are used. The texture analyses include land cover patches of varying sizes, shapes, and spatial pattern of patches. A time series coarse MODIS 250 images are used to improve the representation of land cover patches and reduce the spatial variability. Results show that using a fusion of time series images and properly setting the weights for the Hopfield neural network produce superior accuracy of representing the texture of land cover mapping.","","978-1-5386-0380-2","10.1109/ICSGRC.2017.8070567","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070567","Remote sensing;Hopfield neural network;pixel swapping;texture variable;landscape pattern","MODIS;Spatial resolution;Lakes;Remote sensing;Time series analysis;Earth;Satellites","geophysical image processing;Hopfield neural nets;image enhancement;image fusion;land cover;remote sensing;time series","texture variable analysis;landscape patches;super-resolution mapping techniques;Hopfield neural network;land cover patches;land cover mapping;image enhancement;pixel swapping;MODIS 250 images;time series image fusion","","","","33","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"Change Detection for Mutil-Temporal Remote Sensing Images Based on NSCT and Hierarchical Clustering","Q. Guo; J. Zhang","School of Electronics and Information Engineering, Harbin Institute of Technology, China; School of Electronics and Information Engineering, Harbin Institute of Technology, China","2019 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","16 Jan 2020","2019","","","1","4","Change detection has many applications in remote sensing, such as urban development, environment and damage monitoring and so on. Some typical methods are difficult to maintain detail information and the detection accuracy is also not satisfied. In this paper, a detail-injecting algorithm conducted by non-subsampled contourlet transform (NSCT) and hierarchical clustering is presented to preserve the detail information and increase the separability of the intermediate classes to improve the accuracy. The strategy of detail-injecting based on NSCT is to extract the detail information, and then inject the detail to difference image. After that, the residual image which have been highlighted by histogram contrast (HC) model is used as input of the strategy of hierarchical clustering to obtain the final result. Compare with some tradition methods, the experiments indicate that the proposed outperforms others in detection accuracy for remote sensing image.","","978-1-7281-1708-9","10.1109/ICSPCC46631.2019.8960739","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8960739","NSCT;change detection;hierarchical clustering;detail-injecting","","geophysical image processing;image fusion;object detection;remote sensing;transforms","hierarchical clustering;detection accuracy;remote sensing image;change detection;mutil-temporal remote sensing images;urban development;damage monitoring;nonsubsampled contourlet;intermediate classes;difference image;residual image;histogram contrast model","","","","7","IEEE","16 Jan 2020","","","IEEE","IEEE Conferences"
"Research on Worldview-3 Panchromatic and Shortwave Infrared Image Fusion Method","Q. Wang; Y. Bao","College of Big Data and Artificial Intelligence, Southwest Forestry University, Kunming, China; College of Big Data and Artificial Intelligence, Southwest Forestry University, Kunming, China","2022 International Conference on Automation, Robotics and Computer Engineering (ICARCE)","22 Feb 2023","2022","","","1","5","Worldview 3 is one of the most advanced high-resolution optical satellites. Aiming at the problems of large difference in spatial resolution between panchromatic band and short wave infrared (SWIR) band of Worldview 3 remote sensing satellite data and inconsistent spectral range, resulting in massive effect of fusion results and limited effect of spatial resolution enhancement, pannet network training is used for fusion. Firstly, the network reduces the spatial resolution of panchromatic band and realizes the preliminary integration with SWIR band; Then the preliminary fusion results are fused with the original resolution panchromatic band again. For spectral preservation, pannet adds the sampled multispectral image to the network output, which propagates the spectral information directly to the reconstructed image. The network trains the network parameters in the high pass filter domain rather than the image domain, so as to preserve the spatial structure. The results show that deep learning can achieve good results in image fusion. Pannet network structure can effectively enhance the spatial resolution of SWIR band, and also has a certain reference significance for the integration of traditional panchromatic and short wave infrared band.","","978-1-6654-7548-8","10.1109/ICARCE55724.2022.10046640","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046640","remote sensing image fusion;image fusion framework;deep learning;WorldView-3 images;SWIR;panchromatic","Optical filters;Training;Integrated optics;Satellites;Robot sensing systems;Optical imaging;Optical sensors","","","","","","14","IEEE","22 Feb 2023","","","IEEE","IEEE Conferences"
"Spatial Resolution Enhancement of Optical Images Based on Tensor Decomposition","K. Uto; M. D. Mura; J. Chanussot","School of Computing, Tokyo Institute of Technology, Yokohama, Japan; CNRS, GIPSA-lab, Grenoble, France; CNRS, GIPSA-lab, Grenoble, France","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","8058","8061","There is an inevitable trade-off between spatial and spectral resolutions in optical remote sensing images. A number of data fusion techniques of multimodal images with different spatial and spectral characteristics have been developed to generate optical images with both spatial and spectral high resolution. Although some of the techniques take the spectral and spatial blurring process into account, there is no method that attempts to retrieve an optical image with both spatial and spectral high resolution, a spectral blurring filter and a spectral response simultaneously. In this paper, we propose a new framework of spatial resolution enhancement by a fusion of multiple optical images with different characteristics based on tensor decomposition. An optical image with both spatial and spectral high resolution, together with a spatial blurring filter and a spectral response, is generated via canonical polyadic (CP) decomposition of a set of tensors. Experimental results featured that relatively reasonable results were obtained by regularization based on nonnegativity and coupling.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518769","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518769","Spatial resolution enhancement;pansharpening;tensor decomposition;canonical polyadic (CP) decomposition;coupling","Spatial resolution;Tensile stress;Optical imaging;Integrated optics;Optical sensors;Optical signal processing","geophysical image processing;image enhancement;image filtering;image fusion;image resolution;image restoration;matrix decomposition;optical images;optical information processing;remote sensing;tensors","spatial characteristics;data fusion techniques;optical image retrieval;multiple optical image fusion;canonical polyadic decomposition;spatial blurring filter;spatial resolution enhancement;spectral response;spectral blurring filter;spatial blurring process;spectral process;spectral high resolution;spectral characteristics;multimodal images;optical remote sensing images;spectral resolutions;tensor decomposition","","","","8","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Mapping Cropland Extent by Asynchronous Fusion of Optical and Active Microwave Imagery","S. Chakrabarti; T. Cormier; N. Malizia; D. Potere; D. Sulla-Menashe; K. Zmijewski; M. Friedl","Telluslabs, Inc., Somerville, Massachusetts, USA; Telluslabs, Inc., Somerville, Massachusetts, USA; Telluslabs, Inc., Somerville, Massachusetts, USA; Telluslabs, Inc., Somerville, Massachusetts, USA; Telluslabs, Inc., Somerville, Massachusetts, USA; Telluslabs, Inc., Somerville, Massachusetts, USA; Telluslabs, Inc., Somerville, Massachusetts, USA","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5319","5321","In this study, a machine learning based algorithm is developed for dynamically classifying and determining area of agricultural land-use using optical and active microwave remote sensing. It includes a gradient boosted machine trained in the US using labels derived from the United States Department of Agriculture National Agricultural Statistics Service crop land data layers (CDL). This is then applied to the state of Mato Grosso in Brazil with a hidden Markov model applied to temporally stabilize the land-cover predictions. High resolution remote sensing products such as land surface temperature, normalized difference vegetation index, nadir adjusted bidirectional reflectance, land-cover and radar backscatter were used to develop the classification model. The results in the US were validated using a hold-out set with CDL labels. The results in Brazil were validated using the harvested area statistics reported by Companhia Nacional de Abastecimento.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518892","Crop Mapping;Land Cover;Machine Learning","Agriculture;Remote sensing;Synthetic aperture radar;Biomedical optical imaging;Optical imaging;Optical sensors;MODIS","agriculture;crops;geophysical image processing;hidden Markov models;image classification;image fusion;land surface temperature;learning (artificial intelligence);terrain mapping;vegetation mapping","hidden Markov model;land-cover predictions;high resolution remote sensing products;land surface temperature;normalized difference vegetation index;nadir adjusted bidirectional reflectance;US;CDL labels;Brazil;harvested area statistics;cropland extent;asynchronous fusion;optical microwave imagery;active microwave imagery;machine learning;agricultural land-use;optical microwave remote sensing;active microwave remote sensing;Mato Grosso;United States Department of Agriculture;National Agricultural Statistics Service crop land data layers","","","","9","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"M2-Net: A Multi-scale Multi-level Feature Enhanced Network for Object Detection in Optical Remote Sensing Images","X. Ye; F. Xiong; J. Lu; H. Zhao; J. Zhou","School of Computer Science and Engineering, Nanjing University of Science and Technology, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, China; School of Software Engineering, Jinling Institute of Technology, China; School of Information and Communication Technology, Griffith University, Australia","2020 Digital Image Computing: Techniques and Applications (DICTA)","1 Mar 2021","2020","","","1","8","Object detection in remote sensing images is a challenging task due to diversified orientation, complex background, dense distribution and scale variation of objects. In this paper, we tackle this problem by proposing a novel multi-scale multi-level feature enhanced network ( M2-Net) that integrates a Feature Map Enhancement (FME) module and a Feature Fusion Block (FFB) into Rotational RetinaNet. The FME module aims to enhance the weak features by factorizing the convolutional operation into two similar branches instead of one single branch, which helps to broaden receptive field with less parameters. This module is embedded into different layers in the backbone network to capture multi-scale semantics and location information for detection. The FFB module is used to shorten the information propagation path between low-level high-resolution features in shallow layers and high-level semantic features in deep layers, facilitating more effective feature fusion and object detection especially those with small sizes. Experimental results on three benchmark datasets show that our method not only outperforms many one-stage detectors but also achieves competitive accuracy with lower time cost than two-stage detectors.","","978-1-7281-9108-9","10.1109/DICTA51227.2020.9363420","National Natural Science Foundation of China(grant numbers:62002169,2017YFB1300205,JIT-B-201717,16KJA520003,JIT-FHXM-201808); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363420","Convolutional neural network (CNN);object detection;feature fusion;remote sensing image;multi-scale analysis","Semantics;Object detection;Detectors;Feature extraction;Optical imaging;Task analysis;Remote sensing","feature extraction;geophysical image processing;image classification;image fusion;object detection;remote sensing","high-level semantic features;effective feature fusion;low-level high-resolution features;FFB module;multiscale semantics;backbone network;weak features;FME module;Feature Fusion Block;Feature Map Enhancement module;novel multiscale multilevel feature enhanced network;dense distribution;remote sensing images;optical remote sensing;Net;object detection","","1","","40","IEEE","1 Mar 2021","","","IEEE","IEEE Conferences"
"Mangrove Species Mapping and Above-Ground Biomass Estimation in Suriname Based on Fused Sentinel-1 and Sentinel-2 Imagery and National Forest Inventory Data","J. Feyen; G. Wip; S. Crabbe; V. Wortel; S. P. Sari; F. Van Coillie","Department of Environment, Remote Sensing | Spatial Analysis Lab (REMOSA), Ghent University, Ghent, Belgium; Foundation for Forest Management and Production Control, Paramaribo, Suriname; Foundation for Forest Management and Production Control, Paramaribo, Suriname; Department of Forest Management, Centre for Agricultural Research in Suriname (CELOS), Paramaribo, Suriname; Department of Environment, Remote Sensing | Spatial Analysis Lab (REMOSA), Ghent University, Ghent, Belgium; Department of Environment, Remote Sensing | Spatial Analysis Lab (REMOSA), Ghent University, Ghent, Belgium","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","6072","6075","Obtaining state-of-the art data on the Mangrove cover extent is important to monitor possible responses to environmental changes such as land use change and mangrove ecosystem degradation caused by climate change. In this study, we examined the possibility of species-specific mapping within the mangrove area in Suriname based on the fusion of Sentinel-1 and Sentinel-2 data using the Google Earth Engine platform and a Random Forest classifier. To do this, a 2-level classification scheme was developed. In the first level, the mangrove cover was discriminated from mangrove graveyards and other land cover classes (kappa index of 80.65%). In the second level, the dominating mangrove species were successfully classified within the living mangrove cover (kappa index of 75.21 %). Secondly mangrove above-ground biomass (AGB) was estimated on a national scale, based on fused Sentinel-1 and Sentinel- 2 data and national mangrove forest inventory data by using a Support Vector Regression (SVR) machine learning technique, resulting in a root mean square error (RMSE) of 32.181 Mg.ha−1 and a R2 of 0.542.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555037","European Union; UNDP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555037","mangroves;species mapping;above-ground biomass;sentinel-1;sentinel- 2;support vector regression","Support vector machines;Geoscience and remote sensing;Estimation;Forestry;Biomass;Climate change;Root mean square","forestry;geophysical image processing;image classification;image fusion;land cover;random forests;regression analysis;support vector machines;vegetation;vegetation mapping","land use change;climate change;species-specific mapping;Sentinel-2 data;Google Earth Engine;random forest classifier;mangrove graveyards;land cover classes;mangrove species;national mangrove forest inventory data;above-ground biomass estimation;Sentinel-2 imagery;Sentinel-1 imagery;support vector regression;Suriname","","1","","15","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Land Cover Mapping Based On Multi-Branch Fusion Of Object-Based And Pixel-Based Segmentation With Filtered Labels","Y. Xia; Y. Liao; H. Zhang; G. Yang","The State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, P.R., China; The State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, P.R., China; The State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, P.R., China; School of Electronic Information, Wuhan University, P.R., China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","27 Sep 2021","2020","","","7050","7053","In this paper, a multi-branch fusion framework is proposed to address the land cover mapping issue with low-resolution labels. To obtain homogeneous target objects, a multi-resolution segmentation (MRS) algorithm is applied to yield unsupervised object-based segmentation maps. Through an index-based judgement mechanism, a label filtering principle was designed and employed to screen out samples with noisy labels while retaining samples with clean labels, thus acquiring more accurate training data. A patch-to-point classification network was established based on these filtered training patches, which fully extracts the contextual features and generates pixel-based prediction results. A post-processing step, consisting of fusion and voting operations, was developed to merge the pixel-based and object-based results, and produce a final segmentation map. Verified through the competition website, the proposed method achieved an average accuracy (AA) of 57.22%, ranking second in the first track of 2020 IEEE GRSS Data Fusion Contest.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9547212","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547212","Data fusion;multi-resolution segmentation;label filtering;land cover mapping","Training;Training data;Geoscience and remote sensing;Data integration;Filtering algorithms;Information filters;Feature extraction","feature extraction;geophysical image processing;geophysical signal processing;image classification;image fusion;image resolution;image segmentation;sensor fusion;terrain mapping","pixel-based segmentation;filtered labels;multibranch fusion framework;land cover mapping issue;low-resolution labels;homogeneous target objects;multiresolution segmentation algorithm;unsupervised object-based segmentation maps;index-based judgement mechanism;label filtering principle;noisy labels;clean labels;accurate training data;patch-to-point classification network;filtered training patches;pixel-based prediction results;voting operations;object-based results;final segmentation map;2020 IEEE GRSS Data Fusion Contest","","1","","7","IEEE","27 Sep 2021","","","IEEE","IEEE Conferences"
"Detection of Moroccan coastal upwelling using the alpha blending fusion technique of sea surface chlorophyll images and sea surface temperature images","Z. El Abidi; K. Minaoui; A. Tamim; H. Laanaya","LRIT-CNRST URAC 29, Faculty of sciences In Rabat, Mohammed V university, Rabat, Morocco; LRIT-CNRST URAC 29, Rabat IT Center, Faculty of sciences In Rabat, Mohammed V university, Rabat, Morocco; Department of Marine Fisheries, Higher Institute of Marine Fisheries (ISPM), Agadir, Morocco; Rabat IT Center, Faculty of science, Mohammed V university, Rabat, Morocco","2018 4th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)","24 May 2018","2018","","","1","5","The purpose of this paper is to improve decision making on the location of the Moroccan coastal upwelling zone by using image fusion concept. In fact, this area can be detected by remote sensing from sea surface chlorophyll (SSC) or sea surface temperature images. In this context, We processed 46 images of the year 2014 for each type of the two parameters in order to combine them into a single image more informative and suitable for visual perception. So, in this work, we propose an ensemble algorithm for feature level fusion using Alpha Blending technique set to determine efficiency and comfortably the educated region. The output images has been tested and validated by the oceanographer. He demonstrated that our proposed methodology gives satisfactory and promising results.","","978-1-5386-5239-8","10.1109/ATSIP.2018.8364336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8364336","Moroccan Coastal Upwelling;Feature level fusion;Sea Surface Temperature image;Sea Surface Chlorophyll image;Alpha Blending technique","Ocean temperature;Sea surface;Sea measurements;Surface treatment;Image segmentation;Image fusion","decision making;geophysical image processing;image fusion;image sensors;ocean temperature;oceanographic regions;oceanographic techniques;remote sensing;sensor fusion","fusion technique;sea surface chlorophyll images;sea surface temperature images;Moroccan coastal upwelling zone;image fusion concept;feature level fusion;Alpha Blending technique;SSC;oceanographer","","5","","14","IEEE","24 May 2018","","","IEEE","IEEE Conferences"
"Fusion and assessment of high-resolution WorldView-3 satellite imagery using NNDiffuse and Brovey algotirhms","J. Zhao; L. Huang; H. Yang; D. Zhang; Z. Wu; J. Guo","Anhui Engineering Laboratory of Agro-Ecological Big Data, Anhui University, Hefei, China; Anhui Engineering Laboratory of Agro-Ecological Big Data, Anhui University, Hefei, China; Beijing Research Center for Information Technology in Agriculture, Beijing Academy of Agriculture and Forestry Sciences, Beijing, China; Anhui Engineering Laboratory of Agro-Ecological Big Data, Anhui University, Hefei, China; Ministry of Education, Anhui University, Hefei, China; Ministry of Education, Anhui University, Hefei, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","2606","2609","It is highly necessary to merge high spatial resolution panchromatic images with high spectral resolution multispectral images in image processing tasks and thematic applications. WorldView-3 (WV-3) imagery was investigated for generating pan-sharped multispectral imagery. Brovey and NNDiffuse pan sharpening algorithms were comparatively used to perform the image fusion and the quality was also assessed. The results show that, in comparison with Brovey, NNDiffuse pan sharpened image can generally maintain spectral and texture information. Specifically, the spectral consistency was evaluated using four typical land cover types with a performance of vegetation > waterbody > bare land > built-up area in ascending order. The relative errors were 5.13%, 5.26%, 5.99%, 28.28%, respectively. Entropy was used to evaluate the texture feature and NNDiffuse fused image had extremely similar values in Max, Mean and Stdev compared to original panchromatic image, while there were greater differences for Brovey fused image in Mean and Stdev and the relative errors were -16.04% and 64.88%. Additionally, WV-3 has more clear spatial stereo and edges compared to GF-1 with the same spatial resolution.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729673","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729673","WorldView-3;GF-1;Multi-scale fusion;Quality assessment;Image pan sharpening","Spatial resolution;Satellites;Remote sensing;Image fusion;Earth;Image edge detection","geophysical image processing;remote sensing;sensor fusion","high-resolution worldview-3 satellite imagery;NNDiffuse pan sharpening algorithms;Brovey algorithms;high spectral resolution multispectral images;image processing;multiscale fusion","","7","","14","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Fusion of RISAT-1 SAR Image and Resourcesat-2 Multispectral Images Using Wavelet Transform","S. C. Kulkarni; P. P. Rege","Dept. of Electronics and Telecommunication Engineering, College of Engineering, Pune, Pune, India; Dept. of Electronics and Telecommunication Engineering, College of Engineering, Pune, Pune, India","2019 6th International Conference on Signal Processing and Integrated Networks (SPIN)","13 May 2019","2019","","","45","52","This paper presents a pixel level wavelet-based approach to fuse synthetic aperture radar (SAR) imagery with multispectral (MS) imagery. Image fusion combines information from two or more images to generate a new image, which is rich in information. Due to complementary nature of SAR and multispectral imagery, fusion of these images is of significant interest in the field of remote sensing. The primary objective of this work is to enhance spatial information in multispectral images by injecting structural information derived from SAR image. Due to negative correlation between SAR and multispectral data, conventional component substitution methods face the problem of spectral distortion in the fused image. Wavelet based fusion approaches overcome this problem due to excellent localization in spatial and frequency domain. Here, different wavelet-based fusion rules are applied for fusion of SAR and multispectral images. Fusion rules applied to fuse approximate sub-bands and detail sub-bands of these images consider spectral dis-similarity between them. Results are evaluated visually, as well as using standard quality metrics and are compared with component substitution fusion techniques namely, principal component analysis and generalized IHS transform. Trade-off between spectral and spatial quality of fused image has been observed while fusing SAR and multispectral images.","","978-1-7281-1380-7","10.1109/SPIN.2019.8711589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8711589","Image Fusion;Remote Sensing;SAR Imagery;Multispectral Imagery;Wavelet Transform","Synthetic aperture radar;Wavelet transforms;Radar polarimetry;Spatial resolution;Remote sensing","geophysical image processing;image fusion;image resolution;principal component analysis;radar imaging;remote sensing by radar;sensor fusion;synthetic aperture radar;wavelet transforms","RISAT-1 SAR image;wavelet transform;pixel level wavelet-based approach;synthetic aperture radar imagery;multispectral imagery;image fusion;spatial information;multispectral images;multispectral data;fused image;based fusion approaches;different wavelet-based fusion rules;component substitution fusion techniques;resourcesat-2 multispectral images;remote sensing","","1","","22","IEEE","13 May 2019","","","IEEE","IEEE Conferences"
"Pan-Sharpening Via High-Pass Modification Convolutional Neural Network","J. Wang; Z. Shao; X. Huang; T. Lu; R. Zhang; J. Ma","State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University; State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University; Department of Geosciences, University of Arkansas; Wuhan Institute of Technology; State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University; the Electronic Information School, Wuhan University","2021 IEEE International Conference on Image Processing (ICIP)","23 Aug 2021","2021","","","1714","1718","Most existing deep learning-based pan-sharpening methods have several widely recognized issues, such as spectral distortion and insufficient spatial texture enhancement, we propose a novel pan-sharpening convolutional neural network based on a high-pass modification b lock. Different from existing methods, the proposed block is designed to learn the high-pass information, leading to enhance spatial information in each band of the multi-spectral-resolution images. To facilitate the generation of visually appealing pan-sharpened images, we propose a perceptual loss function and further optimize the model based on high-level features in the near-infrared space. Experiments demonstrate the superior performance of the proposed method compared to the state-of the-art pan-sharpening methods, both quantitatively and qualitatively. The proposed model is open-sourced at https://github.com/jiaming-wang/HMB.","2381-8549","978-1-6654-4115-5","10.1109/ICIP42928.2021.9506568","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506568","Residual enhancement;pan-sharpening;image fusion;deep neural networks","Image edge detection;Conferences;Neural networks;Distortion;Image restoration;Convolutional neural networks","geophysical image processing;geophysical signal processing;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","high-pass modification convolutional neural network;existing deep learning-based pan-sharpening methods;widely recognized issues;spectral distortion;insufficient spatial texture enhancement;pan-sharpening convolutional neural network;high-pass modification b lock;high-pass information;spatial information;multispectral-resolution images;visually appealing pan-sharpened images;high-level features;the-art pan-sharpening methods","","2","","19","IEEE","23 Aug 2021","","","IEEE","IEEE Conferences"
"PAN-Sharpening via residual deep learning","N. Li; N. Huang; L. Xiao","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, P. R. China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, P. R. China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, P. R. China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","5133","5136","One significant advantage of the deep convolutional neural networks (DCNN) is their representational ability for local complex structures. Inspired by this observation, a DCNN based residual learning model is proposed to learn a nonlinear mapping function between the high-resolution (HR) and low-resolution (LR) image patches. The DCNN is trained based on image patches, which are only sampled from the HR/LR panchromatic (PAN) image without other training images. We train the DCNN to obtain a nonlinear mapping function with HR/LR PAN patch pairs using mini-batch gradient descent based on back-propagation. By assuming HR/LR multispectral (MS) image shares the same mapping function between HR/LR PAN image patches from the viewpoint of transfer learning methodology, the HR MS image can be reconstructed from the observed LR MS image using the trained DCNN. Owing to the advantage of the residual learning mechanism, the proposed method can achieve a good geometrical details injection while preserves the spectral features. Experimental results show that the proposed method provides a better performance in both visual perception and numerical measures compared with the conventional methods.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128158","Residual;DCNN;multispectral(MS) image;panchromatic(PAN) image;pan-sharpening","Training;Image reconstruction;Principal component analysis;Image resolution;Machine learning;Image fusion;Remote sensing","gradient methods;image denoising;image fusion;image reconstruction;image representation;image resolution;learning (artificial intelligence);neural nets","HR/LR PAN patch pairs;HR/LR panchromatic image;nonlinear mapping function;residual learning model;deep convolutional neural networks;residual deep learning;PAN-Sharpening;residual learning mechanism;trained DCNN;observed LR MS image;HR MS image;transfer learning methodology;HR/LR PAN image patches;HR/LR multispectral image shares the same mapping function;mini-batch gradient descent","","9","","10","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Multi-Order Feature Fusion Joint Training Network for Remote Sensing Scene Classification","K. Ni; F. Zhou; Y. Wu; X. Hao","School of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","2019 International Conference on Electronic Engineering and Informatics (EEI)","13 Feb 2020","2019","","","332","336","Convolutional neural network (CNN) has shown great powerful feature representation for remote sensing scene classification. Nevertheless, the current state-of-the-art related works only focus on the first-order deep feature statistics (i.e., convolutional layer, fully connected layer), does not consider high-order feature or feature fusion for achieving the discriminative feature representation. To tackle this problem, the multi-order feature fusion joint-training network (MFJN) is presented for remote sensing scene classification. Firstly, the second-order orderless feature statistics is obtained by the parameterized vector of locally aggregated descriptors (VLAD) layer which is embedded into the CNN. Secondly, the deep feature representation of the last convolutional layer is used as the first-order feature statistics, then the convolutional layer of size is used for dimensionality reduction. Finally, multi-order feature statistics is obtained by fusing the first and second-order feature statistics, then the normalization layer is employed in the MFJN. We applied MFJN on the Aerial Image data set (AID) for scene classification, experimental results show better performances than other related works.","","978-1-7281-4076-6","10.1109/EEI48997.2019.00079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8991135","Scene classification;Convolutional neural network;feature representation;second-order;normalization layer","","feature extraction;geophysical image processing;image classification;image fusion;image representation;learning (artificial intelligence);neural nets;remote sensing","first order feature statistics;second-order feature statistics;fusion joint training network;remote sensing scene classification;convolutional neural network;great powerful feature representation;current state-of-the-art related works;first-order deep feature statistics;convolutional layer;fully connected layer;high-order feature;discriminative feature representation;fusion joint-training network;second-order orderless feature statistics;locally aggregated descriptors layer;deep feature representation;first-order feature statistics;multiorder feature statistics","","","","22","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Time series assessment of multi-source spatiotemporal fusion reconstruction data based on dynamic time warping","Y. Chen; D. Li; Q. Han; X. Zhang; Q. Zhang","College of Mining Engineering, Taiyuan University of Technology, Taiyuan, China; China Center for, Resources Satellite Data and Application, Beijing, China; China Center for, Resources Satellite Data and Application, Beijing, China; China Center for, Resources Satellite Data and Application, Beijing, China; College of Mining Engineering, Taiyuan University of Technology, Taiyuan, China","2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)","1 Sep 2020","2020","","","858","864","Time series remote sensing data provide data support for agricultural monitoring, atmospheric and hydrological research, etc. As an effective method to reconstruct time series remote sensing images, remote sensing spatiotemporal information fusion technology can supply the shortage of high spatial resolution data. Aiming at the accuracy of time series remote sensing data generated by spatiotemporal fusion model, this paper proposes a time series assessment method based on Dynamic time warping (DTW), which uses GF-2 and Landsat-8 data to product 2015 year time series data by selected four typical spatiotemporal fusion models (STARFM, FSDAF, ESPFM, Single-pair SPSTFM), and compare synthesized data with reference data. The results show that the time series assessment method has a high consistency with the existing methods which based on spatial quality evaluation. And this method can be used as an effective method to evaluate the accuracy of synthesized time series data.","","978-1-7281-7005-3","10.1109/ICAICA50127.2020.9182494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9182494","Spatiotemporal fusion;time series assessment;DTW;GF-2;Landsat-8","Time series analysis;Remote sensing;Artificial satellites;Earth;Reflectivity;Spatial resolution;Data models","geophysical image processing;image fusion;image resolution;remote sensing","remote sensing spatiotemporal information fusion technology;high spatial resolution data;time series remote sensing data;spatiotemporal fusion model;time series assessment method;Landsat-8 data;spatiotemporal fusion models;reference data;synthesized time series data;multisource spatiotemporal fusion reconstruction data;data support;time series remote sensing images","","","","16","IEEE","1 Sep 2020","","","IEEE","IEEE Conferences"
"Multi-Level Fusion of the Multi-Receptive Fields Contextual Networks and Disparity Network for Pairwise Semantic Stereo","H. Chen; M. Lin; H. Zhang; G. Yang; G. -S. Xia; X. Zheng; L. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, P.R. China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, P.R. China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, P.R. China; School of Electronic Information, Wuhan University, Wuhan, P. R. China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, P.R. China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, P.R. China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, P.R. China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","4967","4970","In this paper, we propose a multi-level fusion framework to address the pairwise semantic stereo issue. For disparity estimation, we adopt the pyramid stereo matching network. For semantic segmentation, the single segmentation network is proposed with respect to the left image, along with the disparity fusion segmentation network for the combination of semantic features and disparity features. Specifically, the multi-receptive fusion block is designed and employed to fully extract and fuse the contextual information. Finally, the refined segmentation result is obtained via yet another fusion of the multi-model results. The proposed method achieved a mean intersection over union (mIoU) of 79.05%, an average endpoint error (EPE) of 1.3966, and an mIoU-3 of 77.75%, ranking first in the Pairwise Semantic Stereo Challenge of the 2019 IEEE GRSS Data Fusion Contest [1],[2].","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899306","Data fusion;deep learning;semantic segmentation;disparity estimation;feature fusion","Image segmentation;Semantics;Convolution;Estimation;Feature extraction;Three-dimensional displays;Task analysis","feature extraction;image fusion;image matching;image segmentation;stereo image processing","multilevel fusion framework;disparity estimation;pyramid stereo matching network;semantic segmentation;single segmentation network;disparity fusion segmentation network;semantic features;disparity features;multireceptive fusion block;contextual information;Pairwise Semantic Stereo Challenge;2019 IEEE GRSS Data Fusion Contest;multireceptive fields contextual networks;disparity network;mean intersection over union;mIoU;average endpoint error;EPE","","5","","8","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Remote Sensing Image Scene Classification Based on SURF Feature and Deep Learning","J. Liang; J. Dang; Y. Wang; J. Yang; Z. Zhang","Gansu Provincial Engineering Research Center for Artificial Intelligence and Graphic & Image Processing, Gansu Provincial Key Lab of System Dynamics and Reliability of Rail Transport Equipment, Lanzhou Bocai Technology Co., Ltd.; Gansu Provincial Engineering Research Center for Artificial Intelligence and Graphic & Image Processing, Gansu Provincial Key Lab of System Dynamics and Reliability of Rail Transport Equipment, Lanzhou Bocai Technology Co., Ltd.; Gansu Provincial Engineering Research Center for Artificial Intelligence and Graphic & Image Processing, Gansu Provincial Key Lab of System Dynamics and Reliability of Rail Transport Equipment, Lanzhou Bocai Technology Co., Ltd.; Gansu Provincial Engineering Research Center for Artificial Intelligence and Graphic & Image Processing, Gansu Provincial Key Lab of System Dynamics and Reliability of Rail Transport Equipment, Lanzhou Bocai Technology Co., Ltd.; Gansu Provincial Engineering Research Center for Artificial Intelligence and Graphic & Image Processing, Gansu Provincial Key Lab of System Dynamics and Reliability of Rail Transport Equipment, Lanzhou Bocai Technology Co., Ltd.","2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","5 Mar 2020","2019","","","1128","1133","Remote sensing image scene classification is one of the key points in remote sensing image interpretation. The traditional remote sensing image scene classification feature performance is not strong, and the deep learning extraction semantic feature process is complex. This paper proposes a fusion feature remote sensing image scene classification method which is based on artificial features and deep learning semantic features. Firstly, the SURF feature of the remote sensing image is extracted and encoded by the VLAD algorithm. The semantic feature of a remote sensing image is extracted by transfer learning. Then the feature reduction is performed by PCA algorithm and feature fusion is performed. Finally, the scene classifier is trained by using the random forest algorithm. The experimental results show that the classification accuracy and Kappa coefficient of this method are higher and the method is effective.","2640-0103","978-1-7281-3248-8","10.1109/APSIPAASC47483.2019.9023118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9023118","","Feature extraction;Remote sensing;Machine learning;Semantics;Image analysis;Classification algorithms;Forestry","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);principal component analysis;remote sensing","feature reduction;feature fusion;scene classifier;classification accuracy;SURF feature;remote sensing image interpretation;deep learning extraction semantic feature process;fusion feature remote sensing image scene classification;artificial features;deep learning semantic features","","","","21","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Pansharpening based on details injection model and online sparse dictionary learning","J. Wang; L. Liu; N. Ai; J. Peng; X. Li","School of Information Science and Technology, Northwest University, Xi'an, Shaanxi Province, China; School of Information Science and Technology, Northwest University, Xi'an, Shaanxi Province, China; School of Information Science and Technology, Northwest University, Xi'an, Shaanxi Province, China; School of Information Science and Technology, Northwest University, Xi'an, Shaanxi Province, China; School of Information Science and Technology, Northwest University, Xi'an, Shaanxi Province, China","2018 13th IEEE Conference on Industrial Electronics and Applications (ICIEA)","28 Jun 2018","2018","","","1939","1944","Pansharpening is a process of combining a high-resolution Panchromatic (PAN) image with a low-resolution Multispectral (MS) image to obtain a high-resolution MS image. Pansharpening can not only overcome the physical and technical limitations of satellite sensors, but also improve the quality of images and obtain a more detailed description of the scene. In this paper, a novel method of Pansharpening is proposed which adopts the ARSIS concept. The spatial information missing in the low-resolution MS image can be extracted from the PAN image. This is achieved by image decomposition based on sparse representation. Then, the information is injected into the MS bands by the details injection model. In the dictionary training, we adapt Online Sparse Dictionary Learning (OSDL), which can shorten the dictionary training time and improve the fusion image effect. Experimental results show that the fusion image of the proposed method has stronger spectral performance and more detailed spatial information.","2158-2297","978-1-5386-3758-6","10.1109/ICIEA.2018.8398026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8398026","Image fusion;Sparse representation;Dictionary training;Online sparse dictionary learning","Dictionaries;Image fusion;Training;Spatial resolution;Machine learning;Image decomposition","geophysical image processing;image fusion;image representation;image resolution;learning (artificial intelligence);remote sensing;sparse matrices","high-resolution MS image;physical limitations;technical limitations;satellite sensors;low-resolution MS image;PAN image;image decomposition;sparse representation;MS bands;online sparse dictionary learning;dictionary training time;fusion image effect;high-resolution panchromatic image;low-resolution multispectral image;spatial information;detail injection model;pansharpening process;ARSIS concept;OSDL","","3","","9","IEEE","28 Jun 2018","","","IEEE","IEEE Conferences"
"Iterative Fusion Based on Amplitude Minimization of Orthogonal Spatial Component and Spectral Error","R. N. Marandi; H. Ghassemian","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2019 27th Iranian Conference on Electrical Engineering (ICEE)","5 Aug 2019","2019","","","1277","1283","Fusion of Multispectral (MS) and Panchromatic (Pan) images is known as pansharpening is a process which increases spatial resolution MS image by injecting high-frequency spatial information of Pan image. There is no pansharpening method that excellent performance on various data set, and usually its performance depends on the data set. So different pansharpening approach appear in this field. A new method is proposed that MS image and Pan image is fused iteratively in this article. The component of Pan, that orthogonal to MS, was considered as details. Because this component contains low-frequency information, spectral distortion occurs. So, a spectral error correction is used in this algorithm. Two data sets are used for evaluation and comparison to state of the art methods. The proposed method successes in visual analysis and quantity evaluation rather than other methods.","2642-9527","978-1-7281-1508-5","10.1109/IranianCEE.2019.8786384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8786384","image fusion;pansharpening;multispectral;panchromatic;spatial information;spectral information","Spatial resolution;Pansharpening;Image fusion;Image resolution;Iterative methods;Spectral analysis;Minimization","image fusion;image resolution;iterative methods;minimisation;remote sensing;spectral analysis","iterative fusion;amplitude minimization;orthogonal spatial component;spatial resolution MS image;high-frequency spatial information;spectral distortion;spectral error correction;data sets;pan image;pansharpening approach;visual analysis;quantity evaluation;multispectral image;panchromatic image","","1","","32","IEEE","5 Aug 2019","","","IEEE","IEEE Conferences"
"Fusion-based Resolution Enhancement of Satellite Images: Comparative Study and Performance Evaluation","T. M. Talal; M. R. metwalli; G. Attiya; F. E. Abd El-Samie; M. I. Dessouky","NARSS, Cairo, Egypt; NARSS, Cairo, Egypt; Faculty of Electronics Engineering, Menoufia University, Menouf, Egypt; Faculty of Electronics Engineering, Menoufia University, Menouf, Egypt; Faculty of Electronics Engineering, Menoufia University, Menouf, Egypt","2018 14th International Computer Engineering Conference (ICENCO)","7 Feb 2019","2018","","","1","6","Fusion becomes an important technique for improving resolution of satellite images. This importance is established from the ability of such technique to combine relevant information from several images, of a scene, into a single image. The constructed image is more informative than any one of the original images. This paper first presents a literature survey of the most recent fusion techniques concerned with satellite images. Then, it gives a performance evaluation of such techniques for enhancing resolution of color images considering full scene images of different regions with resolutions taken by Spot-4 and Landsat-8 satellites. Finally, it presents a comparison among the fusion techniques using different evaluation criteria.","2475-2320","978-1-5386-5117-9","10.1109/ICENCO.2018.8636137","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8636137","image fusion;satellite images;spatial resolution;spectral resolution","Spatial resolution;Discrete wavelet transforms;Satellites;Principal component analysis;Image fusion","image colour analysis;image enhancement;image fusion;image resolution;remote sensing","color image enhancement;Spot-4 satellites;Landsat-8 satellites;scene images;performance evaluation;satellite images;fusion-based resolution enhancement","","1","","23","IEEE","7 Feb 2019","","","IEEE","IEEE Conferences"
"MIHS: A Multiobjective Pan-sharpening Method for Remote Sensing Images","Y. Chen; C. Liu; A. Zhou; G. Zhang","School of Computer Science, Yangtze University, Hubei, China; School of Optical Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai, China; Department of Computer Science and Technology, East China Normal University, Shanghai, China; Department of Computer Science and Technology, East China Normal University, Shanghai, China","2019 IEEE Congress on Evolutionary Computation (CEC)","8 Aug 2019","2019","","","1068","1073","Pan-sharpening aims to integrate the spatial details of high resolution panchromatic image (PAN) with the spectral information of the corresponding low resolution multispectral image (MS) to produce high resolution multispectral image, which is a challenge real-world task. In this paper, we propose a novel pan-sharpening method, called multiobjective Intensity-Hue-Saturation (MIHS) transformation, which combines adaptive Intensity-Hue-Saturation transformation and evolutionary multi-objective optimization techniques for remote sensing images. The basic idea is to convert the pan-sharpening problem into a mutiobjective optimization problem and deal with it by an multiobjective evolutionary algorithm. The proposed method is applied to two Quick-bird images. Experimental results demonstrate that the proposed method does markedly improve the pan-sharpening performance compared to the state-of-the-art methods and an evolutionary method with single objective in terms of both subjective visual effects and objective quality metrics.","","978-1-7281-2153-6","10.1109/CEC.2019.8789901","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789901","Pan-sharpening;Panchromatic image;Multispectral images;Intensity-Hue-Saturation;Multiobjective optimization","Sociology;Statistics;Indexes;Spatial resolution;Biological cells;Optimization","evolutionary computation;geophysical image processing;image colour analysis;image fusion;image resolution;remote sensing","multiobjective pan-sharpening method;remote sensing images;pan-sharpening aims;spatial details;high resolution panchromatic image;spectral information;corresponding low resolution multispectral image;high resolution multispectral image;challenge real-world task;MIHS;multiobjective optimization techniques;multiobjective intensity-hue-saturation transformation;adaptive intensity-hue-saturation transformation;quick-bird images;evolutionary method;pan-sharpening performance;multiobjective evolutionary algorithm;mutiobjective optimization problem;pan-sharpening problem","","3","","18","IEEE","8 Aug 2019","","","IEEE","IEEE Conferences"
"An urban expansion model for African cities using fused multi temporal optical and SAR data","M. Shimoni; J. Lopez; Y. Forget; E. Wolff; C. Michellier; T. Grippa; C. Linard; M. Gilbert","Dept. of Electrical Engineering (SIC-RMA), Signal and Image Centre, Brussels, Belgium; Dept. of Electrical Engineering (SIC-RMA), Signal and Image Centre, Brussels, Belgium; Dept. of Electrical Engineering (SIC-RMA), Signal and Image Centre, Brussels, Belgium; Dept. of Electrical Engineering (SIC-RMA), Signal and Image Centre, Brussels, Belgium; Dept. of Electrical Engineering (SIC-RMA), Signal and Image Centre, Brussels, Belgium; Universite Libre de Bruxelles, Bruxelles, Bruxelles, BE; Dept. of Electrical Engineering (SIC-RMA), Signal and Image Centre, Brussels, Belgium; Dept. of Electrical Engineering (SIC-RMA), Signal and Image Centre, Brussels, Belgium","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","1159","1162","The forecast of human population distribution in Africa is limited by the lack of spatial urban expansion model and the quality of its data sources. One way to overcome this shortcoming is to integrate multi-source and multi-temporal data for improving the delineation and the characterization of human settlements. This paper presents a fully automatic fusion processing scheme of multi-temporal SAR and optical data for improving the segmentation of African urban areas.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325977","Urban expansion;density;fusion processing;support vector machine (SVM)","Synthetic aperture radar;Optical imaging;Sociology;Statistics;Remote sensing;Cities and towns;Adaptive optics","geophysical image processing;image fusion;image segmentation;land use;remote sensing by laser beam;remote sensing by radar;synthetic aperture radar","multitemporal optical data fusion;multitemporal SAR data fusion;synthetic aperture radar;human population distribution;spatial urban expansion model;human settlement characterization;automatic fusion processing scheme;African urban area segmentation","","2","","16","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"A new hyperspectral pansharpening method based on guided fliter","J. Qu; Y. Li; W. Dong","School of Telecommunications Engineering, Xidian University, Xi'an, China; School of Telecommunications Engineering, Xidian University, Xi'an, China; School of Telecommunications Engineering, Xidian University, Xi'an, China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","5125","5128","Because the guided filter can transfer structures, and avoid ringing artifacts, it can be applied to image fusion. A new hyperspectral pansharpening method based on guided filter (GFP) is proposed in this paper. The proposed method which works on each band of the hyperspectral (HS) image successively is different from the traditional methods. The detail information of each band is extracted at first. Then, we sharpen the panchromatic (PAN) image to enhance the details and obtain the difference between the enhanced PAN image and the detail information of each band in turn using the guided filter without causing spectral and spatial distortion. In order to reduce spectral distortion and add enough spatial information, the injection gains matrix is generated. The fused HS image is finally achieved by injecting the corresponding spatial difference into each band of the interpolated HS image. Experiments demonstrate that the proposed method can obtain superior performance in terms of subjective and objective evaluations.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128156","hyperspectral (HS) image;panchromatic (PAN) image;guided filter;hyperspectral pansharpening","","geophysical image processing;hyperspectral imaging;image enhancement;image filtering;image fusion;image resolution;interpolation;matrix algebra;remote sensing","spatial difference;PAN image enhancement;HS image fusion;HS image interpolation;spatial information;spatial distortion;spectral distortion;panchromatic image;hyperspectral image;image fusion;ringing artifacts;guided fliter;hyperspectral pansharpening method","","2","","8","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Pre-trained VGGNet Architecture for Remote-Sensing Image Scene Classification","U. Muhammad; W. Wang; S. P. Chattha; S. Ali","School of Computer and Control Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Computer and Control Engineering, University of Chinese Academy of Sciences, Beijing, China; Yanbu University College, Yanbu, Saudi Arabia; University of Education, Lahore, Pakistan","2018 24th International Conference on Pattern Recognition (ICPR)","29 Nov 2018","2018","","","1622","1627","The visual geometry group network (VGGNet) is used widely for image classification and has proven to be very effective method. Most existing approaches use features of just one type, and traditional fusion methods generally use multiple manually created features. However, to get the benefits of multilayer features remain a significant challenge in the remote-sensing domain. To address this challenge, we present a simple yet powerful framework based on canonical correlation analysis and 4-layer SVM classifier. Specifically, the pretrained VGGNet is employed as a deep feature extractor to extract mid-level and deep features for remote-sensing scene images. We then choose two convolutional (mid-level) and two fully-connected layers produced by VGGNet in which each layer is treated as a separated feature descriptor. Next, canonical correlation analysis (CCA) is used as a feature fusion strategy to refine the extracted features, and to fuse them with more discriminative power. Finally, the support vector machine (SVM) classifier is used to construct the 4-layer representation of the scenes images. Experimenting on a UC Merced and WHU-RS datasets, demonstrate that the proposed approach, even without data augmentation, fine tuning or coding strategy, has a superior performance than state-of-the-art methods used now.","1051-4651","978-1-5386-3788-3","10.1109/ICPR.2018.8545591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8545591","","Feature extraction;Correlation;Support vector machines;Covariance matrices;Remote sensing;Fuses;Semantics","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing;support vector machines","pretrained VGGNet;deep feature extractor;remote-sensing scene images;canonical correlation analysis;feature fusion strategy;support vector machine classifier;scenes images;pre-trained VGGNet architecture;remote-sensing image scene classification;visual geometry group network;image classification;multilayer features;remote-sensing domain;4-layer SVM classifier;feature descriptor;fusion methods;CCA","","31","","23","IEEE","29 Nov 2018","","","IEEE","IEEE Conferences"
"Building Near-Real-Time MODIS Data Fusion Workflow to Support Agricultural Decision-making Applications","L. Lin; L. Di; C. Zhang; L. Guo; J. Tang; E. Yu; M. S. Rahman; H. Zhao; Z. Yu; Z. Sun; J. Gaigalas","Center for Spatial Information Science and Systems, George Mason University, Fairfax, USA; Center for Spatial Information Science and Systems, George Mason University, Fairfax, USA; Center for Spatial Information Science and Systems, George Mason University, Fairfax, USA; Center for Spatial Information Science and Systems, George Mason University, Fairfax, USA; Center for Spatial Information Science and Systems, George Mason University, Fairfax, USA; Center for Spatial Information Science and Systems, George Mason University, Fairfax, USA; Center for Spatial Information Science and Systems, George Mason University, Fairfax, USA; Center for Spatial Information Science and Systems, George Mason University, Fairfax, USA; Center for Spatial Information Science and Systems, George Mason University, Fairfax, USA; Center for Spatial Information Science and Systems, George Mason University, Fairfax, USA; Center for Spatial Information Science and Systems, George Mason University, Fairfax, USA","2019 8th International Conference on Agro-Geoinformatics (Agro-Geoinformatics)","2 Sep 2019","2019","","","1","5","WaterSmart project is an NSF funded projected seeks water consumption reduction using satellite observations. In order to fit the fine temporal resolution requirement, satellites are required to have a high revisit cycle. MODIS is an ideal platform for monitoring the ground thanks to its daily coverage while the spatial resolution is too coarse. Research has demonstrated the possibility to improve the spatial resolution of MODIS using the Landsat 8 images. This research is aimed to establish a workflow to adapt the data fusion algorithm to achieve automatically processing at real-time in order to support short-term decision making.","","978-1-7281-2116-1","10.1109/Agro-Geoinformatics.2019.8820229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8820229","Agriculture;remote sensing;MODIS;data fusion;web service","Remote sensing;MODIS;Cloud computing;Irrigation;Image fusion;Spatial resolution","agriculture;hydrological techniques;image fusion;radiometry","spatial resolution;Landsat 8 images;data fusion algorithm;short-term decision making;building near-real-time MODIS;data fusion workflow;agricultural decision-making applications;WaterSmart project;NSF funded;satellite observations;fine temporal resolution requirement;high revisit cycle;daily coverage;water consumption reduction","","1","","24","IEEE","2 Sep 2019","","","IEEE","IEEE Conferences"
"Mapping land covers of brussels capital region using spatially enhanced hyperspectral images","J. C. -W. Chan; N. Yokoya","Department of Electronics and Informatics, Vrije Universiteit Brussel, Belgium; Sinagl Processing in Earth Observation (SiPEO), Technische Universitat Munchen (TUM), Germany","2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Oct 2017","2016","","","1","5","Hyperspectral data provide indispensable timely information for environmental monitoring. It has become one of the most sought after data set for many specific applications. However, for large areal coverage, spaceborne hyperspectral data are currently acquired at low resolution. Due to the proven usefulness of hyperspectral data and its potential in newer applications, many researchers have investigated novel enhancement methods for Earth Observation hyperspectral images. We have examined four different enhancement methods using a classification scheme at medium level of difficulty. Two of the examined methods are pansharpening methods and the other two are sub-space methods. The results do not show improvements in classification using spatially enhanced images except for the class of Pine trees. However, using full groundtruth of road and buildings, it is clear that spatially enhanced hyperspectral images achieve substantial improvement in classifying small sized houses. Better characterization of road networks can be visualized and also higher accuracy is observed but to a lesser extent than buildings. Among the four methods, a pansharpening method performed best.","2158-6276","978-1-5090-0608-3","10.1109/WHISPERS.2016.8071678","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071678","Hyperspectral;spatial enhancement;image fusion;pansharpening;land cover classification","Image fusion;Spatial resolution;Hyperspectral imaging;Buildings;Sensors","environmental monitoring (geophysics);geophysical image processing;hyperspectral imaging;image classification;image enhancement;image resolution;land cover;terrain mapping","spaceborne hyperspectral data;Earth Observation hyperspectral images;sub-space methods;spatially enhanced hyperspectral images;pansharpening method;brussels capital region;enhancement methods;land covers;houses;buildings;road networks;Pine trees;environmental monitoring","","1","","24","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"Multimodal, multitemporal, and multisource global data fusion for local climate zones classification based on ensemble learning","N. Yokoya; P. Ghamisi; J. Xia","Signal Processing in Earth Observation (SiPEO), Technische Universität München (TUM), Germany; Signal Processing in Earth Observation (SiPEO), Technische Universität München (TUM), Germany; Department of Advanced Interdisciplinary Studies, University of Tokyo, Japan","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","1197","1200","This paper presents a new methodology for classification of local climate zones based on ensemble learning techniques. Landsat-8 data and open street map data are used to extract spectral-spatial features, including spectral reflectance, spectral indexes, and morphological profiles fed to subsequent classification methods as inputs. Canonical correlation forests and rotation forests are used for the classification step. The final classification map is generated by majority voting on different classification maps obtained by the two classifiers using multiple training subsets. The proposed method achieved an overall accuracy of 74.94% and a kappa coefficient of 0.71 in the 2017 IEEE GRSS Data Fusion Contest.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127172","Local climate zones (LCZs);canonical correlation forests;rotation forests;morphological profiles","Remote sensing;Earth;Artificial satellites;Training;Urban areas;Feature extraction;Indexes","geophysical image processing;geophysical techniques;image classification;image fusion;learning (artificial intelligence);sensor fusion","multisource global data fusion;local climate zones classification;ensemble learning techniques;open street map data;spectral-spatial features;spectral reflectance;spectral indexes;morphological profiles;subsequent classification methods;canonical correlation forests;rotation forests;classification step;final classification map;different classification maps;2017 IEEE GRSS Data Fusion Contest","","15","","8","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Relationships analysis of land surface temperature with vegetation indicators and impervious surface fraction by fusing multi-temporal and multi-sensor remotely sensed data","Liwen Huang; H. Shen; P. Wu; L. Zhang; C. Zeng","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","2015 Joint Urban Remote Sensing Event (JURSE)","11 Jun 2015","2015","","","1","4","It is known that vegetation, and impervious surface are very important factors to affect the LST distribution in surface urban heat island (SUHI) analysis. However, the trade-off between temporal resolution and spatial resolution and/or the influence of cloud covering, make it difficult to obtain fine-scale spatial-temporal relationship analysis. To relieve these difficulties, this study employs multi-temporal and multi-sensor fusion methods for summer spatial-temporal relationships of Land surface temperature (LST) with normalized difference vegetation index (NDVI), vegetation fraction (VF) and impervious surface fraction (ISF) analysis on Wuhan city of China. Here, the correlation analysis was extended from two-dimensional to three-dimensional by using the continuous fused data (from 1988 to 2013). Our analysis indicates there is a strong negative relationship between LST and NDVI as well as VF, whereas the relationship between LST and ISF is obvious positive correlation. In addition, we also find that all these relationships are spatial-temporal steady. This result suggest that increasing impervious surface area means enhance LST, whereas increasing vegetation means weaken LST in summer, especially in the “warm edge” area. We believe the use of continuous long-term data weakened the interference of data quality and improve the reliability.","2334-0932","978-1-4799-6652-3","10.1109/JURSE.2015.7120459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120459","","Land surface temperature;Remote sensing;Earth;Satellites;Land surface;Vegetation mapping;Thermal pollution","atmospheric temperature;geophysical image processing;image fusion;land surface temperature;thermal pollution;vegetation;vegetation mapping","land surface temperature;vegetation indicators;impervious surface fraction;multitemporal remotely sensed data;multisensor remotely sensed data;LST distribution;surface urban heat island;SUHI analysis;fine-scale spatial-temporal relationship analysis;normalized difference vegetation index;multitemporal fusion method;multisensor fusion method;continuous fused data;AD 1988 to 2013;warm edge area;data quality","","3","","13","IEEE","11 Jun 2015","","","IEEE","IEEE Conferences"
"An improved non-subsampled contourlet transform-based hybrid pan-sharpening algorithm","X. Lu; J. Zhang; Y. Zhang","Dept. of Information Engineering, Harbin Institute of Technology, Harbin, China; Dept. of Information Engineering, Harbin Institute of Technology, Harbin, China; Dept. of Information Engineering, Harbin Institute of Technology, Harbin, China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","3393","3396","Multi-resolution analysis (MRA) is a useful technique for multispectral (MS) image pan-sharpening. To overcome the limitation of the conventional wavelet-based method, multi-directional MRA approaches have been proposed to provide an efficient directional information representation. In this paper, we present a new hybrid method for MS image pan-sharpening that uses non-subsampled contourlet transform (NSCT) as the MRA tool, and followed by a high-pass detail-injection model. Particularly, to further enhance the spatial resolution of the MS image, and in the meantime, produce a high spectral quality, we take account of the dissimilarity between the spatial details of MS image and decomposed panchromatic image of NSCT, and introduce a fast and effective means to deal with the problem. Experiments on four remote sensing data sets show the superiority of the presented approach over other typical pan-sharpening methods.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127726","Multispectral;contourlet transform;pan-sharpening","Remote sensing;Dogs;Computers","geophysical image processing;geophysical techniques;image fusion;image resolution;remote sensing;wavelet transforms","MS image pan-sharpening;nonsubsampled contourlet;NSCT;MRA tool;spatial resolution;high spectral quality;panchromatic image;hybrid pan-sharpening algorithm;multiresolution analysis;multispectral image pan-sharpening;conventional wavelet;multidirectional MRA approaches;directional information representation","","5","","7","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Cloud Removal for Single Visible Image Based on Modified Dark Channel Prior with Multiple Scale","S. Shi; Y. Zhang; X. Zhou; J. Cheng","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, Heilongjiang Province, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, Heilongjiang Province, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, Heilongjiang Province, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, Heilongjiang Province, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4127","4130","The cloud-contaminated phenomenon in the field of remote sensing has a serious impact on image processing so that a large number of images are unusable. To achieve cloud removal for single visible image, we propose a novel method based on modified dark channel prior with multiple scale (MDCPMS). In the structure of multiple scale, the cloudy image is firstly decomposed into high-frequency and low-frequency components. The former is uniformly amplified to enhance its weak contour, and the latter is processed by modified dark channel prior (DCP), whose estimation of atmospheric light is optimized for better cloud removal. Finally, the cloud-removed image is obtained through multi-scale reconstruction. Experimental results show that the proposed MDCPMS obtains a significant performance with slightest color distortion and is closest to the corresponding real image, compared with DCP and nonlocal method.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553169","Cloud removal;modified dark channel prior with multiple scale;estimation of atmospheric light;visible images","Measurement;Image color analysis;Clouds;Estimation;Channel estimation;Distortion;Remote sensing","clouds;edge detection;geophysical image processing;image colour analysis;image enhancement;image fusion;image processing;image reconstruction;image restoration;image segmentation;remote sensing","cloud removal;single visible image;modified dark channel;multiple scale;cloud-contaminated phenomenon;image processing;cloudy image;low-frequency components;cloud-removed image;multiscale reconstruction;corresponding real image","","","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A Novel Effective Chlorophyll Indicator for Forest Monitoring Using Worldview-3 Multispectral Reflectance","C. Lin","Department of Forestry and Natural Resources, National Chiayi University","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5220","5223","This paper explores the feasibility of deriving multispectral-based effective chlorophyll indicators (MECIs) for foliage chlorophyll concentration (CHLS) estimation. An average fusion method was applied to simulate the multispectral reflectance of the WorldView-3 sensor using hyperspectral data. With the experimental data of CHLS and predictors derived from multispectral reflectance, a series of linear regression analyses were carried out to derive appropriate models for CHLS estimation. Accuracy measures of RMSE and PRMSE were used to evaluate the model performance. Results showed that the coastal-band based MECI (MECIc) and the blue-band based MECI (MECIb) were able to achieve an RMSE of 0.5657 mg/g and 0.5943 mg/g as well as a PRMSE of 36% and 38% respectively. Using the Red edge and Yellow reflectance based NDVI (NDVIREY) as a predictor, the model can reduce uncertainty and achieve an estimation of 0.4089 mg/g and 26% for RMSE and PRMSE respectively. The prediction error made by the CHLS-NDVIREY model and the CHLS-MECI model were 11% and 60% larger than 0.38 mg/g the RMSE of hyperspectral-based CHLS-ECI model. In summary, NDVIREY was able to achieve a better prediction at around a level of 75% accuracy (1-PRMSE) and therefore is able to be an effective indicator of CHLS for forest monitoring.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518275","Chlorophyll indicator;forest health;climate change;hyperspectral remote sensing;multispectral remote sensing","Reflectivity;Predictive models;Forestry;Estimation;Vegetation;Hyperspectral imaging","geophysical image processing;image fusion;reflectivity;regression analysis;remote sensing;vegetation;vegetation mapping","novel effective chlorophyll indicator;forest monitoring;worldview-3 multispectral reflectance;effective chlorophyll indicators;foliage chlorophyll concentration estimation;average fusion method;WorldView-3 sensor;hyperspectral data;experimental data;linear regression analyses;appropriate models;CHLS estimation;accuracy measures;blue-band based MECI;NDVI REY;CHLS-NDVI;REY model;CHLS-MECI model;hyperspectral-based CHLS-ECI model;1-PRMSE;mass 0.5943 mg","","2","","10","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Multiresolution Analysis Pansharpening for the Fusion of Raman and Conventional Brightfield Microscopy Images","C. Pomrehn; D. Klein; A. Kolb; P. Kaul; R. Herpers","Institute of Visual Computing, Bonn-Rhein-Sieg University of Applied Sciences, Germany; Institute of Safety and Security Research, Bonn-Rhein-Sieg University of Applied Sciences, Germany; Institute for Vision and Graphics, University of Siegen, Germany; Institute of Safety and Security Research, Bonn-Rhein-Sieg University of Applied Sciences, Germany; Institute of Visual Computing, Bonn-Rhein-Sieg University of Applied Sciences, Germany","2019 10th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","5 Dec 2019","2019","","","1","6","This contribution investigates the application of multiresolution analysis-based pansharpening algorithms for the fusion of microscopic images generated by Raman and conventional brightfield microscopy. Using hyperspectral and panchromatic image data of a polymer sample, eleven different established algorithms have been applied. Fusion quality assessment has been conducted at reduced and full resolution. It could be demonstrated that among the considered approaches, pansharpening based on high-pass filtering performs best in terms of spatial and spectral consistency, according to established quality measures and a visual image quality assessment.","2158-6276","978-1-7281-5294-3","10.1109/WHISPERS.2019.8921202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8921202","Hyperspectral image;multiresolution analysis;pansharpening;image fusion;Raman microscopy","Spatial resolution;Distortion;Microscopy;Visualization;Hyperspectral imaging;Quality assessment","hyperspectral imaging;image fusion;image resolution;materials science computing;microscopy;polymers","multiresolution analysis pansharpening;multiresolution analysis-based pansharpening algorithms;hyperspectral image data;panchromatic image data;polymer sample;fusion quality assessment;quality measures;visual image quality assessment;established algorithms;Raman microscopy image fusion;conventional brightfield microscopy image fusion;high-pass filtering;spectral consistency;spatial consistency","","1","","17","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Optical and Acoustic Fusion in Borehole Imaging Logging","J. Li; L. Liang; Y. Gong; A. He; Q. Zhu; X. Gao","School of Geoscience and Technology, Southwest Petroleum University; Southwest China Research Institute of Electronic Equipment; School of Geoscience and Technology, Southwest Petroleum University; School of Geoscience and Technology, Southwest Petroleum University; School of Mechatronic Engineering, Southwest Petroleum University; School of Automation Engineering, University of Electronic Science and Technology of China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2852","2855","Optical Televiewer (OTV) and Acoustic Televiewer (ATV) are cornerstones of establishing borehole imaging logging systems. OTV provides the intuitive analysis of lithology and textures, while ATV has advantages in presenting fractures at deep formation. Combining both imaging methods with integrated interpretation is conducive for geophysicists to characterize structures, and therefore the synthesis based on image fusion is studied in this paper. The self-supervised autoencoder with convolutional neural networks (CNN) and transformer modules is established for feature extraction and reconstruction, where the fusion strategy is implemented in the independent stage. The introduction of the self-attention mechanism makes up for the lack of long-range relationships in CNN. The nested networks capture detailed information in convolutional layers, and the semantic features are conveyed with fine-grained decomposition simultaneously. Besides, the introduction of the self-attention mechanism makes up long-range dependencies for intrinsic information. Quali-tative and quantitative experiments prove that the proposed scheme outperforms competitive methods over textured and structural fidelity.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883924","Optical and Acoustic Televiewer;Bore-hole Logging;Image Fusion","Optical losses;Semantics;Optical imaging;Transformers;Acoustics;Optical coupling;Optical sensors","drilling (geotechnical);feature extraction;image classification;image fusion;image representation;learning (artificial intelligence);neural nets;well logging","CNN;transformer modules;feature extraction;fusion strategy;self-attention mechanism;long-range relationships;nested networks;convolutional layers;semantic features;long-range dependencies;textured fidelity;structural fidelity;OTV;ATV;borehole imaging logging systems;intuitive analysis;lithology;deep formation;imaging methods;integrated interpretation;image fusion;self-supervised autoencoder","","","","16","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Fusion of Spaceborne and Airborne SAR Images Using Saliency and Fuzzy Logic for Vessel Detection","D. Zhu; X. Wang; G. Li; X. -P. Zhang","School of Electron. Inf. & Commun., Huazhong University of Science and Technology, China; Department of Electronic Engineering, Tsinghua University, China; Department of Electronic Engineering, Tsinghua University, China; Department of Electrical, Computer & Biomedical Engineering, Ryerson University, Canada","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4468","4471","In the paper, we propose a new method based on multi-order superpixel-level saliency and fuzzy logic (MSSFL) to fuse spaceborne and airborne SAR images for vessel detection. First, we generate a new global regional contrast map (GRCM) by exploiting the multi-order superpixel-level saliency (MSS). In the generated GRCM, the vessel targets are well restored and the backgrounds are suppressed. Next, a new fuzzy logic approach is presented to fuse the MSS information provided by the GRCMs. This GRCM-based fuzzy fusion can further enhance the vessel target regions and filter out the inshore interference regions. Experimental results using Gaofen-3 satellite and unmanned aerial vehicle (UAV) SAR images show that the proposed MSSFL method yields higher target-to-cluster ratio (TCR) of fused images and improved detection performance compared with the commonly utilized image fusion approaches.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554207","National Natural Science Foundation of China(grant numbers:61790551,61901244,61925106); China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554207","Fuzzy logic;saliency;spaceborne and airborne SAR image fusion;vessel target detection","Fuzzy logic;Satellites;Fuses;Object detection;Interference;Information filters;Radar polarimetry","autonomous aerial vehicles;fuzzy logic;image classification;image colour analysis;image fusion;image representation;image segmentation;object detection;radar imaging;remotely operated vehicles;sensor fusion;synthetic aperture radar","airborne SAR images;vessel detection;multiorder superpixel-level saliency;global regional contrast map;generated GRCM;vessel targets;fuzzy logic approach;GRCM-based fuzzy fusion;vessel target regions;inshore interference regions;MSSFL method yields higher target-to-cluster ratio;improved detection performance;commonly utilized image fusion approaches","","","","11","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A Combining Method for Generating Land Surface Temperature with High Spatiotemporal Resolution","Y. Li; H. Wu; Z. -L. Li; C. Gao","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Agricultural Remote Sensing, Ministry of Agriculture/Institute of Agricultural Resources and Regional Planning, Chinese Academy of Agricultural Sciences, Beijing, China; Key Laboratory of Quantitative Remote Sensing Information Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","3747","3750","Satellite-derived high-resolution LST observations are essential for environmental studies. However, the tradeoff between spatial and temporal resolutions largely restricts the application of current LST products. As a consequence, many spatial downscaling or spatiotemporal fusion methods were proposed to overcome this limitation. In this paper, we design a novel empirical weighting method to combine the results from the popular downscaling and fusion methods, thermal sharpening algorithm (TsHARP), and spatial and temporal adaptive reflectance fusion model (STARFM). Specifically, the error of the two methods are firstly estimated and the predictions are blended based on the inverse ratio of the corresponding error. Our method is tested with Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) and Moderate Resolution Imaging Spectroradiometer (MODIS) data in Beijing. Compared with the actual ASTER LST, the combining results could both enhance the accuracy and structure similarity, as our method utilizes spatial-temporal-spectral information. Moreover, our method also has the potential for generating more accurate daily high-resolution LSTs.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884214","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884214","Land surface temperature;Weighted combination;Spatial downscaling;Spatiotemporal fusion","Reflectivity;Land surface;Radiometry;Prediction algorithms;Reflection;Land surface temperature;Spatiotemporal phenomena","atmospheric techniques;geophysical image processing;image fusion;image resolution;land surface temperature;radiometers;radiometry;remote sensing;spatiotemporal phenomena;vegetation mapping","combining method;generating land surface temperature;high spatiotemporal Resolution;satellite-derived high-resolution LST observations;environmental studies;spatial resolutions;temporal resolutions;current LST products;spatial downscaling;spatiotemporal fusion methods;novel empirical weighting method;popular downscaling;thermal sharpening algorithm;spatial reflectance fusion model;temporal adaptive reflectance fusion model;corresponding error;Advanced Spaceborne Thermal Emission;Reflection Radiometer;Moderate Resolution Imaging Spectroradiometer data;actual ASTER LST;combining results;spatial-temporal-spectral information;accurate daily high-resolution LSTs","","","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Evaluation of four Spatiotemporal Gap-Filling Methods in Crop Phenology Monitoring","C. Wang; T. He","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","6061","6064","The high spatial resolution land surface phenology (LSP) monitoring is often limited by the temporal discontinuity of high spatial resolution observations. Many gap-filling methods have been proposed for LSP monitoring, however, a thorough intercomparison and evaluation is still lacking. Four widely-used methods, including the Spatial and Temporal Adaptive Reflectance Fusion Model (STARFM), the Flexible Spatiotemporal DAta Fusion (FSDAF), Multi-year based model, and Spatiotemporal Shape Matching Model (SSMM) were selected to extract green-up date (GUD) based on the Harmonized Landsat and Sentinel-2 (HLS) dataset. The results of the four methods show consistency with those of PhenoCam sites (R2>0.64) and there is a lag phenomenon. Compared within 3×3 VIIRS pixel window, the difference between mean VIIRS GUDs and aggregated 30m GUDs is small and the mean absolute difference is less than 7 days. Comprehensively, SSMM has high consistency (R2=0.75) and smaller bias (Bias=7.3days), which shows more potential in LSP monitoring.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884875","National Natural Science Foundation of China(grant numbers:42090012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884875","land surface phenology;green-up date;HLS;gap-filling","Adaptation models;Uncertainty;Shape;Land surface;Data models;Spatiotemporal phenomena;Data mining","crops;geophysical image processing;geophysical techniques;image fusion;image resolution;phenology;radiometry;remote sensing;sensor fusion;spatiotemporal phenomena;vegetation;vegetation mapping","Spatiotemporal gap-filling methods;crop phenology;high spatial resolution land surface phenology monitoring;temporal discontinuity;high spatial resolution observations;LSP monitoring;Flexible Spatiotemporal DAta Fusion;Multiyear based model;Spatiotemporal Shape Matching Model;Sentinel-2 dataset;3×3 VIIRS pixel window;high consistency;size 30.0 m;time 7.0 d;time 7.3 d","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Automatic Area-Based Registration of Optical and SAR Images Through Generative Adversarial Networks and a Correlation-Type Metric","L. Maggiolo; D. Solarna; G. Moser; S. B. Serpico","University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2089","2092","The automatic registration of multisensor remote sensing images is a highly challenging task due to the inherently different physical, statistical, and textural properties of the input data. In the present paper, this problem is addressed in the case of optical-SAR images by proposing a novel method based on deep learning and area-based registration concepts. The method integrates a conditional generative adversarial network (cGAN), an area-based cross-correlation-type l2 similarity metric, and the COBYLA constrained maximization algorithm. Whereas correlation-type metrics are typically ineffective in the application to multisensor registration, the proposed approach allows exploiting the image translation capabilities of cGAN architectures to enable the use of an l2 similarity metric, which favors high computational efficiency. Experiments with Sentinel-1 and Sentinel-2 data suggest the effectiveness of this strategy and the capability of the proposed method to achieve accurate registration.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323235","European Space Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323235","Multisensor image registration;conditional generative adversarial network;$\ell^{2}$ similarity;COBYLA","Radar polarimetry;Optical imaging;Measurement;Optical sensors;Feature extraction;Training;Generative adversarial networks","geophysical image processing;geophysical signal processing;image classification;image fusion;image registration;radar imaging;remote sensing;remote sensing by radar;sensor fusion;synthetic aperture radar","automatic area-based registration;generative adversarial networks;correlation-type metric;automatic registration;multisensor remote sensing images;textural properties;optical-SAR images;deep learning;area-based registration concepts;conditional generative adversarial network;area-based cross-correlation-type l;image translation capabilities;Sentinel-2 data","","4","","14","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Model based pansharpening method based on TV and MTF deblurring","F. Palsson; J. R. Sveinsson; M. O. Ulfarsson; J. A. Benediktsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","33","36","In the past two decades, many methods have been proposed to fuse low resolution multispectral (MS) and high resolution panchromatic (Pan) images, i.e., pansharpening. Two large families of such methods are component substitution (CS) and multiresolution analysis methods (MRA). We develop a model based method for pansharpening based on minimizing a cost function which includes a data fidelity term, a detail injection term and a total variation (TV) term. The model takes into account the modulation transfer function (MTF) and spectral response of the sensor. The resulting iterative method not only sharpens the MS image with details from the Pan image but is also able to extract important information from the MS image itself via MTF-based deconvolution. We compare the proposed method to a number of state-of-the-art CS and MRA pansharpening methods using a real WorldView-2 dataset and show that it gives excellent results with details that all the CS and MRA methods can not extract.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325690","Image fusion;pansharpening;World View 2;component substitution;multiresolution analysis","TV;Measurement;Cost function;Spatial resolution;Remote sensing;Iterative methods","geophysical image processing;image fusion;image resolution;optical transfer function","MTF deblurring;TV deblurring;real WorldView-2 dataset;MTF-based deconvolution;iterative method;sensor spectral response;modulation transfer function;total variation term;data fidelity;cost function;multiresolution analysis method;component substitution;high resolution panchromatic image;low resolution multispectral image;model based pansharpening method","","5","","8","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Pan-Sharpening with a CNN-Based Two Stage Ratio Enhancement Method","H. Zhou; Q. Liu; Q. Xu; Y. Wang","The State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; Hangzhou Innovation Institute, Beihang University, Hangzhou, China; Beijing University of Chemical Technology, Beijing, China; The State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","216","219","We propose a hybrid method combining the deep learning technique and the ratio enhancement (RE) method for pansharpening. The intuition behind is to utilize the deep learning technique to synthesize a panchromatic (PAN) image for the RE method to reduce the spectral distortion while keeping the spatial details. The method consists of two stages. First, the CNN synthesizer is optimized to generate the downsampled PAN image to guarantee the network have a good initialization. Second, CNN is integrated into the RE method and supervised by the ground truth multi-spectral (MS) to produce an ideal synthesized PAN for the RE method. We conduct experiments on various datasets and compare with widely used methods to demonstrate the superiority of the proposed method.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323505","National Natural Science Foundation of China(grant numbers:41871283); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323505","Image fusion;pan-sharpening;Convolutional Neural Network (CNN);deep learning","Training;Spatial resolution;Measurement;Image resolution;Testing;Satellites;Remote sensing","geophysical image processing;image fusion;image resolution;image sampling;learning (artificial intelligence);spectral analysis","RE method;ground truth multispectral;ideal synthesized PAN;pan-sharpening;CNN-based two stage ratio enhancement method;hybrid method;deep learning technique;pansharpening;panchromatic image;spectral distortion;CNN synthesizer;downsampled PAN image","","2","","18","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Wavefusion: Wavelet Assistant Fusion Model for Pan-Sharpening","Y. Xing; Y. Zhang; Y. Zhang","School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1083","1086","Pan-sharpening refers to obtain a high-resolution multispectral (HRMS) image by fusing a panchromatic (PAN) image and a low-resolution multispectral (LRMS) image. Recently, convolutional neural networks (CNNs) have achieved great success in pan-sharpening. However, the down-sampling operations in commonly used CNN-based models lead to information loss, and the corresponding up-sampling operations usually introduce some undesirable artifacts, resulting in suboptimal fusion results. In this paper, we propose a simple but effective wavelet assistant fusion model (WaveFusion) to address aforementioned issue. The proposed model consists of three parts, namely a wavelet feature extraction (WFE) part, a wavelet feature fusion (WFF) part and a reconstruction part. With the assistance of the wavelet transform and also a simple alignment operation, WaveFusion obtains the best fusion result compared with some state-of-the-art methods, especially for the fusion at the full resolution.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884867","Pan-sharpening;convolutional neural network;image fusion;deep learning;wavelet transform","Wavelet transforms;Wavelet domain;Frequency-domain analysis;Geoscience and remote sensing;Feature extraction;Convolutional neural networks;Task analysis","correlation methods;feature extraction;filtering theory;image enhancement;image fusion;image resolution;neural nets;sensor fusion;wavelet transforms","down-sampling operations;commonly used CNN-based models;up-sampling operations;suboptimal fusion results;simple but effective wavelet assistant fusion model;wavefusion;wavelet feature extraction part;wavelet feature fusion part;simple alignment operation;fusion result;pan-sharpening;high-resolution multispectral image;panchromatic image;low-resolution multispectral image;convolutional neural networks","","","","12","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"A multidimensional scaling optimization and fusion approach for the unsupervised change detection problem in remote sensing images","R. Touati; M. Mignotte","Département d'Informatique et de Recherche Opérationnelle (DIRO), Université de Montréal; Département d'Informatique et de Recherche Opérationnelle (DIRO), Université de Montréal","2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA)","19 Jan 2017","2016","","","1","6","It is generally well known that the overall performance of the most widely used types of unsupervised change detection methods, based on the luminance pixel-wise difference, is mainly relied on the quality of the so-called difference image and the accuracy of the classification method. In order to address these two issues, this work proposes to first estimate, a new and robust similarity feature map, playing the same role as the difference image, by specifying a set of constraints expressed for each pair of pixels existing in the multitemporal images. As a consequence, the proposed change detection method does not require any preprocessing step of the multitemporal images such as radiometric correction/normalization. In addition, input data can be acquired from different sensors. The quadratic complexity in the number of pixels of this new similarity feature map, between the multitemporal images, is reduced to a linear complexity procedure thanks to the FastMap-based optimization algorithm. Second, in order to achieve more robustness, changes are then identified, from this similarity feature map, by combining (fusing) the results of different automatic thresholding algorithms. Experimental results confirm the robustness of the proposed approach.","2154-512X","978-1-4673-8910-5","10.1109/IPTA.2016.7821021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7821021","Change detection;Data fusion;Multidimensional scaling;Multitemporal multimodal images;Thresholding algorithms","Sensors;Robustness;Complexity theory;Image sensors;Optimization;Radiometry","feature extraction;image classification;image fusion;optimisation;remote sensing","multidimensional scaling optimization and fusion approach;unsupervised change detection problem;remote sensing image;luminance pixel-wise difference;classification method;similarity feature map;multitemporal image;FastMap-based optimization algorithm;automatic thresholding algorithm","","6","","27","IEEE","19 Jan 2017","","","IEEE","IEEE Conferences"
"Detect Deeply Only Once: Arbitrary-Oriented Object Detection in Remote Sensing Images Based on Improved YOLOv5","S. Hu; Y. Ge; Y. Fan; H. Wu","School of Software, Nanchang Hangkong University, NanChang, China; School of Software, Nanchang Hangkong University, NanChang, China; School of Software, Nanchang Hangkong University, NanChang, China; College of Mathematics and Information Science, Nanchang Hangkong University, NanChang, China","2022 3rd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)","23 Dec 2022","2022","","","613","618","To solve the problem that the existing arbitrary-oriented object detection cannot satisfy both high accuracy and high efficiency, a model called Detect Deeply Only Once is proposed. The model is based on the YOLOv5s_CSL network and is improved in three ways: (1) Three attention mechanisms are separately added to the appropriate positions of the model. All three attention mechanisms pay more attention to the features of interest and improve the detection accuracy. (2) The repeated weighted bi-directional feature pyramid network and path aggregation network are combined, which strengthens feature fusion ability among multiple layers. (3) Loss rank mining is employed after the final feature map, which filters some elements representing easy examples in the final feature map and forces the detectors to concentrate on hard examples during training. Meanwhile, the role of each module is verified on the HRSC2016 dataset, and the proposed method shows good superiority in terms of higher detection accuracy than the benchmark model and the state-of-the-art models on both HRSC2016 dataset and UCAS_AOD dataset.","","978-1-6654-5160-4","10.1109/ICBAIE56435.2022.9985837","National Natural Science Foundation of China; Nanchang Hangkong University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985837","arbitrary-oriented object detection;attention mechanism;feature fusion;loss rank mining;YOLOv5s_CSL","Training;Semantics;Object detection;Detectors;Feature extraction;Transformers;Data models","data mining;feature extraction;geophysical image processing;image colour analysis;image fusion;image resolution;image segmentation;learning (artificial intelligence);object detection;remote sensing;self-organising feature maps","arbitrary-oriented object detection;attention mechanisms;Detect Deeply;feature fusion ability;HRSC2016 dataset;improved YOLOv;loss rank mining;remote sensing images;repeated weighted bidirectional feature pyramid network;UCAS_AOD dataset;YOLOv5s_CSL network","","","","28","IEEE","23 Dec 2022","","","IEEE","IEEE Conferences"
"Disentangled Non-Local Network for Hyperspectral and LiDAR Data Classification","W. Liu; F. Gao; J. Dong","Institute of Marine Development, Ocean University of China; Institute of Marine Development, Ocean University of China; Institute of Marine Development, Ocean University of China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2397","2400","As the ground objects become increasingly complex, the classification results obtained by single source remote sensing data can hardly meet the application requirements. In order to tackle this limitation, we propose a simple yet effective attention fusion model based on Disentangled Non-local (DNL) network for hyperspectral and LiDAR data joint classification task. In this model, according to the spectral and spatial characteristics of HSI and LiDAR, a multiscale module and a convolutional neural network (CNN) are used to capture the spectral and spatial characteristics respectively. In addition, the extracted HSI and LiDAR features are fused through some operations to obtain the feature information more in line with the real situation. Finally, the above three data are fed into different branches of the DNL module, respectively. Extensive experiments on Houston dataset show that the proposed network is superior and more effective compared to several of the most advanced baselines in HSI and LiDAR joint classification missions.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553313","National Key Research and Development Program of China(grant numbers:2018AAA0100602); National Natural Science Foundation of China(grant numbers:U1706218); Key Research and Development Program of Shandong Province(grant numbers:2019GHY112048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553313","Disentangled Non-local;self-attention;hyperspectral;LiDAR;joint classification","Laser radar;Feature extraction;Data models;Data mining;Convolutional neural networks;Task analysis;Hyperspectral imaging","feature extraction;geophysical image processing;geophysical techniques;image classification;image colour analysis;image fusion;neural nets;object detection;optical radar;remote sensing;remote sensing by laser beam;sensor fusion","Nonlocal network;hyperspectral;LiDAR data classification;single source remote sensing data;application requirements;simple yet effective attention fusion model;LiDAR data joint classification task;spectral characteristics;spatial characteristics;multiscale module;convolutional neural network;feature information;DNL module","","1","","14","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Object detection using optical and LiDAR data fusion","O. Taşar; S. Aksoy","Department of Computer Engineering, Bilkent University, Ankara, Turkey; Department of Computer Engineering, Bilkent University, Ankara, Turkey","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","7204","7207","Fusion of aerial optical and LiDAR data has been a popular problem in remote sensing as they carry complementary information for object detection. We describe a stratified method that involves separately thresholding the normalized digital surface model derived from LiDAR data and the normalized difference vegetation index derived from spectral bands to obtain candidate image parts that contain different object classes, and incorporates spectral and height data with spatial information in a graph cut framework to segment the rest of the image where such separation is not possible. Experiments using a benchmark data set show that the performance of the proposed method that uses small amount of supervision is compatible with the ones in the literature.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730879","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730879","Object detection;data fusion;graph cut","Buildings;Vegetation mapping;Laser radar;Optical imaging;Optical sensors;Eigenvalues and eigenfunctions;Vegetation","geophysical image processing;graph theory;image fusion;image segmentation;object detection;optical radar;remote sensing by laser beam;remote sensing by radar;vegetation mapping","object detection;aerial optical-LiDAR data fusion;remote sensing;stratified method;normalized digital surface model;normalized difference vegetation index;spectral bands;candidate image;object classes;spectral data;height data;spatial information;graph cut framework","","","","8","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Convolutional Neural Network Approach for Mapping Arctic Vegetation Using Multi-Sensor Remote Sensing Fusion","Z. L. Langford; J. Kumar; F. M. Hoffman","University of Tennessee, Knoxville, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA","2017 IEEE International Conference on Data Mining Workshops (ICDMW)","18 Dec 2017","2017","","","322","331","Accurate and high-resolution maps of vegetation are critical for projects seeking to understand the terrestrial ecosystem processes and land-atmosphere interactions in Arctic ecosystems, such as U.S. Department of Energy's Next Generation Ecosystem Experiment (NGEE) Arctic. However, most existing Arctic vegetation maps are at a coarse resolution and with a varying degree of detail and accuracy. Remote sensing-based approaches for mapping vegetation, while promising, are challenging in high latitude environments due to frequent cloud cover, polar darkness, and limited availability of high-resolution remote sensing datasets (e.g., ~ 5 m). This study proposes a new remote sensing based multi-sensor data fusion approach for developing high-resolution maps of vegetation in the Seward Peninsula, Alaska. We focus detailed analysis and validation study around the Kougarok river, located in the central Seward Peninsula of Alaska. We seek to evaluate the integration of hyper-spectral, multi-spectral, radar, and terrain datasets using unsupervised and supervised classification techniques over a ~343.72 km2 area for generating vegetation classifications at a variety of resolutions (5 m and 12.5 m). We fist applied a quantitative goodness-of-fit method, called Mapcurves, that shows the degree of spatial concordance between the public coarse resolution maps and k-means clustering values and relabels the k values based on the best overlap. We develop a convolutional neural network (CNN) approach for developing high resolution vegetation maps for our study region in Arctic. We compare two CNN approaches: (1) breaking up the images into small patches (e.g., 6 × 6) and predict the vegetation class for entire patch and (2) semantic segmentation and predict the vegetation class for every pixel. We also perform accuracy assessments of the developed data products and evaluate varying CNN architectures. The fusion of hyperspectral and optical datasets performed the best, with accuracy values increased from 0.64 to 0.96-0.97 when using a training map produced by unsupervised clustering and Mapcurves labeling for both CNN models.","2375-9259","978-1-5386-3800-2","10.1109/ICDMW.2017.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8215680","Multi-Sensor;Fusion;Remote Sensing;Vegetation Classificaiton","Vegetation mapping;Remote sensing;Arctic;Earth;Biological system modeling;Artificial satellites;Meteorology","geophysical image processing;image classification;image fusion;image segmentation;neural nets;pattern clustering;sensor fusion;terrain mapping;vegetation;vegetation mapping","US Department of Energy;Arctic vegetation mapping;Next Generation Ecosystem Experiment;Mapcurves labeling;unsupervised clustering;hyperspectral-optical dataset fusion;k values;training map;developed data products;vegetation class;CNN approaches;high resolution vegetation maps;public coarse resolution maps;vegetation classifications;unsupervised classification techniques;central Seward Peninsula;validation study;Alaska;multisensor data fusion approach;high-resolution remote sensing datasets;frequent cloud cover;high latitude environments;existing Arctic vegetation maps;Arctic ecosystems;land-atmosphere interactions;terrestrial ecosystem processes;multisensor remote sensing fusion;convolutional neural network approach;size 343.72 km","","5","","44","IEEE","18 Dec 2017","","","IEEE","IEEE Conferences"
"Application of Remote Sensing Technology in Crop Estimation","M. Wei; B. Qiao; J. Zhao; X. Zuo","School of Computer and Information Engineering, Henan University, Kaifeng, China; Institute of Intelligent Network System, Henan University, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China; Institute of Data and Knowledge Engineering, Henan University, Kaifeng, China","2018 IEEE 4th International Conference on Big Data Security on Cloud (BigDataSecurity), IEEE International Conference on High Performance and Smart Computing, (HPSC) and IEEE International Conference on Intelligent Data and Security (IDS)","29 Nov 2018","2018","","","252","257","Recently, with the rapid development of big data, the accuracy, speed, efficiency and the cost of remote sensing yield estimation have been greatly improved. Based on the relevant literature researches of domestic and foreign scholars in the remote sensing yield estimation for nearly ten years, we divide the process into four stages: the crop planting area extraction, the crop growth monitoring, the crop yield estimation model, and the model accuracy evaluation. In this paper, we also conclude the characteristics of each stage, evaluate the advantages and disadvantages of the commonly used methods in each stage, and explore solutions to some main problems in the field of remote sensing yield estimation, finally, through analysis and summary, we give some feasible suggestions to use the models in different situations to provide references for the future research.","","978-1-5386-4399-0","10.1109/BDS/HPSC/IDS18.2018.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8552319","Remote sensing technology;the crop planting area extraction;the crop growth monitoring;the crop yield estimation model;the model accuracy evaluation","Agriculture;Monitoring;Remote sensing;Vegetation mapping;Yield estimation;Indexes;Biological system modeling","crops;feature extraction;geophysical image processing;image classification;image fusion;image registration;image segmentation;vegetation mapping","crop yield estimation model;crop growth monitoring;crop planting area extraction;remote sensing yield estimation;remote sensing technology","","3","","23","IEEE","29 Nov 2018","","","IEEE","IEEE Conferences"
"Multisource and Multitemporal Data Fusion in Remote Sensing: A Comprehensive Review of the State of the Art","P. Ghamisi; B. Rasti; N. Yokoya; Q. Wang; B. Hofle; L. Bruzzone; F. Bovolo; M. Chi; K. Anders; R. Gloaguen; P. M. Atkinson; J. A. Benediktsson","Electrical and computer engineering, University of Iceland, Reykjavik; Department of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Aerospace engineering, University of Tokyo; College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; GIScience and 3D spatial data processing, Institute of Geography, Heidelberg University, Germany; Telecommunications, University of Trento, Italy; Communication and information technologies, University of Trento, Italy; Electrical engineering, Changchun University of Science and Technology, China; Geography, Heidelberg University, Germany; Geosciences Communitatis Europae, University of Western Brittany, Brest, France; Geography, University of Nottingham, United Kingdom; Electrical engineering, University of Iceland, Reykjavik","IEEE Geoscience and Remote Sensing Magazine","20 Mar 2019","2019","7","1","6","39","This article brings together the advances of multisource and multitemporal data fusion approaches with respect to the various research communities and provides a thorough and discipline-specific starting point for researchers at different levels (i.e., students, researchers, and senior researchers) willing to conduct novel investigations on this challenging topic by supplying sufficient detail and references. More specifically, this work provides a bird's-eye view of many important contributions specifically dedicated to the topics of pansharpening and resolution enhancement, point cloud data fusion, hyperspectral and lidar data fusion, multitemporal data fusion, and big data and social media. In addition, the main challenges and possible future research in each area are outlined and discussed.","2168-6831","","10.1109/MGRS.2018.2890023","National Natural Science Foundation of China(grant numbers:71331005); State Key Research and Development Program of China(grant numbers:2016YFE0100300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672156","","Data integration;Remote sensing;Spatial resolution;Sensors;Optical variables measurement;Atmospheric modeling;Data models;Data fusion","geophysical image processing;image enhancement;image fusion;remote sensing","resolution enhancement;pansharpening;lidar data fusion;hyperspectral data fusion;point cloud data fusion;remote sensing;multitemporal data fusion;multisource data fusion","","203","","205","IEEE","20 Mar 2019","","","IEEE","IEEE Magazines"
"Machine Learning in Pansharpening: A benchmark, from shallow to deep networks","L. -j. Deng; G. Vivone; M. E. Paoletti; G. Scarpa; J. He; Y. Zhang; J. Chanussot; A. Plaza","School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; Institute of Methodologies for Environmental Analysis, Tito Scalo, Italy; Computer Science, Complutense University of Madrid, Madrid, Spain; Department of Electrical Engineering and Information Technology, University Federico II, Naples, Italy; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; University of Grenoble Alpes, Grenoble, France; Department of Technology of Computers and Communications, Escuela Politécnica, University of Extremadura, Cáceres, Spain","IEEE Geoscience and Remote Sensing Magazine","2 Nov 2022","2022","10","3","279","315","Machine learning (ML) is influencing the literature in several research fields, often through state-of-the-art approaches. In the past several years, ML has been explored for pansharpening, i.e., an image fusion technique based on the combination of a multispectral (MS) image, which is characterized by its medium/low spatial resolution, and higher-spatial-resolution panchromatic (PAN) data. Thus, ML for pansharpening represents an emerging research line that deserves further investigation. In this article, we go through some powerful and widely used ML-based approaches for pansharpening that have been recently proposed in the related literature. Eight approaches are extensively compared. Implementations of these eight methods, exploiting a common software platform and ML library, are developed for comparison purposes. The ML framework for pansharpening will be freely distributed to the scientific community. Experimental results using data acquired by five commonly used sensors for pansharpening and well-established protocols for performance assessment (both at reduced resolution and at full resolution) are shown. The ML-based approaches are compared with a benchmark consisting of classical and variational optimization (VO)-based methods. The pros and cons of each pansharpening technique, based on the training-by-examples philosophy, are reported together with a broad computational analysis. The toolbox is provided in https://github.com/liangjiandeng/DLPan-Toolbox.","2168-6831","","10.1109/MGRS.2022.3187652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9844267","","Pansharpening;Task analysis;Spatial resolution;Pansharpening;Machine learning;Optimization;Philosophical considerations;Image fusion;Benchmark testing","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);optimisation;remote sensing","computational analysis;deep networks;higher-spatial-resolution panchromatic data;image fusion technique;machine learning;ML-based approaches;multispectral image;pansharpening technique;variational optimization-based methods","","5","","126","IEEE","28 Jul 2022","","","IEEE","IEEE Magazines"
"Multimodal and Multitemporal Spatial Data Analysis in Google Earth Engine Cloud Computing Platform to Detect Human Settlements Without Electricity: A Case Study of Bangalore City","M. B. Ujjinakoppa; U. Kumar; R. Thottolil; A. Dasgupta","Samsung R&D Institute, Bangalore, India; Spatial Computing Laboratory, Center for Data Sciences, International Institute of Information Technology Bangalore (IIITB), Bangalore, India; Spatial Computing Laboratory, Center for Data Sciences, International Institute of Information Technology Bangalore (IIITB), Bangalore, India; Spatial Computing Laboratory, Center for Data Sciences, International Institute of Information Technology Bangalore (IIITB), Bangalore, India","2021 IEEE International India Geoscience and Remote Sensing Symposium (InGARSS)","13 Jun 2022","2021","","","238","241","Accurate information about the human settlements without electricity is essential for monitoring the areas deprived of access to electricity and to end the darkness. Motivated by the 2021 IEEE GRSS Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society (GRSS), the objective of this research was to assess the human settlements without electricity for areas in and around Bangalore City. We used multimodal and multitemporal data of the year 2019 with 27 layers such as Landsat-8 OLI bands, Sentinel-1 C Band (SAR data) with VV and VH polarization, spectral indices (EVI, NDVI, MNDWI, NDBI, NDMI, BSI, SAVI, IBI, BuEI and SoEI), Texture parameters (DISS, Entropy and Angular Second Moment), Topological data (slope and elevation), and land surface temperature to detect land use map with urban builtup, vegetation, water and barren land classes with a spatial resolution of 30 m using object-based Random Forest algorithm. To overcome the computational limitations, all the analyses were carried out in Google Earth Engine (GEE) cloud-based platform that has planetary-scale analysis capabilities. Overall, 39 experiments on classification were carried out with various combinations of feature vectors to obtain the most accurate land use map with 4 classes. Composite of Landsat bands and advantages of other spectral indices with thresholds rendered the highest classification accuracy of 93.49%. The final mapping results of human settlements without electricity was obtained by comparing binary classified maps with resampled VIIRS night-time light imagery. The results revealed that the total area of human settlements without electricity in Bangalore City is approximately 36.57 sq. km. accounting for 6.2% of the total study area.","","978-1-6654-4249-7","10.1109/InGARSS51564.2021.9792041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792041","Urban builtup;Google Earth Engine;multimodal;multitemporal;remote sensing;machine learning","Earth;Cloud computing;Temperature distribution;Artificial satellites;Urban areas;Data integration;Vegetation mapping","cloud computing;data analysis;geophysical image processing;geophysical signal processing;geophysical techniques;image classification;image fusion;land surface temperature;remote sensing;sensor fusion;synthetic aperture radar;terrain mapping","Google Earth Engine cloud computing platform;Bangalore City;2021 IEEE GRSS Data Fusion Contest;Image Analysis;Data Fusion Technical Committee;multimodal data;multitemporal data;Landsat-8 OLI bands;SAR data;spectral indices;land surface temperature;barren land classes;object-based Random Forest algorithm;Google Earth Engine cloud-based platform;planetary-scale analysis capabilities;land use map;Landsat bands","","","","7","IEEE","13 Jun 2022","","","IEEE","IEEE Conferences"
"A HSV-Based Fusion of InIRA SAR and GoogleEarth Optical Images","D. Li; Y. Zhang; X. Dong; X. Shi; W. Zhai","Chinese Acadcmy of Sceicnes, Key Laboratory of Microwave Remote Sensing National Space Science Center, Beijing, China; Chinese Acadcmy of Sceicnes, Key Laboratory of Microwave Remote Sensing National Space Science Center, Beijing, China; Chinese Acadcmy of Sceicnes, Key Laboratory of Microwave Remote Sensing National Space Science Center, Beijing, China; Chinese Acadcmy of Sceicnes, Key Laboratory of Microwave Remote Sensing National Space Science Center, Beijing, China; Chinese Acadcmy of Sceicnes, Key Laboratory of Microwave Remote Sensing National Space Science Center, Beijing, China","2018 Asia-Pacific Microwave Conference (APMC)","17 Jan 2019","2018","","","848","850","Interferometric Imaging Radar Altimeter (InIRA) on board Chinese TG-2 space laboratory is dedicated to enable a wide-swath measure of the sea surface height by interferometric processing synthetic aperture radar (SAR) images. In this paper, a simple fusion algorithm is developed for InIRA SAR image and GoogleEarth optical image based on hue-saturation-value (HSV) color model. While the H and S components of optical image are reserved for the fusion, the V component of the fusion image is a weighted linear combination of SAR image and the V component of optical image, with the image entropy as weight. Experimental results exhibit not only the nice performance of the fusion scheme but also the potential of InIRA in inland water observation.","","978-4-9023-3945-1","10.23919/APMC.2018.8617352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8617352","HSV;image entropy;image fusion;synthetic aperture radar;optical sensor","Synthetic aperture radar;Optical imaging;Optical interferometry;Optical scattering;Optical sensors;Rivers","entropy;geophysical image processing;image colour analysis;image fusion;oceanographic techniques;radar altimetry;radar imaging;radar interferometry;synthetic aperture radar","Chinese TG-2 space laboratory;fusion algorithm;HSV-based fusion;fusion scheme;image entropy;fusion image;hue-saturation-value color model;GoogleEarth optical image;InIRA SAR image;interferometric processing synthetic aperture radar images;sea surface height;wide-swath measure;Interferometric Imaging Radar Altimeter","","1","","6","","17 Jan 2019","","","IEEE","IEEE Conferences"
"Automatic fusion and classification using random forests and features extracted with deep learning","A. Merentitis; C. Debes","AGT International, Darmstadt, Germany; AGT International, Darmstadt, Germany","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","2943","2946","Fusion of different sensor modalities has proven very effective in numerous remote sensing applications. However, in order to benefit from fusion, advanced feature extraction mechanisms that rely on domain expertise are typically required. In this paper we present an automated feature extraction scheme based on deep learning. The feature extraction is unsupervised and hierarchical. Furthermore, computational efficiency (often a challenge for deep learning methods) is a primary goal in order to make certain that the method can be applied in large remote sensing datasets. Promising classification results show the applicability of the approach for both reducing the gap between naive feature extraction and methods relying on domain expertise, as well as further improving the performance of the latter in two challenging datasets.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326432","","Feature extraction;Machine learning;Laser radar;Hyperspectral imaging;Data integration;Correlation","feature extraction;geophysical image processing;image classification;image fusion;remote sensing by laser beam","automatic fusion;automatic classification;random forests;deep learning feature extraction;sensor modality fusion;remote sensing applications;advanced feature extraction mechanisms;automated feature extraction scheme;remote sensing datasets","","10","","7","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"A Line Objects Recognition Algorithm based on Non-subsampled Contourlet Transform","Q. Jiang; Y. Yang; M. Zhang; S. Zhang","State Key Laboratory for Strength and Vibration of Mechanical Structures, Xi’an Jiaotong University, Xi’an; State Key Laboratory for Strength and Vibration of Mechanical Structures, Xi’an Jiaotong University, Xi’an; State Key Laboratory for Strength and Vibration of Mechanical Structures, Xi’an Jiaotong University, Xi’an; State Key Laboratory for Strength and Vibration of Mechanical Structures, Xi’an Jiaotong University, Xi’an","2020 39th Chinese Control Conference (CCC)","9 Sep 2020","2020","","","3050","3055","Aiming at the recognition of line features represented by bridges and airport runways in remote sensing data, this paper presents a line objects extraction algorithm based on Non-subsampled Contourlet Transform(NSCT). Firstly, NSCT is used to decompose the source image into multiple layers and directions to obtain the corresponding frequency sub-bands. Then the Line Segment Detection(LSD) algorithm with visual saliency is used to detect the line segments. Finally, line objects recognition is completed by multi-scale and multi-direction line features combination according to the established fusion rules. Comparison with some traditional algorithms verifies that the proposed algorithm can improve the recognition accuracy.","1934-1768","978-9-8815-6390-3","10.23919/CCC50068.2020.9188900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9188900","Line feature;NSCT;LSD;fusion","Image segmentation;Object recognition;Image edge detection;Feature extraction;Visualization;Transforms;Image fusion","feature extraction;image fusion;image segmentation;object recognition;remote sensing;transforms","airport runways;remote sensing data;extraction algorithm;source image;frequency subbands;line segments;line object recognition algorithm;multidirection line feature combination;nonsubsampled contourlet transform;NSCT;LSD algorithm;fusion rules;visual saliency","","2","","11","","9 Sep 2020","","","IEEE","IEEE Conferences"
"Fusion of themis and TES for accurate Mars surface characterization","C. Kwan; B. Ayhan; B. Budavari","Signal Processing, Inc.; Signal Processing, Inc.; Signal Processing, Inc.","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","3381","3384","This paper presents a novel approach to fusing Thermal Emission Imaging System (THEMIS) and Thermal Emission Spectrometer (TES) satellite images, aiming to improve Mars surface characterization performance from orbit. Our approach includes proven registration and advanced pansharpening algorithms developed by us and others. Preliminary experiments show that the fusion approach is highly promising despite the extremely high resolution difference of THEMIS and TES (30 to 1). We also observed some potential issues that require further research.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127723","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127723","image fusion;pansharpening;THEMIS;TES;registration","","geophysical image processing;image fusion;image resolution;Mars;planetary surfaces;remote sensing","TES;Mars surface characterization performance;fusion approach;thermal emission spectrometer satellite images;advanced pansharpening algorithms","","10","","25","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Pan-Sharpening Performance Comparison for Land Use Classification Application, and Its Effect on LISA LAPAN-A3 in Accuracy Improvement","A. Wahyudiono; E. Asti Anggari; A. Herawan; P. Rachman Hakim; A. Hadi Syafrudin; E. Rachim","Research Center for Satellite Technology, National Research and Innovation Agency (BRIN), Bogor, Indonesia; Research Center for Satellite Technology, National Research and Innovation Agency (BRIN), Bogor, Indonesia; Research Center for Satellite Technology, National Research and Innovation Agency (BRIN), Bogor, Indonesia; Research Center for Satellite Technology, National Research and Innovation Agency (BRIN), Bogor, Indonesia; Research Center for Satellite Technology, National Research and Innovation Agency (BRIN), Bogor, Indonesia; Research Center for Satellite Technology, National Research and Innovation Agency (BRIN), Bogor, Indonesia","2022 IEEE International Conference on Aerospace Electronics and Remote Sensing Technology (ICARES)","30 Dec 2022","2022","","","1","6","Pan-sharpening is one data fusion application that aims to increase the spatial resolution of the multi-spectral image by merging a low-resolution multispectral image with a high-resolution panchromatic image. This process is commonly used to increase the quality of images in the application of land use classification. This research aims to see and learn about the performance of the pan-sharpening method in terms of Land Use Classifications. 5 different methods are compared to see each performance in classification. Moreover, not only using a single-platform data, which is multi-spectral (MS) and panchromatic (Pan) image from Landsat 8, this research also tries to fuse 2 data from a different platform, which are MS from LISA LAPAN-A3 and Pan from Landsat 8. It found that each pan-sharpening method has a different result in terms of accuracy when applied to single-platform data and cross-platform data, nevertheless, some improvements in accuracy were slightly found in pan-sharpened LISA's product to a 9.31% increase.","","978-1-6654-6191-7","10.1109/ICARES56907.2022.9993548","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9993548","pan-sharpening;image fusion;land use classification;lapan-a3;Lisa;Landsat","Earth;Artificial satellites;Fuses;Merging;Data integration;Aerospace electronics;Spatial resolution","geophysical image processing;geophysical signal processing;image fusion;image resolution;remote sensing;sensor fusion","5 different methods;cross-platform data;data fusion application;different platform;high-resolution panchromatic image;Land Use Classifications;Landsat 8;LISA LAPAN-A3;low-resolution multispectral image;pan-sharpened LISA's product;pan-sharpening method;Pan-sharpening performance comparison;single-platform data;spatial resolution","","","","19","IEEE","30 Dec 2022","","","IEEE","IEEE Conferences"
"A New Spatio-Temporal Fusion Method for Blending Landsat and MODIS Data in Heterogeneous Area","B. Ping; Y. Meng","School of Earth System Science, Institute of Surface-Earth System Science, Tianjin University, Tianjin, China; National Marine Data and Information Service, Tianjin, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2560","2563","Landsat and MODIS imagery have been widely employed for many applications because of their rich archives and free availability, however, the trade-off between the spatial and temporal resolutions has limited their capacities in monitoring detailed spatio-temporal dynamics. Linear spatio-temporal fusion methods which consider the varieties between fine- and coarse-resolution data as linear can effectively solve this problem, and the coefficients of the linear model are key parameters for fusion accuracy. The existing linear fusion methods either adopt regression algorithms to fit these parameters or set these parameters as constants, both of which may introduce some errors. In this article, we proposed a new method to eliminate the influences of parameters calculation. Instead of calculating the slope and intercept of the linear model directly, these parameters can be both eliminated by equation transformation. In addition, spectrally similar pixels and their corresponding weights were used to mitigate block effects. Compared to the STARFM method, the proposed method can get better fusion accuracy.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553231","spaio-temporal fusion;Landsat;MODIS;image fusion","Earth;Visualization;Artificial satellites;Mathematical models;Data models;Spatiotemporal phenomena;Spatial resolution","geophysical image processing;geophysical techniques;image fusion;image resolution;regression analysis;remote sensing;sensor fusion;spatiotemporal phenomena;vegetation mapping","blending landsat;MODIS data;heterogeneous area;rich archives;free availability;spatial resolutions;temporal resolutions;detailed spatio-temporal dynamics;linear spatio-temporal fusion methods;coarse-resolution data;linear model;fusion accuracy;existing linear fusion methods;parameters calculation;STARFM method;new spatio-temporal fusion method","","","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Improved Multiresolution Analysis Method for Hyperspectral Pansharpening","X. Hu; Y. Shi; W. Li; R. Tao","College of Information Science & Technology, Beijing University of Chemical Technology; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","2778","2781","The fusion of Panchromatic (PAN) and Hyperspectral image (HSI) aims at improving resolution in spatial and spectral domain simultaneously. Multiresolution analysis is a widely used method for Mutispectral or HSI pansharpening. However, only detail information from PAN is considered while ignoring the detail information from HSI. In this paper, an improved approach based on multiresolution analysis is proposed, which extracts detail information from both PAN and HSI by choosing optimal multiresolution layers. Another contribution is that we discuss the weight when fusing the detail information. The experimental results demonstrate that the proposed method can provide better quality metrics and visual effects when compared with some existing methods.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898539","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898539","Hyperspectral Image;Panchromatic Image;Multiresolution;Image Fusion.","Spatial resolution;Data mining;Principal component analysis;Hyperspectral imaging;Multiresolution analysis;Information filtering","geophysical image processing;image fusion;image resolution;remote sensing","multiresolution analysis method;hyperspectral pansharpening;PAN;HSI;spatial domain;spectral domain;optimal multiresolution layers;quality metrics;visual effects","","","","15","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Thermal sharpening of VIIRS data","G. Picaro; P. Addesso; R. Restaino; G. Vivone; D. Picone; M. Dalla Mura","DIEM, University of Salerno, Salerno, Italy; DIEM, University of Salerno, Salerno, Italy; DIEM, University of Salerno, Salerno, Italy; NATO STO Centre for Maritime Research and Experimentation, La Spezia, Italy; DIEM, University of Salerno, Salerno, Italy; GIPSA-Lab, Grenoble Institute of Technology, France","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","7260","7263","Thermal Sharpening (TS) is usually referred to techniques widely used in several Earth Observation applications in order to increase the spatial resolution of thermal images. Profiting from the particular design of the Visible Infrared Imaging Radiometer Suite (VIIRS) sensor mounted on board of the Suomi National Polar-orbiting Partnership (NPP) satellite, we propose here a new approach for obtaining synthetic thermal data with increased spatial resolution and spectral diversity. The method exploits classical Pansharpening algorithms, which are very popular in the field of Visible and Near-InfraRed (VNIR) image fusion, for combining the VIIRS thermal bands with partially overlapping spectral responses. We evaluate the effectiveness of several algorithms by performing a Reduced Resolution (RR) assessment on VIIRS real data, showing the importance of an adequate knowledge of the sensor characteristics.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730893","Remote Sensing;Thermal Sharpening","Spatial resolution;Remote sensing;Algorithm design and analysis;Data integration;Multiresolution analysis;Earth","geophysical image processing;image resolution;remote sensing","thermal sharpening;VIIRS data;Earth Observation applications;image spatial resolution;Visible Infrared Imaging Radiometer Suite sensor;Suomi National Polar orbiting Partnership satellite;spectral diversity","","5","","10","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Fused Recurrent Network Via Channel Attention For Remote Sensing Satellite Image Super-Resolution","X. Li; D. Zhang; Z. Liang; D. Ouyang; J. Shao","University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China","2020 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2020","2020","","","1","6","Remote sensing satellite images often suffer from low spatial resolution. Image super-resolution plays an important role in remote sensing image processing. However, existing methods show that increasing network depth will inevitably lead to the dramatic increase of model parameters and the over-fitting problem. Besides, most methods treat different types of information (low-frequency and high-frequency) equally. Motivated by these observations, we propose a fused recurrent network via channel attention (CA-FRN) in this paper. The basic module, recursive channel attention block (RCAB), pays enough attention to the high-frequency information and diminishes the low-frequency information adaptively through channel attention. Based on RCAB, we render our model effective by retaining and fusing hierarchical local information of both low-resolution and high-resolution, and we enhance the network performance simply by increasing the number of RCABs without adding extra parameters. We evaluate the proposed model on satellite images from different datasets, and the proposed CA-FRN is superior to the state-of-the-art methods. Code is available at https://github.com/lxy0922/CAFRN.","1945-788X","978-1-7281-1331-9","10.1109/ICME46284.2020.9102948","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9102948","Satellite image super-resolution;fused recurrent network;channel attention","Satellites;Remote sensing;Spatial resolution;Recurrent neural networks;Feature extraction;Image reconstruction","artificial satellites;geophysical image processing;image fusion;image resolution;recurrent neural nets;remote sensing","remote sensing satellite image super-resolution;low spatial resolution;remote sensing image processing;fused recurrent network;recursive channel attention block;RCAB;high-frequency information;low-frequency information;hierarchical local information;CA-FRN","","1","","23","IEEE","9 Jun 2020","","","IEEE","IEEE Conferences"
"Change Detection of Tidal Flat Images based on Siamese Network","Z. Mei; W. Zheng; Y. Fan; W. Hu","School of Data Science Engineering, East China Normal University, Shanghai, China; Information Technology Services, East China Normal University, Shanghai, China; School of Data Science Engineering, East China Normal University, Shanghai, China; School of Data Science Engineering, East China Normal University, Shanghai, China","2022 4th International Conference on Advances in Computer Technology, Information Science and Communications (CTISC)","15 Aug 2022","2022","","","1","7","In recent times, deep learning-based method have attained great success in the problem of change detection in tidal flats’s satellite remote sensing images. However, the observation of changes requires processing multiple time series images, which leads to excessive computation of deep learning models. In addition, remote sensing images often have problems such as missing pixels, which challenges the receptive field of the model. In this paper, we advance a innovative method of MSIC mosaic synthesis, which is used to synthesize the multi-series remote sensing satellite images into the highest and lowest tide level maps. The MSIC mosaic synthesis greatly reduces the computational complexity of the model. In the meantime, a model based on the Siamese network structure is also proposed, which takes bi-temporal images as input, and improves the model effect by fusing the feature pyramid module. We use the self-made landsat8 satellite remote sensing images as data to conduct experiments, and achieve a dice coefficient value of 72.8%.Experimental results show that our model obtain remarkable performance, and exceeds several state-of-the-art methods.","","978-1-6654-5872-6","10.1109/CTISC54888.2022.9849717","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9849717","remote sensing images;Siamese network;MSIC mosaic synthesis;feature pyramid","Learning systems;Image segmentation;Information science;Satellites;Image synthesis;Fuses;Computational modeling","geophysical image processing;image classification;image fusion;image representation;image resolution;learning (artificial intelligence);remote sensing;time series","highest tide level maps;lowest tide level maps;MSIC mosaic synthesis;computational complexity;Siamese network structure;bi-temporal images;model effect;landsat8 satellite remote;change detection;tidal flat images;deep learning-based method;tidal flats;multiple time series images;excessive computation;deep learning models;remote sensing images;receptive field;innovative method;multiseries remote sensing satellite images","","","","25","IEEE","15 Aug 2022","","","IEEE","IEEE Conferences"
"Improving Spectral Quality of IHS-Pansharpening Result by Integrating Equalization Process using SVE-DWT for Satellite Imagery Data","D. P. Trijayanto; H. Tjandrasa","Department of Informatics, Sepuluh Nopember Institute of Technology (ITS), Surabaya, Indonesia; Department of Informatics, Sepuluh Nopember Institute of Technology (ITS), Surabaya, Indonesia","2019 12th International Conference on Information & Communication Technology and System (ICTS)","30 Sep 2019","2019","","","1","6","Remote Sensing satellite such as WorldView-4 has limitations in producing images that have advantages on both spectral and spatial resolution. This problem makes it necessary to divide the images into high spectral resolution multispectral (MS) and high spatial resolution panchromatic (PAN) images. The method of image fusing such as intensity-hue-saturation (IHS) pansharpening causes spectral distortion. This drawback is caused by overusing the pixel values from a PAN image. In this study, a new method was proposed that integrating IHS-pansharpening with Singular Value Equalization (SVE) as equalization process and Discrete Wavelet Transform (DWT) for preserving edge and detail information of the PAN image. The SVE method equalizes values on PAN images by intensity values in the IHS transformation process. The spectral quality produced from the proposed method shows the superiority to the original method when tested using spectral quality metrices like: RMSE, Spectral Angle Mapper (SAM) and Relative Dimensionless Synthesis Global Error (ERGAS).","","978-1-7281-2133-8","10.1109/ICTS.2019.8850974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8850974","Remote Sensing;IHS-Pansharpening;DWT;SVE","Spatial resolution;Satellites;Discrete wavelet transforms;Mathematical model;Distortion;Remote sensing","discrete wavelet transforms;geophysical image processing;geophysical techniques;image fusion;image resolution;image sensors;remote sensing;wavelet transforms","spectral quality metrices;IHS transformation process;intensity values;SVE method;Singular Value Equalization;PAN image;pixel values;spectral distortion;intensity-hue-saturation pansharpening;image fusing;high spatial resolution panchromatic images;high spectral resolution multispectral;WorldView-4;Sensing satellite;satellite imagery data;SVE-DWT;equalization process;IHS-pansharpening result","","","","23","IEEE","30 Sep 2019","","","IEEE","IEEE Conferences"
"Pan-sharpening based on sparse representation","S. Ayas; E. T. Görmüs; M. Ekinci","Bilgisayar Mühendisliği Bölümü, Karadeniz Teknik Üniversitesi, Trabzon, Türkiye; Harita Mühendisliği Bölümü, Karadeniz Teknik Üniversitesi, Trabzon, Türkiye; Bilgisayar Mühendisliği Bölümü, Karadeniz Teknik Üniversitesi, Trabzon, Türkiye","2017 25th Signal Processing and Communications Applications Conference (SIU)","29 Jun 2017","2017","","","1","4","Pan-sharpening in remote sensing aims to obtain a multispectral image with high spectral and spatial information by combining spectral information of a low resolution multispectral image and spatial information of a high resolution panchromatic image. In this paper, a pan-sharpening method based on sparse representation was proposed. Firstly, a dictionary was learned from the multispectral image patches. Then, sparse coefficients of low resolution multispectral image and high resolution panchromatic image were calculated. Thus, pan-sharpened multispectral image was obtained by using these sparse coefficients and dictionary. The IKONOS satellite image was used to test the proposed method. The quantitative and visual results demonstrate the effectiveness of the proposed method in pan-sharpening of the remote sensing images.","","978-1-5090-6494-6","10.1109/SIU.2017.7960662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960662","Pan-sharpening;sparse representation;dictionary learning;IKONOS","Remote sensing;Spatial resolution;Dictionaries;Satellites;Image fusion;Principal component analysis","geophysical image processing;image representation;image resolution;remote sensing;sparse matrices","remote sensing;spectral information;spatial information;low-resolution multispectral image;high-resolution panchromatic image;sparse representation;dictionary learning;multispectral image patches;sparse coefficients;pan-sharpened multispectral image;IKONOS satellite image;quantitative analysis;visual analysis","","","","12","IEEE","29 Jun 2017","","","IEEE","IEEE Conferences"
"Multispectral and hyperspectral data fusion based on SAM minimization band assignment approach","D. Picone; R. Restaino; G. Vivone; P. Addesso; M. Dalla Mura; J. Chanussot","Department of Information Eng, University of Salerno; Department of Information Eng, University of Salerno; NATO STO Centre for Maritime Research and Experimentation; Department of Information Eng, University of Salerno; GIPSA-Lab, Grenoble Institute of Technology, France; Faculty of Electrical and Computer Engineering, University of Iceland","2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Oct 2017","2016","","","1","5","The sharpening of hyperspectral (HS) images introduces novel questions that have never been faced by classical pansharpening, which deals with the fusion of multispectral and panchromatic images. In this paper, we focus on the fusion of high resolution MultiSpectral (MS) and low resolution HS data, namely tackling the problem of assigning the optimal MS channel for each HS band through the minimization of the Spectral Angle Mapper (SAM) metric. The performance is assessed on two datasets, both composed by a HS and a MS image acquired by the Hyperion and the ALI sensors, respectively. Several MultiResolution Analysis pansharpening approaches are used for evaluating the performance improvements with respect to existing methods.","2158-6276","978-1-5090-0608-3","10.1109/WHISPERS.2016.8071722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071722","Data fusion;Hyperspectral data;Multispectral images;Hyperion sensor;ALI sensor","Indexes;Image resolution;Sensors;Image sensors;Hyperspectral sensors;Algorithm design and analysis;Optimized production technology","hyperspectral imaging;image fusion;image resolution;remote sensing","multispectral data fusion;hyperspectral data fusion;SAM minimization band assignment approach;high resolution MultiSpectral;low resolution HS data;optimal MS channel;Spectral Angle Mapper metric;MultiResolution Analysis pansharpening approaches;hyperspectral image sharpening;multispectral image fusion;panchromatic image fusion;Hyperion sensors;ALI sensors","","","","21","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"An application of panoramic mosaic in UAV aerial image","J. Hu; Y. Zhou; C. Zhao; Q. Pan; K. Zhang; Z. Xu","School of Automation, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Automation, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Automation, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Automation, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, Shaanxi, China","2017 13th IEEE International Conference on Control & Automation (ICCA)","7 Aug 2017","2017","","","1049","1053","Panoramic mosaic plays an important role in the field of computer vision, robot navigation and virtual reality. This paper summarizes the specific process of panoramic stitching and proposes a coordinate transformation stitching method based on down-sampling. In order to reduce the processing time, images are compressed by down-sampling before processing, and spliced in the corresponding points of original images. Especially, the middle image is chosen as the reference image and the others are directly spliced onto it by transformation matrix. Considering the illumination of UAV aerial images, just one overlapping region of the adjacent images is remained when doing the fusion. In the experiment, images with different resolutions are tested. The results show the performance of good efficiency and little time consuming.","1948-3457","978-1-5386-2679-5","10.1109/ICCA.2017.8003206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8003206","","Feature extraction;Image resolution;Image registration;Splicing;Image coding;Lighting;Image fusion","data compression;geophysical image processing;image coding;image fusion;image resolution;image sampling;image segmentation;lighting;remote sensing;robot vision","panoramic mosaic;panoramic stitching;coordinate transformation stitching method;down-sampling;image compression;transformation matrix;UAV aerial images illumination;image fusion;image resolution","","1","","22","IEEE","7 Aug 2017","","","IEEE","IEEE Conferences"
"An effective pansharpening method for WorldView-2 satellite images","Xu Li; Weifeng Qi","School of Electronics and Information, Northwestern Polytechnical University, Xi'an, P. R. China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, P. R. China","2015 International Conference on Estimation, Detection and Information Fusion (ICEDIF)","1 Oct 2015","2015","","","88","92","Pansharpening has been an important tool in remote sensing applications, which transforms a set of low-spatial-resolution multispectral images to high-spatial-resolution images by fusing a co-registered fine-spatial-resolution panchromatic image. The new style very high-resolution WorldView-2 satellite images have posed challenges to the image fusion techniques. An effective pansharpening method based on correspondence analysis is presented in this paper. The experimental results show that the presented method can effectively obtain a better trade-off between the spectral fidelity and the spatial resolution enhancement compared to some existing methods.","","978-1-4799-6418-5","10.1109/ICEDIF.2015.7280168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280168","pansharpening;resolution;multispectral;panchromatic","Spatial resolution;Linear regression;Roads;Buildings;Indexes","image fusion;remote sensing","effective pansharpening method;WorldView-2 satellite image;remote sensing application;low-spatial-resolution multispectral image;high-spatial-resolution image;co-registered fine-spatial-resolution panchromatic image fusion;image fusion technique;correspondence analysis;spectral fidelity;spatial resolution enhancement","","","","11","IEEE","1 Oct 2015","","","IEEE","IEEE Conferences"
"Sar And Optical Data Fusion Based On Anisotropic Diffusion With Pca And Classification Using Patch-Based Svm With Lbp","A. Shakya; M. Biswas; M. Pal","Computer Engineering Department, National Institute of Technology, Kurukshetra, India; Computer Engineering Department, National Institute of Technology, Kurukshetra, India; Civil Engineering Department, National Institute of Technology, Kurukshetra, India","2020 IEEE India Geoscience and Remote Sensing Symposium (InGARSS)","23 Feb 2021","2020","","","25","28","SAR (VV and VH polarization) and optical data are widely used in image fusion to use the complimentary information of each other and to obtain the better-quality image (in terms of spatial and spectral features) for the improved classification results. The optical data acquisition depends on whether conditions while SAR data can acquire the data in presence of clouds. This paper uses anisotropic diffusion with PCA for the fusion of SAR (Sentinel 1 (S1)) and Optical (Sentinel 2 (S2)) data for patch-based SVM Classification with LBP (LBP-PSVM). Fusion results with VV polarization performed better than VH polarization using considered fusion method. Classification results suggests that the LBP-PSVM classifier is more effective in comparison to SVM and PSVM classifiers for considered data.","","978-1-7281-3114-6","10.1109/InGARSS48198.2020.9358949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9358949","Fusion;SVM;Anisotropic Diffusion;Principal Component Analysis;Local Binary Pattern","Support vector machines;Optical polarization;Optical imaging;Geometrical optics;Optical sensors;Synthetic aperture radar;Principal component analysis","feature extraction;geophysical image processing;image classification;image fusion;principal component analysis;radar imaging;remote sensing by radar;support vector machines;synthetic aperture radar","optical data fusion;anisotropic diffusion;VH polarization;image fusion;complimentary information;better-quality image;spatial features;spectral features;improved classification results;optical data acquisition;SAR data;PCA;VV polarization;fusion method;LBP-PSVM classifier;patch-based SVM classification","","1","","15","IEEE","23 Feb 2021","","","IEEE","IEEE Conferences"
"CNN-Based Fusion and Classification of Multi-Temporal Sentinel-1 & -2 Satellite Data","A. Shakya; M. Biswas; M. Pal","Computer Engineering Department, National Institute of Technology, Kurukshetra, India; Computer Engineering Department, National Institute of Technology, Kurukshetra, India; Civil Engineering Department, National Institute of Technology, Kurukshetra, India","2021 IEEE International India Geoscience and Remote Sensing Symposium (InGARSS)","13 Jun 2022","2021","","","57","60","SAR and optical data are widely used in image fusion to provide the complimentary information of each other and obtain the spatial and spectral features for improved classifications. This paper proposes to use multi-temporal data form Sentinel-1 (VV & VH polarization) and Sentinel-2 sensors for the fusion and classification over an agricultural area. Convolutional Neural Network (CNN)- based Pyramid method for fusion and Bayesian Optimized 2-D CNN for classification of fused multi-temporal data was used to extract spatial-spectral information. Results in terms of classification accuracy suggests slightly better performance by VV polarized fused images than the VH and also suggests an improved performance by multi-temporal data in comparison to the single date data over the study area.","","978-1-6654-4249-7","10.1109/InGARSS51564.2021.9791998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9791998","Fusion;Classification;Convolutional Neural Network (CNN);Bayesian Optimization","Satellites;Optical polarization;Neural networks;Sensor fusion;Optical imaging;Bayes methods;Convolutional neural networks","agriculture;convolutional neural nets;geophysical image processing;image classification;image fusion;remote sensing;synthetic aperture radar","multitemporal Sentinel-1 satellite data;multitemporal Sentinel-2 satellite data;Bayesian optimized 2D CNN;Sentinel-2 sensors;VV & VH polarization;spectral features;spatial features;complimentary information;image fusion;optical data;CNN-based fusion;single date data;VV polarized fused images;classification accuracy;spatial-spectral information;fused multitemporal data;convolutional neural network","","","","10","IEEE","13 Jun 2022","","","IEEE","IEEE Conferences"
"The Time Variable in Data Fusion: A Change Detection Perspective","F. Bovolo; L. Bruzzone","Fondazione Bruno Kessler, Center for Information and Communication Technologies, Povo, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Povo, Trento, Italy","IEEE Geoscience and Remote Sensing Magazine","30 Sep 2015","2015","3","3","8","26","This paper presents an overview on the image fusion concept in the context of multitemporal remote sensing image processing. In the remote sensing literature, multitemporal image analysis mainly deals with the detection of changes and land-cover transitions. Thus the paper presents and analyses the most relevant literature contributions on these topics. From the perspective of change detection and detection of land-cover transitions, multitemporal image analysis techniques can be divided into two main groups: those based on the fusion of the multitemporal information at feature level, and those based on the fusion of the multitemporal information at decision level. The former mainly exploit multitemporal image comparison techniques, which aim at highlighting the presence/absence of changes by generating change indices. These indices are then analyzed by unsupervised algorithms for extracting the change information. The latter rely mainly on classification and include both supervised and semi/partially-supervised/unsupervised methods. The paper focuses the attention on both standard (and largely used) methods and techniques proposed in the recent literature. The analysis is conducted by considering images acquired by optical and SAR systems at medium, high and very high spatial resolution.","2168-6831","","10.1109/MGRS.2015.2443494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284786","","Feature extraction;Optical sensors;Optical imaging;Remote sensing;Nonlinear optics;Data integration;Data mining","geophysical image processing;image classification;image fusion;image resolution;land cover;optical radar;radar imaging;radar resolution;remote sensing by radar;synthetic aperture radar","data fusion;image fusion concept;multitemporal remote sensing image processing;land-cover transition;multitemporal image analysis technique;change information extraction;semipartially-supervised-unsupervised method;image classification;SAR system;image resolution;optical system","","110","","145","IEEE","30 Sep 2015","","","IEEE","IEEE Magazines"
"Fusion of RADARSAT-2 imagery with LANDSAT-8 multispectral data for improving land cover classification performance using SVM","C. Sukawattanavijit; J. Chen","School of Electronics and Information Engineering, Beihang University Beijing, CHINA; School of Electronics and Information Engineering, Beihang University Beijing, CHINA","2015 IEEE 5th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)","29 Oct 2015","2015","","","567","572","Study of the land cover classification using multi-source data are very important for eco-environment monitoring, land use planning and climatic change detection. In this study, the utility of multi-source RADARSAT-2 and LANDSAT-8 multi-spectral images for improving land cover classification performance using Support Vector Machine (SVM) classifier. HH polarized C band RADARSAT-2 images were fused with the three band (6, 5, and 4) LANDSAT-8 multispectral image for land cover classification. Wavelet-based fusion (WT) techniques are implemented in the data fusion process. The Radial Basic Function (RBF) kernel function were used for SVM classifier in order to classify land cover types in the study area. The results of the SVM classification were compared with those using standard method Maximum Likelihood (ML) classifier, and it demonstrates a higher accuracy. Finally, it was indicated by the study that the fusion of SAR and optical images can significantly improve the classification accuracy with respect to use single dataset, and the SVM classifier could clearly outperform the standard method the ML classifier.","","978-1-4673-7297-8","10.1109/APSAR.2015.7306273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306273","image fusion;RADARSAT-2;LANDSAT-8;land cover classification;Support Vector Machine (SVM)","Support vector machines;Remote sensing;Satellites;Earth;Accuracy;Kernel;Training","climatology;environmental monitoring (geophysics);geophysical image processing;image classification;image fusion;land cover;land use planning;radar imaging;radial basis function networks;remote sensing by radar;satellite communication;support vector machines;wavelet transforms","HH polarized C band RADARSAT-2 imagery fusion;LANDSAT-8 multispectral data;land cover classification performance improvement;SVM classifier;support vector machine classifier;multisource data;eco-environment monitoring;land use planning;climatic change detection;wavelet-based fusion techniques;WT techniques;data fusion process;radial basic function kernel function;RBF kernel function","","5","","27","IEEE","29 Oct 2015","","","IEEE","IEEE Conferences"
"A CNN-Based Pansharpening Method with Perceptual Loss","S. Vitale","Dipartimento di Ingegneria, Università di Napoli Parthenope (I)","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3105","3108","Pansharpening is a classical data fusion task that is often necessary when dealing with data sensed through multiresolution acquisition systems. These systems, in fact, provide a single panchromatic band at full spatial resolution coupled with a multispectral lower resolution image of the same scene, which must be fused (pansharpened) to generate a full spatial-spectral resolution datacube. In the last few years, there has been a methodological shift in pansharpening towards the deep learning (DL) paradigm. Most DL solutions proposed thus far use self-supervised learning. Training is carried out on data at downgraded resolution, where ground truth data are also available. Then, the trained network is applied to perform pansharpening on native resolution data. As a consequence, such solutions show good results on low-resolution datasets, but less convincing results on full-resolution data, due to limited generalization ability. In this work, to address this problem, we enrich the training loss function with a perceptual term computed on full-resolution data, obtaining promising experimental results.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900390","Data fusion;deep learning;convolutional neural network;super-resolution","Spatial resolution;Training;Feature extraction;Data integration;Deep learning;Remote sensing","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);remote sensing","deep learning paradigm;spatial-spectral resolution datacube;pansharpened;multispectral lower resolution image;spatial resolution;single panchromatic band;multiresolution acquisition systems;classical data fusion task;perceptual loss;CNN-based pansharpening;training loss function;full-resolution data;low-resolution datasets;native resolution data;ground truth data;downgraded resolution;self-supervised learning","","9","","20","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Spatial Consistency for Full-Scale Assessment of Pansharpening","L. Alparone; A. Garzelli; G. Vivone","Department of Information Engineering, University of Florence, Florence, Italy; Department of Information Engineering and Mathematics, University of Siena, Siena, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5132","5134","Pansharpening usually refers to the fusion of a high spatial resolution panchromatic image with a low spatial resolution multispectral image. One of the most debated issue in this research field regards the quality assessment of fused products. The two exploited quality assessments are at reduced resolution and at full resolution. The former is an accurate procedure, but the main drawback is that it works on synthetic (with lower spatial resolutions) products. The latter is able to work at full resolution paying it with a reduced accuracy due to the absence of a ground-truth. In this work, we will focus on the assessment at full resolution by introducing a new measure of spatial consistency based on multivariate linear regression of the panchromatic image towards the multispectral bands. Simulations with an IKONOS dataset and six fusion methods show that the proposed spatial index is the ideal counterpart of Khan's spectral consistency index.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518869","Multispectral pansharpening;Multivariate regression;Quality assessment;Spectral consistency;Spatial consistency","Spatial resolution;Remote sensing;Protocols;Quality assessment;Spatial indexes","image fusion;image resolution;remote sensing","spatial consistency;pansharpening;high spatial resolution panchromatic image;low spatial resolution multispectral image;Khan's spectral consistency index;multivariate linear regression;IKONOS dataset;fusion method","","8","","10","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Spatiotemporal reflectance fusion based on location regularized sparse representation","X. Liu; C. Deng; B. Zhao","Beijing Institute of Technology, China; Beijing Institute of Technology, China; Beijing Institute of Technology, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","2562","2565","Spatiotemporal reflectance fusion plays an important role in providing earth observation with both high-spatial and high-temporal resolutions, and sparse representation is one of the popular strategies to implement spatiotemporal fusion. However, the existing methods generally suffers from instability of sparse representation for the fine and coarse image pairs. In this paper, we demonstrate that such instability can be addressed by exploiting spatial correlations among the neighboring fine images, which is mathematically formulated as a location regularized term. A fast iterative shrinkage-thresholding algorithm (FISTA) is then employed to find the optimal solution. Experimental results show that the performance of proposed method outperforms other relevant state-of-the-art fusion approaches.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729662","spatiotemporal fusion;location regularized sparse representation;fast iterative shrinkage-thresholding algorithm (FISTA)","Satellites;Remote sensing;Earth;MODIS;Spatiotemporal phenomena;Dictionaries;Image resolution","geophysical image processing;image fusion;iterative methods;remote sensing","spatiotemporal reflectance fusion;location regularized sparse representation;Earth observation;fast iterative shrinkage-thresholding algorithm;FISTA","","6","","9","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Multibranch Cnn-Based Pansharpening With Skip Connection","M. E. A. Larabi; M. S. Karoui; S. Chaib; K. Bakhti; M. I. Tchenar","Agence Spatial Algérienne,Centre des Techniques Spatiale, Arzew, Algérie; Agence Spatial Algérienne,Centre des Techniques Spatiale, Arzew, Algérie; School of computer science, Harbin Institute of Technology, China; Agence Spatial Algérienne,Centre des Techniques Spatiale, Arzew, Algérie; The State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China","2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium (M2GARSS)","2 Jun 2020","2020","","","137","140","Recent research on multispectral (MS) and panchromatic (PN) images fusion that known as pansharpening has progressed with the development of Convolutional Neural Networks (CNN). However, the states-of-the-arts methods are principally based on simple networks with shallow architectures that may limit their performance. Recently, residual learning (ResNet) exhibit improved performance in many application domains. At the same time, numerous upsampling methods were developed, from the classical interpolation to learning based methods. In this paper, ResNet is employed to make the full exploitation of the high nonlinearity of CNN. Moreover, an ensemble of upsampling methods were joined in the developed Multibranch Pansharpening Network (MPN) that prove good performance to reconstruct high-resolution MS images. The proposed approach is applied to QuickBird data, its efficiency is assessed with universally used performance criteria in spatial and spectral domains. Experimental results of the proposed approach show better spatial performance than classical methods and competitive spectral performance against the state-of-the-art approaches.","","978-1-7281-2190-1","10.1109/M2GARSS47143.2020.9105231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9105231","CNN;Deep Learning;super-resolution;pansharpening;Transposed Convolution.","Training;Learning systems;Interpolation;Convolution;Pansharpening;Convolutional neural networks;Remote sensing","convolutional neural nets;geophysical image processing;image fusion;image reconstruction;image resolution;image sampling;interpolation;learning (artificial intelligence);remote sensing","skip connection;convolutional neural networks;shallow architectures;residual learning;ResNet;upsampling methods;interpolation;spatial domains;spectral domains;multibranch pansharpening network;multibranch CNN-based pansharpening;multispectral images fusion;panchromatic images fusion;high-resolution MS images;image reconstruction","","3","","22","IEEE","2 Jun 2020","","","IEEE","IEEE Conferences"
"SAR/optical data fusion for flood detection","A. D'Addabbo; A. Refice; G. Pasquariello; F. Lovergine","CNR-ISSIA, Bari, Italy; CNR-ISSIA, Bari, Italy; CNR-ISSIA, Bari, Italy; CNR-ISSIA, Bari, Italy","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","7631","7634","In precision flood monitoring it is important to follow the temporal evolution of an event. Often, however, sufficient temporal coverage of events spanning several days can be attained only by recurring to multi-sensor data, due to different acquisition characteristics and schedules of different types of sensors. We present an example of a successful fusion of data coming from both SAR (COSMO-SkyMed stripmap, 3-m resolution) and optical (RapidEye, multispectral, 5 m-resolution) data, covering a flood event in southern Italy. The data fusion is performed through a Bayesian network approach, a reliable means to infer probabilistic information from heterogeneous sources. Results show accordance with independent model-based flood maps reaching accuracies of up to 96%.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730990","","Floods;Remote sensing;Image resolution;Optical imaging;Optical sensors","Bayes methods;data acquisition;floods;geophysical image processing;image fusion;remote sensing by radar;synthetic aperture radar;terrain mapping","precision flood monitoring;temporal evolution;temporal coverage;multisensor data;data acquisition characteristics;SAR-optical data fusion;flood event;southern Italy;COSMO-SkyMed stripmap;RapidEye data;multispectral data;Bayesian network approach;probabilistic infomation;heterogeneous sources;independent model-based flood maps;flood detection","","3","","10","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Multiple Feature Fusion for Fine Classification of Crops in UAV Hyperspectral Imagery","Y. Liang; L. Wei; Q. Lu","Faculty of Resources and Environmental Science, Hubei University, Wuhan, China; Hubei Key Laboratory of Regional Development and Environmental Response, Hubei University, Wuhan, China; Hubei Key Laboratory of Regional Development and Environmental Response, Hubei University, Wuhan, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","5059","5062","UAV hyperspectral imagery has been widely applied in the fine classification of crops because of its high spectral resolution and high spatial resolution. As the crops in hyperspectral image show complicated characteristics, only the spectral information is insufficient to distinguish them. Therefore, we use multiple feature fusion method for fine classification of crops in UAV hyperspectral imagery. In our work, the GLCM texture, morphological profile, and endmember abundance feature, are extracted. Meanwhile, three fusion strategies, namely decision fusion, probability fusion, and stacking fusion, are employed to obtain the classification results. The experimental results illustrate the superiority of the multiple fusion approaches in the crop fine classification with hyperspectral imagery.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553490","UAV hyperspectral imagery;fine classification of crops;multiple feature fusion","Stacking;Crops;Geoscience and remote sensing;Feature extraction;Spatial resolution;Hyperspectral imaging","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image texture;probability;remote sensing;sensor fusion","crops;UAV hyperspectral imagery;high spectral resolution;high spatial resolution;hyperspectral image show complicated characteristics;multiple feature fusion method;endmember abundance feature;fusion strategies;decision fusion;probability fusion;multiple fusion approaches;crop fine classification","","2","","14","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Analysis of binary land cover change detection methods using optical and radar data","M. S. Reis; S. J. Siqueira Sant'Anna","Image Processing Division (DPI), Brazilian National Institute for Space Research (INPE), São José dos Campos, SP, Brazil; Image Processing Division (DPI), Brazilian National Institute for Space Research (INPE), SP, Brazil","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","4236","4239","This work evaluates change classifications obtained using four binary change detection methods based on region, applied to optical, Synthetic Aperture Radar (SAR) and fused data. Although optical data has presented the best results, in the cases that such data is unavailable, it is possible to detect changes with high accuracy using SAR data. The use of fused images didn't improve change classification when compared to the use of single optical or SAR data.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326761","Change Detection;data fusion;SAR","Image segmentation;Synthetic aperture radar;Adaptive optics;Optical imaging;Optical sensors;Standards;Remote sensing","geophysical image processing;geophysical techniques;image classification;image fusion;land cover;remote sensing by radar;synthetic aperture radar","binary land cover change detection methods;optical data;radar data;change classifications;synthetic aperture radar;fused data;SAR data;fused images","","1","","8","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Progressive Band-Separated Convolutional Neural Network for Multispectral Pansharpening","S. -S. Xiao; C. Jin; T. -J. Zhang; R. Ran; L. -J. Deng","School of Information and Communication Engineering; School of Optoelectronic Science and Engineering; Yingcai Honors College; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4464","4467","Recently, convolutional neural networks (CNNs) have been introduced to pansharpening for enhancing fusion accuracy and overcoming the drawbacks of the conventional methods. However, most of methods based on CNN fail to distinguish the difference of multispectral bands, and only use a uniform set of convolutional kernels to extract features. In this paper, we design a progressive, band-separated convolutional network architecture for discriminatively learning the features and relation among spectral bands, aiming to address the problem mentioned before. More specifically, the proposed architecture mainly consists of three aspects. First, to accurately preserve the spectral peculiarities, we divide the multispectral input image in terms of its bands into several groups. Second, our original panchromatic and multispectral inputs are filtered by a high-pass operation to further yield more spatial details. Third, we use a spectral fusion module (SFM) for each group and associate them to progressively assemble the whole architecture. It is worth mentioning that the architecture could be integrated into any other competitive CNNs to improve the performance. Both visual and quantitative experiments have demonstrated that our proposed method outperforms recent state-of-the-art pansharpening techniques.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554024","Band-Separated Convolutional Neural Network;Pansharpening;Multispectral Image;Progressive Network","Visualization;Convolution;Neural networks;Geoscience and remote sensing;Pansharpening;Network architecture;Feature extraction","convolutional neural nets;feature extraction;geophysical image processing;high-pass filters;image colour analysis;image filtering;image fusion;neural net architecture;remote sensing","progressive band-separated convolutional neural network;multispectral pansharpening;convolutional neural networks;fusion accuracy;multispectral bands;convolutional kernels;spectral peculiarities;multispectral input image;panchromatic inputs;spectral fusion module;CNNs;feature extraction;band-separated convolutional network architecture;high-pass operation;SFM;filtering","","1","","11","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A Comparison of Two Spatio-Temporal Data Fusion Schemes to Increase the Spatial Resolution of Mapping Actual Evapotranspiration","T. Wang; R. Tang; Z. -L. Li; B. Tang; H. Wu; Y. Jiang; M. Liu","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Agri-informatics, Chinese Academy of Agricultural Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","7023","7026","Continuous monitoring of high spatial resolution evapotranspiration (ET) is critical for water resources management at both regional and local scales. This research employs a multi-sensor satellite data fusion approach (ESTARFM: Enhanced Spatial and Temporal Adaptive Reflectance Fusion Model) combined with a Two-Source N95 model and a constant evaporative fraction method to compute daily ET at 30 m spatial resolution. Two schemes are followed: the first scheme is to apply ESTARFM on the LST data to estimate daily ET at 30 m spatial resolution. The second scheme is to apply ESTARFM on the ET derived from MODIS and Landsat 8 images. The results show that the ET fused by both schemes is in good agreement with the reference ET data from the Landsat 8, while the first scheme (applying the ESTARFM on LST) is observed with more variations.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517768","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517768","evapotranspiration;fusion;ESTARFM;Landsat 8;MODIS","Remote sensing;Earth;Artificial satellites;Spatial resolution;Land surface temperature;MODIS;Reflectivity","evaporation;geophysical image processing;hydrological techniques;image fusion;image resolution;land surface temperature;remote sensing;sensor fusion;spatiotemporal phenomena;transpiration;water resources","Landsat 8 images;MODIS images;Enhanced Spatial;multisensor satellite data fusion approach;water resources management;high spatial resolution evapotranspiration;mapping actual evapotranspiration;spatio-Temporal data Fusion schemes;reference ET data;LST data;ESTARFM;constant evaporative fraction method;Two-Source N95 model;Temporal Adaptive Reflectance Fusion Model;size 30.0 m","","1","","8","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"On Hyperspectral Super-Resolution","J. Chanussot","Inria, CNRS, Grenoble INP, LJK, Univ. Grenoble Alpes, Grenoble, France","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","29","32","In this paper we will review seminal contributions of Prof. Jose Bioucas Dias for the improvement of the spatial resolution of hyperspectral images. Be it through the extension of pansharpening algorithms with spatial and spectral sparsity priors, using spectral unmixing, using a low-rank assumption from complementary multisource data, or by designing an edge-preserving convex formulation, Jose Bioucas Dias set up very solid and rigourous foundations for countless subsequent works.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553903","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553903","hyperspectral imaging;super-resolution;pansharpening","Image edge detection;Superresolution;Geoscience and remote sensing;Pansharpening;Solids;Spatial resolution;Hyperspectral imaging","convex programming;geophysical image processing;hyperspectral imaging;image fusion;image resolution;iterative methods;remote sensing","low-rank assumption;complementary multisource data;edge-preserving convex formulation;hyperspectral super-resolution;seminal contributions;spatial resolution;hyperspectral images;spatial sparsity priors;spectral sparsity priors","","1","","13","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Coupled Nonnegative Matrix Factorization With Local Neighborhood Weights For Data Fusion","A. Ertürk","Kocaeli University Laboratory of Image and Signal Processing, Turkey","2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium (M2GARSS)","2 Jun 2020","2020","","","41","44","Unmixing based hyperspectral (HS) – multispectral (MS) data fusion is a relatively recent addition to data fusion literature, and has been shown to provide robust and stable performance. Coupled nonnegative matrix factorization (CNMF) is an unmixing based data fusion method based on alternating unmixing of the HS and MS data while relating the results by point spread function (PSF) and spectral response function (SRF). However, the well-established CNMF method operates solely on the spectral information of the HS and MS data, and disregards the spatial distribution of the data. This paper proposes the integration of spatial information into the update rules used for the abundances in unmixing based fusion under the CNMF framework, based on local neighborhood weights. The proposed approach highlights that the integration of spatial information into the fusion process results in enhanced fusion performance","","978-1-7281-2190-1","10.1109/M2GARSS47143.2020.9105321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9105321","Data fusion;local;NMF;weights","Graphical models;Data integration;Geoscience and remote sensing;Computational efficiency;Spatial resolution;Hyperspectral imaging;Distribution functions","geophysical image processing;hyperspectral imaging;image enhancement;image fusion;image resolution;matrix decomposition;remote sensing;spectral analysis","coupled nonnegative matrix factorization;unmixing based data fusion method;spectral response function;CNMF;local neighborhood weights;hyperspectral data fusion;MS data fusion;point spread function;multispectral data fusion;PSF;SRF;HS data fusion;hyperspectral images","","1","","14","IEEE","2 Jun 2020","","","IEEE","IEEE Conferences"
"Small Vessel Detection Based on Adaptive Dual-Polarimetric Sar Feature Fusion and Attention-Enhanced Feature Pyramid Network","F. Zhang; Y. Zhou; F. Zhang; Q. Yin; F. Ma","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P. R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P. R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P. R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P. R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P. R. China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2218","2221","Small vessels in synthetic aperture radar (SAR) images usually have weak scattering intensity and occupy only a few numbers of image pixels, resulting in a high miss detection rate during the detection process. Regarding the problem, two solutions were presented in this paper. Firstly, dual-polarimetric SAR data were used and dual-polarimetric features were adaptively fused. Comparing to single-polarization and conventional non-adaptive fusion method, it optimally enhanced the characteristics of small vessels. Secondly, the conventional feature pyramid network (FPN) was enhanced by reducing the downsampling factor, adding spatial attention, and channel attention. The added spatial attention enhanced the significant features of small vessels on the large-scale feature map; the added channel attention filtered out the spliced features maps that were benefiting small vessel detection and reduced feature redundancy. Experimental results on the small vessel data set of Sentinel-1 verified that it not only reduced the miss detection rate but also improved calculation efficiency.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555096","National Natural Science Foundation of China(grant numbers:61871413,61801015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555096","Synthetic aperture radar (SAR);convolutional neural network (CNN);small vessel detection;feature fusion;attention enhanced","Adaptive systems;Surveillance;Oceans;Redundancy;Neural networks;Scattering;Geoscience and remote sensing","feature extraction;image fusion;radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","vessel detection;adaptive dual-polarimetric SAR feature fusion;attention-enhanced feature pyramid network;vessels;synthetic aperture radar images;weak scattering intensity;image pixels;high miss detection rate;detection process;dual-polarimetric SAR data;dual-polarimetric features;nonadaptive fusion method;conventional feature pyramid network;spatial attention;large-scale feature map;added channel attention;spliced features maps;reduced feature redundancy;vessel data","","1","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Feature-Level Fusion of Landsat-8 OLI-SWIR and TIR Images for Fine Burned Area Change Detection","S. Liu; Y. Zheng; M. Dalponte; X. Tong; Q. Du","College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; Dept. of Sustainable Agro-ecosystems and Bioresources, Research and Innovation Centre, Fondazione E. Mach, San Michele all'Adige, TN, Italy; College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; College of Surveying and Geo-Informatics, Tongji University, Shanghai, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","9964","9967","This paper proposes a novel feature-level fusion approach for burned area change detection at a fine level. The proposed approach relies on two features. The first feature is a modified normalized burn ratio (MNBR) fire index based on Landsat-8 OLI SWIR data, and the second feature is the Bright temperature (BT) based on Landsat-8 TIR data. Then two features are combined by using the gradient transfer fusion algorithm and a change detection technique to generate a fine burned area change map. A real Landsat-8 data set covering a complex fire disaster scenario is utilized to test the performance of the proposed approach. Experimental results demonstrate the effectiveness of the proposed feature-level fusion approach comparing with the reference methods in term of higher separability value and detection accuracy.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900221","feature fusion;change detection;burned area;normalized burn ratio-SWIR;brightness temperature;gradient transfer fusion","Feature extraction;Remote sensing;Artificial satellites;Earth;Indexes;Vegetation mapping;Fires","geophysical image processing;image fusion;remote sensing;wildfires","fine burned area change detection;modified normalized burn ratio fire index;Landsat-8 OLI SWIR data;Landsat-8 TIR data;gradient transfer fusion algorithm;change detection technique;fine burned area change map;Landsat-8 data set;complex fire disaster scenario;feature-level fusion approach;higher separability value","","1","","6","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Spectral Constrained Residual Attention Network for Hyperspectral Pansharpening","Z. Zhou; J. Feng; X. Wu; J. Shi; X. Zhang","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi'an, China; Northwestern Polytechnical University, School of Electronics and Information, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi'an, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2386","2389","Deep learning methods have been widely used in the task of hyperspectral pansharpening. However, most of these methods regard the Panchromatic (PAN) image as a kind of auxiliary information, which is mainly used as spatial details to add on the hyperspectral image (HSI) after processing. Obviously, this kind of methods utilize the PAN image insufficiently, resulting in the imbalance of spatial preservation and spatial preservation. In this paper, a spectral constrained residual attention network (SCRAN) is proposed by using the PAN image as the foundation of the pansharpening task and concerning on the spectral and spatial learning. The proposed SCRAN method consists of three parts: a spectral feature extraction net, an attention spatial residual net and a spectral reconstruction net. A spectral constrained loss function is designed to enhance the spectral learning ability of SCRAN. Additionally, in SCRAN, a deep back-projection network (DBPN) is operated to upsample the HSI, and the histogram matching is applied to the PAN image to make it closer to the HSI in terms of spectral bands.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883551","National Natural Science Foundation of China(grant numbers:61871306,61836009,62172600,62077038); Natural Science Basic Research Program of Shaanxi(grant numbers:2022JC-45,2022GY-065); Fundamental Research Funds for the Central Universities(grant numbers:JB211901); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883551","hyperspectral pansharpening;attention mechanism;super resolution;deep residual neural network;histogram matching","Deep learning;Histograms;Geoscience and remote sensing;Pansharpening;Feature extraction;Task analysis;Spatial resolution","feature extraction;geophysical image processing;geophysical signal processing;image classification;image fusion;image reconstruction;image resolution;image sampling;learning (artificial intelligence);remote sensing","spectral constrained residual attention network;hyperspectral pansharpening;Panchromatic image;spatial details;hyperspectral image;HSI;PAN image;spatial preservation;pansharpening task;spatial learning;SCRAN method;spectral feature extraction net;attention spatial residual net;spectral reconstruction net;spectral constrained loss function;spectral learning ability;deep back-projection network;spectral bands","","1","","12","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Pansharpening with Spatial Hyper-Laplacian and Spectral Sparse Constraints","P. Liu; S. Tang; L. Huang","Jiangsu Key Laboratory of Big Data Security & Intelligent Processing, China; Department of Criminal Science and Technology, Nanjing Forest Police College, China; School of Computer Science and Technology, Jinling Institute of Technology, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","3762","3765","This paper proposed a new pansharpening model with spatial hyper- Laplacian and spectral sparse constraints (PSHSS), which finally generated the high resolution multispectral (HR MS) images. Based on the panchromatic (PAN) and low resolution multispectral (LR MS) images, an enhanced PAN image was first constructed. To-gether with the local spectral consistency constraint, then the pro-posed PSHSS model particularly exploited the spatial gradient hyper-Laplacian sparse constraint between enhanced PAN and HR MS for spatial prior modeling, and the spectral gradient sparse constraint of HR MS for spectral prior modeling. Then, the proposed PSHSS model was optimized under the alternating direction method of multipliers (ADMM) framework. Finally, the fusion experiment demonstrated the superiority of PSHSS method.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883841","National Natural Science Foundation of China(grant numbers:61802202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883841","Pansharpening;spatial hyper-Laplacian sparse constraint;spectral sparse constraint","Laplace equations;Geoscience and remote sensing;Pansharpening;Convex functions;Spatial resolution","geophysical image processing;gradient methods;graph theory;image fusion;image reconstruction;image resolution;optimisation;remote sensing","enhanced PAN image;local spectral consistency constraint;PSHSS model;spatial gradient hyper-Laplacian;spatial prior modeling;spectral gradient sparse constraint;HR MS;spectral prior modeling;PSHSS method;spatial hyper-Laplacian;pansharpening model;spectral sparse constraints;high resolution multispectral images;low resolution multispectral images;LR MS","","","","16","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Two-Stage Fusion based CNN for Hyperspectral Pansharpening","J. Xie; L. He","School of Automation Science and Engineering, South China University of Technology; School of Automation Science and Engineering, South China University of Technology","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1091","1094","Hyperspectral (HS) pansharpening convolutional neural networks (CNNs) usually pre-interpolate the low spatial resolution (LR) HS image before pansharpening, which incurs heavy computation burden and insufficient fusion. Therefore, we propose a novel two-stage fusion based CNN (TSF-CNN) in this work. In particular, the HS pansharpening is split into two fusion stages, i.e. the low-resolution (LRF) fusion stage and the high-resolution fusion (HRF) stage. At the LRF stage, we subsapmle the panchromatic (PAN) image and fuse it with the LRHS image, which not only reduces the computation cost but also make use of the low-frequency information of the PAN. Then, the high-frequency information of the PAN is extracted and fused via a residual dense channel attention block (RDCAB) in the subsequent HRF stage. Moreover, a new spectral similarity auxiliary L1 loss is designed for better spectral fidelity. Experimental results from the Chikusei dataset demonstrate the effectiveness and efficiency of the proposed method.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884253","Hyperspectral pansharpening;two-stage fusion;residual dense channel attention;spectral similarity;CNN","Costs;Fuses;Convolution;Neural networks;Geoscience and remote sensing;Pansharpening;Convolutional neural networks","convolutional neural nets;geophysical image processing;image fusion;image resolution;interpolation;remote sensing","hyperspectral pansharpening;pansharpening convolutional neural networks;low spatial resolution HS image;heavy computation burden;two-stage fusion;TSF-CNN;HS pansharpening;low-resolution fusion stage;high-resolution fusion stage;LRF stage;panchromatic image;PAN;LRHS image;computation cost;low-frequency information;high-frequency information;subsequent HRF stage","","","","9","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"3D Point Cloud Generation Using Adversarial Training for Large-Scale Outdoor Scene","T. Shinohara; H. Xiu; M. Matsuoka","Department of Architecture and Building Engineering, Tokyo Institute of Technology, Yokohama, Japan; Department of Architecture and Building Engineering, Tokyo Institute of Technology, Yokohama, Japan; Department of Architecture and Building Engineering, Tokyo Institute of Technology, Yokohama, Japan","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2935","2938","Three-dimensional (3D) point clouds are becoming an important part of the geospatial domain. During research on 3D point clouds, deep-learning models have been widely used for the classification and segmentation of 3D point clouds observed by airborne LiDAR. However, most previous studies used discriminative models, whereas few studies used generative models. Specifically, one unsolved problem is the synthesis of large-scale 3D point clouds, such as those observed in outdoor scenes, because of the 3D point clouds' complex geometric structure. In this paper, we propose a generative model for generating large-scale 3D point clouds observed from airborne LiDAR. Generally, because the training process of the famous generative model called generative adversarial network (GAN) is unstable, we combine a variational autoen-coder and GAN to generate a suitable 3D point cloud. We experimentally demonstrate that our framework can generate high-density 3D point clouds by using data from the 2018 IEEE GRSS Data Fusion Contest.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554523","KAKENHI(grant numbers:19H02408); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554523","Generative Adversarial Network;Variational Autoencoder;Deep Learning;Point Clouds;Airborne LiDAR","Training;Solid modeling;Three-dimensional displays;Laser radar;Atmospheric modeling;Geoscience and remote sensing;Generative adversarial networks","geophysical image processing;geophysical signal processing;image classification;image fusion;learning (artificial intelligence);optical radar;remote sensing by laser beam;sensor fusion;terrain mapping","airborne LiDAR;large-scale 3D point clouds;famous generative model;generative adversarial network;suitable 3D point cloud;high-density 3D point clouds;3D point cloud generation;large-scale outdoor scene;three-dimensional point clouds;deep-learning models","","","","16","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Cross-Frequency Detail Compensation Network for Pansharpening","X. -N. Zhao; C. -Y. Zhao; T. -J. Zhang; L. -J. Deng","Yingcai Honors College; Yingcai Honors College; Yingcai Honors College; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","3770","3773","Pansharpening is a fusion technique aiming at improving the spatial resolution of multispectral images while preserving spectral information. Previous attempts to adopt CNNs have led to significant progress in pansharpening, but always with a cumbersome network structure, and there exists redundancy in both spatial and channel of feature maps learned by CNNs. Considering the distinct properties of components with different frequencies in the feature map, we propose a cross-frequency detail compensation network (CFDCNet) by processing low, medium, and high frequency separately. Specifically, a cross-frequency convolution block is designed to produce a representation that captures the different frequency classes while achieving the more efficient detail extraction. Overall pipeline is progressive, and the learned features are fused in an interactive compensation manner to obtain the final output. Experimental results demonstrate the superiority of CFDCNet over state-of-the-art pansharpening methods in terms of visual quality and quantitative metrics.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883203","","Measurement;Visualization;Convolution;Redundancy;Pipelines;Geoscience and remote sensing;Pansharpening","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);remote sensing","feature map;cross-frequency detail compensation network;low frequency;medium, frequency;cross-frequency convolution block;different frequency classes;interactive compensation manner;state-of-the-art pansharpening methods;fusion technique;spatial resolution;CNNs;cumbersome network structure;spatial channel","","","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Cycle GAN Based Heterogeneous Spatial-Spectral Fusion for Soil Moisture Downscaling","M. Jiang; H. Shen; J. Li","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","4819","4822","Soil moisture (SM) downscaling aims to solve the coarse resolution problem of passive microwave SM products. On the basis of SMAP SM products and related MODIS products, this study develops a deep residual cycle generative adversarial network (GAN) based heterogeneous spatial-spectral fusion method to downscale SMAP SM from 36km to 9km. On the one hand, the proposed method creatively regards the MODIS products that can reflect the SM state as the spectral features of SM in a broad sense and performs the heterogeneous spatial-spectral fusion between the low-resolution (LR) SM product and high-resolution (HR) MODIS products. On the other hand, considering the spatial correlation of SM, the proposed method utilizes a deep residual cycle generative adversarial network (GAN) to extract and fuse features of heterogeneous images through convolutions. Both qualitative and quantitative evaluation of experimental results shows that the proposed method can generate high accuracy SM products.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884702","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884702","Soil moisture;downscale;heterogeneous spatial-spectral fusion;cycle GAN","Fuses;Soil moisture;Geoscience and remote sensing;Generative adversarial networks;Feature extraction;Microwave theory and techniques;Spatial resolution","hydrological techniques;image fusion;image resolution;microwave measurement;moisture;remote sensing;soil","cycle GAN;heterogeneous spatial-spectral fusion;soil moisture downscaling;coarse resolution problem;passive microwave SM products;SMAP SM products;related MODIS products;deep residual cycle generative adversarial network;spatial-spectral fusion method;downscale SMAP SM;SM state;spectral features;low-resolution SM product;heterogeneous images;high accuracy SM products;size 9.0 km to 36.0 km","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Joint Image Registration and Blur Kernel Learning for Pansharpening","A. Guo; Y. Wu; S. Li","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2636","2639","Image registration and the estimation of spatial and spectral blur kernels are essential steps before fusing panchromatic image (PAN) and multispectral image (MSI). Usually, these basic steps are performed separately, which will lead to error accumulation and ultimately affect the fusion performance. In this paper, we propose a novel deep learning (DL) based framework which can jointly register images and learn the blur kernels of original PAN and MSI. Specifically, we first construct a convolutional neural network (CNN) to learn the offsets between PAN and MSI, and the offsets are utilized to align the two images. Then, we analyze the relationship between the registered PAN and MSI, and design a tiny network for blur kernel learning. After solving the gradient derivation problems, we can combine the two networks and train them end-to-end. Experimental results on GF-2 satellite images demonstrate that the proposed method can significantly improve the fusion performance of some popular pansharpening methods.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554618","National Natural Science Fund of China(grant numbers:61890962,61520106001,61801179); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554618","Pansharpening;image registration;blur kernel learning;deep learning","Training;Image registration;Satellites;Estimation;Geoscience and remote sensing;Pansharpening;Registers","geophysical image processing;image fusion;image registration;image resolution;learning (artificial intelligence);neural nets;remote sensing","multispectral image;MSI;basic steps;fusion performance;deep learning based framework;original PAN;convolutional neural network;offsets;registered PAN;blur kernel learning;GF-2 satellite images;joint image registration;spatial blur kernels;spectral blur kernels;essential steps","","","","7","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Quantitative Evaluation of Algae Detection Based on Deep Neural Network Multi-Source Data Fusion","L. Gao; X. Li; Y. Guo; J. Qi; B. Zhang","Institute of Oceanography, Chinese Academy of Sciences, Qingdao, China; Institute of Oceanography, Chinese Academy of Sciences, Qingdao, China; Institute of Oceanography, Chinese Academy of Sciences, Qingdao, China; Institute of Oceanography, Chinese Academy of Sciences, Qingdao, China; Institute of Oceanography, Chinese Academy of Sciences, Qingdao, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","7561","7563","This study developed a deep-learning-based model for macroalgae detection in optical Moderate Resolution Imaging Spectroradiometer (MODIS) and microwave synthetic aperture radar (SAR) images. The model reached the accuracy of 97.51(99.37)% and a mean Intersection over Union (IoU) of 42.62(86.22)% for the MODIS(SAR) images, based on the labeled 1055/4071 pairs of MODIS/SAR samples. For macroalgae detection, we designed the model based on the U-Net model with a specific-tailored modification: overfitting processing based on algae image features. We applied the model to images acquired in the Yellow Sea region frequently affected by the modifications blooming and obtained one new scientific discovery: SAR image with high-resolution has more powerful detection capabilities than MODIS image with coarse-resolution.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554235","Macroalgae detection;deep-learning model;MODIS;SAR","Laser radar;Algae;Geoscience and remote sensing;Optical imaging;Feature extraction;Radar polarimetry;Optical sensors","deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;neural nets;oceanographic regions;oceanographic techniques;radar imaging;remote sensing by radar;synthetic aperture radar","MODIS image;deep neural network multisource data fusion;macroalgae detection;U-Net model;algae image features;SAR image;optical Moderate Resolution Imaging Spectroradiometer;microwave synthetic aperture radar;MODIS images;SAR images;Yellow Sea region;deep-learning-based model","","","","16","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Full-Resolution Image Segmentation Model Combining Multi-Source Input Information","C. Feng; X. Wang; X. Wang; M. Liu; J. Wu","School of Computer Science, Shaanxi Normal University, China; School of Computer Science, Shaanxi Normal University, China; School of Physics and Electronic Electrical Engineering, Ningxia University, China; School of Computer Science, Shaanxi Normal University, China; School of Computer Science, Shaanxi Normal University, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3864","3867","In this paper, a full-resolution image segmentation model with multi-source input information is proposed and applied to road extraction. The convolution-deconvolution network is adopted as the backbone network, and a full resolution network branch is added into the backbone network. A data exchange mechanism is established between the backbone network and the full resolution branch, which not only overcomes the problems of reduced feature resolution and loss of detailed information caused by repeated pooling operations, but also aggregates multi-scale features in convolution stage. The aggregated features are transferred to the corresponding layers in deconvolution stage, which enhances the feature fusion. Multi-source images are used as input, and the predictions are fused by weighting at the end of the network to highlight the target while effectively suppressing the misclassification. Experiments on Road Detection Dataset show that the results of the proposed method are superior to those of state-of-the-art comparison methods.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898592","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898592","Image segmentation;road extraction;multi-source input;full-resolution network;fusion","Image segmentation;Feature extraction;Convolution;Roads;Deconvolution;Image resolution;Remote sensing","convolutional neural nets;deconvolution;geophysical image processing;image fusion;image segmentation;remote sensing;roads;traffic engineering computing","convolution stage;feature fusion;multisource images;full-resolution image segmentation model;multisource input information;convolution-deconvolution network;data exchange mechanism;reduced feature resolution;road detection dataset","","","","8","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Assessing Buildings Damage from Multi-Temporal Sar Images Fusion using Semantic Change Detection","L. Pang; F. Zhang; L. Li; Q. Huang; Y. Jiao; Y. Shao","Deqing Academy of Satellite Applications, Laboratory of Target Microwave Properties, Huzhou, China; Deqing Academy of Satellite Applications, Laboratory of Target Microwave Properties, Huzhou, China; Deqing Academy of Satellite Applications, Laboratory of Target Microwave Properties, Huzhou, China; Deqing Academy of Satellite Applications, Laboratory of Target Microwave Properties, Huzhou, China; Deqing Academy of Satellite Applications, Laboratory of Target Microwave Properties, Huzhou, China; Deqing Academy of Satellite Applications, Laboratory of Target Microwave Properties, Huzhou, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","6292","6295","A prompt and accurate assessment of buildings' damage is critical for disaster management and emergency response. With the development of high-resolution synthetic aperture radar (SAR) and deep-learning methods, more efficient damage assessment techniques based on building-units are possible. This paper proposes a new building damage assessment method using high-resolution SAR images based on semantic change detection. It utilizes a Siamese-based module for damage change detection together with an attention mechanism-based module for semantic segmentation of the damage maps. To evaluate the proposed model, a new damage assessment dataset is constructed from the SAR imagery originated from the battle of Aleppo, Syria, for model training and testing. The experiments performed on this dataset show an overall accuracy of 88.3%. The proposed method effectively identifies the damaged areas of the buildings and grade the damage condition.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884915","National Natural Science Foundation of China(grant numbers:41671359,61471358); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884915","SAR;Buildings damage;Semantic change detection;Siamese network","Training;Open Access;Buildings;Semantics;Geoscience and remote sensing;Disaster management;Emergency services","geophysical image processing;image fusion;image segmentation;learning (artificial intelligence);radar imaging;remote sensing by radar;synthetic aperture radar","damage change detection;attention mechanism-based module;semantic segmentation;damage maps;damage assessment dataset;SAR imagery;damaged areas;damage condition;buildings damage;multitemporal sar images fusion;semantic change detection;prompt assessment;disaster management;emergency response;high-resolution synthetic aperture radar;deep-learning methods;efficient damage assessment techniques;building-units;building damage assessment method;high-resolution SAR images;Siamese-based module","","","","16","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Three-Branch Multilevel Attentive Fusion Network for Hyperspectral Pansharpening","P. Guan; E. Y. Lam","Department of Electrical and Electronic Engineering, University of Hong Kong, Pokfulam, Hong Kong SAR, China; Department of Electrical and Electronic Engineering, University of Hong Kong, Pokfulam, Hong Kong SAR, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1087","1090","In this paper, we propose a three-branch multilevel attentive fusion network (TMA-Net) for hyperspectral pansharpening, which aims to merge low-resolution hyperspectral images (LR-HSIs) and high-resolution panchromatic images (HR-PANs) to obtain HSIs with high resolution. We construct three branches to extract rich features of the two images and the correlation between them, which enables us to capture abundant useful information for pansharpening. We merge the multilevel features extracted by each branch in multiple steps to fully fuse the useful information. An attentive fusion module (AFM) is designed to guide the fusion procedure. It explores the relation between different features and employs attention mechanism to refine them adaptively. The experimental results illustrate the superiority of the TMA-Net.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883218","University of Hong Kong(grant numbers:104005864); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883218","Hyperspectral pansharpening;three-branch;multilevel fusion;attention mechanism","Image resolution;Correlation;Fuses;Merging;Geoscience and remote sensing;Pansharpening;Feature extraction","feature extraction;geophysical image processing;geophysical signal processing;hyperspectral imaging;image classification;image fusion;image resolution;remote sensing;sensor fusion","hyperspectral pansharpening;three-branch multilevel attentive fusion network;TMA-Net;low-resolution hyperspectral images;LR-HSIs;high-resolution panchromatic images;rich features;abundant useful information;multilevel features;attentive fusion module;fusion procedure;attention mechanism","","","","14","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Learning Image Downscaling for Pansharpening Using an Improved UNet","M. E. A. Larabi; M. Iftene; M. I. Tchenar; K. Bakhti","The State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, China; Agence Spatiales Algérienne, Centre des Techniques Spatiales, Algérie; The State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, China; Agence Spatiales Algérienne, Centre des Techniques Spatiales, Algérie","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4460","4463","Pan-sharpening is a challenging ill-posed problem, which aims to restore a high-resolution multispectral image (MSHR) from its low-resolution (MSLR) image combined with a high-resolution panchromatic image (PAN). Powerful deep learning based techniques have achieved state-of-the-art performance in pan-sharpening. However, they can underperform when handling images with unknown land structures and areas. In this paper, a new designed UNet like network, able to learn the relationship between a set of randomly degraded MSLR images and their corresponding original MSHR images is developed. We propose to employ a degradation module on the training images in addition to learn a down- /up-sampling model by the deep network, allowing the construction of UNet like architecture for pansharpening. Experimental results show that the proposed model improves the visual quality of the obtained MSHR images while keeping a low reconstruction error.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553240","Pan-sharpening;deep learning;low-resolution;high-resolution;multispectral;panchromatic","Degradation;Training;Deep learning;Visualization;Geoscience and remote sensing;Pansharpening;Feature extraction","geophysical image processing;image classification;image fusion;image reconstruction;image resolution;learning (artificial intelligence);remote sensing","MSLR images;corresponding original MSHR images;degradation module;training images;deep network;pansharpening;image downscaling;pan-sharpening;challenging ill-posed problem;high-resolution multispectral image;low-resolution image;high-resolution panchromatic image;powerful deep learning based techniques;unknown land structures;designed UNet","","","","15","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Framework for multi-sensor data fusion using template based matching","G. Palubinskas","German Aerospace Center DLR, Remote Sensing Technology Institute Oberpfaffenhofen, Wessling, Germany","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","621","624","A general approach or framework is proposed for multi-sensor data fusion using template based matching (TBM). The main advantage of TBM is that it allows defining features/templates using a priori information from the scene/image. The approach works as follows: first quite complex features e.g. roundabouts/junctions are extracted in the optical image, then these features are simulated in SAR and finally they are compared or matched on patch/area basis with the SAR image. The proposed framework is illustrated for common tie points extraction in very high resolution satellite optical WorldView-2 and radar TerraSAR-X imagery.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325840","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325840","Optical;SAR;image fusion;registration;template extraction;simulation;matching","Optical imaging;Adaptive optics;Feature extraction;Optical sensors;Synthetic aperture radar;Data mining;Accuracy","geophysical image processing;geophysical techniques;image fusion","multisensor data fusion;template based matching;roundabouts-junctions;optical image;SAR;radar TerraSAR-X imagery;very high resolution satellite optical WorldView-2","","","2","18","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"EFP-Net: High-Precision Remote Sensing Image Object Detection","L. Weihao; P. Wu; Q. Pan; J. Yan","Guangdong Company Limited, China Mobile Group; Engineering institute, Shantou University, Shantou, China; Engineering institute, Shantou University, Shantou, China; Engineering institute, Shantou University, Shantou, China","2022 2nd International Conference on Frontiers of Electronics, Information and Computation Technologies (ICFEICT)","28 Nov 2022","2022","","","483","490","Due to its advantages of high resolution and wide capturing range, satellite remote sensing images have been widely used in object detection nowadays. Nevertheless, the multi-scale character of satellite remote sensing images poses difficulties to our research and applications. To solve this problem, most people choose to use a convolution neural network (CNN) for feature extraction. However, the existing methods are not able to extract useful information efficiently and accurately. This paper proposes a new feature extraction network based on ResNet50, which introduces an Atrous convolution module to enhance the feature extraction of the object. On this basis, we propose an Enhanced Feature Pyramid Network (EFPN), which can effectively extract the multi-scale information by Bi-directional feature fusion. Furthermore, experiments were conducted in three datasets, Dota, NWPU VHR-10, and UCAS-AOD, in which the detection accuracy on NWPU VHR-10 reached 98.2%. The experiments show that our method not only improve the efficiency of multi-scale detection, but also greatly promote detection accuracy.","","978-1-6654-5476-6","10.1109/ICFEICT57213.2022.00091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9951414","Atrous Convolution;Enhanced Feature Pyramid Network;Feature Fusion;Network Structure;Remote Sensing Images","Training;Satellites;Convolution;Neural networks;Object detection;Feature extraction;Sensors","convolutional neural nets;feature extraction;geophysical image processing;image fusion;image resolution;object detection;remote sensing","Atrous convolution module;bidirectional feature fusion;convolution neural network;detection accuracy;EFP-net;EFPN;enhanced feature pyramid network;feature extraction network;high-precision remote sensing image object detection;multiscale character;multiscale detection;multiscale information extraction;NWPU VHR-10;satellite remote sensing images;wide capturing range","","","","30","IEEE","28 Nov 2022","","","IEEE","IEEE Conferences"
"Remote sensing image registration with spatial restraint based on moment invariants and fast generalized fuzzy clustering","W. Yue; G. Maoguo; J. Jia; W. Ma","Xidian University, Xian, Shaanxi, CN; Xidian University, Xian, Shaanxi, CN; Xidian University, Xian, Shaanxi, CN; Xidian University, Xian, Shaanxi, CN","2015 Conference on Technologies and Applications of Artificial Intelligence (TAAI)","15 Feb 2016","2015","","","97","104","Image registration is a key component in remote sensing image processing. In this paper, we present a remote sensing image registration method by incorporating spatial restraint based on moment invariants and fast generalized fuzzy clustering. Seven moment invariants are extracted as features of objects obtained by the fast generalized fuzzy c-means (FGFCM) algorithm. The objects are matched through these features. Then, we detect the keypoints in corresponding matching regions. Through the spatial restraint, the outliers are removed and the correct matches are increased. The proposed algorithm is evaluated on multi-spectral images, multi-temporal images, and multi-sensor images. Extensive experimental studies prove that the proposed algorithm is promising.","2376-6824","978-1-4673-9606-6","10.1109/TAAI.2015.7407062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407062","Image registration;remote sensing;spatial restraint;moment invariants;FCM","Image segmentation;Adaptation models","fuzzy set theory;geophysical image processing;image fusion;image matching;image registration;object detection;pattern clustering;remote sensing","multisensor image;multitemporal images;multispectral image;keypoint detection;object matching;FGFCM algorithm;fast generalized fuzzy c-means algorithm;remote sensing image processing;fast generalized fuzzy clustering;moment invariants;spatial restraint;remote sensing image registration","","","","27","IEEE","15 Feb 2016","","","IEEE","IEEE Conferences"
"A New Network Structure for Semantic Segmentation of Ship Targets in Remote Sensing","C. Mi; C. Yaqi; L. Yafei; Z. Jing; X. Wei; P. Jiazheng","Research Institute of Information Fusion, Naval Aviation University, Yantai, China; Research Institute of Information Fusion, Naval Aviation University, Yantai, China; Research Institute of Information Fusion, Naval Aviation University, Yantai, China; Naval Aviation University, Yantai, China; Research Institute of Information Fusion, Naval Aviation University, Yantai, China; Naval Aviation University, Yantai, China","2019 22th International Conference on Information Fusion (FUSION)","27 Feb 2020","2019","","","1","8","Accurate detection of ship targets is a research hotspot in computer vision. Most of the researches have achieved instance-level detection in the way of bounding box. But we intend to achieve more accurate detection of ship targets in pixel-level through semantic segmentation. However, there are still two main challenges: the first one is the difficulty to segment small targets caused by the difference among ship targets' scales, and the other one is the lack of localization information caused by insufficient recovery ability of the decoder part. In this paper, we propose an effective solution. First, a multi-scale pooling fusion module is proposed to fuse multi-scale feature maps and acquire more multi-scale context information, then we improve the capability of precise decoding by taking the place of convolution operation with deconvolution in the decoder part to gather more localization information. At last, we integrate above two schemes into an encoder-decoder symmetry training network with less training parameters and less training time. Furthermore, we construct a dataset for ship semantic segmentation called HRSC2016-SS by labeling HRSC2016 dataset to evaluate our solution. Experiments show that comparing with the existing methods, our proposed solution has a better performance.","","978-0-9964527-8-6","10.23919/FUSION43075.2019.9011331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9011331","HRSC2016-SS;multi-scale;deconvolution;remote sensing;semantic segmentation","Marine vehicles;Image segmentation;Semantics;Remote sensing;Feature extraction;Convolution;Decoding","computer vision;convolution;decoding;deconvolution;image fusion;image segmentation;marine engineering;neural nets;object detection;remote sensing;ships","ship target detection;research hotspot;instance-level detection;pixel-level;localization information;decoder part;multiscale pooling fusion module;multiscale feature maps;multiscale context information;encoder-decoder symmetry training network;ship semantic segmentation;small target segmentation;remote sensing;convolution operation;deconvolution","","1","","37","","27 Feb 2020","","","IEEE","IEEE Conferences"
"When to fuse what? random forest based fusion of low-, mid-, and high-level information for land cover classification from optical and SAR images","R. Hänsch; O. Hellwich","Computer Vision & Remote Sensing, Technische Universität Berlin, Germany; Computer Vision & Remote Sensing, Technische Universität Berlin, Germany","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","3587","3590","With increasing availability of different sensors for earth observation, data fusion gained more and more importance. While previous publications focussed on new sensor combinations, new fusion techniques, or new applications, this work investigates at which stage of the image analysis pipeline the fusion process is most beneficial. The fusion of an optical and a SAR image for the task of land cover classification serves as an example. The experimental results indicate, that although the fusion of complementary data is generally advantageous, it is most helpful at later stages of the classification process.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729929","Data fusion;Random Forests;SAR","Optical imaging;Adaptive optics;Synthetic aperture radar;Optical sensors;Data integration;Feature extraction","geophysical image processing;image classification;image fusion;land cover","random forest;Earth observation sensor;data fusion;image fusion technique;image analysis pipeline;optical image;SAR image;land cover classification;high-level information;midlevel information;low-level information","","1","","9","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"A Conditional Generative Adversarial Network to Fuse Sar And Multispectral Optical Data For Cloud Removal From Sentinel-2 Images","C. Grohnfeldt; M. Schmitt; X. Zhu","Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1726","1729","In this paper, we present the first conditional generative adversarial network (cGAN) architecture that is specifically designed to fuse synthetic aperture radar (SAR) and optical multi-spectral (MS) image data to generate cloud- and haze-free MS optical data from a cloud-corrupted MS input and an auxiliary SAR image. Experiments on Sentinel-2 MS and Sentinel-l SAR data confirm that our extended SAR-Opt-cGAN model utilizes the auxiliary SAR information to better reconstruct MS images than an equivalent model which uses the same architecture but only single-sensor MS data as input.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519215","SAR;optical remote sensing;data fusion;deep learning;generative adversarial network (GAN);cloud-removal","Synthetic aperture radar;Optical sensors;Optical imaging;Clouds;Remote sensing;Adaptive optics;Generative adversarial networks","geophysical image processing;image fusion;radar imaging;synthetic aperture radar","cloud-free MS optical data;Sentinel-l SAR data;single-sensor MS data;reconstruct MS images;auxiliary SAR information;extended SAR-Opt-cGAN model;auxiliary SAR image;cloud-corrupted MS input;haze-free MS optical data;multispectral image data;synthetic aperture radar;conditional generative adversarial network architecture;Sentinel-2;cloud removal;multispectral optical data","","51","3","12","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"An unsupervised automatic change detection approach based on visual attention mechanism","D. Liu; J. Zhang; X. Lu","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","3045","3048","In change detection analysis, it is important to distinguish the real change targets and pseudo change targets accurately. Supervised change detection has been regarded as the best way to reduce the effects of pseudo change information. This is because human visual system has the ability to find the real changes. By imitating human visual characteristic, visual attention mechanism can bring the improvement of accuracy and speed of unsupervised change detection. In this paper, a change detection approach based on visual attention mechanism is proposed to reduce the influence of pseudo change information. Experiments show that the proposed method significantly reduces the false alarm rate and missed alarm rate and also shows insensitive to noise.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326458","Change detection;remote sensing image;visual attention;feature extraction;saliency map","Visualization;Remote sensing;Feature extraction;Computational modeling;Satellites;Visual systems;Computer vision","geophysical image processing;image fusion;image texture;visual perception","unsupervised automatic change detection approach;visual attention mechanism;pseudo change targets;pseudo change information;human visual system;human visual characteristic;false alarm rate;missed alarm rate","","7","","11","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"MPDFF: Multi-source Pedestrian detection based on Feature Fusion","L. Meng; J. Zhou; J. Ma; Z. Wang","School of Resources and Environment, Center for Information Geoscience, University of Electronic Science and Technology of China, Chengdu, China; School of Resources and Environment, Center for Information Geoscience, University of Electronic Science and Technology of China, Chengdu, China; School of Resources and Environment, Center for Information Geoscience, University of Electronic Science and Technology of China, Chengdu, China; School of Resources and Environment, Center for Information Geoscience, University of Electronic Science and Technology of China, Chengdu, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","7906","7909","Pedestrian detection from UAV images is vital for many fields. Given that visible images are susceptible to bad illumination, thermal images with the ability to characterize the temperature of an object can provide auxiliary information. It is useful to fuse the visible and thermal images to improve the pedestrian detection performance. Unfortunately, studies on pedestrian detection with UAV visible-thermal image pairs are still rare. Therefore, we propose a method for Multi-source Pedestrian Detection based on Feature Fusion (MPDFF). With the registered visible and thermal image pairs as the input, MPDFF can achieve better characterization of pedestrians by concatenating the features from the two images. MPDFF performs much better than the methods that use only single-source images, which demonstrates that visible and thermal images are complementary in pedestrian detection.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884864","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884864","pedestrian detection;feature fusion;thermal;visible;UAV remote sensing","Fuses;Lighting;Feature extraction;Remote sensing;Optimization","image classification;image colour analysis;image fusion;image sensors;infrared imaging;object detection;pedestrians","Feature Fusion;MPDFF;registered visible image pairs;single-source images;visible images;thermal images;Multisource Pedestrian detection;UAV images;pedestrian detection performance;UAV visible-thermal image pairs;Multisource Pedestrian Detection","","","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"ReAFFPN: Rotation-Equivariant Attention Feature Fusion Pyramid Networks for Aerial Object Detetcion","C. Sun; Y. Xu; Z. Wu; Z. Wei","School of Computer Science and Enginnering, Nanjing University of Science and Technology, Nanjing, Jiangsu Province, China; School of Computer Science and Enginnering, Nanjing University of Science and Technology, Nanjing, Jiangsu Province, China; School of Computer Science and Enginnering, Nanjing University of Science and Technology, Nanjing, Jiangsu Province, China; School of Computer Science and Enginnering, Nanjing University of Science and Technology, Nanjing, Jiangsu Province, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","3055","3058","This paper proposes a Rotation-equivariant Attention Feature Fusion Pyramid Networks for Aerial Object Detection named ReAFFPN. ReAFFPN aims at improving the effect of rotation-equivariant features fusion between adjacent layers which suffers from the semantic and scale discontinuity. Due to the particularity of rotational equivariant convolution, general methods are unable to achieve their original effect while ensuring rotation equivariance of the network. To solve this problem, we design a new Rotation-equivariant Channel Attention which has the ability to both generate channel attention and keep rotation equivariance. Then we embed a new channel attention function into Iterative Attentional Feature Fusion (iAFF) module to realize Rotation-equivariant Attention Feature Fusion. Experimental results demonstrate that ReAFFPN achieves a better rotation-equivariant feature fusion ability and significantly improve the accuracy of the Rotation-equivariant Convolutional Networks.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884235","Object Detection;Remote Sensing;Rotation Equivariance;Attention;Feature Fusion","Convolution;Semantics;Geoscience and remote sensing;Object detection;Feature extraction;Sensors;Iterative methods","convolutional neural nets;feature extraction;image fusion;iterative methods;object detection","ReAFFPN;Rotation-equivariant Attention Feature Fusion Pyramid Networks;rotation-equivariant features fusion;rotational equivariant convolution;rotation equivariance;Rotation-equivariant Channel Attention;channel attention function;Iterative Attentional Feature Fusion module;rotation-equivariant feature fusion ability;Rotation-equivariant Convolutional Networks","","","","7","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"An interactive color visualization method with multi-image fusion for hyperspectral imagery","D. Liu; L. Wang; J. A. Benediktsson","The Faculty of Electrical and Computer Engineering, University of Iceland; The College of Information and Communication Engineering, Harbin Engineering University; The Faculty of Electrical and Computer Engineering, University of Iceland","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","1088","1091","An interactive color visualization method is proposed for hyperspectral imagery (HSI). The method visualizes complex information through different fusion results of multiple images in a color space which is under the interactive control of the observers. In order to solve the main problem of traditional visualization methods, i.e., they can at most display information from three bands in one image, this paper proposes an easy, vivid and effective method for color visualization. In the proposed approach, observers interactively control a cursor position to change the output fusion images and their fusion coefficients. In the approach, the dynamic display will include more than three bands of HSIs. The proposed method is also applicable for visualization of other multi-images, e.g., multispectral images, output images of direction filters, multi-focus images, and multi-temporal images, etc.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325959","Hyperspectral imagery (HSI);interactive visualization;multiple images","Image color analysis;Visualization;Hyperspectral imaging;Aerospace electronics;Observers","data visualisation;geophysical image processing;geophysical techniques;hyperspectral imaging;image colour analysis;image fusion","interactive color visualization method;hyperspectral imagery;multiimage fusion;complex information;multiple color space imagery;traditional visualization methods;fusion images;fusion coefficients;multispectral image;multifocus image;multitemporal image","","4","","7","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Urban TanDEM-X Raw DEM Fusion Based ON TV-L1 and Huber Models","H. Bagheri; M. Schmitt; X. X. Zhu","Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany; Remote Sensing Technology Institute, German Aerospace Center, Oberpfaffenhofen, Wessling, Germany","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","7251","7254","Recently, the TanDEM-X DEM has been produced as a global DEM with unprecedented relative accuracy. One important step of the chain of global DEM generation is to mosaic multiple raw DEM tiles by DEM fusion methods to reach the best possible target accuracy. Currently, Weighted Averaging (WA) is used as a fast and simple method for TanDEM-X raw DEM fusion in which the weights are computed from height error maps delivered from the Interferometric TanDEM-X Processor (ITP). In this paper, we investigate the efficiency of variational models such as TV-L1 and Huber model for the TanDEM-X raw DEM fusion task in comparison to WA. The results illustrate that using variational models can improve the quality of DEM fusion outputs especially for areas with high-frequency contents and more complex morphological features like urban areas. Using variational models could improve the DEM quality by up to about 1m.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518870","Data fusion;norm total variation;Weight map;Huber model;TanDEM-X DEM","Urban areas;Data models;Remote sensing;Three-dimensional displays;Data integration;Laser radar;Computational modeling","digital elevation models;geophysical image processing;image fusion;radar interferometry;terrain mapping","urban TanDEM-X raw DEM fusion;DEM quality;TanDEM-X raw DEM fusion task;Huber model;TV-L1;variational models;Interferometric TanDEM-X Processor;Weighted Averaging;DEM fusion methods;mosaic multiple raw DEM tiles;global DEM generation","","","","11","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Sharpening the 20 M Bands of SENTINEL-2 Image Using an Unsupervised Convolutional Neural Network","H. V. Nguyen; M. O. Ulfarsson; J. R. Sveinsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2875","2878","This paper proposes a novel method for sharpening the 20 m bands of the multispectral images acquired by the Sentinel-2 (S2) constellation. We formulate the S2 sharpening as an inverse problem and solve it using an unsupervised convolutional neural network (CNN), called S2UCNN. The proposed method extends the deep image prior provided by a CNN structure with S2 domain knowledge. We incorporate a modulation transfer function-based degradation model as a network layer. We add the 10 m bands to both the network input and output to take advantage of the multitask learning. Experimental results with a real S2 dataset show that the proposed method outperforms the competitive methods on reduced-resolution data and gives very high quality sharpened image on full-resolution data.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555082","Remote sensing;Sentinel-2;image fusion;sharpening;super-resolution;unsupervised convolutional neural network","Degradation;Inverse problems;Modulation;Convolutional neural networks;Remote sensing;Electronics packaging","convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;image resolution;inverse problems;optical transfer function;unsupervised learning","unsupervised convolutional neural network;multispectral images;Sentinel-2 constellation;deep image;CNN structure;S2 domain knowledge;network layer;high quality sharpened image;modulation transfer function-based degradation;S2UCNN;Sentinel-2 image sharpening;S2 sharpening;inverse problem;multitask learning;S2 dataset;full-resolution data","","","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Spectral Diversity Enhancement for Pansharpening","L. Zhou; X. Luo; J. Yin; X. Shi","Image Processing Center, Beihang University, China; Image Processing Center, Beihang University, China; Image Processing Center, Beihang University, China; School of Electronic and Information Engineering, Beihang University, China","2018 25th IEEE International Conference on Image Processing (ICIP)","6 Sep 2018","2018","","","1867","1871","Pansharpening is to generate a synthetic image with high spatial resolution and high spectral resolution via fusing panchromatic (PAN) and multispectral (MS) images. In most traditional pansharpening methods, the original MS image is firstly interpolated to the same size of PAN image by analytical interpolations. However, these interpolated methods could cause false spectral information due to ignoring mixed spectral characteristics in the MS image. To enhance the spectral diversity of upsampled MS image, we use the spatial structure information in PAN image to support the pansharpening in this paper. By introducing the superpixel structure for PAN image, the processible mixed pixels can be screened out in corresponding MS locations, and the other MS locations are considered to be occupied by pure pixels. For the pure and mixed pixels, their upsampling MS results can be obtained via a directly expending manner and a sparse representation manner respectively. Two different detail injection strategies are used for assessing the performance of analytical interpolations and our approach for pansharpening. Experimental results demonstrate that our method achieves the appreciable improvements with respect to analytical interpolations.","2381-8549","978-1-4799-7061-2","10.1109/ICIP.2018.8451459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451459","pansharpening;remote sensing;segmentation;sparse representation","Interpolation;Indexes;Spatial resolution;Dictionaries;Remote sensing;Distortion;Optimization","image fusion;image representation;image resolution;image segmentation;interpolation;spectral analysis","PAN image;upsampling MS results;analytical interpolations;spectral diversity enhancement;high spatial resolution;high spectral resolution;traditional pansharpening methods;original MS image;interpolated methods;false spectral information;mixed spectral characteristics;sparse representation;superpixel structure;multispectral image fusion;panchromatic image fusion;synthetic image generation;spatial structure information;upsampled MS image","","1","","15","IEEE","6 Sep 2018","","","IEEE","IEEE Conferences"
"Oriented object detection based on cross-scale information fusion","C. Li; T. Zhao; C. Mao; W. Hu","Hubei Province Key Laboratory of Intelligent Robot, Wuhan Institute of Technology, Wuhan, China; Hubei Province Key Laboratory of Intelligent Robot, Wuhan Institute of Technology, Wuhan, China; Hubei Province Key Laboratory of Intelligent Robot, Wuhan Institute of Technology, Wuhan, China; Hubei Province Key Laboratory of Intelligent Robot, Wuhan Institute of Technology, Wuhan, China","2022 International Conference on Artificial Intelligence and Computer Information Technology (AICIT)","1 Nov 2022","2022","","","1","4","Due to the enormous size discrepancies between classes and within classes, as well as the high degree of resemblance across classes of oriented objects, traditional remote sensing object detection was made difficult. Although coping with huge scale differences and high inter-class similarity was made possible by multi-scale information fusion, the multiscale weight fusion technique neglected the impact of cross-scale on picture semantic feature extraction, leading to subpar detection performance. The performance of the delayed inference was caused by the rotating region proposal network, which produced high-quality ideas while expanding the network’s capacity. In this study, a cross-scale shift oriented object detection method was suggested. First, by creating a feature pyramid network, the multi-layer feature maps were successfully fused. First, the multi-layer feature maps were effectively fused by reconstructing a feature pyramid network. A cross-scale shift module was simultaneously introduced to FPN to enhance the correlation between multi-scale properties. Finally, to raise the quality of the bounding boxes produced, an oriented region proposal network (ORPN) was used. On remote sensing datasets from DOTA-V1.5, the proposed method fared better than the control group in terms of detection accuracy.","","978-1-6654-5087-4","10.1109/AICIT55386.2022.9930256","Wuhan Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9930256","oriented objects;remote sensing object detection;feature pyramid network;cross-scale shift;region proposal network","Correlation;Semantics;Object detection;Feature extraction;Sensors;Proposals;Information technology","feature extraction;image fusion;object detection;object recognition;remote sensing;sensor fusion","high-quality ideas;cross-scale shift oriented object detection method;feature pyramid network;multilayer feature maps;cross-scale shift module;multiscale properties;oriented region proposal network;remote sensing datasets;detection accuracy;cross-scale information fusion;enormous size discrepancies;oriented objects;traditional remote sensing object detection;huge scale differences;high inter-class similarity;multiscale information fusion;multiscale weight fusion technique;picture semantic feature extraction;detection performance;rotating region proposal network","","1","","15","IEEE","1 Nov 2022","","","IEEE","IEEE Conferences"
"Disaster risk reduction using image fusion of optical and SAR data before and after tsunami","Y. Kwak; A. Yorozuya; Y. Iwami","Dokuritsu Gyosei Hojin Doboku Kenkyujo, Tsukuba, Ibaraki, JP; Dokuritsu Gyosei Hojin Doboku Kenkyujo, Tsukuba, Ibaraki, JP; Dokuritsu Gyosei Hojin Doboku Kenkyujo, Tsukuba, Ibaraki, JP","2016 IEEE Aerospace Conference","30 Jun 2016","2016","","","1","11","This study applied supervised change detection to identify and estimate damaged urban surface conditions before and after a tsunami event in order to provide more accurate information for the implementation and enhancement of disaster risk reduction policies and strategies. Advanced remote sensing is crucial to support cost-effective emergency response activities for disaster risk assessment and management. This preliminary study, as an effort to propose a good case study in risk management, suggested that three main steps, i.e., filtering, fusing and classifying, should be adopted to perform change detection before and after a natural disaster. We fused very high-spatial-resolution multi-temporal optical images (0.6 m spatial resolution) and X-band SAR images (2.5 m spatial resolution). The study also revealed that the decision-level method, i.e. morphological transform, was the most promising in image fusion of filtered images to classify urban surfaces in tsunami damage assessment, compared with the pixel-level method, i.e. wavelet transform, and feature-level method, i.e. segmentation extraction. This paper reports, coupled with the results from the image fusion, that the preliminary results are good enough to obtain urban impervious surface estimation of a wide disaster risk area but not good enough to make clear amplitude images to identify individual buildings of dwelling zone. The proposed normalized change index (NCI), an important indicator for detecting changes, was found capable of providing better estimation of urban impervious surfaces, such as transport-related land (e.g., bridges and parking lots) and building roof tops in residential and industrial areas over a coastal zone in Rikuzen-takada City, devastated in the 2011 Great East Japan Earthquake.","","978-1-4673-7676-1","10.1109/AERO.2016.7500520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7500520","","Risk management;Tsunami;Synthetic aperture radar;Earthquakes;Optical imaging;Optical sensors;Optical surface waves","disasters;earthquakes;geophysical image processing;image classification;image filtering;image fusion;image resolution;remote sensing by radar;risk management;synthetic aperture radar;tsunami","image fusion;optical data;SAR data;supervised change detection;damaged urban surface conditions;tsunami event;disaster risk reduction policies;disaster risk reduction strategies;remote sensing;cost-effective emergency response activities;disaster risk assessment;disaster risk management;natural disaster;high-spatial-resolution multitemporal optical images;spatial resolution;X-band SAR images;decision- level method;morphological transform;filtered images;urban surfaces;tsunami damage assessment;pixel-level method;wavelet transform;feature-level method;segmentation extraction;urban impervious surface estimation;disaster risk area;amplitude images;individual building dwelling zone;normalized change index;urban impervious surfaces;transport-related land;building roof;residential area;industrial area;coastal zone;Rikuzen-takada City;Great East Japan Earthquake;AD 2011","","3","","35","IEEE","30 Jun 2016","","","IEEE","IEEE Conferences"
"Bayesian fusion of multispectral and panchromatic images","G. Khademi; H. Ghassemian","Image Processing and Information Analysis Lab., Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Lab., Tarbiat Modares University, Tehran, Iran","2017 10th Iranian Conference on Machine Vision and Image Processing (MVIP)","23 Apr 2018","2017","","","20","25","In this paper, a model-based pansharpening method based on the super-resolution (SR) paradigm is developed. The spatial and spectral relationships between the desired high resolution multispectral (MS) image and the observed low resolution MS and panchromatic (Pan) images are combined with an appropriate image prior model via Bayesian theory. Maximum a posteriori (MAP) estimation is employed to convert the inverse problem of restoring the desired MS image into a constrained optimization problem. The final desired MS image is obtained by the conjugate gradient (CG) algorithm. Experimental results on two datasets show the effectiveness of the proposed method compared to the well-known pansharpening methods according to the quantitative evaluation metrics and visual inspection.","2166-6784","978-1-5386-4405-8","10.1109/IranianMVIP.2017.8342363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8342363","Image fusion;maximum a posteriori (MAP) estimation;pansharpening;remote sensing;super-resolution (SR)","Spatial resolution;Sensors;Bayes methods;Estimation;Probability density function","Bayes methods;geophysical image processing;image fusion;image resolution;inverse problems;remote sensing","well-known pansharpening methods;final desired MS image;constrained optimization problem;inverse problem;maximum a posteriori estimation;Bayesian theory;appropriate image prior model;observed low resolution MS;desired high resolution multispectral image;spectral relationships;spatial relationships;super-resolution paradigm;panchromatic images;multispectral images;Bayesian fusion","","7","","32","IEEE","23 Apr 2018","","","IEEE","IEEE Conferences"
"A Retina-Inspired Multiresolution Analysis Framework for Pansharpening","M. Maneshi; H. Ghassemian; G. Khademi; M. Imani","Image Processing and Information Analysis Lab. Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Lab. Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Lab. Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Lab. Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2020 International Conference on Machine Vision and Image Processing (MVIP)","15 Jun 2020","2020","","","1","5","Technical limitations on the satellite sensors make a trade-off between the spectral and spatial resolution in remotely sensed images. To deal with this issue, pansharpening has been emerged to prepare a single image with the high spatial and spectral resolution, simultaneously. This paper presents a pansharpening approach based on the retina-inspired model and the multiresolution analysis (MRA) framework. The retina- inspired model is simplified by the difference of Gaussian (DoG) operator, and we apply it to the panchromatic image to extract the spatial details. Furthermore, the injection gains in the MRA framework are calculated through an iterative process where the gains at each iteration are updated based on the fusion result obtained from its previous iteration. To investigate the performance of the proposed model, it is compared with some classical pansharpening approaches with two data sets captured by the GeoEye-1 and Pléiades satellite imagery sensors. The experimental results show the proposed retina-inspired pansharpening method acts well in injecting the spatial information along with reducing the spectral distortion.","2166-6784","978-1-7281-6832-6","10.1109/MVIP49855.2020.9116884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9116884","Difference of Gaussian (DoG);Image fusion;Pansharpening;Retina model;Remote sensing","Image sensors;Analytical models;Satellites;Pansharpening;Dogs;Distortion;Retina","geophysical image processing;geophysical signal processing;image fusion;image resolution;remote sensing","retina-inspired multiresolution analysis framework;technical limitations;satellite sensors;spectral resolution;remotely sensed images;single image;high spatial resolution;pansharpening approach;retina-inspired model;retina- inspired model;Gaussian operator;panchromatic image;spatial details;injection gains;MRA framework;iterative process;classical pansharpening approaches;Pléiades satellite imagery sensors;retina-inspired pansharpening method;spatial information;spectral distortion","","4","","25","IEEE","15 Jun 2020","","","IEEE","IEEE Conferences"
"Fusion of hyperspectral and LiDAR data using morphological component analysis","X. Xu; J. Li; A. Plaza","CIGIA, Sun Yat-sen University, Guangzhou, China; CIGIA, Sun Yat-sen University, Guangzhou, China; Department of Technology of Computers and Communications, University of Extremadura, Cáceres, Spain","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","3575","3578","This paper presents a new classification framework for the fusion of hyperspectral and LiDAR data. The proposed approach aims at exploiting the complementarity of the features, i.e., textural features in the hyperspectral data and the height features in the LiDAR data, respectively. In this work, we use a morphological component analysis (MCA) method for textural feature extraction. The classification is then executed by a multinomial logistic regression classifier (MLR). Our obtained experimental results reveal that the proposed feature fusion method can lead to very good classification results.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729926","Remote sensing image classification;morphological component analysis (MCA);feature fusion;LiDAR;Hyperspectral classification","Laser radar;Feature extraction;Hyperspectral imaging;Data mining;Kernel","feature extraction;hyperspectral imaging;image classification;image fusion;image texture;optical radar;remote sensing by laser beam","hyperspectral data;data fusion;LiDAR data;morphological component analysis;height feature;textural feature extraction;multinomial logistic regression classifier","","12","","16","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Fusion of high and very high density LiDAR data for 3D forest change detection","D. Marinelli; C. Paris; L. Bruzzone","Dept. of Information Engineering and Computer Science, University of Trento, Trento, Italy; Dept. of Information Engineering and Computer Science, University of Trento, Trento, Italy; Dept. of Information Engineering and Computer Science, University of Trento, Trento, Italy","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","3595","3598","Light Detection And Ranging (LiDAR) data have proven to be very effective in the estimation of parameters for forestry applications. However, little research has been done regarding the multitemporal analysis of these data. In this paper we propose a novel hierarchical change detection approach that first performs the detection of major changes (e.g., harvested trees) and then focuses on the detection of minor changes (e.g., single tree growth), using multitemporal LiDAR data having different point densities. Splitting the change detection problem allows us to analyze the different types of changes with different techniques. In particular, the detection of minor changes is carried out directly on the point clouds in order to exploit all the informative content of the LiDAR data. The approach has been tested on a dataset acquired in 2010 and 2014 on a complex forest area located in the Southern Italian Alps. The experimental results confirm the effectiveness of the proposed approach.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729931","3D change detection;multitemporal analysis;Light detection and Ranging (LiDAR);remote sensing;forestry","Vegetation;Three-dimensional displays;Laser radar;Ellipsoids;Solid modeling;Estimation;Data models","forestry;geophysical image processing;image fusion;optical radar;remote sensing by laser beam;vegetation","very-high-density LiDAR data fusion;3D forest change detection;Light Detection And Ranging;hierarchical change detection;harvested tree detection;single tree growth detection;multitemporal LiDAR data;AD 2010;AD 2014;complex forest area;southern Italian alps","","5","","7","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Automatic Fine Alignment of Multispectral and Panchromatic Images","A. Arienzo; L. Alparone; B. Aiazzi; A. Garzelli","Institute of Applied Physics (IFAC-CNR), Research Area of Florence, Sesto F.no (FI), Italy; Department of Information Engineering (DINFO), University of Florence, Florence, Italy; Institute of Applied Physics (IFAC-CNR), Research Area of Florence, Sesto F.no (FI), Italy; Department of Information Engineering and Mathematics, University of Siena, Siena, Italy","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","228","231","In this paper, we propose a totally unsupervised procedure to cope with the residual local misalignment between a higher-resolution panchromatic (Pan) image and a series of lower-resolution multispectral (MS) bands, preliminarily interpolated to the pixel size of Pan. The proposed method exploits the different resolutions of the MS and Pan datasets to force the former to match a lowpass version of the latter. Specifically, the space-varying residue of the multivariate regression between resampled MS bands and lowpass-filtered Pan image, which locally measures the extent of MS-to-Pan misalignment, is injected into each the MS bands, after being weighted by the pixel-varying multiplicative injection gain of each band. Tests on a GeoEye-1 image, with space-varying shifts, highlight improvements in the spatial alignment.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324689","Multispectral pansharpening;multivariate regression;co-registration;satellite remote sensing","Image color analysis;Pansharpening;Geometry;Spatial resolution;Satellites;Indexes;Automobiles","geophysical image processing;geophysical signal processing;geophysical techniques;image fusion;image resolution;remote sensing","GeoEye-1 image;space-varying shifts;spatial alignment;automatic fine alignment;totally unsupervised procedure;residual local misalignment;higher-resolution panchromatic image;lower-resolution multispectral bands;lowpass version;space-varying residue;resampled MS bands;Pan image;MS-to-Pan misalignment;pixel-varying multiplicative injection gain","","1","","16","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Resolution Enhancement of Unsupervised Classification Maps Through Data Fusion of Spectral and Visible Images from Different Sensing Instruments","F. Kizel","Department of Mapping and Geoinformation Engineering, Technion-Israel Institute of Technology","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2887","2890","We propose a new methodology for enhancing the spatial resolution of unsupervised classification through a fusion of multispectral and visible images. The new method, DFuSIAL-C (Data Fusion through Spatial Information-Aided Learning for Classification), relies on automatically extracted invariant points (IPs), assumed to have the same land cover type in the two data sources. In contrast to typical methods, DFuSIAL-C does not require a full spatial, spectral, and temporal overlapping between the data sources and allows for the fusion of data from different sensors. An evaluation of the proposed method, compared to a state-of-the-art pansharpening fusion method, is carried out using Landsat-8 and Sentinel-2 images. Our experimental results show that the DFuSIAL-C obtains unsupervised classification maps with a significantly enhanced spatial resolution and an overall accuracy (OA) of 85%. Furthermore, we show that the proposed method is preferable when full overlapping is not available due to the acquisition by different instruments.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555098","Spectral Remote Sensing;Classification;Data Fusion;Spatial Information;Pansharpening;Machine Learning;Neural Networks","Earth;Artificial satellites;Instruments;Data integration;Pansharpening;Sensor fusion;Spatial databases","feature extraction;geophysical image processing;image classification;image fusion;image resolution;remote sensing","sensing instruments;resolution enhancement;enhanced spatial resolution;unsupervised classification maps;Sentinel-2 images;state-of-the-art pansharpening fusion method;data sources;land cover type;automatically extracted invariant points;Spatial Information-Aided;Data Fusion;DFuSIAL-C;visible images","","","","21","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Fusion of microwave and infrared data for enhancing its spatial resolution","I. Yanovsky; A. Behrangi; M. Schreier; V. Dang; B. Wen; B. Lambrigtsen","Joint Institute for Regional Earth System Science and Engineering, University of California Los Angeles, Los Angeles, CA, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","2625","2628","The images acquired by microwave sensors are blurry and of low-resolution. On the other hand, the images obtained using infrared/visible sensors are of sufficiently high-resolution. In this paper, we develop a data fusion methodology and apply it to enhance resolution of a microwave image using the data from a collocated infrared/visible sensor. Such an approach takes advantage of the spatial resolution of the infrared instrument and the sensing accuracy of the microwave instrument. We tested our method using precipitation scenes captured with the Advanced Microwave Sounding Unit (AMSU) microwave instrument and the Advanced Very High Resolution Radiometer (AVHRR).","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127533","Data fusion;inverse problems;microwave imaging;remote sensing;sparse optimization;spatial resolution;super-resolution","Microwave imaging;Microwave theory and techniques;Spatial resolution;Data integration;Microwave radiometry;Sensors","atmospheric precipitation;geophysical image processing;image fusion;image resolution;microwave imaging;remote sensing","low-resolution;data fusion methodology;microwave image;infrared instrument;microwave instrument;Advanced Microwave Sounding Unit;Advanced Very High Resolution Radiometer;infrared sensor;visible sensor;precipitation scene","","","","16","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"High spatial resolution hyperspectral image using fusion technique","K. Suchitha; B. S. Premananda; A. K. Singh","R.V. College of Engineering, Bengaluru, India; R.V. College of Engineering, Bengaluru, India; ISRO Satellite Centre, Bengaluru, India","2017 International Conference on Trends in Electronics and Informatics (ICEI)","22 Feb 2018","2017","","","348","353","Now a day's remote sensing imaging is extensively used in the study of land resources, surface geology, water resources, landslide study, forest study, urban development at large scale. Always there is a need to improve the spatial and spectral information of remote sensing data. This can be done either by building new satellites with high resolution power or by using image processing techniques. Building a new satellite with high power is much more expensive, so it is an advantage of using image processing techniques. Hyperspectral imaging sensor provides better spectral resolution, but provide poor spatial resolution and multispectral imaging sensor provides good spatial resolution but provide poor spectral resolution. There are many fusion techniques such as Gram Schmidt Pan Sharpening, Principal Component Transform, High Pass Filtering are used to sharpen Multispectral Image with panchromatic image. To sharpen the hyperspectral image we are trying to fuse the HSI with MSI with the existing techniques and are quantified using Mean Square Error, Peak Signal to Noise Ratio, Entropy, and Universal Image Quality Index.","","978-1-5090-4257-9","10.1109/ICOEI.2017.8300946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8300946","FLAASH;Hyperspectral;QUAC;Image Fusion;Multispectral","Spatial resolution;Atmospheric waves;Hyperspectral imaging;Satellites;Transforms;Atmospheric modeling","geophysical image processing;geophysical techniques;hyperspectral imaging;image fusion;remote sensing","spectral resolution;urban development;Gram Schmidt pan sharpening;principal component transform;high pass filtering;mean square error;peak signal-to-noise ratio;entropy;Universal Image Quality Index;panchromatic image;High Pass Filtering;multispectral imaging sensor;hyperspectral imaging sensor;image processing techniques;high resolution power;remote sensing data;spectral information;spatial information;forest study;landslide study;water resources;surface geology;land resources;fusion technique;High spatial resolution hyperspectral image","","","","14","IEEE","22 Feb 2018","","","IEEE","IEEE Conferences"
"Class-Aware Regularized Self-Distillation Learning Method for Land Cover Classification","Q. Zang","School of Artificial Intelligence, Xidian University, Xi'an, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","4603","4606","The rapid development of remote sensing provides abundant data for land cover classification. Nevertheless, densely labeling newly-acquired data is an expensive job. With labeled data from different imaging locations, obtaining good performance on unlabeled test data has two challenges: (1) Due to the geographic shift, deep models trained on labeled data lack generalization to other geographic locations. (2) The samples of different classes in remote sensing images with diverse scenes are extremely imbalanced. To conquer these obstacles, we propose a class-aware regularized self-distillation learning method. For the former, the model trained on labeled data is multi-round self-distilled on unlabeled data with the supervision of pseudo-labels, and each class is set with specific distillation rounds. For the latter, we assign a tailored weight to each class during the self-distillation learning. In the end, our proposal achieves a mIoU of 49.83%, ranking third place in Track SLM of 2022 IEEE GRSS Data fusion contest.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884217","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884217","land cover classification;deep learning;self-distillation learning;class-aware regularization","Learning systems;Imaging;Data integration;Interference;Data models;Reliability;Proposals","geophysical image processing;geophysical signal processing;image classification;image fusion;learning (artificial intelligence);pattern classification;remote sensing;sensor fusion;terrain mapping","land cover classification;abundant data;different imaging locations;unlabeled test data;labeled data lack generalization;geographic locations;remote sensing images;class-aware regularized self-distillation learning method;unlabeled data;pseudolabels;specific distillation rounds;2022 IEEE GRSS Data fusion contest","","1","","9","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"A Two-Layers Super-Resolution Based Generation Adversarial Spatiotemporal Fusion Model","S. Fang; Q. Guo; Y. Cao; J. Zhang","Anhui Province Key Laboratory of Industry Safety and Emergency Technology, Hefei, China; Key Laboratory of Knowledge Engineering with Big Data, Hefei University of Technology, Ministry of Education, Hefei, China; University of Science and Technology of China, Hefei, Anhui, China; Key Laboratory of Knowledge Engineering with Big Data, Hefei University of Technology, Ministry of Education, Hefei, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","891","894","Remote sensing image spatiotemporal fusion (STF) algorism plays an important role by supplementing the lack of original high-resolution remote sensing satellite images in the study scenarios of dense time-series data. In recent years, the deep-learning-based STF algorithm has become a research hotspot with comparatively higher accuracy and robustness. However, due to the lack of sufficient high-quality images for training and the huge resolution gap between low-resolution images and high-resolution images, it is difficult to recover detailed information, especially for areas of land-cover change. In this paper, we propose a two-layers super-resolution based generation adversarial spatiotemporal fusion model(TLSRSTF) using smaller inputs to reduce pressure on data requirements and a mutual affine convolution to reduce model parameters. Specifically, we only use a pair of high-resolution and low-resolution images and a high-resolution image at any time. A spatial degradation consistency is constructed to adaptively determine the ratio of two layers of the super-resolution STF model. The quantitative and qualitative experimental results on public spatiotemporal fusion datasets demonstrate our superiority over the state-of-the-art methods.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883547","National Natural Science Foundation of China(grant numbers:61872327,61175033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883547","Spatiotemporal Fusion;Generative Adversarial Networks(GAN);Mutual Affine Convolution","Training;Adaptation models;Satellites;Convolution;Superresolution;Data models;Robustness","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);remote sensing;spatiotemporal phenomena","sensing image spatiotemporal fusion algorism;high-resolution remote sensing satellite images;dense time-series data;STF algorithm;comparatively higher accuracy;robustness;high-quality images;huge resolution gap;low-resolution images;high-resolution image;super-resolution STF model;public spatiotemporal fusion datasets","","","","9","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Water Pollution Detection in Acapulco Coasts Using Merged Data from the Sentinel-2 and Sentinel-3 Satellites","R. Lomelí-Huerta; H. Avila-George; J. P. Rivera-Caicedo; M. De-la-Torre","Departamento de Ciencias Computacionales e Ingenierías, Universidad de Guadalajara; Departamento de Ciencias Computacionales e Ingenierías, Universidad de Guadalajara; CONACyT-UAN, Secretaría de Investigation y Posgradom, Universidad Autónoma de Nayarit; Departamento de Ciencias Computacionales e Ingenierías, Universidad de Guadalajara","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","1518","1521","Acapulco coasts are occasionally contaminated by illegal discharges originated by temporary or permanent floods that disembogue to the pacific ocean. Plumes formed by contaminated water running through the ocean can be distinguished in satellite imagery, and their reflectance is related to the polluting elements. Although some spacial agencies provide data from diverse multispectral sensors, application-specific requirements are fulfilled by merging heterogeneous imagery (differences in spatial, temporal, and spectral resolutions). This paper proposes a continuous monitoring strategy to detect pollution in water discharges by combining data from Sentinel-2 and Sentinel-3 platforms. First, the region of interest to be monitored is detected using the bands with high spatial resolution. Then, distance-based supervised machine learning is employed to detect pixel-wise pollution in water. Finally, the historic detections over time are presented to detect recurrent discharges.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553929","Sentinel;satellite image fusion;contaminated water;monitoring system;remote sensing","Reflectivity;Image sensors;Satellites;Oceans;Merging;Machine learning;Water pollution","environmental monitoring (geophysics);floods;geophysical image processing;marine pollution;oceanographic regions;oceanographic techniques;remote sensing;supervised learning;water pollution measurement","water pollution detection;Sentinel-2 satellite;Sentinel-3 satellite;illegal discharges;temporary floods;permanent floods;Pacific Ocean;plumes;contaminated water;satellite imagery;diverse multispectral sensors;heterogeneous imagery;water discharges;distance-based supervised machine learning;pixel-wise pollution;Acapulco coasts;Mexico;permanent flood","","1","","11","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"SOON: Specifically Optimized One-Stage Network for Object Detection in Remote Sensing Imagery","Z. Wang; H. Qin; Y. Li; J. Lei; W. Xie","State Key Laboratory of Integrated Service Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China","2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)","13 Feb 2020","2019","","","531","538","With great significance in military and civilian applications, detecting indistinguishable small objects in wide-scale remote sensing images is still a challenging topic. In this work, we propose a specially optimized one-stage network (SOON) focusing on extracting spatial information of high-resolution images by understanding and analyzing the combination of feature and semantic information of small objects, which consists of feature enhancement, multi-scale detection, and feature fusion. The first part is implemented by constructing a receptive field enhancement (RFE) module and incorporating it into the specific parts of the network where the information of small objects mainly exists. The second part is achieved by four detectors with different sensitivities accessing to the fused and enhanced features, which enables the network to make full use of features in different scales. The third part consolidates the high-level and low-level features by adopting up-sampling, concatenation and convolution operations to build a feature pyramid structure, which explicitly yields strong feature representation and semantic information. In addition, we introduce the Soft-NMS to preserve accurate bounding boxes in the post-processing stage for densely arranged objects. Note that the split and merge strategy, as well as the multi-scale training strategy, are employed in this work. Extensive experiments and thorough analysis are performed on the NWPU VHR-10-v2 dataset and the ACS dataset as compared with several state-of-the-art methods, in which satisfactory performance verifies the effectiveness of the design and optimization. The code will be released for reproduction.","2375-0197","978-1-7281-3798-8","10.1109/ICTAI.2019.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8995401","object detection, remote sensing images, feature enhancement, multi-scale detection, feature fusion","","convolutional neural nets;feature extraction;geophysical image processing;image enhancement;image fusion;image representation;image resolution;object detection;remote sensing","specifically optimized one-stage network;object detection;remote sensing imagery;military applications;civilian applications;wide-scale remote sensing images;spatial information;high-resolution images;semantic information;feature enhancement;multiscale detection;receptive field enhancement module;low-level features;convolution operations;feature pyramid structure;feature representation;post-processing stage;densely arranged objects;multiscale training strategy;optimization;SOON;feature fusion","","1","","35","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Multiresolution analysis based sparse dictionary learning for remotely sensed image retrieval","A. Yadav; A. Aryasomayajula; R. AhmedAnsari","Department of Electrical Engineering, Veermata Jijabai Technological Institute, Mumbai, India; Department of Electrical Engineering, Veermata Jijabai Technological Institute, Mumbai, India; Department of Electrical Engineering, Veermata Jijabai Technological Institute, Mumbai, India","2019 Women Institute of Technology Conference on Electrical and Computer Engineering (WITCON ECE)","14 May 2020","2019","","","76","80","Sparse representation using over-complete dictionaries offers a compact representation with minimal error and yields in compression ratios that are relatively higher than other conventional lossless compression algorithms. Multiresolution analysis (MRA) offers a framework to decompose and observe a signal at various resolutions thus allows representing the signal in a way that makes the retrieval convenient.In this paper, we propose a wavelet based MRA framework for image compression by sparse representation of the decomposition of an image, trained on over-complete dictionary. The dictionaries are trained using algorithms like the singular value decomposition (SVD), method of optimal directions (MOD) and the sparse representation is done using the pursuit algorithm. The method is tested on remotely sensed images captured by Worldview-1 satellite and reconstructions from the dictionaries using proposed framework also yield a low root mean square error and a high spatial similarity index (SSIM) which attests to the high quality of reconstructed images. The results obtained show the storage of the over-complete dictionaries trained on the various subbands, keep on reducing as the level of decomposition increases lead to a better compression ratio than the sparse representation of an image at a single resolution while maintaining the same image quality.","","978-1-7281-5204-2","10.1109/WITCONECE48374.2019.9092901","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9092901","sparse coding;image retrieval;multiresolution analysis;image compression;remote sensing","Dictionaries;Image coding;Image reconstruction;Multiresolution analysis;Remote sensing;Spatial resolution","data compression;geophysical image processing;geophysical techniques;image coding;image fusion;image reconstruction;image representation;image resolution;image retrieval;learning (artificial intelligence);mean square error methods;remote sensing;singular value decomposition;wavelet transforms","multiresolution analysis;sparse dictionary;remotely sensed image retrieval;sparse representation;over-complete dictionary;compact representation;minimal error;compression ratio;conventional lossless compression algorithms;MRA framework;image compression;remotely sensed images;reconstructed images;image quality","","1","","11","IEEE","14 May 2020","","","IEEE","IEEE Conferences"
"Comparison of Different Pan Sharpening Techniques using Landsat 8 Imagery","N. R. Govind; C. A. Rishikeshan; H. Ramesh","Dept. of Applied Mechanics & Hydraulics, National Institute of Technology Karnataka, Mangalore, India; Dept. of Applied Mechanics & Hydraulics, National Institute of Technology Karnataka, Mangalore, India; Dept. of Applied Mechanics & Hydraulics, National Institute of Technology Karnataka, Mangalore, India","2019 IEEE 5th International Conference for Convergence in Technology (I2CT)","12 Mar 2020","2019","","","1","4","Pan sharpening technique is a widely used image processing technique which combines the data available from various sensors and exploits its varied capabilities. In this study, the efficiency of four diverse pan sharpening methods namely High Pass filter, Modified Intensity Hue Saturation, Ehlers fusion and Hyperspectral Colour Sharpening was evaluated. The pan sharpening approaches are applied to Landsat 8 imagery of an urban area. The spatial and spectral quality of the fused images is assessed using different indices like Bias, RMSE, Correlation Coefficient and ERGAS. The fused images obtained have improved spatial resolution and visual appearance compared to the original MS image. The fused images have a spatial resolution comparable to that of the PAN image. According to visual analysis, Modified IHS method yielded a fused image with better visual interpretability. The statistical analysis shows that the high pass filter is the most suitable pan sharpening method for this dataset. On testing for Bias, RMSE, ERGAS and CC, the high pass filter method performed best followed by Modified Intensity Hue saturation, Ehlers fusion and Hyperspectral Colour Sharpening while Ehlers fusion showed a higher correlation, compared to Modified IHS.","","978-1-5386-8075-9","10.1109/I2CT45611.2019.9033659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9033659","pan sharpening;remote sensing;urban area;spatial resolution","Spatial resolution;Remote sensing;Artificial satellites;Earth;Visualization;Urban areas","geophysical image processing;high-pass filters;image colour analysis;image fusion;image resolution;remote sensing;statistical analysis","Landsat 8 imagery;pan sharpening technique;image processing technique;diverse pan sharpening methods;Modified Intensity Hue Saturation;Ehlers fusion;Hyperspectral Colour Sharpening;spatial quality;spectral quality;spatial resolution;original MS image;PAN image;Modified IHS method;fused image;suitable pan sharpening method;high pass filter method;Modified Intensity Hue saturation;Correlation Coefficient;ERGAS","","1","","14","IEEE","12 Mar 2020","","","IEEE","IEEE Conferences"
"Construction of Yunnan's Agricultural Ecological Civilization Based on Intelligent UAV and SAR Image Analysis","W. Li; X. Chen; G. Li; Y. Bi","College of Humanities and Social Sciences, Yunnan Agricultural University, Kunming, Yunnan, China; College of Economics and Management, Yunnan Agricultural University, Kunming, Yunnan, China; Yunnan Plateau Characteristic Agricultural Industry Research Institute, Kunming, Yunnan, China; Yunnan Agricultural University, Kunming, Yunnan, China","2022 4th International Conference on Smart Systems and Inventive Technology (ICSSIT)","25 Feb 2022","2022","","","1639","1642","This paper studies the construction of Yunnan's agricultural ecological civilization based on the analysis of intelligent drones and SAR images. This article deeply discusses several key problems faced by multi-rotor UAV SAR imaging processing from the aspects of imaging algorithm, motion compensation and auto-focusing, and puts forward an effective imaging processing scheme. The SIFT algorithm is used to extract the image feature points, and the BBF algorithm calculates and generates a matching point set. According to the matching point set, the inter-image perspective transformation model is calculated to complete the image registration, and the wavelet transform algorithm is used to realize the registration image fusion. Based on SAR image analysis and dimensionality reduction of remote sensing data, dimensionality reduction, extraction and classification of remote sensing images of agricultural ecology in Yunnan are carried out.","","978-1-6654-0118-0","10.1109/ICSSIT53264.2022.9716302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9716302","Yunnan;Agricultural Ecological Civilization;Intelligent UAV;SAR Image","Dimensionality reduction;Wavelet transforms;Satellites;Imaging;Reconnaissance;Feature extraction;Radar polarimetry","autonomous aerial vehicles;feature extraction;geophysical image processing;image fusion;image matching;image processing;image registration;motion compensation;radar imaging;remote sensing;remotely operated vehicles;synthetic aperture radar;transforms;wavelet transforms","image registration;wavelet transform algorithm;registration image fusion;SAR image analysis;remote sensing images;agricultural ecology;Yunnan's agricultural ecological civilization;intelligent UAV;intelligent drones;SAR images;multirotor UAV SAR imaging processing;motion compensation;auto;effective imaging processing scheme;SIFT algorithm;image feature points;BBF algorithm;matching point;inter-image perspective transformation model","","","","24","IEEE","25 Feb 2022","","","IEEE","IEEE Conferences"
"Joint dictionary learning with ridge regression for pansharpening","S. Tang; L. Xiao; B. Naz; P. Liu; Y. Chen","Nanjing University of Science and Technology, Nanjing, Jiangsu, CN; Nanjing University of Science and Technology; Nanjing University of Science and Technology; Nanjing University of Science and Technology; Nanjing University of Science and Technology, Nanjing, Jiangsu, CN","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","613","616","A novel pansharpening method is proposed for creating a fused image of high spatial and spectral resolutions through merging a panchromatic (PAN) image with a multispectral (MS) image. To replace the patch pairs sampled from the images directly as the dictionary pairs, a joint learning model is proposed to learn a pair of compact dictionaries. Meanwhile, instead of restricting the coding coefficients of low resolution (LR) MS and high resolution (HR) MS image patches to be equal, ridge regression model is employed to describe their relation. Then, the fused MS image is calculated by combining the mapped sparse coefficients and the dictionary for the HR MS image. By comparing with some well-known methods in terms of several universal quality evaluation indexes, the simulated experimental results demonstrate the superiority of our method.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325838","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325838","","Dictionaries;Spatial resolution;Remote sensing;DH-HEMTs;Joints;Sensors","image fusion;regression analysis","dictionary learning;pansharpening method;image fusion;panchromatic image;multispectral image;coding coefficient;ridge regression model;sparse coefficient;universal quality evaluation index","","1","","10","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Object-based image analysis for urban land cover classification in the city of Campinas - SP, Brazil","D. G. M. França; R. G. Lotte; C. M. de Almeida; S. M. O. Siani; T. S. Körting; L. G. M. Fonseca; L. T. da Silva","National Institute for Space Research (INPE), Brazil; National Institute for Space Research (INPE), Brazil; National Institute for Space Research (INPE), Brazil; National Institute for Space Research (INPE), Brazil; National Institute for Space Research (INPE), Brazil; National Institute for Space Research (INPE), Brazil; National Institute for Space Research (INPE), Brazil","2015 Joint Urban Remote Sensing Event (JURSE)","11 Jun 2015","2015","","","1","4","Classifiers that make use of pixel-by-pixel approaches are limited in the high spatial and radiometric resolution of urban areas, that happens mostly because of the similarity between the target's spectral response like ceramic roofs and bare soil. Because of that, the literature favors approaches that make use of object-oriented analysis for image interpretation, those approaches make a better use of the high spatial resolution and do not use only the target spectral response. Assuming that the object-oriented analysis is a favorable approach to be employed for intra-urban image classification, this paper will assess the results of such approach through an implementation of it in an urbanized area from the city of Campinas (Brazil), which has a size close to twelve square kilometers. Making use of the fusion of high spatial resolution image from Worldview-2 sensor and it's panchromatic band, the experiments were performed with the use of eCognition Developer 8 as the segmentation platform, and the classification being based on a decision tree generated by J48 (C4.5) algorithm on the software WEKA. This work also assess which approach best suits the experiment needs, being an optimal attribute selection achieved through a Wrapper filter, with a final kappa statistic of 0.9425.","2334-0932","978-1-4799-6652-3","10.1109/JURSE.2015.7120351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120351","","Data mining;Image segmentation;Remote sensing;Spatial resolution;Decision trees;Image analysis","decision trees;geophysical image processing;image classification;image fusion;image segmentation;land cover","object-based image analysis;urban land cover classification;Campinas city;Brazil;classifier;urban area radiometric resolution;ceramic roof;bare soil;object-oriented analysis;image interpretation;intraurban image classification;image fusion;Worldview-2 sensor;panchromatic band;eCognition developer 8;segmentation platform;decision tree;J48 C4.5 algorithm;software WEKA;Wrapper filter;kappa statistics","","","","16","IEEE","11 Jun 2015","","","IEEE","IEEE Conferences"
"Classification of pixel-level fused hyperspectral and lidar data using deep convolutional neural networks","S. Morchhale; V. P. Pauca; R. J. Plemmons; T. C. Torgersen","Departments of Computer Science and Mathematics, Wake Forest University, Winston-Salem; Departments of Computer Science and Mathematics, Wake Forest University, Winston-Salem; Departments of Computer Science and Mathematics, Wake Forest University, Winston-Salem; Departments of Computer Science and Mathematics, Wake Forest University, Winston-Salem","2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Oct 2017","2016","","","1","5","We investigate classification from pixel-level fusion of Hyperspectral (HSI) and Light Detection and Ranging (LiDAR) data using convolutional neural networks (CNN). HSI and LiDAR imaging are complementary modalities increasingly used together for geospatial data collection in remote sensing. HSI data is used to glean information about material composition and LiDAR data provides information about the geometry of objects in the scene. Two key questions relative to classification performance are addressed: the effect of merging multi-modal data and the effect of uncertainty in the CNN training data. Two recent co-registered HSI and LiDAR datasets are used here to characterize performance. One was collected, over Houston TX, by the University of Houston National Center for Airborne Laser Mapping with NSF sponsorship, and the other was collected, over Gulfport MS, by Universities of Florida and Missouri with NGA sponsorship.","2158-6276","978-1-5090-0608-3","10.1109/WHISPERS.2016.8071715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071715","LiDAR;Hyperspectral Imaging;Convolutional Neural Networks;Data Fusion","Laser radar;Training;Feature extraction;Training data;Hyperspectral imaging;Neurons;Biological neural networks","geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);neural nets;optical radar;remote sensing by laser beam;terrain mapping","lidar data;deep convolutional neural networks;pixel-level fusion;LiDAR imaging;geospatial data collection;remote sensing;HSI data;material composition;classification performance;CNN training data;LiDAR datasets;hyperspectral data;Light Detection and Ranging;Texas;Houston;Mississippi;Gulfport;Missouri","","14","","16","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"An image sharpening strategy based on multiframe super resolution for multispectral data","J. Sun; Q. Lv; Z. Tan; Y. Liu","Key Laboratory of Computational Optical Imaging Technology, Academy of Opto-Electronics, Chinese Academy of Sciences, Beijing, Beijing, CN; Key Laboratory of Computational Optical Imaging Technology, Academy of Opto-Electronics, Chinese Academy of Sciences, Beijing, Beijing, CN; Key Laboratory of Computational Optical Imaging Technology, Academy of Opto-Electronics, Chinese Academy of Sciences, Beijing, Beijing, CN; Key Laboratory of Computational Optical Imaging Technology, Academy of Opto-Electronics, Chinese Academy of Sciences, Beijing, Beijing, CN","2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Oct 2017","2016","","","1","5","Spatial resolution is one of the most important assessments to evaluate an image. Enhancing spatial resolution consequently becomes a hot issue. As is all known, multispectral (MS) image, which is widely studied in remote sensing (RS) field, can be fused with the corresponding high-resolution panchromatic image to promote spatial-quality. In this paper, we consider the question regarding how to enhance the spatial resolution of multispectral image in the case that we do not have high-resolution panchromatic image. The only inputs are the MS data and the same spatial-resolution multi-frame panchromatic image. This paper addresses the application of super-resolution (SR) reconstruction technique and provides a suggestion for MS image sharpening. We generate a high-resolution panchromatic image based on SR. Then we adjust it and the low-resolution MS image into the same size. At last, we fuse them via a hybrid image sharpening technique. Experiments demonstrated an effective processing result and a good performance.","2158-6276","978-1-5090-0608-3","10.1109/WHISPERS.2016.8071718","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071718","Multispectral image;spatial resolution;super resolution;image sharpening","Spatial resolution;Optical filters;Image reconstruction;Filtering algorithms;Principal component analysis;Wavelet transforms","image enhancement;image fusion;image reconstruction;image resolution;image sensors;remote sensing","image sharpening strategy;multiframe super resolution;multispectral data;remote sensing field;spatial-quality;spatial-resolution multiframe panchromatic image;super-resolution reconstruction technique;MS image sharpening;low-resolution MS image;hybrid image sharpening technique;multispectral image;high-resolution panchromatic image","","","","13","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"A Multichannel SAR-GMTI Method Based on Multi-Polarization SAR Image Fusion","L. Song; A. Liu; Z. Huang","Nanjing Research Institute of Electronics Technology, Nanjing, China; Nanjing Research Institute of Electronics Technology, Nanjing, China; Nanjing Research Institute of Electronics Technology, Nanjing, China","2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","5 Apr 2021","2021","5","","2678","2684","Displaced phase center array (DPCA) is a classical multichannel synthetic aperture radar(SAR)-based ground moving target indication (SAR-GMTI) method. It has been widely used in the real SAR system because of its simple operation and low computational complexity. However, in the complex clutter environment, there are often many false alarms in the DPCA results, such as the urban area. In this paper, a new GMTI method based on the multi-polarization SAR image information is proposed. Since the amplitude of the urban area in the cross-polarization SAR image is weaker than that in the copolarization image, the urban area in the copolarization image can be replaced with the cross-polarization image, then the DPCA processing is performed for the fusional image. Compared with traditional DPCA technique, this method can reduce the false alarms in the complex environment, such as the urban area. Finally, the effectiveness of the proposed method is verified by the real multichannel quadrature-polarization SAR data.","2689-6621","978-1-7281-8028-1","10.1109/IAEAC50856.2021.9390769","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9390769","synthetic aperture radar(SAR);displaced phase center array(DPCA);ground moving target indication(GMTI);polarization","Phased arrays;Urban areas;Buildings;Apertures;Radar polarimetry;Clutter;Synthetic aperture radar","airborne radar;image fusion;radar antennas;radar imaging;remote sensing by radar;synthetic aperture radar","complex clutter environment;false alarms;DPCA results;urban area;multipolarization SAR image information;cross-polarization SAR image;copolarization image;cross-polarization image;DPCA processing;fusional image;traditional DPCA technique;complex environment;multichannel quadrature-polarization SAR data;multichannel SAR-GMTI method;target indication method;SAR system;low computational complexity","","1","","6","IEEE","5 Apr 2021","","","IEEE","IEEE Conferences"
"Object-oriented change detection for multi-source images using multi-feature fusion","B. Zhang; J. Lu; H. Guo; J. Xu; C. Zhao","Department of Photogrammtery, Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China; Department of Photogrammtery, Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China; Department of Photogrammtery, Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China; Department of Photogrammtery, Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China; Department of Photogrammtery, Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China","2016 Third International Conference on Artificial Intelligence and Pattern Recognition (AIPR)","13 Oct 2016","2016","","","1","6","With the development of remote sensing technology, the source of data is getting more abundant and the resolution is becoming higher. Consequently, conventional change detection method can't meet the application requirements any more. In this paper, an object-oriented change detection method for multisource remote sensing images using multi-feature fusion was proposed to solve this problem. On the basis of objects acquisition and multiple features extraction, SVM was adopted for its outstanding character in high dimensional data classification. Through the efficient combination of binary classification algorithm based on SVM and object-oriented change detection, the accuracy and reliability of change detection for multi-source images were increased. With manual visual judgment, a computing method for ground objects oriented evaluation index was designed. The experiments were conducted among multi-source and multi-temporal images, and the change detection accuracy of different ground objects were counted, which verified the effectiveness of this method.","","978-1-4673-9187-0","10.1109/ICAIPR.2016.7585215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7585215","change detection;multi-source images;objectoriented;muti-feature fusion;SVM","Feature extraction;Remote sensing;Support vector machines;Image segmentation;Spatial resolution;Earth;Data mining","computer vision;data handling;feature extraction;image classification;image fusion;object detection;remote sensing;support vector machines","multifeature fusion;object-oriented change detection method;multisource remote sensing images;feature extraction;object acquisition;SVM;high-dimensional data classification;manual visual judgment;ground object oriented evaluation index;multitemporal images","","","","18","IEEE","13 Oct 2016","","","IEEE","IEEE Conferences"
"A Pixel Level Scaled Fusion Model to Provide High Spatial-Spectral Resolution for Satellite Images Using LSTM Networks","C. A. Theran; M. A. Álvarez; E. Arzuaga; H. Sierra","Laboratory for Applied Remote Sensing, Imaging and Photonics, University of Puerto Rico Mayaguez; Laboratory for Applied Remote Sensing, Imaging and Photonics, University of Puerto Rico Mayaguez; Laboratory for Applied Remote Sensing, Imaging and Photonics, University of Puerto Rico Mayaguez; Laboratory for Applied Remote Sensing, Imaging and Photonics, University of Puerto Rico Mayaguez","2019 10th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","5 Dec 2019","2019","","","1","5","Pixel-level fusion of satellite images coming from multiple sensors allows for an improvement in the quality of the acquired data both spatially and spectrally. In particular, multispectral and hyperspectral images have been fused to generate images with a high spatial and spectral resolution. In literature, there are several approaches for this task, nonetheless, those techniques still present a loss of relevant spatial information during the fusion process. This work presents a multi scale deep learning model to fuse multispectral and hyperspectral data, each with high-spatial-and-low-spectral resolution (HSaLS) and low-spatial-and-high-spectral resolution (LSaHS) respectively. As a result of the fusion scheme, a high-spatial-and-spectral resolution image (HSaHS) can be obtained. In order of accomplishing this result, we have developed a new scalable high spatial resolution process in which the model learns how to transition from low spatial resolution to an intermediate spatial resolution level and finally to the high spatial-spectral resolution image. This step-by-step process reduces significantly the loss of spatial information. The results of our approach show better performance in terms of both the structural similarity index and the signal to noise ratio.","2158-6276","978-1-7281-5294-3","10.1109/WHISPERS.2019.8921269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8921269","Data Fusion;Long Short Term Memory;Pixel level;Super resolution;hyperspectral image;multispectral image","Spatial resolution;Hyperspectral imaging;Image sensors;Sensors;Signal resolution","image classification;image fusion;image resolution;learning (artificial intelligence);object detection","pixel level scaled fusion model;satellite images;pixel-level fusion;multispectral images;hyperspectral images;spatial information;fusion process;multiscale deep learning model;multispectral data;hyperspectral data;high-spatial-and-low-spectral resolution;fusion scheme;high-spatial- resolution image;scalable high spatial resolution process;low spatial resolution;intermediate spatial resolution level;spatial-spectral resolution image;high spatial-spectral resolution;signal to noise ratio;structural similarity index","","6","","12","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"FGF-GAN: A Lightweight Generative Adversarial Network for Pansharpening via Fast Guided Filter","Z. Zhao; J. Zhan; S. Xu; K. Sun; L. Huang; J. Liu; C. Zhang","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China","2021 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2021","2021","","","1","6","Pansharpening is a widely used image enhancement technique for remote sensing. Its principle is to fuse the input high-resolution single-channel panchromatic (PAN) image and low-resolution multi-spectral image and to obtain a high-resolution multi-spectral (HRMS) image. The existing deep learning pansharpening method has two shortcomings. First, features of two input images need to be concatenated along the channel dimension to reconstruct the HRMS image, which makes the importance of PAN images not prominent, and also leads to high computational cost. Second, the implicit information of features is difficult to extract through the manually designed loss function. To this end, we propose a generative adversarial network via the fast guided filter (FGF) for pansharpening. In generator, traditional channel concatenation is replaced by FGF to better retain the spatial information while reducing the number of parameters. Meanwhile, the fusion objects can be highlighted by the spatial attention module. In addition, the latent information of features can be preserved effectively through adversarial training. Numerous experiments illustrate that our network generates high-quality HRMS images that can surpass existing methods, and with fewer parameters.","1945-788X","978-1-6654-3864-3","10.1109/ICME51207.2021.9428272","National Key Research and Development Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428272","Pansharpening;Fast guided filter;Generative adversarial network;Image fusion","Training;Earth;Fuses;Pansharpening;Feature extraction;Information filters;Generative adversarial networks","deep learning (artificial intelligence);geophysical image processing;image enhancement;image fusion;image resolution;remote sensing","lightweight generative adversarial network;fast guided filter;image enhancement technique;remote sensing;input high-resolution single-channel panchromatic image;low-resolution multispectral image;high-resolution multispectral image;channel dimension;PAN images;high computational cost;channel concatenation;adversarial training;high-quality HRMS images;FGF-GAN;deep learning pansharpening method","","3","","20","IEEE","9 Jun 2021","","","IEEE","IEEE Conferences"
"Unsupervised Multiple-Change Detection in VHR Multisensor Images Via Deep-Learning Based Adaptation","S. Saha; F. Bovolo; L. Bruzzone","Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; University of Trento, Trento, Italy","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","5033","5036","Change Detection (CD) using multitemporal satellite images is an important application of remote sensing. In this work, we propose a Convolutional-Neural-Network (CNN) based unsupervised multiple-change detection approach that simultaneously accounts for the high spatial correlation among pixels in Very High spatial Resolution (VHR) images and the differences in multisensor images. We accomplish this by learning in an unsupervised way a transcoding between multisensor multitemporal data by exploiting a cycle-consistent Generative Adversarial Network (CycleGAN) that consists of two generator CNN networks. After unsupervised training, one generator of the CycleGAN is used to mitigate multisensor differences, while the other is used as a feature extractor that enables the computation of multitemporal deep features. These features are then compared pixelwise to generate a change detection map. Changed pixels are then further analyzed based on multitemporal deep features for identifying different kind of changes (multiple-change detection). Results obtained on multisensor multitemporal dataset consisting of Quickbird and Pleiades images confirm the effectiveness of the proposed approach.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900173","Change detection;Very High Resolution;Multisensor images;Multitemporal images;Generative Adversarial Network;Deep Change Vector Analysis;Deep learning","Feature extraction;Transcoding;Spatial resolution;Generators;Training data;Semantics","feature extraction;geophysical image processing;image classification;image fusion;image resolution;neural nets;remote sensing;unsupervised learning","change detection map;changed pixels;multitemporal deep features;multisensor multitemporal dataset;VHR multisensor images;deep-learning based adaptation;multitemporal satellite images;remote sensing;Convolutional-Neural-Network;unsupervised multiple-change detection approach;high spatial correlation;High spatial Resolution images;multisensor multitemporal data;cycle-consistent Generative Adversarial Network;CycleGAN;generator CNN networks;unsupervised training;multisensor differences;feature extractor","","25","","10","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"A Two-Branch Network with Semi-Supervised Learning for Hyperspectral Classification","S. Fang; D. Quan; S. Wang; L. Zhang; L. Zhou","School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","3860","3863","In order to promote progress on fusion and analysis methodologies for multi-source remote sensing data, The Image Analysis and Data Fusion Technical Committee organized the 2018 IEEE GRSS Data Fusion contest. In this contest, we proposed a two-branch convolution network for hyperspectral image classification with a data re-sampling strategy and semi-supervised learning to address three existing problems, i.e. multi-scale feature learning, data imbalance, and small size of the dataset. The contest showed that our proposal achieved the best performance on two metrics: the overall accuracy of 77.39% and a kappa coefficient of 0.76 on the hyperspectral images provided by 2018 IEEE GRSS Data Fusion Contest.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517816","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517816","Hyperspectral image;image classification;semi-supervised learning;deep learning","Training;Hyperspectral imaging;Semisupervised learning;Training data;Feature extraction;Convolution;Image classification","hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);remote sensing","semisupervised learning;hyperspectral classification;multisource remote sensing data;two-branch convolution network;hyperspectral image classification;data re-sampling strategy;data imbalance;multiscale feature learning","","16","","11","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Hypersharpening by a Multiplicative Joint-Criterion NMF Method Addressing Spectral Variability","M. S. Karoui; F. Z. Benhalouche; S. E. Brezini; Y. Deville; Y. K. Benkouider","LSI, Université des Sciences et de la Technologie d'Oran Mohamed Boudiaf, Oran, Algérie; LSI, Université des Sciences et de la Technologie d'Oran Mohamed Boudiaf, Oran, Algérie; LSI, Université des Sciences et de la Technologie d'Oran Mohamed Boudiaf, Oran, Algérie; IRAP, Université de Toulouse, UPS-OMP, CNRS, CNES, Toulouse, France; LSI, Université des Sciences et de la Technologie d'Oran Mohamed Boudiaf, Oran, Algérie","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4472","4475","In this work, a hypersharpening approach, creating fused hyperspectral remote sensing images with high spatial and spectral resolutions, is introduced. This approach, linked to linear spectral unmixing (LSU) methods and based on a multiplicative nonnegative matrix factorization (NMF) technique, extends the Joint-Criterion NMF (JCNMF) algorithm, by addressing the spectral variability phenomenon. This method is designed for combining low spatial resolution hyperspectral and high spatial resolution multispectral data. It optimizes the considered criterion that deals with the spectral variability phenomenon by using a specific structure of involved matrices. The introduced algorithm, which uses multiplicative and iterative update rules, is applied to realistic synthetic data, and its effectiveness, in the spatial and spectral domains, is evaluated by considering commonly used assessment protocol and performance criteria. The obtained results prove that the introduced algorithm yields fused hyperspectral data with good spectral and spatial fidelities. These results also illustrate that the proposed algorithm significantly outperforms two tested literature ones that do not take the spectral variability phenomenon into account.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553972","Hyper/multispectral imaging;hypersharpening;linear spectral unmixing;nonnegative matrix factorization;spectral variability;spatial degradation model","Degradation;Protocols;Iterative algorithms;Spatial resolution;Spectral analysis;Hyperspectral imaging","geophysical image processing;image classification;image fusion;image processing;image resolution;matrix algebra;matrix decomposition;remote sensing","considered criterion;spectral variability phenomenon;multiplicative update rules;iterative update rules;spatial domains;spectral domains;introduced algorithm yields;hyperspectral data;good spectral fidelities;spatial fidelities;multiplicative Joint-Criterion NMF method addressing spectral;hypersharpening approach;fused hyperspectral remote sensing images;high spatial resolutions;spectral resolutions;spectral unmixing methods;multiplicative nonnegative matrix factorization technique;Joint-Criterion NMF algorithm;low spatial resolution hyperspectral;high spatial resolution multispectral data","","2","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"SOFNet: SAR-Optical Fusion Network for Land Cover Classification","D. Zhang; M. Gade; J. Zhang","Universität Hamburg, Fachbereich Informatik, Hamburg, Germany; Universität Hamburg, Institut für Meereskunde, Hamburg, Germany; Universität Hamburg, Fachbereich Informatik, Hamburg, Germany","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2409","2412","The objective of this research is to realize automatic land cover classification from synthetic aperture radar (SAR) and multispectral remote sensing imagery. We develop a SAR-optical fusion network (SOFNet) with the symmetric cross entropy (SCE) loss to utilize both the SAR and optical information in a novel deep neural network. The proposed framework has been trained on the public SEN12MS dataset and tested on the 2020 IEEE-GRSS Data Fusion Contest (DFC2020) dataset. Experimental results show that our approach takes full advantage of multimodal information and outperforms the state-of-the-art convolutional architectures.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554070","land cover classification;SAR;multispectral;multimodal fusion;deep learning","Optical losses;Ultraviolet sources;Semantics;Data integration;Optical fiber networks;Optical imaging;Adaptive optics","deep learning (artificial intelligence);entropy;geophysical image processing;image classification;image fusion;land cover;radar imaging;remote sensing by radar;synthetic aperture radar","automatic land cover classification;synthetic aperture radar;multispectral remote sensing imagery;SAR-optical fusion network;symmetric cross entropy loss;optical information;deep neural network;public SEN12MS dataset;2020 IEEE-GRSS Data Fusion Contest;SOFNet;SCE;DFC2020;multimodal information","","2","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Road Extraction and Road Width Estimation Via Fusion of Aerial Optical Imagery, Geospatial Data, and Street-Level Images","A. Grillo; V. A. Krylov; G. Moser; S. B. Serpico","University of Genoa, Italy; Dublin City University, Ireland; University of Genoa, Italy; University of Genoa, Italy","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2413","2416","Road information extraction based purely on remote sensing can be affected by occlusions of the road surface caused by trees, shadows, and buildings. We propose a multimodal fusion method that addresses road extraction and road width estimation by combining aerial imagery, monocular images taken at ground level (street-level), and geospatial data (Open-StreetMap). The method combines semantic segmentation through convolutional neural networks, Voronoi diagram processing, and graph matching.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554540","Road extraction;Road width estimation;Optical imagery;Street-level imagery;OpenStreetMap","Image segmentation;Roads;Semantics;Estimation;Geospatial analysis;Topology;Data mining","convolutional neural nets;feature extraction;geographic information systems;geophysical image processing;image fusion;image matching;image segmentation;remote sensing;roads","geospatial data;street-level images;road information extraction;remote sensing;road surface;multimodal fusion method;road width estimation;monocular images;ground level;aerial optical imagery;semantic segmentation;convolutional neural network;Voronoi diagram processing;graph matching","","2","","8","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Semi-Supervised Land-Cover Mapping Based on Multimodal Fusion and Pseudo-Label","Y. Gao; X. Ding; G. Yang","School of Electronic Information, Wuhan University, Wuhan, P.R. China; School of Electronic Information, Wuhan University, Wuhan, P.R. China; School of Electronic Information, Wuhan University, Wuhan, P.R. China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","4599","4602","Land-cover mapping is of great significance for remote sensing and earth observation. However, due to the high cost of label acquisition, how to use limited labeled samples and multimodal data to achieve large-scale and high-precision land-cover mapping is still a great challenge. In this paper, a multimodal fusion and pseudo-label based method is proposed for semi-supervised land-cover mapping (SLM). For the problem of domain incompatibility, we use strong data enhancement and multimodal fusion module to strengthen the generalization performance of the method from data level and model level respectively. For a large amount of unlabeled data, we combine the pseudo-label self-training technology and propose Fusion-Finetune-Fusion training strategy to achieve large-scale, high-precision land-cover mapping under semi-supervised conditions. In the track SLM of the 2022 Data Fusion Contest (DFC22-SLM), the proposed method achieves a mean intersection over union (mIoU) of 0.4962 in phase 2, ranking fourth place.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884536","National Natural Science Foundation of China(grant numbers:61871298,42071322); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884536","Semi-supervised;Land-cover mapping;Multimodal fusion","Training;Earth;Costs;Data integration;Benchmark testing;Data models;Robustness","geophysical image processing;geophysical signal processing;image classification;image fusion;learning (artificial intelligence);remote sensing;sensor fusion;terrain mapping","2022 Data Fusion Contest;semisupervised land-cover mapping;remote sensing;earth observation;label acquisition;labeled samples;multimodal data;high-precision land-cover mapping;pseudolabel based method;strong data enhancement;multimodal fusion module;data level;model level;Fusion-Finetune-Fusion training strategy;semisupervised conditions","","1","","6","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Performance analysis of similarity measures between multichannel optical and multipolarization radar images","M. L. Uss; V. V. Lukin; B. Vozel; K. Chehdi","Department of Transmitters, National Aerospace University, Kharkov, Ukraine; Department of Transmitters, National Aerospace University, Kharkov, Ukraine; IETR UMR CNRS 6164, University of Rennes 1, Enssat Lannion, France; IETR UMR CNRS 6164, University of Rennes 1, Enssat Lannion, France","2017 IEEE Microwaves, Radar and Remote Sensing Symposium (MRRS)","19 Oct 2017","2017","","","107","110","This paper investigates the problem of measuring similarity between multimodal Remote Sensing (RS) images using both area-based and feature-based structural similarity measures (SMs). For many RS platforms, optical image is multichannel and radar image is multipolarization. Thus, vector-to-vector SMs could be applied to optical-to-radar image pairs in contrast to scalar-to-scalar SMs considered in the literature. Using two real Landsat8 - SIR-C image pairs, we demonstrate that vector variants of state-of-the-art SMs outperform their scalar counterparts. This is especially evident for Normalized Correlation Coefficient (NCC), which in vector case performs as good as or better than advanced structural SMs.","","978-1-5090-5391-9","10.1109/MRRS.2017.8075039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075039","multimodal registration;similarity measure;multichannel;multipolarization images;optical image;radar image","Laser radar;Optical imaging;Optical sensors;Radar imaging;Biomedical optical imaging;Optical polarization","correlation methods;geophysical image processing;image fusion;image resolution;radar imaging;remote sensing by radar;vectors","RS platforms;optical image;multipolarization;vector-to-vector SMs;optical-to-radar image pairs;scalar-to-scalar SMs;Landsat8 - SIR-C image pairs;vector variants;advanced structural SMs;performance analysis;multichannel optical;multimodal Remote Sensing images;feature-based structural similarity measures;area-based structural similarity measures","","1","","13","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"Fusing Sentinel-2 Satellite Images and Aerial RGB Images","J. Sigurdsson; M. O. Ulfarsson; J. R. Sveinsson","Dept. Electrical Eng., University of Iceland, Reykjavik, ICELAND; Dept. Electrical Eng., University of Iceland, Reykjavik, ICELAND; Dept. Electrical Eng., University of Iceland, Reykjavik, ICELAND","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4444","4447","Sentinel-2 (S2) is a constellation of two satellites that frequently acquire optical imagery over land and coastal waters. The S2 sensors have three spatial resolutions: 10, 20, and 60 m. Many remote sensing applications require the spatial resolution to be at the highest resolution, i.e., 10 m for S2. To address this demand, researchers have proposed various methods that exploit the spectral and spatial correlation in multispectral data to sharpen the S2 bands to 10 m. In this paper, we fuse S2 data with high-resolution aerial RGB images. A method called S2Sharp is modified to include the red, green, and blue bands of the aerial image and sharpen S2 data to the resolution of the RGB image. The method, termed S2PF, is evaluated using an S2 image and aerial photographs of Reykjavik, Iceland.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554406","Data fusion;image sharpening;Sentinel-2 (S2) constellation;superresolution;RGB images","Integrated optics;Satellites;Correlation;Fuses;Sea measurements;Optical imaging;Optical sensors","geophysical image processing;image fusion;image resolution;image sensors;remote sensing","Sentinel-2 satellite images;optical imagery;coastal waters;spatial resolution;remote sensing applications;highest resolution;spectral correlation;spatial correlation;multispectral data;high-resolution aerial RGB images;S2Sharp;red bands;green bands;blue bands;Reykjavik;Iceland","","1","","16","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Spectral and Spatial Residual Attention Network for Joint Hyperspectral and Lidar Data Classification","J. Wang; J. Zhou; X. Liu; F. Jahan","Commonwealth Scientific and Industrial Research Organisation, Canberra, Australia; School of Information and Communication Technology, Griffith University, Nathan, Australia; School of Information Technology and Electrical Engineering, University of Queensland, Brisbane, Australia; Department of Computer Science and Engineering, University of Chittagong, Chittagong, Bangladesh","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","278","281","Hyperspectral (HS) imaging and light detection and ranging (LiDAR) are widely used in remote sensing to acquire data from a same area of earth surface. HS image and LiDAR data contain complementary information of the target objects. Jointly using these two data modalities has great potential in land cover classification. In recent years, deep learning based fusion methods demonstrated promising performance on this task. However, how to better model the relationship of heterogeneous features from HS and LiDAR and their importance for the classification remains a challenging task. In this paper, we propose a spectral and spatial residual attention network for HS and LiDAR fusion and classification. A spectral residual attention module and a spatial residual attention module are designed in the network for better feature learning and fusion. Experiments on widely adopted Houston dataset demonstrate the superiority of the proposed method.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554312","Hyperspectral;LiDAR;data fusion;residual attention;convolutional neural network","Earth;Deep learning;Laser radar;Neural networks;Data integration;Imaging;Feature extraction","deep learning (artificial intelligence);geophysical image processing;hyperspectral imaging;image fusion;optical radar;remote sensing by laser beam","spectral residual attention network;spatial residual attention network;joint hyperspectral;lidar data classification;light detection and ranging;remote sensing;HS image;data modalities;land cover classification;spectral attention network;spectral residual attention module;spatial residual attention module;hyperspectral imaging;earth surface;target objects;deep learning based fusion methods;heterogeneous features","","1","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Dual Unet: A Novel Siamese Network for Change Detection with Cascade Differential Fusion","K. Jiang; J. Liu; F. Liu; W. Zhang; Y. Liu; J. Shi","Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Northwestern Polytechnical University, Xian, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1428","1431","Change detection (CD) of remote sensing images is to detect the change region by analyzing the difference between two bitemporal images. It is extensively used in land resource planning, natural hazards monitoring and other fields. In our study, we propose a novel Siamese neural network for change detection task, namely Dual-UNet. In contrast to previous individually encoded the bitemporal images, we design an encoder differential-attention module to focus on the spatial difference relationships of pixels. In order to improve the generalization of networks, it computes the attention weights between any pixels between bitemporal images and uses them to engender more discriminating features. In order to improve the feature fusion and avoid gradient vanishing, multi-scale weighted variance map fusion strategy is proposed in the decoding stage. Experiments demonstrate that the proposed approach consistently outperforms the most advanced methods on popular seasonal change detection datasets.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883140","change detection;multi-scale;fully convolutional Siamese network;self-attention;variance fusion","Image color analysis;Semantics;Neural networks;Feature extraction;Decoding;Planning;Task analysis","feature extraction;geophysical image processing;image classification;image fusion;image representation;image resolution;learning (artificial intelligence);neural nets;remote sensing","change region;bitemporal images;land resource planning;natural hazards monitoring;novel Siamese neural network;change detection task;encoder differential-attention module;spatial difference relationships;attention weights;feature fusion;multiscale weighted variance map fusion strategy;popular seasonal change detection datasets;Dual Unet;novel siamese network;cascade differential fusion;remote sensing images","","","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Hypersharpening Based on Extended Coupled Nonnegative Matrix Factorization Addressing Spectral Variability","F. Z. Benhalouche; M. S. Karoui; S. E. Brezini; Y. Deville; Y. K. Benkouider","LSI, Université des Sciences et de la Technologie d'Oran Mohamed Boudiaf, Oran, Algérie; LSI, Université des Sciences et de la Technologie d'Oran Mohamed Boudiaf, Oran, Algérie; LSI, Université des Sciences et de la Technologie d'Oran Mohamed Boudiaf, Oran, Algérie; IRAP, Université de Toulouse, UPS-OMP, CNRS, CNES, Toulouse, France; LSI, Université des Sciences et de la Technologie d'Oran Mohamed Boudiaf, Oran, Algérie","2022 IEEE Mediterranean and Middle-East Geoscience and Remote Sensing Symposium (M2GARSS)","9 Aug 2022","2022","","","70","73","Hypersharpening results in creating unobservable hyperspectral remote sensing data with high spectral and spatial resolutions, by combining a hyperspectral image, characterized by a high spectral resolution, with a high spatial resolution multispectral one. In this paper, a new hypersharpening method, based on the linear spectral unmixing concept, is proposed. This one, based on a multiplicative Nonnegative Matrix Factorization (NMF) technique, extends the Coupled NMF (CNMF) algorithm by addressing the spectral variability phenomenon. The introduced algorithm that uses multiplicative and iterative update rules minimizes a cost function that takes into account the specific structure of involved matrices, in order to deal with the spectral variability phenomenon. The proposed method is applied to realistic synthetic data, and its effectiveness is evaluated with established performance criteria. The obtained results show that the developed extended CNMF algorithm yields sharpened hyperspectral data with fine spectral and spatial fidelities. Also, these results demonstrate that the proposed technique outperforms the tested hypersharpening literature approaches.","","978-1-6654-2795-1","10.1109/M2GARSS52314.2022.9840261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9840261","Hyper/multispectral imaging;data fusion;hypersharpening;spectral variability;linear spectral unmixing;nonnegative matrix factorization","Cost function;Iterative algorithms;Spatial resolution;Hyperspectral imaging","geophysical image processing;hyperspectral imaging;image fusion;image resolution;matrix algebra;matrix decomposition;remote sensing;sensor fusion;spectral analysis","high spatial resolution multispectral;hypersharpening method;linear spectral unmixing concept;multiplicative Nonnegative Matrix Factorization technique;Coupled NMF algorithm;spectral variability phenomenon;introduced algorithm;multiplicative update rules minimizes;iterative update rules minimizes;realistic synthetic data;developed extended CNMF algorithm yields;hyperspectral data;fine spectral fidelities;spatial fidelities;tested hypersharpening literature approaches;extended Coupled Nonnegative Matrix Factorization addressing spectral variability;unobservable hyperspectral remote sensing data;high spectral resolutions;spatial resolutions;hyperspectral image;high spectral resolution","","","","14","IEEE","9 Aug 2022","","","IEEE","IEEE Conferences"
"Megh Sansuchak: A Cloud Mask Algorithm for High Resolution Panchromatic Satellite Imagery","S. K. Joshi; I. Baranwal; V. Malhotra; S. Prakash; B. Kartikeyan","Space Applications Centre, Indian Space Research Organisation, Ahmedabad, India; Space Applications Centre, Indian Space Research Organisation, Ahmedabad, India; Space Applications Centre, Indian Space Research Organisation, Ahmedabad, India; Space Applications Centre, Indian Space Research Organisation, Ahmedabad, India; Space Applications Centre, Indian Space Research Organisation, Ahmedabad, India","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4103","4106","In the era of Remote Sensing application where generation of meaningful hypothesis plays an important role in decision making of certain situation, getting high resolution data with precise features is a necessary and important task. In that context, presence of clouds in satellite imagery sometimes misleads to high reflectance features. This problem becomes more challenging when working with panchromatic (gray band) images as the signature of cloud and high reflectance features like dessert and salt land are somewhere similar in terms of intensity. To address this issue, a cloud mask generation algorithm is proposed in this paper. This algorithm consists of fully connected convolutional neural network (CNN) based on U-net architecture. This algorithm which is named as Megh-Sansuchak requires no specific preprocessing. Experimental results show a significantly accurate cloud mask on a long strip of Cartosat-1 Panchromatic Imagery. Performance matrices for Megh-Sansuchak are estimated with respect to different loss functions.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553282","Cloud Mask;High Resolution;Panchromatic;Semantic Segmentation;superpixel","Reflectivity;Strips;Image segmentation;Visualization;Image resolution;Satellites;Snow","geophysical image processing;image fusion;image resolution;neural nets;remote sensing","Megh Sansuchak;cloud mask algorithm;high resolution Panchromatic satellite Imagery;Remote Sensing application;decision making;high reflectance features;panchromatic images;salt land;cloud mask generation algorithm;fully connected convolutional neural network;Cartosat-1 Panchromatic Imagery","","","","13","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A scale-aware pansharpening method with rolling guidance filter","Y. Gao; X. Li; A. Gao; L. Li; S. Yue","School of Electronics and Information, Northwestern Polytechnical University, P. R. China; School of Electronics and Information, Northwestern Polytechnical University, P. R. China; School of Electronics and Information, Northwestern Polytechnical University, P. R. China; School of Electronics and Information, Northwestern Polytechnical University, P. R. China; School of Computer Science, University of Lincoln, Lincoln, UK","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","5121","5124","Pansharpening technology has been an important tool in remote sensing applications. It aims at increasing the spatial resolution of multispectral (MS) image with the aid of panchromatic (PAN) image. A key point of pansharpening is spatial detail extraction and injection. Since MS and PAN images contain objects in different sizes and structures of various scales, scale-sensitive detail extraction is desired. In this paper, we present a scale-aware pansharpening method which uses rolling guidance filter to separate structure from details and injects the details through Gram-Schmidt transformation. The experimental results show that our proposed method can obtain high-quality sharpened results and outperforms some existing methods.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128155","Rolling guidance filter;scale-aware;pansharpening","Indexes;Erbium","geophysical image processing;geophysical signal processing;geophysical techniques;image fusion;image resolution;remote sensing","guidance filter;scale-sensitive detail extraction;spatial detail extraction;panchromatic image;multispectral image;spatial resolution;remote sensing applications;pansharpening technology;scale-aware pansharpening method","","","","14","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Cross Residual Fusion for Pansharpening","M. Iftene; M. E. Amin Larabi; M. I. Tchenar; K. Bakhti","Agence Spatiale Algérienne, Centre des Techniques Spatiales, Algeria; Agence Spatiale Algérienne, Centre des Techniques Spatiales, Algeria; The State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, China; Agence Spatiale Algérienne, Centre des Techniques Spatiales, Algeria","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2644","2647","In this work, a deep learning approach has been developed to carry out optical remote sensing pansharpening by the fusion of high spectral and spatial information from two different sources. In the proposed approach, the combination of multimodal information is achieved at multiple levels. The cross fusion deep network (CNet) is designed to directly integrate information from training dataset; this is accomplished by using trainable cross connections between the Multispectral (MS) and the Panchromatic (PAN) images processing branches. To further highlight the benefits of using multiples cross fusion levels for pansharpening, comparison with baselines networks was carried out in this work using three fusion strategies: early, late, and the newly proposed cross fusion. The proposed fusion strategy was evaluated on images from Quickbird sensor and achieved good performance.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554603","Pansharpening;multispectral;panchromatic;cross fusion;Quickbird sensor","Training;Deep learning;Satellites;Pansharpening;Feature extraction;Optical imaging;Optical sensors","deep learning (artificial intelligence);image fusion;remote sensing","cross residual fusion;deep learning;optical remote sensing pansharpening;high spectral information;spatial information;multimodal information;cross fusion deep network;panchromatic images;multispectral images;CNet;Quickbird sensor","","","","17","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"An Empirical Review on Image Dehazing Techniques for Change Detection of Land Cover","C. M. S. Kumar; R. S. Valarmathi; S. Aswath","Electronics and Communication Engineering, Vel Tech Rangarajan Dr.Sagunthala R&D Institute of Science and Technology, Chennai, India; Electronics and Communication Engineering, Vel Tech Rangarajan Dr.Sagunthala R&D Institute of Science and Technology, Chennai, India; Electronics and Communication Engineering, Vel Tech Rangarajan Dr.Sagunthala R&D Institute of Science and Technology, Chennai, India","2021 Asian Conference on Innovation in Technology (ASIANCON)","4 Oct 2021","2021","","","1","9","The turbulence effects in the weather like smog, haze, and fog severely degrade the visibility and performance of outdoor images by causing texture attenuation and color decay in brightness regions, making it difficult to identify object features in the images captured or scenes. The presence of haze can affect the techniques of preprocessing, segmentation, and classification of outdoor images similar to aerial and Remote Sensing images used in the Land Use/ Land Cover (LU/ LC) change analysis. To remove atmospheric effects, dark channel prior-based and learning-based dehazing techniques have been suggested. Prior-based algorithms have the disadvantage of oversaturating the sky region, resulting in artifacts, and longer execution time. Image dehazing techniques based on image enhancement, image fusion, image reconstruction, image segmentation, and deep learning-based approaches are discussed in this review article. The review also discusses the techniques for obtaining minimum haze removal parameters and assigning appropriate values to dehazing attributes. The effectiveness of various techniques was assessed with metrics that were specific to the datasets. In the end, future research directions for image dehazing are rendered.","","978-1-7281-8402-9","10.1109/ASIANCON51346.2021.9544917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9544917","Turbulence effects;Texture attenuation;Remote Sensing Images;Land Use Land Cover;Image Enhancement;Image Fusion;Image Segmentation;Deep learning;Dehazing attributes","Deep learning;Training;Image segmentation;Technological innovation;Uncertainty;Computational modeling;Real-time systems","deep learning (artificial intelligence);fog;geophysical image processing;image colour analysis;image enhancement;image fusion;image reconstruction;image segmentation;remote sensing","deep learning;haze removal;change detection;turbulence effects;outdoor images;texture attenuation;color decay;aerial images;remote sensing images;atmospheric effects;image enhancement;image fusion;image reconstruction;image segmentation;image dehazing;land use;land cover","","1","","80","IEEE","4 Oct 2021","","","IEEE","IEEE Conferences"
"A Survey of Hyperspectral Image Super-Resolution Technology","M. Zhang; X. Sun; Q. Zhu; G. Zheng","School of Geography and Information Engineering, China University of Geosciences, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; School of Geography and Information Engineering, China University of Geosciences, Wuhan, China; School of Geography and Information Engineering, China University of Geosciences, Wuhan, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4476","4479","Hyperspectral images (HSIs) have very high spectral resolution, which can reflect the characteristics of different materials well. However, compared with RGB image or multispectral image (MSI), the spatial resolution of HSI is much lower, which limits its applications. Therefore, many super-resolution (SR) techniques have been proposed to reconstruct HSI with high spatial resolution image. To the best of our knowledge, there has not, to date, that been a study aimed at expatiating and summarizing the current research situation. Therefore, this is our motivation in this survey. In view of the promising development prospects in this field, this paper systematically reviews the existing SR methods of HSI. Specifically, two major categories are summarized, one is fusion-based methods, and the other is single HSI SR methods. At the end of the paper, several future development directions for HSI SR are given.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554409","National Natural Science Foundation of China(grant numbers:41901306); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554409","Hyperspectral remote sensing;image super-resolution;image fusion;singe HSI SR;deep learning","Image sensors;Superresolution;Sensors;Spatial resolution;Image reconstruction;Hyperspectral imaging","geophysical image processing;hyperspectral imaging;image reconstruction;image resolution","hyperspectral image super-resolution technology;high spectral resolution;multispectral image;super-resolution techniques;high spatial resolution image;current research situation;single HSI SR methods;RGB image;MSI","","5","","31","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A New Pansharpening Method Using Objectness Based Saliency Analysis and Saliency Guided Deep Residual Network","L. Zhang; J. Zhang; X. Lyu; J. Ma","College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China","2019 IEEE International Conference on Image Processing (ICIP)","26 Aug 2019","2019","","","4529","4533","Pansharpening is a fundamental and crucial task in the remote sensing community. For remote sensing images, there is a significant difference in demands for spatial and spectral resolution in different regions. From this perspective, we propose a new pansharpening method using objectness based saliency analysis and saliency guided deep residual network to boost the fusion accuracy. We first develop an objectness based saliency analysis by incorporating texture feature and objectness measurements to estimate saliency values in images and thereby help discriminate different demands for spatial improvement and spectral preservation. Inspired by the impressive performance of deep learning, we subsequently construct a saliency guided deep residual network to implement pansharpening. In addition, in order to produce images with subtler details, we design a new loss function, the normalized mean square error, particularly for the pansharpening task. Experiments support the superiority of our proposal over six competing methods.","2381-8549","978-1-5386-6249-6","10.1109/ICIP.2019.8803477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803477","Image fusion;pansharpening;deep residual network;saliency;normalized mean square error","Microsoft Windows;Proposals;Task analysis;Remote sensing;Spatial resolution;Mean square error methods","geophysical image processing;image resolution;image texture;learning (artificial intelligence);neural nets;remote sensing","pansharpening method;objectness based saliency analysis;remote sensing images;texture feature;objectness measurements;saliency values;saliency guided deep residual network;spectral resolution;spatial resolution","","1","","21","IEEE","26 Aug 2019","","","IEEE","IEEE Conferences"
"Contextual Information Fusion for Small Object Detection","J. Chen; X. Chen; L. Luo; G. Wang","Engineering Research Center of Intelligent Technology for Geo-Exploration, Ministry of Education; Engineering Research Center of Intelligent Technology for Geo-Exploration, Ministry of Education; School of Automation, China University of Geosciences, Wuhan, China; Engineering Research Center of Intelligent Technology for Geo-Exploration, Ministry of Education","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","7971","7975","In view of the difficulty and low accuracy of small object detection for remote sensing images, this paper proposes a small object detection algorithm based on contextual information fusion to solve the problem of real-time detection accuracy of small object. In this paper, we use bottom-up VGG16 network to realize multi-scale feature extraction to deal with the problem of insufficient image feature extraction. To direct at the problem that the feature information of each feature layer is single, the shallow feature layer and the deep feature layer are fused through the feature fusion module, which achieves the purpose that some feature layers have more abundant fusion features in the structure level. Aiming at the problem that the detection objects in remote sensing images are mainly small and medium-sized objects, this paper proposes to use the multivariate information of four different scale feature layers for classification prediction and regression prediction, so as to reduce the complexity of network model. The experimental results show that the proposed small object detection algorithm based on the fusion of four scale deep and shallow contextual information can obtain good accuracy and real-time performance on the NWPU VHR-10 dataset, improve the detection accuracy on the basis of ensuring the real-time detection, and perform well in the small object detection task of remote sensing images.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9550159","National Natural Science Foundation of China; National Natural Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9550159","remote sensing;deep learning;small object detection;SSD;feature fusion","Training;Semantics;Object detection;Feature extraction;Real-time systems;Inference algorithms;Proposals","feature extraction;geophysical image processing;image fusion;object detection;remote sensing","shallow contextual information;object detection task;remote sensing images;contextual information fusion;small object detection;object detection algorithm;real-time detection accuracy;VGG16 network;multiscale feature extraction;insufficient image feature extraction;feature information;shallow feature layer;deep feature layer;feature fusion module;abundant fusion features;medium-sized objects;multivariate information;different scale feature layers","","","","21","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Impact of hybrid pansharpening approaches applied to hyperspectral images","G. Licciardi; M. A. Veganzones; G. Vivone; L. Loncan; J. Chanussot","Grenoble-INP, GIPSA-Iab, Saint Martin d'Heres, France; Institut Polytechnique de Grenoble, Grenoble, RhÃ´ne-Alpes, FR; Universita degli Studi Salerno, Italy; Grenoble-INP, GIPSA-Iab, Saint Martin d'Heres, France; Grenoble-INP, GIPSA-Iab, Saint Martin d'Heres, France","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","23 Oct 2017","2015","","","1","4","Pansharpening techniques can be divided into component substitution (CS) and multi-resolution analysis (MRA) based methods. Generally, the CS methods result in fused images having high spatial quality but the fused images suffer from spectral distortions. On the other hand, images obtained using MRA techniques are not as sharp as CS methods but they are spectrally consistent. Both substitution and filtering approaches are considered adequate when applied to multi-spectral and PAN images, but have many drawbacks when the low-resolution image is a hyperspectral image. Based on these findings, the use of a hybrid approach, combining the better spatial information of CS and the more accurate spectral information of MRA techniques, may result in an improvement in terms of spectral quality, spatial sharpness as well as computational time.","2158-6276","978-1-4673-9015-6","10.1109/WHISPERS.2015.8075402","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075402","Pansharpening;Hyperspectral;multiresolution analysis;component substitution","Spatial resolution;Hyperspectral imaging;Principal component analysis;Indexes;Nonlinear distortion","hyperspectral imaging;image filtering;image fusion;remote sensing","spectral quality;hyperspectral image;pansharpening techniques;CS methods;spectral distortions;MRA techniques;PAN images;component substitution method;image filtering approaches;spectral information;image fusion;multiresolution analysis method","","1","","11","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"A new pansharpening method using multi resolution analysis framework and deep neural networks","A. Azarang; H. Ghassemian","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2017 3rd International Conference on Pattern Recognition and Image Analysis (IPRIA)","20 Jul 2017","2017","","","1","6","Present work describes a promising method in image fusion remote sensing applications. Due to intrinsic properties of deep neural networks (DNN) in image reconstruction, a novel pansharpening method presents based on multi resolution analysis (MRA) framework. First, a low resolution Panchromatic (LR Pan) image is constructed using its high resolution (HR) version. Then, the relationship between LR/HR Pan images are used to reconstruct the HR Multispectral (MS) image utilizing the LR MS. For our work, two datasets are considered and for each of them, the effect of several parameters such as window size, overlapping percentage and number of training samples on spectral distortion are considered. After training DNN, the LR MS image is given to the trained network as input to obtain MS image with better spatial details and finally the fused image obtains using MRA framework. Comparison with state of art methods, the proposed method has better results from objective and visual perspectives.","","978-1-5090-6454-0","10.1109/PRIA.2017.7983017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7983017","Pansharpening;multi resolution analysis;deep neural networks","Training;Image resolution;Distortion;Neural networks;Noise reduction;Feature extraction","geophysical image processing;image coding;image fusion;image reconstruction;image resolution;learning (artificial intelligence);neural nets;remote sensing","pansharpening method;multiresolution analysis framework;deep neural networks;image fusion remote sensing applications;image reconstruction;MRA framework;low-resolution panchromatic image;LR-pan image;high-resolution image;HR multispectral image reconstruction;HR-MS image reconstruction;window size;overlapping percentage;spectral distortion;DNN training;LR MS image;spatial details;objective perspectives;visual perspectives","","28","","26","IEEE","20 Jul 2017","","","IEEE","IEEE Conferences"
"DATFuse: Infrared and Visible Image Fusion via Dual Attention Transformer","W. Tang; F. He; Y. Liu; Y. Duan; T. Si","School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China","IEEE Transactions on Circuits and Systems for Video Technology","","2023","PP","99","1","1","The fusion of infrared and visible images aims to generate a composite image that can simultaneously contain the thermal radiation information of an infrared image and the plentiful texture details of a visible image to detect targets under various weather conditions with a high spatial resolution of scenes. Previous deep fusion models were generally based on convolutional operations, resulting in a limited ability to represent long-range context information. In this paper, we propose a novel end-to-end model for infrared and visible image fusion via a dual attention Transformer termed DATFuse. To accurately examine the significant areas of the source images, a dual attention residual module (DARM) is designed for important feature extraction. To further model long-range dependencies, a Transformer module (TRM) is devised for global complementary information preservation. Moreover, a loss function that consists of three terms, namely, pixel loss, gradient loss, and structural loss, is designed to train the proposed model in an unsupervised manner. This can avoid manually designing complicated activity-level measurement and fusion strategies in traditional image fusion methods. Extensive experiments on public datasets reveal that our DATFuse outperforms other representative state-of-the-art approaches in both qualitative and quantitative assessments. The proposed model is also extended to address other infrared and visible image fusion tasks without fine-tuning, and the promising results demonstrate that it has good generalization ability. The source code is available at https://github.com/tthinking/DATFuse.","1558-2205","","10.1109/TCSVT.2023.3234340","National Key RD Program of China(grant numbers:2019YFC1509604); National Natural Science Foundation of China(grant numbers:62072348); Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies)(grant numbers:2019AEA170); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10006826","Image fusion;Transformer;attention mechanism;infrared image;residual learning","Transformers;Image fusion;Feature extraction;Task analysis;Transmission line measurements;Decoding;Computational modeling","","","","","","","IEEE","5 Jan 2023","","","IEEE","IEEE Early Access Articles"
"DOES multispectral / hyperspectral pansharpening improve the performance of anomaly detection?","Y. Qu; H. Qi; B. Ayhan; C. Kwan; R. Kidd","EECS Department, The University of Tennessee, Knoxville, TN; EECS Department, The University of Tennessee, Knoxville, TN; Applied Research LLC, Rockville, MD; Applied Research LLC, Rockville, MD; Jet Propulsion Lab, Pasadena, CA","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","6130","6133","Pansharpening refers to the fusion of a high spatial resolution panchromatic image with high spectral resolution multispectral or hyperspectral images (MSI or HSI) to yield high resolution data in both spectral and spatial domains. It has been widely adopted as a primary preprocessing step for numerous applications. In this paper, we perform a literature survey of various pansharpening algorithms including the most advanced deep learning approaches for both multispectral and hyperspectral images. We further evaluate the effect of the resolution difference on anomaly detection. Synthetic multispectral and hyperspectral images are generated to evaluate the performance of anomaly detection on high resolution images. Eight state-of-the-art MSI and HSI pansharpening methods are compared in this paper. Experimental results show that, performing anomaly detection on high resolution images improves the detection rate, and at the mean time suppresses the false alarm rate.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128408","Hyperspectral images;multispectral images;pansharpening;anomaly detection;deep learning","Anomaly detection;Hyperspectral imaging;Spatial resolution;Bayes methods;Machine learning","geophysical image processing;hyperspectral imaging;image fusion;image resolution;learning (artificial intelligence);remote sensing","deep learning;multispectral images;multispectral pansharpening;hyperspectral pansharpening;high spatial resolution panchromatic image fusion;high spectral resolution;high resolution images;anomaly detection;hyperspectral images;pansharpening algorithms","","32","","22","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"VHR time-series generation by prediction and fusion of multi-sensor images","Y. T. S. Correa; F. Bovolo; L. Bruzzone","Fondazione Bruno Kessler, Trento, Trentino-Alto Adige, IT; Fondazione Bruno Kessler, Center for Information and Communication Technology, Trento, Italy; Dept. of Information Engineering and Computer Science, University of Trento, Trento, Italy","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","3298","3301","The availability of multitemporal images acquired by several very high geometrical resolution (VHR) optical sensors makes it possible to build VHR image Time-Series (TS) with a temporal resolution better than the one achievable when considering a single sensor. However, such TS include images showing different characteristics from the geometrical, radiometrical and spectral viewpoint. Thus, there is a need of methods for building consistent VHR optical TS when using multispectral Multi-Sensor (MS) images. Here we focus on the spectral domain only, by designing a method to transform one image in an MS-TS into the spectral domain of another image in the same MS-TS, but acquired by a different sensor. To this end, a prediction-based approach relying on Artificial Neural Networks (ANN) is employed. In order to mitigate the impacts of possible changes occurred on the ground, the prediction model estimation is based on unchanged samples only. Experimental results obtained on VHR optical MS images confirm the effectiveness of the proposed approach.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326523","Change Detection;VHR Time-Series;Multi-Sensor fusion;Radiometric Normalization;Prediction","Training;Image resolution;Neurons;Radiometry;Optical sensors;Optical imaging;Artificial neural networks","geophysical image processing;image fusion;image resolution;neural nets;optical sensors;remote sensing;time series","multitemporal images;very high geometrical resolution optical sensors;VHR image optical time-series generation;temporal resolution;multispectral multisensor image fusion;spectral domain;prediction-based approach;artificial neural networks;prediction model estimation;unchanged samples","","4","","6","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"A new classification approach based on source separation and feature extraction","H. Elmannai; M. Loghmari; M. S. Naceur","Ecole Supérieure des Communications de Tunis; Université de Tunis El Manar, Institut Supérieur d'Informatique d'El Manar, Ariana, Tunisie; Laboratoire de Télédétection et Système d'Informations à Référence Spatiale, ENIT, Tunis, Tunisia","2016 International Symposium on Signal, Image, Video and Communications (ISIVC)","12 Apr 2017","2016","","","137","141","Pattern recognition for multispectral data aims to identify land cover thematics for environmental monitoring and disaster risk reduction. Multispectral images contain data acquired from different channels within the frequency spectrum. They represent a mixture of latent signals. This paper represents a pattern recognition contribution for remote sensing. We propose a new classification framework based on nonlinear source separation and linear feature fusion. The first stage performs a nonlinear separation model based on multilayer neuron network. The underlying sources are Gaussians and a misfit function between the approximated source distributions and their prior's will be minimized iteratively. The second stage performs feature extraction and fusion. The linear feature model considers that feature descriptors allow cooperative description for land pattern recognition. Classification tasks are performed by Support Vector Machine. Experimentation results demonstrate that the proposed classification method enhances the recognition accuracy and provides a powerful tool for land identification.","","978-1-5090-3611-0","10.1109/ISIVC.2016.7893976","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7893976","Pattern recognition;Classification;Feature extraction;Nonlinear separation;Support Vector Machine","Feature extraction;Source separation;Pattern recognition;Lakes;Remote sensing;Urban areas;Support vector machines","feature extraction;geophysical image processing;image classification;image fusion;land cover;neural nets;remote sensing;source separation;support vector machines","nonlinear source separation;feature extraction;multispectral data;land cover thematics;environmental monitoring;disaster risk reduction;multispectral images;frequency spectrum;remote sensing;linear feature fusion;multilayer neuron network;approximated source distribution;linear feature model;feature descriptors;cooperative description;land pattern recognition;classification tasks;support vector machine","","1","","18","IEEE","12 Apr 2017","","","IEEE","IEEE Conferences"
"Pan-Sharpening Based On Parallel Pyramid Convolutional Neural Network","S. Fang; X. Wang; J. Zhang; Y. Cao","Key Laboratory of Knowledge Engineering with Big Data, (Hefei University of Technology), Hefei, China; Key Laboratory of Knowledge Engineering with Big Data, (Hefei University of Technology), Hefei, China; Key Laboratory of Knowledge Engineering with Big Data, (Hefei University of Technology), Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, Anhui, China","2020 IEEE International Conference on Image Processing (ICIP)","30 Sep 2020","2020","","","453","457","Existing deep learning-based pan-sharpening methods mainly learn spatial information from a high-resolution (HR) panchromatic (PAN) image for each spectral channel. However, due to the own characteristics of remote sensing image data, the spatial information of PAN image often shows weak correlation with some spectral channel, especially for channels non-overlapped by PAN channel. In this paper, we propose a parallel pyramid network (PPN) for pan-sharpening. First, a three-branch parallel structure is proposed for dealing with PAN image detail, multispectral (MS) images detail and spectral property respectively. Second, pyramid network structure is introduced in two detail branches to solve the problem of weak correlation due to scale difference. Third, the feature level fusion in two detail branches is implemented, which utilizes redundancy between channels to solve detail representation of channels non-overlapped by PAN channel. The qualitative and quantitative experimental results on various data sets demonstrate the superiority of our proposed method over the state-of-the-art methods.","2381-8549","978-1-7281-6395-6","10.1109/ICIP40778.2020.9191153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9191153","parallel pyramid network;pan-sharpening;feature level fusion;spatial information;learning-based","Spatial resolution;Satellites;Feature extraction;Remote sensing;Correlation;Distortion","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;image colour analysis;image fusion;image representation;image resolution;remote sensing","parallel pyramid convolutional neural network;deep learning;spatial information;high-resolution panchromatic image;spectral channel;remote sensing image data;PAN channel;parallel pyramid network;three-branch parallel structure;PAN image detail;multispectral images detail;spectral property;pyramid network structure;detail branches;pan-sharpening;PPN;weak correlation;scale difference;feature level fusion;detail representation","","1","","21","IEEE","30 Sep 2020","","","IEEE","IEEE Conferences"
"Optical SAR Fusion of Sentinel-2 Images for Mapping High Resolution Land Cover","Yuhendra; E. Yulianti; J. Na'am","Informatics Engineering Department, Padang Institute of Technology, Padang-West Sumatera, Indonesia; Informatics Engineering Department, Padang Institute of Technology, Padang-West Sumatera, Indonesia; Informatics Engineering Department, Padang Institute of Technology, Padang-West Sumatera, Indonesia","2018 International Conference on System Science and Engineering (ICSSE)","4 Nov 2018","2018","","","1","4","Sentinel-2 is a very new programme of the European Space Agency (ESA) that is designed for fine spatial resolution global monitoring. Land cover-land use (LCLU) classification tasks can take advantage of the fusion of radar and optical remote sensing data, leading generally to increase mapping accuracy. Here we propose a methodological approach to fuse information from the new European Space Agency Sentinel-1 and Sentinel-2 imagery for accurate land cover mapping of a portion of the South Solok region, West Sumatera. Data pre-processing was carried out using the European Space Agency's Sentinel Application Platform and the SEN2COR toolboxes. The two main objectives of this study are to evaluate the potential use and synergetic effects of ESA Sentinel-1A C-band SAR and Sentinel-2A Optical data for classification and mapping of LCLU. As a result of the research, two main advantages. First, the pre-processing chain supported by sensor-specific toolboxes developed by ESA represents a reliable and fast approach for the preparation of ready-to-process imagery. Second, investigation to derive a methodological framework to integrate Sentinel-1 and Sentinel-2 imagery for land cover mapping by integrating of radar and optical imagery have been set up and tested.","2325-0925","978-1-5386-6285-4","10.1109/ICSSE.2018.8520099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8520099","Sentinel-1;Sentinel-2;SAR;land cover mapping;data fusion;segmentation;South Solok","Remote sensing;Optical imaging;Spatial resolution;Laser radar;Image segmentation;Optical sensors;Earth","geophysical image processing;geophysical techniques;image classification;image fusion;land cover;radar imaging;remote sensing by radar;synthetic aperture radar;terrain mapping","land cover mapping;South Solok region;West Sumatera;optical imagery;ready-to-process imagery;pre-processing chain;Optical data;Sentinel-2A;ESA Sentinel-1A;SEN2COR toolboxes;European Space Agency's Sentinel Application Platform;data pre-processing;Sentinel-2 imagery;European Space Agency Sentinel-1;mapping accuracy;optical remote sensing data;land cover-land use classification tasks;fine spatial resolution global monitoring;mapping high resolution land cover;Optical SAR fusion","","","","6","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"An Improved Method for Pan-Sharpening Based on Pan-GAN","Y. Li; J. Li; X. Du; Y. Huang; J. Lei","School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China","2022 7th International Conference on Image, Vision and Computing (ICIVC)","19 Sep 2022","2022","","","282","286","Pan-sharpening refers to the fusion of pan images and low-resolution multispectral remote sensing images to obtain high-resolution multispectral images. Generative adversarial network (GAN)-based pan-sharpening methods have recently became popular due to the lack of ground-truth data during training. However, GAN-based methods suffer from training instability and convergence difficulties. To deal the issues, we propose a novel GAN-based pan-sharpening method using additional constraint. First, we adopt the geometric consistency constraint to enforce the network to preserve the spatial structure of image. Second, we introduce an attention mechanism in the generator to extract useful information through the features of pan image and multispectral images and pay more attention to meaningful regions. Experimental results show the effectiveness of our method in terms of the quantitative and visual results.","","978-1-6654-6734-6","10.1109/ICIVC55077.2022.9887169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887169","pan-sharpening;geometric consistency;attention mechanism;Generative adversarial network","Training;Visualization;Feature extraction;Generative adversarial networks;Generators;Data mining;Remote sensing","geophysical image processing;image enhancement;image fusion;image resolution;remote sensing","GAN-based methods;training instability;convergence difficulties;novel GAN-based pan-sharpening method;pan image;pan-GAN;low-resolution multispectral remote sensing images;high-resolution multispectral images;generative adversarial network-based pan-sharpening methods;ground-truth data","","","","19","IEEE","19 Sep 2022","","","IEEE","IEEE Conferences"
"A novel pan sharpening method via sparse representation over learned dictionary","S. Ayas; E. T. Görmüş; M. Ekinci","Karadeniz Teknik Universitesi, Trabzon, TR; Karadeniz Teknik Universitesi, Trabzon, TR; Karadeniz Teknik Universitesi, Trabzon, TR","2018 26th Signal Processing and Communications Applications Conference (SIU)","9 Jul 2018","2018","","","1","4","Remote sensing pan sharpening aims to enhance spatial resolution of multispectral image by injecting spatial details of a panchromatic image to multispectral image. In this study, a novel sparse representation based pan sharpening method is proposed to overcome the disadvantages of traditional methods such as color distortion and blurring effect. A data set acquired for each IKONOS and Quickbird satellites are used to evaluate the performance and robustness of the proposed algorithm. The proposed method is compared with four traditional methods using several quality measurement indices with reference image. The experimental results demonstrate that the proposed algorithm is competitive or superior to other conventional methods in terms of visual and quantitative analysis as it preserves spectral information and provides high quality spatial details in the final product image.","","978-1-5386-1501-0","10.1109/SIU.2018.8404354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8404354","dictionary learning;multispectral image;panchromatic image;pan sharpening;sparse representation","Sensors;Spatial resolution;Satellites;Geoscience;Remote sensing;Dictionaries;Image color analysis","artificial satellites;geophysical image processing;image fusion;image reconstruction;image representation;image resolution;learning (artificial intelligence);remote sensing","learned dictionary;spatial resolution;multispectral image;panchromatic image;color distortion;blurring effect;reference image;sparse representation based pan sharpening method;remote sensing pan sharpening;IKONOS satellites;Quickbird satellites;quality measurement indices;visual analysis;quantitative analysis","","","","","IEEE","9 Jul 2018","","","IEEE","IEEE Conferences"
"DSNet:Multi-resolution Dense Encoder and Stack Decoder Network for Aerial Image Segmentation","Y. Chong; C. Nie; Y. Tao; S. Pan","State Key laboratory for information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key laboratory for information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key laboratory for information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key laboratory for information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","2019 Chinese Automation Congress (CAC)","13 Feb 2020","2019","","","2748","2753","Semantic segmentation in high resolution aerial image is faced with a challenge caused by ubiquitous fine-structure objects. Traditional encoder-decoder structure losses some detail information during the process of down-sampling, which is harmful to the location of fine-structure objects. In this work, we present a multi-resolution dense encoder and stack decoder network to deal with this problem. On the one hand, the dense encoder embeds shallow detailed feature into deep semantic feature through proposed information-reserved down-sampling method called CE-Pooling. On the other hand, the stack decoder gradually enhances the detailed feature through iterative attention fusion. Extensive experiments on several benchmark datasets have been conducted, which shows that our method is superior than the state-of-the-art approaches.","2688-0938","978-1-7281-4094-0","10.1109/CAC48633.2019.8996431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8996431","semantic segmentation;encoder-decoder;fine-structure;ISPRS","Decoding;Semantics;Feature extraction;Training;Image segmentation;Task analysis;Remote sensing","decoding;feature extraction;image coding;image fusion;image resolution;image sampling;image segmentation;iterative methods","DSNet:multiresolution;stack decoder;aerial image segmentation;semantic segmentation;high resolution aerial image;ubiquitous fine-structure objects;traditional encoder-decoder structure losses some detail information;multiresolution dense encoder;dense encoder embeds shallow detailed feature;deep semantic feature;proposed information-reserved down-sampling method","","","","20","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Effective Feature Fusion Network in BIFPN for Small Object Detection","J. Chen; H. Mai; L. Luo; X. Chen; K. Wu","Ministry of Education, Engineering Research Center of Intelligent Technology for Geo-Exploration; Ministry of Education, Engineering Research Center of Intelligent Technology for Geo-Exploration; School of Mechanical Engineering and Electronic Information, China University of Geosciences; Ministry of Education, Engineering Research Center of Intelligent Technology for Geo-Exploration; Ministry of Education, Engineering Research Center of Intelligent Technology for Geo-Exploration","2021 IEEE International Conference on Image Processing (ICIP)","23 Aug 2021","2021","","","699","703","In view of the difficulty and low accuracy of small object detection in remote sensing images, this paper proposes a bidirectional cross-scale connection feature fusion network with an information direct connection layer and a shallow information fusion layer. Aiming at the problem that the detection targets in remote sensing images are mainly small and medium-sized targets, we fuse the shallow feature maps with rich spatial information in the bidirectional cross-scale connection feature fusion network instead of directly using the shallow feature maps for regression and classification. While ensuring the model inference speed, the detection accuracy of small objects is improved. At the same time, we use the information direct connection layer to perform feature fusion with the initial information in each iteration of the bidirectional cross-scale connection feature fusion pyramid to prevent the loss of small object information. Experimental results show that the algorithm proposed in this paper can obtain good accuracy and real-time performance on the NWPU VHR-10 dataset.","2381-8549","978-1-6654-4115-5","10.1109/ICIP42928.2021.9506347","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506347","Remote sensing;deep learning;small object detection;feature fusion","Training;Graphics;Fuses;Conferences;Memory management;Object detection;Feature extraction","feature extraction;geophysical image processing;image classification;image fusion;image resolution;iterative methods;object detection;remote sensing","shallow feature maps;rich spatial information;bidirectional cross-scale connection feature fusion network;information direct connection layer;object information;small object detection;remote sensing images;shallow information fusion layer;BIFPN","","11","","18","IEEE","23 Aug 2021","","","IEEE","IEEE Conferences"
"Unsupervised approach for change map generation","H. N. Mahulkar; B. Sonawane","Computer Engineering Department, Shah and Anchor kutcchi College of Engineering, Mumbai, India; Computer Engineering Department, Shah and Anchor kutcchi College of Engineering, Mumbai, India","2016 International Conference on Communication and Signal Processing (ICCSP)","24 Nov 2016","2016","","","0037","0041","Change detection is the process of automatically identifying and analyzing region that have undergone spatial or spectral changes from multi temporal images. Detecting and representing change provides valuable information of the possible transformations a given scene has suffered over time. Change detection is used in several applications (eg. Disaster management, deforestation, urbanization, etc). In the proposed unsupervised method co-registered and radiometrically corrected temporal images are used as input. Using this, absolute valued image and log ratio image is calculated to get difference image. These difference images are fused using Discrete Wavelet Transform (DWT). Then, min-mean normalization is applied to the get filtered data. The normalized data is clustered into two groups using K-means clustering algorithm as changed pixels and unchanged pixels. Experiment result is also calculated using two different ways. In first, fused image data is given to Principal Component Analysis (PCA) and clustering is done using K-means algorithm and in second way Fuzzy c-means clustering algorithm is used to cluster image data.","","978-1-5090-0396-9","10.1109/ICCSP.2016.7754322","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7754322","Unsupervised;change detection;DWT;PCA;k-means;FCM;Remote sensing;Image difference;image ratioing","Discrete wavelet transforms;Principal component analysis;Clustering algorithms;Algorithm design and analysis;Change detection algorithms;Remote sensing","discrete wavelet transforms;image fusion;image representation;object detection;pattern clustering;principal component analysis","change map generation;change detection;multitemporal images;change representation;absolute valued image;log ratio image;difference image;discrete wavelet transform;DWT;image fusion;min-mean normalization;K-means clustering algorithm;principal component analysis;PCA;fuzzy c-means clustering algorithm","","","","19","IEEE","24 Nov 2016","","","IEEE","IEEE Conferences"
"Multiple Instance Choquet Integral with Binary Fuzzy Measures for Remote Sensing Classifier Fusion with Imprecise Labels","X. Du; A. Zare; D. T. Anderson","Naval Architecture and Marine Engineering, University of Michigan, Ann Arbor, MI, USA; Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Electrical Engineering and Computer Science, University of Missouri, Columbia, MO, USA","2019 IEEE Symposium Series on Computational Intelligence (SSCI)","20 Feb 2020","2019","","","1154","1162","Classifier fusion methods integrate complementary information from multiple classifiers or detectors and can aid remote sensing applications such as target detection and hyperspectral image analysis. The Choquet integral (CI), parameterized by fuzzy measures (FMs), has been widely used in the literature as an effective non-linear fusion framework. Standard supervised CI fusion algorithms often require precise ground-truth labels for each training data point, which can be difficult or impossible to obtain for remote sensing data. Previously, we proposed a Multiple Instance Choquet Integral (MICI) classifier fusion approach to address such label uncertainty, yet it can be slow to train due to large search space for FM variables. In this paper, we propose a new efficient learning scheme using binary fuzzy measures (BFMs) with the MICI framework for two-class classifier fusion given ambiguously and imprecisely labeled training data. We present experimental results on both synthetic data and real target detection problems and show that the proposed MICI-BFM algorithm can effectively and efficiently perform classifier fusion given remote sensing data with imprecise labels.","","978-1-7281-2485-8","10.1109/SSCI44817.2019.9002801","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9002801","Choquet integral;fuzzy measure;classifier fusion;hyperspectral;target detection","Global Positioning System;Object detection;Training data;Frequency modulation;Hyperspectral imaging","feature extraction;fuzzy set theory;geophysical image processing;image classification;image fusion;learning (artificial intelligence);object detection;remote sensing","binary fuzzy measures;remote sensing classifier fusion;classifier fusion methods;multiple classifiers;remote sensing applications;hyperspectral image analysis;nonlinear fusion framework;CI fusion algorithms;precise ground-truth labels;training data point;remote sensing data;Multiple Instance Choquet Integral classifier fusion approach;label uncertainty;two-class classifier fusion;target detection problems","","5","","58","IEEE","20 Feb 2020","","","IEEE","IEEE Conferences"
"Remote sensing image scene classification via multi-feature fusion","R. Liu; X. Bian; Y. Sheng","School of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan, China; School of Information Science and Engineenring, Wuhan University of Science and Technology, Wuhan, China","2018 Chinese Control And Decision Conference (CCDC)","9 Jul 2018","2018","","","3495","3500","In this papers, classification of remote sensing image scene is investigated. A scene classification approach based on multi-feature fusion has been proposed. In the proposed approach, three types of features are extracted. Specifically, extended multi-attribute profile (EMAP)-based texture feature, saliency-based shape feature and color ones. The texture features are extracted by EMAP. Furthermore, the Hu invariant moments are extracted from the saliency map, where the saliency map is obtained by frequency-tuned saliency detection. Meanwhile, the color moments are extracted as the color features from the image scenes. As for EMAP-based features, dimension reduction via principal component analysis (PCA) is first performed and combined with other two types of features to form a compact feature representation. Finally, support vector machine (SVM) is employed to classify the remote sensing image scenes. The experiments on the two challenging image scene datasets are performed to show that the proposed method is simple, yet efficient to implement, comparing with the state-of-the-arts.","1948-9447","978-1-5386-1244-6","10.1109/CCDC.2018.8407728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8407728","Scene classification;morphological attribute profile;saliency map;feature extraction","Feature extraction;Image color analysis;Image analysis;Shape;Remote sensing;Saliency detection;Support vector machines","feature extraction;image classification;image colour analysis;image fusion;image representation;image texture;principal component analysis;remote sensing;support vector machines","support vector machine;principal component analysis;features extraction;image scene classification;image scene datasets;extended multiattribute profile;compact feature representation;EMAP-based features;color features;color moments;frequency-tuned saliency detection;saliency map;Hu invariant moments;texture features;saliency-based shape feature;multifeature fusion;remote sensing image scene","","1","","15","IEEE","9 Jul 2018","","","IEEE","IEEE Conferences"
"Inclination from conventional to contemporary image alignment techniques in remote sensing","B. Sirisha; P. C. Sekhar; A. S. C. Sastry; B. Sandhya","MVSREC, Osmania University, Hyderabad, INDIA; Department of ECE, KL University, Vijayawada, INDIA; University College of Engineering, Osmania University, Hyderabad, INDIA; Department of CSE, MVSREC, Hyderabad, INDIA","2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)","31 Oct 2016","2016","","","362","367","Advancement in Remote Sensing Imaging sensors provides massive rise in the accessibility and quality of timely and repetitive multisensor, multimodal, multi band, multiview, multiresolution earth surveillance information on a global scale. Image alignment is a fundamental task in remote sensing. Though there is an extensive range of alignment techniques developed for different applications, the pipeline process involved in Conventional and Contemporary techniques is almost same, but the selection of algorithms involved in each stage of registration varies dynamically depending on the category of image, image acquisition sensors, Light source, viewpoint, time, their distortions, applications and domains. There is no universal method that holds good for the above said variations. This paper mainly aims to develop rule based blind alignment frameworks that can automatically map and select algorithm for the given pair of multisensory data. In the process of framework development a thorough comparative study of Multiple Conventional and Contemporary alignment technique is required. The key idea of the paper is to analyze the comparative study and identify and map the registration algorithm required for the given input image pair and a brief discussion of major alignment technique is given.","","978-9-3805-4421-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724288","Feature detection;feature extraction;image matching;RANSAC;View Synthesis","Feature extraction;Measurement;Detectors;Remote sensing;Image sensors;Correlation","feature extraction;geophysical image processing;image fusion;image matching;image resolution;image sensors;remote sensing","image alignment techniques;remote sensing imaging sensors;multisensor-multimodal-multiband-multiview-multiresolution Earth surveillance information;Image registration;image acquisition sensors;light source;image viewpoint;time factor;image distortions;image applications;image domains;rule based blind alignment frameworks;automatic algorithm selection;automatic algorithm mapping","","","","36","","31 Oct 2016","","","IEEE","IEEE Conferences"
"Intelligent neural computing-based way for multi-sensor imaging radar data fusion","Y. V. Shkvarko; J. A. Lopez; S. R. Santos; G. García-Torales","Department of Electrical Engineering, CINVESTAV-IPN, Guadalajara, Mexico; Department of Electrical Engineering, CINVESTAV-IPN, Guadalajara, Mexico; Department of Electrical and Computer Engineering, University of Guadalajara, Guadalajara, Mexico; Department of Electrical and Computer Engineering, University of Guadalajara, Guadalajara, Mexico","2016 23rd International Conference on Pattern Recognition (ICPR)","24 Apr 2017","2016","","","757","762","We address and compare two new frameworks for neural network (NN) computing-based feature enhanced (FE) fusion of remote sensing (RS) imagery acquired with different coherent radar sensing modalities. Both approaches exploit aggregation of the descriptive experiment design regularization (DEDR) based and the theoretical informatics inspired maximum entropy (ME) regularization paradigms for iterative minimization of the energy function (EF) of the multistate MENN with adaptive adjustments of the NN's synaptic weights and bias inputs. Two distinct ways employed for the construction and minimization of the NNs' EFs specify two corresponding different fusion frameworks addressed as the DEDR-MENN(1) and the DEDR-MENN(2) techniques, respectively. The DEDR-MENN(1) framework is based on the weighted aggregation of the individual fused sensor's objective functions, while the DEDR-MEN(2) performs the FE fusion employing the theoretical informatics inspired unified model-based MENN framework. We compare the computational implementation and performance issues of both addressed methods and feature through computer simulations with the real-world radar imagery the superior operational performances attained with the DEDR-MENN(2) approach.","","978-1-5090-4847-2","10.1109/ICPR.2016.7899726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899726","Maximum Entropy;Multi-Sensor Imaging Radars;Neural Network;Remote Sensing;Sensor Fusion","Artificial neural networks;Iron;Minimization;Radar imaging;Computational modeling;Neurons;Imaging","image fusion;maximum entropy methods;neural nets;radar imaging;remote sensing","multisensor imaging radar data fusion;intelligent neural computing;aggregated maximum entropy model;neural network;feature enhanced fusion;remote sensing imagery;radar sensing modalities;descriptive experiment design regularization;maximum entropy regularization;iterative minimization;energy function;multistate MENN;objective functions;FE fusion;theoretical informatics;DEDR-MENN techniques","","","","9","IEEE","24 Apr 2017","","","IEEE","IEEE Conferences"
"Fusion of Panchromatic and Multispectral Images via Morphological Operator and Improved PCNN in Mixed Multiscale Domain","J. Jiao; W. Lingda","Department of Graduate Management, Space Engineering University, Beijing, China; Science and Technology on Complex Electronic System, Simulation Laboratory Space Engineering University, Beijing, China","2018 10th IAPR Workshop on Pattern Recognition in Remote Sensing (PRRS)","11 Oct 2018","2018","","","1","11","In order to effectively combine the spectral information of the multispectral (MS) image with the spatial details of the panchromatic (PAN) image and improve the fusion quality, a fusion method based on morphological operator and improved pulse coupled neural network (PCNN) in mixed multi-scale (MM) domain is proposed. Firstly, the MS and PAN images are decomposed by nonsubsampled shearlet transform (NSST) to low- and high-frequency coefficients, respectively; secondly, morphological filter-based intensity modulation (MFIM) technology and stationary wavelet transform (SWT) are applied to the fusion of the low-frequency coefficients; an improved PCNN model is employed to the fusion of the high-frequency coefficients; thirdly, the final coefficients are reconstructed with inverse NSST. The experimental results on QuickBird satellite demonstrate that the proposed method is superior to five other kinds of traditional and popular methods: HIS, PCA, SWT, NSCT-PCNN and NSST-PCNN. The proposed method can improve the spatial resolution effectively while maintaining the spectral information well. The experimental results show that the proposed method outperforms the other methods in visual effect and objective evaluations.","2377-0198","978-1-5386-8479-5","10.1109/PRRS.2018.8486292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8486292","Image fusion;multispectral and panchromatic images;nonsubsampled shearlet transform;stationary wavelet transform;mixed multiscale analysis;morphological operator;pulse coupled neural network","Wavelet transforms;Image fusion;Spatial resolution;Remote sensing;Image reconstruction;Principal component analysis","image denoising;image fusion;image resolution;neural nets;wavelet transforms","multispectral image;morphological operator;spectral information;MS;spatial details;panchromatic image;PAN;fusion quality;fusion method;pulse coupled neural network;mixed multiscale domain;nonsubsampled shearlet;morphological filter-based intensity modulation technology;SWT;improved PCNN model;final coefficients;inverse NSST;NSCT-PCNN;NSST-PCNN;spatial resolution;visual effect;stationary wavelet transform","","1","","20","IEEE","11 Oct 2018","","","IEEE","IEEE Conferences"
"Random Forest Fusion Classification of Remote Sensing PolSAR and Optical Image Based on LASSO and IM Factor","F. Hong; Y. Kong","College of Electrical and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electrical and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","5048","5051","In view of the redundancy of features in the feature level fusion of PolSAR and optical images, the Support Vector Machine Recursive Feature Elimination (SVM-RFE) algorithm is adopted. However, due to the low efficiency of the algorithm for a large number of feature datasets, an improved feature screening algorithm is proposed. Firstly, extract various characteristics of PolSAR image and optical image, then through the LASSO algorithm construct a penalty function to delete redundant features, and combining with the SVM-RFE method, get the best of sub sets; Secondly, by combining random forest and conditional random field, it is proposed to take the feature importance of random forest as the interactive weight of conditional random field, so that the mixed feature set can achieve the best classification effect. Experimental results show that the feature subset optimized by LASSO-SVM-RFE algorithm can significantly reduce the number of features without decreasing the accuracy, while RF-IM-CRF method can further improve the accuracy of classification, so as to obtain the best classification results.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553357","National Natural Science Foundation of China(grant numbers:61501228); Aeronautical Science Foundation of China(grant numbers:20152052029,20182052012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553357","Image Fusion;Feature Selection;Classification;SVM;RF;CRF;Machine Learning","Support vector machines;Redundancy;Feature extraction;Optical imaging;Classification algorithms;Optical sensors;Random forests","","","","2","","7","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"3DRRDB: Super Resolution of Multiple Remote Sensing Images using 3D Residual in Residual Dense Blocks","M. R. Ibrahim; R. Benavente; F. Lumbreras; D. Ponsa","Computer Vision Center, Campus UAB, Barcelona, Spain; Computer Vision Center, Campus UAB, Barcelona, Spain; Computer Vision Center, Campus UAB, Barcelona, Spain; Computer Vision Center, Campus UAB, Barcelona, Spain","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","23 Aug 2022","2022","","","322","331","The rapid advancement of Deep Convolutional Neural Networks helped in solving many remote sensing problems, especially the problems of super-resolution. However, most state-of-the-art methods focus more on Single Image Super-Resolution neglecting Multi-Image Super-Resolution. In this work, a new proposed 3D Residual in Residual Dense Blocks model (3DRRDB) focuses on remote sensing Multi-Image Super-Resolution for two different single spectral bands. The proposed 3DRRDB model explores the idea of 3D convolution layers in deeply connected Dense Blocks and the effect of local and global residual connections with residual scaling in Multi-Image Super-Resolution. The model tested on the Proba-V challenge dataset shows a significant improvement above the current state-of-the-art models scoring a Corrected Peak Signal to Noise Ratio (cPSNR) of 48.79 dB and 50.83 dB for Near Infrared (NIR) and RED Bands respectively. Moreover, the proposed 3DRRDB model scores a Corrected Structural Similarity Index Measure (cSSIM) of 0.9865 and 0.9909 for NIR and RED bands respectively","2160-7516","978-1-6654-8739-9","10.1109/CVPRW56347.2022.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857363","","Training;Solid modeling;Three-dimensional displays;PSNR;Convolution;Superresolution;Pattern recognition","edge detection;geophysical image processing;image fusion;image representation;image resolution;neural nets;remote sensing","3D convolution layers;global residual connections;3DRRDB model scores;multiple remote sensing images;deep convolutional neural networks;remote sensing problems;remote sensing multiimage super-resolution;residual dense blocks model;single image super-resolution;multiimage superresolution;corrected structural similarity index measure;NIR;Proba-V challenge;noise figure 50.83 dB;noise figure 48.79 dB;V","","","","41","IEEE","23 Aug 2022","","","IEEE","IEEE Conferences"
"Aircraft Targets Detection in Remote Sensing Images with Feature Optimization","Q. Hu; R. Li; C. Pan; Y. Bao; H. Zhang","Information Engineering University, Zhengzhou, China; Information Engineering University, Zhengzhou, China; Information Engineering University, Zhengzhou, China; Information Engineering University, Zhengzhou, China; Information Engineering University, Zhengzhou, China","2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)","19 Jul 2021","2021","4","","1542","1549","Due to background interference, small size and dense arrangement in remote sensing images, the aircraft targets features are unconspicuous and easy to miss detection or misdetect. Aiming at this problem, a feature optimization algorithm based on YOLOv4 was proposed for aircraft targets detection in remote sensing images in this paper. In this algorithm, the feature fusion method of YOLOv4 was changed, and the multi-feature layers weighted fusion was used to reconstruct and optimize the different scale features. By expanding the scale range of the fusion features, the small targets detection ability is improved. Secondly, coordinate attention was used to effectively capture the relationship between feature-channels and the position information of feature-space. It can effectively reduce the interference of background and further enhance the feature information of the targets. The comparative trial results on the joint datasets of DIOR, RSOD and LEVIR show that the improved algorithm can effectively reduce the interference of complex background, significantly improve the detection effect of dense-small aircraft, and the performance is more superior.","2693-2776","978-1-7281-8535-4","10.1109/IMCEC51613.2021.9482093","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9482093","aircraft target detection;remote sensing images;feature weighted fusion;coordinate attentional","Object detection;Interference;Aerospace electronics;Feature extraction;Sensors;Information management;Aircraft","aircraft;feature extraction;image fusion;object detection;optimisation;remote sensing","feature optimization;YOLOv4;aircraft target detection;remote sensing images;feature fusion;multifeature layers weighted fusion;scale features;targets detection ability;feature-channels;feature-space;feature information;dense-small aircraft;background interference;aircraft targets features;DIOR;RSOD;LEVIR;coordinate attention","","1","","26","IEEE","19 Jul 2021","","","IEEE","IEEE Conferences"
"Explainable Multi-Criteria Data-Driven Environmental Status Assessment from Remote Sensing","D. Stroppiana; M. Boschetti; P. A. Brivio; G. Bordogna","IREA CNR, Milano, Italy; IREA CNR, Milano, Italy; IREA CNR, Milano, Italy; IREA CNR, Milano, Italy","2022 IEEE 21st Mediterranean Electrotechnical Conference (MELECON)","3 Aug 2022","2022","","","471","476","The paper proposes a multi-criteria and data driven fusion approach whose semantics can be explained in terms of attitude towards decisions. It is exemplified to assess environmental status from remote sensing images in order to identify hot spot of critical situations and anomalies induced by wildfires, floods, desertification, erosion etc. by fusing multiple factors defined by experts knowledge. The fusion function is an Ordered Weighted Averaging (OWA) operator, whose behaviour is here characterized by degrees of pessimism and democracy. The paper proposes to explain the semantics of the fusion function learnt from few ground truth data available, i.e., the OWA operator, by computing its degrees of pessimism/optimism and democracy/monarchy, which are defined as semantic interpretations of both orness and dispersions respectively. Pessimism indicates if the fused map is more prone to commission (overestimation) or omission (underestimation) errors, while democracy indicates how many factors contribute to the generation of the map. The approach is exemplified to map the flooded areas from remote sensing by considering different models based on distinct spectral indexes and domain experts.","2158-8481","978-1-6654-4280-0","10.1109/MELECON53508.2022.9842919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9842919","remote sensing;decision attitude;fusion of multiple partial evidence","Semantics;Decision making;Fires;Linguistics;Optical imaging;Open wireless architecture;Optical sensors","decision making;floods;fuzzy set theory;geophysical image processing;image fusion;remote sensing;sensor fusion","explainable multicriteria data;fusion approach;semantics;environmental status;remote sensing images;critical situations;experts knowledge;Ordered Weighted Averaging operator;democracy;fusion function learnt;ground truth data;OWA operator;semantic interpretations;fused map;flooded areas","","","","22","IEEE","3 Aug 2022","","","IEEE","IEEE Conferences"
"Image Vusion: Image and Video Fusion","M. Hossny; S. Nahavandi","Centre for Intelligent Systems Research (CISR), Deakin University; Centre for Intelligent Systems Research (CISR), Deakin University","2015 IEEE International Conference on Systems, Man, and Cybernetics","14 Jan 2016","2015","","","425","429","It is not uncommon in many image acquisition solutions to balance a trade off between obtaining high resolution images at very low frame rates or acquiring a burst of low resolution images at higher frame rates. This paper introduces a novel image fusion framework for producing a high resolution video by augmenting analysed motion in a low resolution video to a single high resolution image. Many application domains such as remote sensing, low radiation medical imaging and battlefield automation will benefit from this novel fusion framework. The results show that a captured high resolution 30 frames per second video can be produced with 95% cost reduction while maintaining 94% structural similarity.","","978-1-4799-8697-2","10.1109/SMC.2015.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379217","Image and video fusion;temporal data;fluoroscopy","Image resolution;Image sequences;Heart rate;Biomedical imaging;Image fusion;Injuries;X-rays","image fusion;image resolution;video signal processing","image vusion;video fusion;image fusion;high resolution video;low resolution video;high resolution image","","1","","19","IEEE","14 Jan 2016","","","IEEE","IEEE Conferences"
"Mangrove Species Mapping Using Deep Learning with Fusion of Hyperspectral and High-Resolution Multispectral Images","L. Wan; H. Zhang; P. Ma; G. Lin","The Institute of Space and Earth Information Science, The Chinese University of Hong Kong, Hong Kong, China; Department of Geography, The University of Hong Kong, Hong Kong, China; Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China; Division of Ocean Science and Technology, Shenzhen International Graduate School, Tsinghua University, Shenzhen, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","5892","5895","Accurate mapping of mangroves species is essential for mangrove management, and deep learning of hyperspectral images (HSIs) shows a great advantage in classification with the fine spectrum. However, the sparely available annotations of HSIs are key challenges for accurate mapping using deep learning, especially for mangrove species within small patches. In this work, a high spatial resolution HSI is synthesized using the method of hyperspectral-multispectral image fusion with spectral variability, providing augmented samples as well as spatial information of mangroves. Secondly, the latest 3D convolutional neural network (3DCNN) was investigated to explore spatial and spectral information for mangrove species mapping. Compared to Gaofen 5 using conventional machine learning methods, the synthetic image provides manyfold samples and higher accuracy for mangrove species mapping using 3DCNNs. This work is expected to improve the situation of sample shortage and spatial information deficiency for mangrove species mapping using deep learning with HSIs.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554028","Research Grants Council (RGC) of Hong Kong(grant numbers:HKU27602020,HKU14605917,CUHK14504219); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554028","Mangroves;fusion;deep learning","Deep learning;Three-dimensional displays;Annotations;Geoscience and remote sensing;Convolutional neural networks;Spatial resolution;Image fusion","","","","","","18","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Detection of Transmission Towers and Insulators in Remote Sensing Images with Deep Learning","T. Wang; R. Wei; L. Wang; L. Zhu; E. Zhou; S. Liu; H. Yang; S. Wang","Guangdong Key Laboratory of Electric Power Equipment Reliability, Electric Power Research Institute of Guangdong Power Grid Co., Ltd., Guangzhou, China; Guangdong Key Laboratory of Electric Power Equipment Reliability, Electric Power Research Institute of Guangdong Power Grid Co., Ltd., Guangzhou, China; Guangdong Key Laboratory of Electric Power Equipment Reliability, Electric Power Research Institute of Guangdong Power Grid Co., Ltd., Guangzhou, China; Guangdong Power Grid Co., Ltd., Guangzhou, China; Guangdong Key Laboratory of Electric Power Equipment Reliability, Electric Power Research Institute of Guangdong Power Grid Co., Ltd., Guangzhou, China; Guangdong Key Laboratory of Electric Power Equipment Reliability, Electric Power Research Institute of Guangdong Power Grid Co., Ltd., Guangzhou, China; Tianjin Zhongwei Aerospace Data System Technology Co.,Ltd., Tianjin, China; Tianjin Zhongwei Aerospace Data System Technology Co.,Ltd., Tianjin, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","3298","3303","Intelligent inspections of high-voltage electronic power grids with high-altitude aerial or satellite remote sensing images (RSIs) have attracted more and more attention. The detection of transmission tower and insulator in smart electronic power is of great importance. Traditional image recognition based methods have been proven to be difficult to complete this task effectively and efficiently, lots of deep learning based methods have been adopted due to their promising performance. However, collecting a large number of labeled aerial/satellite imaging data for deep learning requires a lot of manpower/financial costs and the deep models trained on small size of samples are often easy to over-fit. For this reason, it has practical significance to study the automatic detection of towers and insulators in the case of small samples. Aiming at the problem of object detection under small samples, a deep learning framework for simultaneous towers and insulators detection in RSIs based on Faster-RCNN and neural style image synthesis is proposed. Firstly, to alleviate the small sample size problem, a sample generation method based on neural style transfer and alpha channel image fusion techniques is proposed, which randomly combines the foreground towers and background images to expand the training data set. Secondly, upon the expanded training data, an object detection model for towers and insulators based on Faster-RCNN is further trained. Experiments show that the object detection model trained with the extended training data has better generalization performance and can better suppress false alarms.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728166","China Southern Power Grid; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728166","power grids;tower detection;insulator detection;object detection;neural style transfer","Deep learning;Satellites;Poles and towers;Training data;Object detection;Insulators;Data models","feature extraction;geophysical image processing;image fusion;image recognition;learning (artificial intelligence);neural nets;object detection;poles and towers;remote sensing","high-voltage electronic power grids;high-altitude aerial;RSIs;transmission tower;insulator;smart electronic power;traditional image recognition based methods;deep learning based methods;deep models;automatic detection;insulators;deep learning framework;simultaneous towers;Faster-RCNN;neural style image synthesis;sample size problem;sample generation method;neural style transfer;alpha channel image fusion techniques;foreground towers;background images;training data set;expanded training data;object detection model;extended training data;transmission towers;remote sensing images;intelligent inspections","","","","24","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Decision-Level Fusion of DNN Outputs for Improving Feature Detection Performance on Large-Scale Remote Sensing Image Datasets","A. B. Cannaday II; R. L. Chastain; J. A. Hurt; C. H. Davis; G. J. Scott; A. J. Maltenfort","Center for Geospatial Intelligence, University of Missouri, Columbia, MO, USA; Center for Geospatial Intelligence, University of Missouri, Columbia, MO, USA; Center for Geospatial Intelligence, University of Missouri, Columbia, MO, USA; Center for Geospatial Intelligence, University of Missouri, Columbia, MO, USA; Center for Geospatial Intelligence, University of Missouri, Columbia, MO, USA; National Geospatial-Intelligence Agency, Springfield, VA, USA","2019 IEEE International Conference on Big Data (Big Data)","24 Feb 2020","2019","","","5428","5436","Here we demonstrate how Deep Neural Network (DNN) detections of multiple constitutive or component objects that are part of a larger, more complex, and encompassing feature can be spatially fused to improve the detection performance of a larger complex feature. A wide variety of experiments were conducted using the public domain xView dataset to develop and evaluate multiple fusion strategies to improve the detection of Construction Sites using DNN detections of constitutive/component objects commonly associated with construction activity, e.g. cement mixers, dump trucks, etc. The results demonstrate that spatial fusion of multi-scale component object DNN detections can reduce the total detection error rate of Construction Sites by ~30-40%. The best results were obtained when local spatial clustering was used to reduce noise in component vehicle object detections generated by scanning candidate Construction Site locations. This multi-scale spatial fusion approach can be easily extended to improve detection performance in a wide variety of other challenging feature/object search and detection problems in large-scale remote sensing image datasets.","","978-1-7281-0858-2","10.1109/BigData47090.2019.9006502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9006502","Deep Neural Networks;Spatial Clustering;Feature Detection;Object Detection;Data Fusion;Large-Scale Datasets","Feature extraction;Multiprotocol label switching;Training;Remote sensing;Detectors;Automobiles;Containers","construction components;feature extraction;geophysical image processing;image fusion;neural nets;object detection;pattern clustering;remote sensing;sensor fusion;vehicles","candidate construction site locations;multiscale spatial fusion approach;component vehicle object detections;local spatial clustering;multiscale component object DNN detections;public domain xView dataset;deep neural network detections;large-scale remote sensing image datasets;feature detection;DNN outputs;decision-level fusion","","1","","18","IEEE","24 Feb 2020","","","IEEE","IEEE Conferences"
"MIGN: Multiscale Image Generation Network for Remote Sensing Image Semantic Segmentation","J. Nie; C. Wang; S. Yu; J. Shi; X. Lv; Z. Wei","Ocean University of China, QIngdao, Shandong, China; Faculty of Information Science and Engineering, Ocean University of China, QIngdao, Shandong, China; Faculty of Information Science and Engineering, Ocean University of China, QIngdao, Shandong, China; Faculty of Information Science and Engineering, Ocean University of China, QIngdao, Shandong, China; Faculty of Information Science and Engineering, Ocean University of China, QIngdao, Shandong, China; Ocean University of China, QIngdao, Shandong, China","IEEE Transactions on Multimedia","","2022","PP","99","1","14","With the development of computer vision, the semantic segmentation of remote sensing images, which has become an important topic, has been utilized in various applications for image content analysis and understanding, such as urban planning, natural disaster monitoring, and land resource management. Many approaches have been proposed to address these problems. However, due to obvious differences in resolution, spatial structure, and semantics between remote sensing images and ordinary images, the semantic segmentation of remote sensing images is still challenging. In this paper, we propose a novel multiscale image generation network (MIGN) that can efficiently generate high-resolution segmentation results by considering both details and boundary information. In particular, a multi-attention mechanism method for semantic segmentation of remote sensing images is designed. The attention weight is calculated by capturing the interaction of cross dimensions in a two-branch structure, which can learn the underlying feature information and guarantee the performance of each pixel feature for final classification. We also propose an edge supervised module to ensure that the segmentation boundary has a more accurate performance. A multiscale image fusion algorithm based on the Bayes model is proposed to improve the accuracy of the segmentation module. The performance of our model is evaluated on the ISPRS Vaihingen and Potsdam datasets. The results show that our method is superior to the most advanced image segmentation methods in terms of MIoU and pixel accuracy.","1941-0077","","10.1109/TMM.2022.3197369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9852293","semantic segmentation;remote sensing;multiscale;multi-attention;edge supervised;image fusion","Image segmentation;Semantics;Feature extraction;Remote sensing;Image edge detection;Convolution;Decoding","","","","","","","IEEE","8 Aug 2022","","","IEEE","IEEE Early Access Articles"
"Mapping urban impervious surfaces by fusing optical and SAR data at decision level","Y. Bai; G. Sun; Y. Ge; Y. Zhang; Y. Li","Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; National Astronomical Observatories, Chinese Academy of Sciences, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","6336","6339","The extraction of urban impervious surface information plays a key role in the studies of urbanization and its related environmental issues. Optical and SAR remote sensing provides complementary information to improve the accuracy of impervious mapping. However, the fusing of information acquired by different sensors is challenging. Optical and SAR features have distinct characteristics, and require different classification strategy and classification types. In this study, a strategy of fusing multi-spectral optical and polarimetric SAR data at decision-level is proposed. Features are extracted from optical and SAR data, then staked auto-encoder is applied to achieve the land use and land cover classification separately. D-S evidence theory is used to fuse the classification result and the imperious surface map is derived. The experiment was conducted in a highly complex urban area of Hong Kong and the results proves the soundness of the method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898039","impervious surface;decision-level fusion;land use and land cover;multi-spectrum;synthetic aperture radar","","geophysical image processing;geophysical signal processing;image classification;image fusion;radar polarimetry;remote sensing;remote sensing by radar;synthetic aperture radar;terrain mapping","related environmental issues;complementary information;impervious mapping;different classification strategy;classification types;polarimetric SAR data;decision-level;optical SAR data;land use;land cover classification;classification result;imperious surface map;highly complex urban area;urban impervious surfaces;decision level;urban impervious surface information","","","","8","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Efficient superpixel-oriented multi-task joint sparse representation classification for hyperspectral imagery","J. Li; H. Zhang; L. Zhang","The State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, and the Collaborative Innovation Center for Geospatial Technology, Wuhan University, P. R. China; The State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, and the Collaborative Innovation Center for Geospatial Technology, Wuhan University, P. R. China; The State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, and the Collaborative Innovation Center for Geospatial Technology, Wuhan University, P. R. China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","2592","2595","With regard to the specific role of each pixel within a spatial parcel of a hyperspectral image (HSI), we propose a novel superpixel-oriented sparse representation classification method with a multi-task learning approach. The proposed algorithm exploits the class-level sparsity prior for multiple-feature fusion, and also the correlation and distinctiveness of pixels in a spatial local region. Compared with the state-of-the-art hyperspectral classifiers, the superiority of the spatial prior utilization, the multiple-feature fusion, and the computational efficiency are maintained at the same time in the proposed method. The proposed classification framework was tested on two HSIs. The experimental results suggest that the proposed algorithm performs better than the other representation-based classification algorithms and some popular hyperspectral multiple-feature classifiers.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326342","Joint sparsity;multi-task learning;superpixel representation;hyperspectral image classification","Hyperspectral imaging;Joints;Feature extraction;Classification algorithms;Training","geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;learning (artificial intelligence)","superpixel-oriented multitask joint sparse representation classification;hyperspectral imagery;multitask learning;class-level sparsity prior;multiple-feature fusion;hyperspectral classifiers;HSI","","","","9","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Delineation of Moroccan Coastal Upwelling Using The Principal Component Analysis Fusion Algorithm on SSC and SST Images","Z. EL ABIDI; K. MINAOUI; A. TAMIM; H. LAANAYA","LRIT-CNRST URAC 29, Mohammed V university, Morocco; LRIT-CNRST URAC 29, Mohammed V university, Morocco; Department of Marine Fisheries, Higher Institute of Marine Fisheries (ISPM), Agadir, Morocco; Rabat IT Center, Mohammed V university, Morocco","2018 9th International Symposium on Signal, Image, Video and Communications (ISIVC)","9 May 2019","2018","","","174","178","The current paper presents a novel methodology with the goal of detecting the Moroccan coastal upwelling area. Realistically, our region of interest is characterized by lower temperature degree and higher chlorophyll concentration. The distribution of this two indicators in the ocean is observed by remote sensing from sea surface chlorophyll (SSC) and sea surface temperature (SST) images. In this context, we process 46 images of the year 2014 for each kind set to detect efficiently the desired zone by applying Principal Component Analysis fusion algorithm. The validation made by the oceanographer indicates that the results of our approach is promising in term of Moroccan coastal upwelling delimitation.","","978-1-5386-8173-2","10.1109/ISIVC.2018.8709227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8709227","Moroccan Coastal Upwelling;Principal Component Analysis;Sea Surface Temperature image;Sea Surface Chlorophyll image;spatial domain fusion","Ocean temperature;Sea measurements;Sea surface;Principal component analysis;Surface treatment;Image fusion","geophysical image processing;image fusion;ocean composition;ocean temperature;oceanographic regions;organic compounds;principal component analysis;remote sensing","Moroccan coastal upwelling area;Principal Component Analysis fusion algorithm;Moroccan coastal upwelling delimitation;chlorophyll concentration;sea surface chlorophyll;sea surface temperature;AD 2014","","3","","15","IEEE","9 May 2019","","","IEEE","IEEE Conferences"
"Remote Sensing Image Scene Classification via Multi-Level Representation Learning","W. Fu; L. Yang","College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China","2022 26th International Conference on Pattern Recognition (ICPR)","29 Nov 2022","2022","","","2942","2948","Remote sensing image scene classification (RSSC), which assigns semantic labels to remote sensing images, is very important for remote sensing image interpretation. Thanks to the rapid development of deep learning, RSSC achieves significant breakthroughs by the use of convolutional neural network (CNN). However, CNN relies on local receptive fields and is difficult to capture long-range and global scene information. Moreover, the information of salient objects, which contributes to discriminate the category of scenes (e.g., airplanes indicate the airport scene), should be also exploited. To address this issue, a deep learning method, named multi-level representation learning (MLRL), is proposed to collaboratively extract pixel-level, patch-level, and object-level features, which respectively contain local, global, and object-oriented information. Specifically, pixel-level features are obtained by pixel-wise convolution operations within a CNN. Patch-level features are achieved by a patch-wise self-attention network. Object-level features are acquired by applying a CNN to a cropped sub-image, which conveys important information of salient objects. To this end, a three-branch network structure to respectively extract above features, is built. Finally, a decision fusion method is adopted to integrate multi-level features, and gives rise to refined classification results. Experiments conducted on widely-used datasets demonstrate the effectiveness of the proposed method.","2831-7475","978-1-6654-9062-7","10.1109/ICPR56361.2022.9956398","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956398","","Deep learning;Representation learning;Measurement;Image analysis;Fuses;Semantics;Feature extraction","convolutional neural nets;decision theory;deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image fusion;image representation;image segmentation;remote sensing","airport scene;CNN;convolutional neural network;cropped sub-image;decision fusion method;deep learning method;global scene information;local receptive fields;multilevel features;multilevel representation learning;object-level feature extraction;patch-level feature extraction;patch-wise self-attention network;pixel-level feature extraction;pixel-wise convolution operations;remote sensing image interpretation;remote sensing image scene classification;RSSC;salient objects;semantic labels;three-branch network structure","","","","30","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"DFFNet: Dynamic Feature Fusion Network for Weakly Supervised Object Detection in Remote Sensing Images","M. Zhu; S. Wan; P. Jin; P. Zhang","School of Computer Science and Technolopy, University of Science and Technology of China, Hefei, China; School of Computer Science and Technolopy, University of Science and Technology of China, Hefei, China; School of Computer Science and Technolopy, University of Science and Technology of China, Hefei, China; School of Computer Science and Technolopy, University of Science and Technology of China, Hefei, China","2022 IEEE International Conference on Big Data (Big Data)","26 Jan 2023","2022","","","1409","1414","In recent years, weakly supervised object detection (WSOD) methods using only image-level labels have received increasing attention, Due to the difficulty of manually labeling large-scale remote sensing images. However, existing methods cannot generate high-quality proposals when applying proposal generation methods to RSIs. Meanwhile, these methods ignore the fact that there are a large number of objects of different scales in RSIs. To address these issues, we propose a unique end-to-end dynamic feature fusion network (DFFNet) for WSOD in RSIs. First, we propose an intersection-over-union selective search (IoU-SS) algorithm to generate high-quality proposals by preferentially merging regions with high IoU. Furthermore, we design a novel and flexible dynamic feature fusion (DFF) module to dynamically acquire features of objects at different scales based on the information of the input image. The performance of WSOD in RSIs is further improved by using high-quality proposals and dynamically fused features. Comprehensive experiments and comparisons with state-of-the-art methods on two datasets of RSIs, i.e., NWPU VHR-10.v2 and DIOR, demonstrate the superiority of our proposed method.","","978-1-6654-8045-1","10.1109/BigData55660.2022.10020414","Natural Science Foundation of Anhui Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10020414","Dynamic feature fusion (DFF);remote sensing images (RSIs);weakly supervised object detection (WSOD)","Heuristic algorithms;Merging;Object detection;Detectors;Big Data;Feature extraction;Sensors","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);object detection;remote sensing","applying proposal generation methods;DFFNet;dynamic feature fusion network;dynamically fused features;high IoU;high-quality proposals;image-level labels;input image;intersection-over-union selective search;large-scale remote sensing images;RSIs;supervised object detection methods;unique end-to-end;weakly supervised object detection;WSOD","","","","22","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"AF-Net: All-scale Feature Fusion Network for Road Extraction from Remote Sensing Images","S. Zou; F. Xiong; H. Luo; J. Lu; Y. Qian","School of Computer Science and Engineering, Nanjing University of Science and Technology, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, China; College of Computer Science, Zhejiang University, China","2021 Digital Image Computing: Techniques and Applications (DICTA)","23 Dec 2021","2021","","","1","8","Road extraction from high-resolution remote sensing images (RSIs) is a challenging task due to occlusion, irregular structures, complex background, etc. A typical solution for road extraction is semantic segmentation that tries to segment the road region directly from the background region at the pixel level. Because of the narrow and slender structures of roads, high-quality multi-resolution and diverse semantic feature representations are necessary for this task. To this end, this paper introduces an all-scale feature fusion network named as AF-Net to extract roads from RSIs. AF-Net adopts an encoder-decoder architecture, whose encoder and decoder are connected by the introduced all-scale feature fusion module (AF-module). AF-module contains multiple feature fusion stages, corresponding to features of different scales. At each stage of feature fusion, all-scale all-level feature representations are employed to recursively integrate the features from two paths. One path propagates the high-resolution spatial features to the current scale feature and another path merges the current scale feature with high-level semantic features. In this way, we effectively employ all-scale features with varied spatial information and semantic information in each fusion stage, facilitating producing more accurate spatial information and richer semantic information for road extraction. Moreover, a convolutional block attention module is embedded into AF-module to suppress unconducive features from the surrounding background and improve the quality of extracted roads. Due to the features with richer semantic information and more precise spatial information, the proposed AF-Net outperforms other state-of-the-art methods on two benchmark datasets.","","978-1-6654-1709-9","10.1109/DICTA52665.2021.9647235","National Natural Science Foundation of China(grant numbers:62002169,62071421); National Key Research and Development Program of China(grant numbers:2018AAA0100500); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647235","road extraction;remote sensing images;multiscale feature representation;deep learning;attention","Image segmentation;Roads;Digital images;Semantics;Benchmark testing;Feature extraction;Sensors","feature extraction;geophysical image processing;image fusion;image resolution;image segmentation;remote sensing;roads","road extraction;semantic segmentation;road region;high-quality multiresolution;diverse semantic feature representations;all-scale feature fusion network;AF-Net adopts;all-scale feature fusion module;AF-module;multiple feature fusion stages;all-level feature representations;high-resolution spatial features;current scale feature;high-level semantic features;all-scale features;fusion stage;richer semantic information;unconducive features;extracted roads;AF-Net outperforms;high-resolution remote sensing images","","","","33","IEEE","23 Dec 2021","","","IEEE","IEEE Conferences"
"Unsupervised classification of remote sensing imagery using multi-sensor data fusion","A. K. Agarwalla; S. Minz","TERI University, New Delhi; JNU, School of Computer & Systems Sciences, New Delhi","2017 International Conference on Signal Processing and Communication (ICSPC)","5 Mar 2018","2017","","","227","233","Remotely sensed imagery accounts for sensor specific information. The following paper deals with making use of data from multiple sources with similar temporal resolution to improve classification accuracy. This was done by clustering five masks or samples of 100 × 100 pixels selected randomly from multispectral data from Landsat TM and evaluation of cluster quality to find the number of naturally occurring clusters. This was followed by clustering the entire study area Landsat TM data using k-means algorithm and evaluation of the resulting cluster quality using silhouette coefficient to identify loosely classified pixels and mean silhouette value (threshold of the scene). Hyper-spectral data from Hyperion was used for only the loosely classified pixels identified above and was clustered using the k-means algorithm. Finally, soft decision level fusion method was applied to the clustering output from HS data with good quality clusters (clusters with silhouette coefficient above the mean) from the multi-spectral imagery to produce final classification maps. In the fused imagery, the overall Classification accuracy and Kappa Statistics increased significantly as compared to the multispectral imagery. Cluster validity indices like Silhouette coefficient is used to evaluate cluster quality and predict naturally occurring clusters. The decision level fusion of selective data from multiple sources has exhibited better classification results at reduced computational overheads.","","978-1-5090-6730-5","10.1109/CSPC.2017.8305844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305844","Unsupervised Classification;Cluster Validity Index;Silhouette Coefficient;Data fusion;Decision-fusion","Clustering algorithms;Signal processing algorithms;Remote sensing;Classification algorithms;Data integration;Prediction algorithms;Signal processing","feature extraction;geophysical image processing;geophysical signal processing;geophysical techniques;image classification;image fusion;pattern classification;pattern clustering;remote sensing;sensor fusion","Kappa Statistics;Hyperion;k-means algorithm;remotely sensed imagery accounts;hyperspectral data;cluster quality;Landsat TM data;multisensor data fusion;remote sensing imagery;cluster validity indices;fused imagery;multispectral imagery;soft decision level fusion method;mean silhouette value;loosely classified pixels;silhouette coefficient;multispectral data;classification accuracy","","","","31","IEEE","5 Mar 2018","","","IEEE","IEEE Conferences"
"SAR image change detection method based on shearlet transform","Y. Zhang; S. Wang; C. Wang; H. Zhang; F. Wu; M. Liu; Q. Fu; Y. Wang","School of Communication Engineering of Jilin University, Changchun, China; School of Communication Engineering of Jilin University, Changchun, China; University of Chinese Academy of Sciences, Beijing, China; Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, Beijing, China; Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, Beijing, China; Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, Beijing, China; China Center for Resources Satellite Date and Application, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","2017 Progress in Electromagnetics Research Symposium - Fall (PIERS - FALL)","19 Feb 2018","2017","","","1223","1229","Multi-temporal synthetic aperture radar (SAR) images have been successfully used for the detection of different types of terrain changes. However, SAR image change detection based on wavelet transform is still restrained from the existence of speckle noise and the nature of wavelet transform. In this paper, an unsupervised SAR image change detection fusion framework based on shearlet transform is proposed. In the proposed method, The Gauss filtering is combined with log-ratio to impair speckle. Then the difference map (DM) of Gauss-log ratio and the difference map of ratio based on Gabor feature are fused with shearlet transform. Meanwhile, DM is decomposed to low frequency image and four high frequency images, different fusion rules are used in multi-scales images respectively, the work of noise reduction is operated with mean filtering. After an inverse shearlet transformation, the final change map can be obtained via a simple OSTU segmentation. The real SAR image pairs in Bern area are used to verify proposed change detection method. The experimental results demonstrate the robustness of the proposed method.","","978-1-5386-1211-8","10.1109/PIERS-FALL.2017.8293318","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8293318","","Synthetic aperture radar;Wavelet transforms;Electromagnetics;Speckle;Image segmentation;Feature extraction","feature extraction;geophysical image processing;image fusion;image segmentation;remote sensing by radar;speckle;synthetic aperture radar;terrain mapping;unsupervised learning;wavelet transforms","fusion rules;Gauss-log ratio;Gabor feature;OSTU segmentation;inverse shearlet transformation;Gauss filtering;unsupervised SAR image change detection;speckle noise;wavelet transform;terrain changes;multitemporal synthetic aperture radar images;SAR image change detection method;SAR image pairs","","","","11","IEEE","19 Feb 2018","","","IEEE","IEEE Conferences"
"Blending Ultra Spectral Images of Multi-Source Remote Sensors","V. S. Chilkuri; D. Bharathi; R. Karthi","Department of Computer Science and Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Department of Computer Science and Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Department of Computer Science and Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India","2022 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)","30 Dec 2022","2022","","","1","5","Images and procedures from remote sensing are effective tools for studying the earth's surface. Data quality is essential for improving remote sensing applications and producing crisp, noise-free images. Due to the different gathering methods, obtaining a free set of data is quite challenging in most cases. So, picture or information fusion is crucial in far-flung sensing applications. Spatiotemporal fusion (STF) is a method for fusing pix with the proper temporal and spatial decision by integrating (temporally dense) coarse-resolution pictures with (temporally sparse) fine-resolution pictures. This paper makes a specialty of enforcing spatiotemporal fusion of multi-supply remote sensing pictures. In this paper, STF of multi-source remote sensing images, specifically Landsat and Sentinel sensors images is performed using the ESTARFM fusion method. In total 2 experiments were conducted with Landsat 7, Landsat 8, and Sentinel 2 data. In experiment 1 Landsat 7, and Sentinel 2 images are considered fine and coarse resolution images respectively, and in experiment 2 Landsat 8, and Sentinel 2 as fine and coarse resolution images. The metrics suggest that by applying the STF method, the similarity between the fused image and the original image at the prediction time of experiment 2 is more when compared to the corresponding image results of experiment 1.","","978-1-6654-7095-7","10.1109/ICECCME55909.2022.9988603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9988603","Spatiotemporal Fusion;Coarse resolution;Fine resolution;ESTARFM","Earth;Image sensors;Artificial satellites;Satellites;Sensor fusion;Prediction algorithms;Sensors","geophysical image processing;geophysical techniques;hydrological techniques;image fusion;image resolution;remote sensing","blending ultra spectral images;coarse resolution images;coarse-resolution pictures;corresponding image results;data quality;different gathering methods;earth;ESTARFM fusion method;experiment 1 Landsat 7;experiment 2 Landsat 8;far-flung sensing applications;fine resolution images;fine-resolution pictures;fused image;multisource remote sensing images;multisource remote sensors;multisupply remote sensing pictures;noise-free images;proper temporal decision;remote sensing applications;Sentinel 2 data;Sentinel 2 images;spatial decision;spatiotemporal fusion;STF method;time 2.0 as;total 2 experiments","","","","9","IEEE","30 Dec 2022","","","IEEE","IEEE Conferences"
"Multi-exposure image fusion method based on space-borne area array CMOS camera for high dynamic scene","J. Qian; J. Ma; L. Xin; F. Li",NA; NA; NA; NA,"ICMLCA 2021; 2nd International Conference on Machine Learning and Computer Application","17 Mar 2022","2021","","","1","5","In aerospace remote sensing, the dynamic range images cannot be captured well due to the short integration time of the area array CMOS camera, resulting in dim images and loss of details. In order to solve the problems, a multi-exposure image fusion method based on space-borne area array CMOS cameras combining image registration with digital time delay integration (TDI) technology for high dynamic scenes was proposed in this paper. A multi-exposure image fusion algorithm with deblurring was used to generate a high dynamic range (HDR) image with adaptive exposure based on multi-exposure images captured by the area array CMOS camera. Experimental results show that our method has superior image quality compared with other exposure fusion methods. It can effectively solve the limitation of the dynamic range of aerospace remote sensing cameras.","","978-3-8007-5739-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9736902","","","","","","","","","","17 Mar 2022","","","VDE","VDE Conferences"
"Object-based fusion of hyperspectral and LiDAR data for classification of urban areas","P. R. Marpu; S. S. Martinez","Institute Center for Water and Environment (iWATER), Masdar Institute of Science and Technology, Abu Dhabi, United Arab Emirates; Institute Center for Water and Environment (iWATER), Masdar Institute of Science and Technology, Abu Dhabi, United Arab Emirates","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","23 Oct 2017","2015","","","1","4","This paper presents an object-based fusion of hyperspectral data with LIDAR data for efficient classification of urban areas. Image segmentation is performed on the features extracted from hyperspectral data at multiple levels in a hierarchical way for utilizing spatial information at various scales. Additional information on the classes is derived from the LIDAR data to aid in the classification process. While fusing hyperspectral data with the LIDAR data, the proposed approach is resilient to small misregistration of images as we only consider the information with maximum overlap based on the median operator. The final classification is performed using the random forest classifier.","2158-6276","978-1-4673-9015-6","10.1109/WHISPERS.2015.8075399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075399","Data fusion;LIDAR;hyperspectral;object-based","Hyperspectral imaging;Laser radar;Image segmentation;Feature extraction;Data integration;Vegetation mapping","feature extraction;forestry;geophysical image processing;hyperspectral imaging;image classification;image fusion;image registration;image segmentation;optical radar;radar imaging;remote sensing by laser beam;remote sensing by radar","hyperspectral data;LIDAR data;urban areas;object-based fusion;image segmentation;feature extraction;efficient classification process;image misregistration;median operator;random forest classifier","","3","","7","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Fusion of hyperspectral and LiDAR data using random feature selection and morphological attribute profiles","S. Samiappan; L. Dabbiru; R. Moorhead","Geosystems Research Institute, Mississippi State University, USA; Geosystems Research Institute, Mississippi State University, USA; Geosystems Research Institute, Mississippi State University, USA","2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Oct 2017","2016","","","1","4","Hyperspectral imagery provides detailed information about land-cover materials over a wide spectral range. Land-cover classification using hyperspectral data has been an active topic of research. Elevation data from light detection and ranging (LiDAR) can aid the classification process in discriminating complex classes. Fusion of hyperspectral and LiDAR data has been investigated in the past where the goal was to extract features from both sources and combine them to improve the accuracy of land-cover classification. In this paper, we present a new fusion approach based on random feature selection (RFS) and morphological attribute profiles (AP). Our experimental study, conducted on a hyperspectral image and digital surface model (DSM) derived from first return LiDAR data collected over the Samford ecological research facility, Queensland, Australia indicate that the proposed approach yields excellent classification results.","2158-6276","978-1-5090-0608-3","10.1109/WHISPERS.2016.8071662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071662","Hyperspectral;light detection;ranging (LiDAR);digital surface model (DSM);data fusion","Hyperspectral imaging;Laser radar;Feature extraction;Support vector machines;Buildings","feature selection;geophysical image processing;image classification;image fusion;land cover;optical radar;remote sensing;remote sensing by laser beam","elevation data;classification process;land-cover classification;fusion approach;random feature selection;hyperspectral image;hyperspectral imagery;hyperspectral data;light detection and ranging;LiDAR data;digital surface model;Samford ecological research facility;Queensland;Australia","","1","","10","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"Fuzzy aggregation for multimodal remote sensing classification","K. Nock; E. Gilmour","U.S. Naval Research Laboratory, Washington, DC, USA; U.S. Naval Research Laboratory, Washington, DC, USA","2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)","26 Aug 2020","2020","","","1","7","This paper investigates methods of fusing hyperspectral imagery (HSI) and LiDAR data for urban land use and land cover classification. A variety of fusion methods including combination rules, deep neural networks, and fuzzy aggregation are compared against using any single modality for classification. The experimental results demonstrate that the two fuzzy aggregation methods, the linear order statistic neuron (LOSN) and the Choquet integral (ChI), achieved the best overall and average classification accuracy, respectively. We further discuss how the fuzzy aggregation methods provides advantages with difficult samples and the opportunity to gain network explainability.","1558-4739","978-1-7281-6932-3","10.1109/FUZZ48607.2020.9177691","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9177691","hyperspectral imagery;LiDAR;Choquet integral;fuzzy aggregation;remote sensing","Laser radar;Neural networks;Hyperspectral imaging;Feature extraction;Data integration","fuzzy set theory;geophysical image processing;hyperspectral imaging;image classification;image fusion;integral equations;land cover;land use;neural nets;optical radar;remote sensing;statistical analysis","fuzzy aggregation;multimodal remote sensing classification;urban land use;land cover classification;fusion methods;combination rules;deep neural networks;linear order statistic neuron;hyperspectral imagery;LiDAR data;Choquet integral","","1","","19","USGov","26 Aug 2020","","","IEEE","IEEE Conferences"
"Fusion of Hyperspectral and Lidar Images Using Non-Subsampled Shearlettransform","M. R. Soleimanzadeh; A. Karami; P. Scheunders","Faculty of Physics, Shahid Bahonar University of Kerman, Kerman, Iran; Faculty of Physics, Shahid Bahonar University of Kerman, Kerman, Iran; Visionlab, University of Antwerp, Belgium","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","8873","8876","In this paper, a new fusion method for merging the spectral and spatial contents of hyperspectral images (HSI) with the height information of light detection and ranging (LiDAR) for increasing the classification accuracy of HSI is introduced. First, 2D non-subsampled shearlet transform (NSST) is applied to each band of hyperspectral and LiDAR data separately in order to extract the spatial features. Second, principal component analysis (PCA) is applied to all shearlet subbands of HSI in order to reduce their dimension. Third, the spectral information of HSI and obtained spatial features are integrated and classified using subspace multinomial logistic regression (MLRsub). We evaluate the performance of the proposed method over University of Houston, USA and a rural one captured over Trento, Italy. The obtained results show that the proposed method can efficiently classify the joint hyperspectral and LiDAR images.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519547","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519547","Fusion;Hyperspectral Images;LiDAR;Classification;Shearlet transform","Laser radar;Hyperspectral imaging;Feature extraction;Support vector machines;Transforms;Principal component analysis","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;optical radar;principal component analysis;regression analysis;remote sensing by laser beam;remote sensing by radar;transforms","fusion method;spectral contents;spatial contents;hyperspectral images;HSI;height information;classification accuracy;spatial features;principal component analysis;shearlet subbands;spectral information;subspace multinomial logistic regression;nonsubsampled shearlet transform;light detection and ranging;joint hyperspectral and LiDAR image classification","","4","","8","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"An efficient method for text detection from indoor panorama images using Extremal Regions","Y. Liu; K. Zhang; J. Yao; T. He; Y. Liu; J. Tu","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, Hubei, P.R. China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, Hubei, P.R. China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, Hubei, P.R. China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, Hubei, P.R. China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, Hubei, P.R. China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, Hubei, P.R. China","2015 IEEE International Conference on Information and Automation","1 Oct 2015","2015","","","781","786","Text detection in complex real images, such as panorama images, remains great challenging in Computer Vision. A general method often focuses on the small test images with single background which makes it easier to do the detection and recognition. In this paper, we find a novel approach, as it can automatically deal with the indoor panorama images which contains distortion and illumination problems to extract the multi-scale trademark. Our method fuses edge information, color probability detection and geometric characteristics to segment the text and non-text part, and exploits Extremal Regions (ERs) which is robust to blur, illumination, color and texture variation to deal with low contrast text and find the accurate localization. Effectiveness of algorithm has been discussed in the experimental result section where the performance has been compared for different number of feature used.","","978-1-4673-9104-7","10.1109/ICInfA.2015.7279390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279390","Text Detection;Extremal Regions;indoor Panorama images;Multi-Scale trademark","Image edge detection;Feature extraction;Lighting;Image color analysis;Trademarks;Noise;Histograms","computer vision;feature extraction;image colour analysis;image fusion;image restoration;image texture;text detection","text detection;indoor panorama images;extremal regions;computer vision;multiscale trademark extraction;edge information;color probability detection;geometric characteristics;blur variation;illumination variation;color variation;texture variation","","2","","19","IEEE","1 Oct 2015","","","IEEE","IEEE Conferences"
"Experimental Comparison of Multi-Sharpening Methods Applied To Sentinel-2 MSI and Sentinel-3 OLCI Images","A. Alboody; M. Puigt; G. Roussel; V. Vantrepotte; C. Jamet; T. K. Tran","Univ. Littoral Côte d’Opale, Longuenesse, France; Univ. Littoral Côte d’Opale, Longuenesse, France; Univ. Littoral Côte d’Opale, Longuenesse, France; Univ. Littoral Côte d’Opale, Wimereux, France; Univ. Littoral Côte d’Opale, Wimereux, France; Univ. Littoral Côte d’Opale, Wimereux, France","2021 11th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Jul 2021","2021","","","1","5","Multi-spectral images are crucial to detect and to understand phenomena in marine observation. However, in coastal areas, these phenomena are complex and their analyze requires multi-spectral images with both a high spatial and spectral resolution. Unfortunately, no satellite is able to provide both at the same time. As a consequence, multi-sharpening techniques-a.k.a. fusion or super- resolution of multi-spectral and/or hyper-spectral images-were proposed and consist of combining information from at least two multi-spectral images with different spatial and spectral resolutions. The fused image then combines their best characteristics. Various methods-based on different strategies and tools-have been proposed to solve this problem. This article presents a comparative review of fusion methods applied to Sentinel-2 MSI (13 spectral bands with a spatial resolution ranging from 10 to 60 m) and Sentinel-3 OLCI (21 spectral bands with a spatial resolution of 300 m) images. Indeed, both satellites are extensively used in marine observation and, to the best of the authors' knowledge, the fusion of their data was partially investigated (and not in the way we aim to do in this paper). To that end, we provide both a quantitative analysis of the performance of some state-of-the-art methods on simulated images, and a qualitative analysis on real images.","2158-6276","978-1-6654-3601-4","10.1109/WHISPERS52202.2021.9484009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9484009","Image fusion;Remote sensing;Sentinel-2 MSI;Sentinel-3 OLCI;Simulations;Real data","Satellites;Statistical analysis;Conferences;Sea measurements;Distance measurement;Sensors;Spatial resolution","geophysical image processing;geophysical techniques;hyperspectral imaging;image fusion;image resolution;remote sensing","fusion methods;multi-sharpening techniques;multisharpening methods;multisharpening techniques;simulated images;Sentinel-3 OLCI;spectral bands;fused image;spectral resolutions;hyper-spectral images;spectral resolution;high spatial resolution;multispectral images;Sentinel-2 MSI","","2","","21","IEEE","19 Jul 2021","","","IEEE","IEEE Conferences"
"Object Detection in Remote Sensing Images entrenched Feature Fusion and Improved metric","B. Li; Z. Li; J. Shi; P. Liu","Taiyuan University of Technology School of Big Data, Jinzhong, Shanxi, China; Taiyuan University of Technology School of Big Data, Jinzhong, Shanxi, China; Taiyuan University of Technology School of Big Data, Jinzhong, Shanxi, China; Taiyuan University of Technology School of Big Data, Jinzhong, Shanxi, China","2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE)","21 Feb 2022","2022","","","197","202","The main aim of optical remote sensing image (ORSI) object detection is to ascertain the area as well as class of all targets in given images. Nowadays, object detection methods established from deep learning are progressively applied to the analysis of the information from ORSI. But because of the compound background as well as extensive object scale shown in ORSI, the difficulty of object detection has increased. There are still some challenges to be solved. First, when the two bounding boxes don't overlap, current indexes like intersection over union (IoU) cannot identify the distance between them. Secondly, the existing methods using convolutional neural network (CNN) could not take advantage of the characteristics of multi-level features so that these methods are not very good at recognizing small-sized objects. To deal with these above problems, this paper comes up with a ORSI object detection method established on multi-level feature fusion as well as improved boundary box regression compared to IoU. Firstly, a new measure called generalized intersection over union (GIoU) is practiced. That metric could measure the similarity betwixt two bounding boxes, regardless of whether they overlap or not. At the same time, we also directly use GIoU as loss. Lastly, a multi-level feature fusion structure is practiced and it is also integrated into the existing convolutional neural network. In this way, our method could take advantage of the multi-level features. Quantitative comparison of the addressed method with the baseline approach on large-scale dataset named Dior are conducted. The comparison with the most advanced method shows that our method has reached the most advanced performance.","","978-1-6654-0886-8","10.1109/ICCECE54139.2022.9712704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712704","object detection;ORSI;feature fusion;GIOU;CNN","Measurement;Deep learning;Object detection;Feature extraction;Optical imaging;Convolutional neural networks;Optical sensors","convolutional neural nets;feature extraction;geophysical image processing;image fusion;image representation;image segmentation;learning (artificial intelligence);object detection;regression analysis;remote sensing","remote sensing images entrenched feature fusion;optical remote sensing image object detection;extensive object scale;bounding boxes;IoU;multilevel features;small-sized objects;ORSI object detection method;improved boundary box regression;multilevel feature fusion structure;convolutional neural network;generalized intersection over union;large-scale dataset;Dior","","","","21","IEEE","21 Feb 2022","","","IEEE","IEEE Conferences"
"A Semantic Feature Extraction Method For Hyperspectral Image Classification Based On Hashing Learning","M. Zhao; C. Yu; M. Song; C. -I. Chang","Center for Hyperspectral Imaging in Remote Sensing (CHIRS), Dalian Maritime University, Dalian, China; Center for Hyperspectral Imaging in Remote Sensing (CHIRS), Dalian Maritime University, Dalian, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xian, China; Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore, MD, USA","2018 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","27 Jun 2019","2018","","","1","5","Aiming at extraction the semantic feature of hyperspectral image, a semantic feature extraction method based on supervised hashing learning is proposed in the paper. Firstly, a set of hash functions are defined based on hyperspectral target subspace constraint which take into account the locality and discriminability between classes. Secondly, a semantic subspace is obtained through discriminative learning algorithm by the label information of hyperspectral image. Finally, the sparse binary hash codes are obtained by eigenvector mapping which represents the semantic features of targets. In the method, hashing learning uses the similarity binary codes to express the similarity of the original hyperspectral spatial data, and it uses both spectral features and spatial neighborhood features, which leads to strong distinguishing ability on a certain class of hyperspectral image. The experimental on real hyperspectral images classification results show that the fusion of the extracted semantic features with the original hyperspectral features can effectively improve the classification accuracy.","2158-6276","978-1-7281-1581-8","10.1109/WHISPERS.2018.8747106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747106","Semantic Feature Extraction (SFE);Hashing Learning;Binary Codes;Hyperspectral Image classification","Feature extraction;Hyperspectral imaging;Semantics;Support vector machines;Hash functions;Integrated circuits","binary codes;feature extraction;hyperspectral imaging;image classification;image coding;image fusion;supervised learning","sparse binary hash codes;spectral features;spatial neighborhood features;extracted semantic features;semantic feature extraction method;hyperspectral image classification;supervised hashing learning;hash functions;hyperspectral target subspace constraint;semantic subspace;discriminative learning algorithm;similarity binary codes;eigenvector mapping;hyperspectral spatial data;hyperspectral features","","4","","13","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"Fusion multiscale superpixel features for classification of hyperspectral images","S. Li; B. Zhang; X. Jia; H. Wu","Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, China; Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, China; University of New South Wales, Canberra Campus, Australia; State Key Laboratory of Resources and Environment Information System, Chinese Academy of Sciences, China","2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Oct 2017","2016","","","1","4","A novel multiscale superpixel-based fusion classification approach is proposed for hyperspectral images in this study. Superpixels are considered as basic processing unit for spectral-spatial based classification. The proposed technique consists of three steps. In the first step, an improved superpixel segmentations are proposed to perform from coarse to fine scales for the original hyperspectral image. In the second step, features of each superpixel are used for classification at each scale. Multiple classifiers are trained for each scale separately. Finally, the multiscale classification is obtained via decision fusion. Experiments are presented for two hyperspectral images and compared with pixelwise and spectral-spatial classification based on segmentation approaches. The results demonstrate that the proposed method works effectively and the accuracy improvement with fusion of multiscale superpixel for hyperspectral classification.","2158-6276","978-1-5090-0608-3","10.1109/WHISPERS.2016.8071751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071751","hyperspectral image;classification;superpixel;segmentation;fusion","Hyperspectral imaging;Image segmentation;Feature extraction;Support vector machines","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image resolution","fusion multiscale superpixel features;hyperspectral images;novel multiscale superpixel;fusion classification approach;spectral-spatial based classification;improved superpixel segmentations;multiscale classification;segmentation approaches;hyperspectral classification","","1","","12","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"Graph-regularized coupled spectral unmixing for multisensor time-series analysis","N. Yokoya; X. X. Zhu; A. Plaza","Signal Processing in Earth Observation (SiPEO), Technische Universitat Munchen (TUM), Germany; Signal Processing in Earth Observation (SiPEO), Technische Universitat Munchen (TUM), Germany; Departrnent of Technology of Computers and Communications, University of Extremadura, Spain","2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Oct 2017","2016","","","1","5","A new methodology that solves unmixing problems involving a set of multisensor time-series spectral images is proposed in order to understand dynamic changes of the surface at a subpixel scale. The proposed methodology couples multiple unmixing problems via regularization on graphs between the multisensor time-series data to obtain robust and stable unmixing solutions beyond data modalities owing to different sensor characteristics and the effects of non-optimal atmospheric correction. A synthetic dataset that includes seasonal and trend changes on the surface and the residuals of non-optimal atmospheric correction is used for numerical validation. Experimental results demonstrate the effectiveness of the proposed methodology.","2158-6276","978-1-5090-0608-3","10.1109/WHISPERS.2016.8071760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071760","Coupled spectral unmixing;multisensor data fusion;time-series analysis;change detection","Hyperspectral imaging;Couplings;Manifolds;Robustness;Concrete;Market research","graph theory;image fusion;spectral analysis;time series","data modalities;nonoptimal atmospheric correction;multisensor time-series analysis;multisensor time-series spectral images;graph-regularized coupled spectral unmixing","","","","19","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"Spatially Continuous and High-Resolution Land Surface Temperature Product Generation: A review of reconstruction and spatiotemporal fusion techniques","P. Wu; Z. Yin; C. Zeng; S. -B. Duan; F. -M. Göttsche; X. Ma; X. Li; H. Yang; H. Shen","Resources and Environmental Engineering, Anhui University, Hefei, China; Resources and Environmental Engineering, Anhui University, Hefei, China; Resources and Environmental Science, Wuhan University, Wuhan, China; Institute of Agricultural Resources and Regional Planning, Chinese Academy of Agricultural Sciences, Beijing, China; Institute of Meteorology and Climate Research, Forschungszentrum, Karlsruhe, Germany; Resources and Environmental Engineering, Anhui University, Hefei, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Institutes of Physical Science and Information Technology, Anhui University, Hefei, China; School of Resource and Environmental Sciences (SRES), Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Magazine","29 Sep 2021","2021","9","3","112","137","Remotely sensed land surface temperature (LST) with spatial continuity and high spatiotemporal resolution (hereafter referred to as high resolution) is a crucial parameter for studying the thermal environment and has important applications in many fields. However, adverse atmospheric conditions, sensor malfunctioning, and scanning gaps between orbits frequently introduce spatial discontinuities into satellite-retrieved LST products. For a single sensor, a tradeoff occurs between temporal and spatial resolutions; therefore, it is almost impossible to obtain images in high resolution.","2168-6831","","10.1109/MGRS.2021.3050782","National Key Research and Development Program(grant numbers:2018YFA0605500); National Natural Science Foundation of China(grant numbers:41501376,41871275); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9353399","","Land surface temperature;Spatial resolution;Land surface;Satellite broadcasting;Spatiotemporal phenomena;Remote sensing;Land surface temperature","atmospheric techniques;geophysical image processing;image fusion;image reconstruction;land surface temperature;remote sensing","spatiotemporal fusion techniques;land surface temperature;spatial continuity;high spatiotemporal resolution;adverse atmospheric conditions;spatial discontinuities;satellite-retrieved LST products;temporal resolutions;spatial resolutions","","37","","153","IEEE","11 Feb 2021","","","IEEE","IEEE Magazines"
"Building change detection in satellite stereo imagery based on belief functions","J. Tian; P. Reinartz; J. Dezert","Remote Sensing Technology Institute, German Aerospace Center, Germany; Remote Sensing Technology Institute, German Aerospace Center, Germany; ONERA - The French Aerospace Lab, France","2015 Joint Urban Remote Sensing Event (JURSE)","11 Jun 2015","2015","","","1","4","3D Building change detection has become a popular research topic along with the improvement of image quality and computer science. When only building changes are of interest, both the multi-temporal images and Digital Surface Models provide valuable but not comprehensive information in the change detection procedure. Therefore, in this paper, belief functions have been adopted for fusing information from these two sources. In the first step, two change indicators are proposed by focusing on building changes. Both indicators have been projected to a sigmoid curve, in which both the concordance and discordance indexes are considered. In order to fuse the concordance and discordance indexes and further fuse the two change indicators, two belief functions are considered. One is the original Dempster-Shafer Theory (DST), and the most recent one is Dezert-Smarandache Theory (DSmT). This paper shows how these belief-based frameworks can help in building change detection problem. Besides using different belief functions in obtaining the global BBAs, four decision-making criteria are tested to extract final building change masks. The results have been validated by compared to the manually extracted change reference mask.","2334-0932","978-1-4799-6652-3","10.1109/JURSE.2015.7120482","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120482","","Fuses","buildings (structures);decision making;geophysical image processing;image fusion;inference mechanisms;stereo image processing;uncertainty handling","satellite stereo imagery;belief function;3D building change detection;image quality;computer science;digital surface model;multitemporal imaging;sigmoid curve;discordance index;concordance index;Dempster-Shafer theory;DST;Dezert-Smarandache theory;DSmT;belief-based framework;global BBA;decision-making criteria;change reference mask extraction","","6","","16","IEEE","11 Jun 2015","","","IEEE","IEEE Conferences"
"Feature fusion of hyperspectral and lidar data using extinction profiles and total variation","P. Ghamisi; B. Rasti; X. X. Zhu","Signal Processing in Earth Observation, Technische Universität München (TUM); Faculty of Electrical and Computer Engineering, Ubiversity of Iceland; Signal Processing in Earth Observation, Technische Universität München (TUM)","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","2621","2624","To improve the classification of hyperspectral images, this paper proposes an approach for multi-sensor data fusion of LiDAR and hyperspectral data using extinction profiles and Orthogonal Total Variation Component Analysis (OTVCA). Results on the benchmark Houston data indicate the superior performance of the proposed approach compared to other approaches used in the experiments based on classification accuracies.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127532","Feature Fusion;Orthogonal Total Variation Component Analysis;Extinction Profiles;Random Forest;Support Vector Machines","Feature extraction;Laser radar;Support vector machines;Radio frequency;Hyperspectral imaging;Sensors","data handling;geophysical image processing;hyperspectral imaging;image classification;image fusion;optical radar;sensor fusion","hyperspectral images;hyperspectral data;extinction profiles;Orthogonal Total Variation Component Analysis;benchmark Houston data;classification accuracies;feature fusion;LiDAR data;multisensor data fusion;OTVCA","","","","9","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Multi-temporal images classification with evidential fusion of manifold alignment","M. Zhang; T. Liu; G. Gao; Y. Gu","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","819","822","Multi-temporal remote sensing images classification have attracted more and more attention in the last decade because of a wide range of applications of multi-temporal images in long-term environmental monitoring and land cover change detection and increasing multi-temporal data available. At present, most papers investigated two temporal remote-sensing images classification. In fact, there is lots of distinctive information to be unexploited between two or more temporal images which can enhance classification effect and improve ability of detecting change area. In this paper, we present an evidential fusion framework of manifold alignment to combine more than two multi-temporal remote sensing images. Embedding of multi-groups two temporal images pairs after MA can be intergraded based a layered structure of D-S theory. The proposed method was evaluated using five Landsat 8 images. Results confirmed that the proposed algorithm performed better than those with only two temporal images.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127078","Manifold alignment;Domain adaptation;Evidence theory;Multi-temporal classification","Manganese;Hafnium","geophysical image processing;image classification;image fusion;land cover;terrain mapping","multitemporal images classification;manifold alignment;multitemporal remote sensing images classification;long-term environmental monitoring;land cover change detection;remote-sensing images classification;temporal images;classification effect;evidential fusion framework;Landsat 8 images;multitemporal data;distinctive information","","","","4","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Multiple regularizations deep learning for paddy growth stages classification from LANDSAT-8","I. H. Ikasari; V. Ayumi; M. I. Fanany; S. Mulyono","Faculty of Computer Science, University of Indonesia, Depok, Indonesia; Faculty of Computer Science, University of Indonesia, Depok, Indonesia; Faculty of Computer Science, University of Indonesia, Depok, Indonesia; Center of Technology for Regional Resources Development (CTRRD), Agency for the Assessment and. Application of Technology (BPPT), Jakarta, Indonesia","2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS)","9 Mar 2017","2016","","","512","517","This study uses remote sensing technology that can provide information about the condition of the earth's surface area, fast, and spatially. The study area was in Karawang District, lying in the Northern part of West Java-Indonesia. We address a paddy growth stages classification using LANDSAT 8 image data obtained from multi-sensor remote sensing image taken in October 2015 to August 2016. This study pursues a fast and accurate classification of paddy growth stages by employing multiple regularizations learning on some deep learning methods such as DNN (Deep Neural Networks) and 1-D CNN (1-D Convolutional Neural Networks). The used regularizations are Fast Dropout, Dropout, and Batch Normalization. To evaluate the effectiveness, we also compared our method with other machine learning methods such as (Logistic Regression, SVM, Random Forest, and XGBoost). The data used are seven bands of LANDSAT-8 spectral data samples that correspond to paddy growth stages data obtained from i-Sky (eye in the sky) Innovation system. The growth stages are determined based on paddy crop phenology profile from time series of LANDSAT-8 images. The classification results show that MLP using multiple regularization Dropout and Batch Normalization achieves the highest accuracy for this dataset.","","978-1-5090-4629-4","10.1109/ICACSIS.2016.7872790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872790","Classification;Paddy Growth Stage;Remote Sensing;LANDSAT 8;Machine Learning;Deep Learning;Fast Dropout Training","Remote sensing;Training;Earth;Satellites;Biological neural networks;Indexes","convolution;crops;image classification;image fusion;learning (artificial intelligence);neural nets;remote sensing;time series","multiple regularization deep learning;paddy growth stage classification;Karawang District;West Java;Indonesia;multisensor remote sensing image;DNN;deep neural networks;1D CNN;1D convolutional neural networks;fast dropout regularizations;batch normalization;LANDSAT-8 spectral data samples;i-Sky Innovation system;paddy crop phenology profile;LANDSAT-8 image time series","","10","","20","IEEE","9 Mar 2017","","","IEEE","IEEE Conferences"
"A Dense Pointnet++ Architecture for 3D Point Cloud Semantic Segmentation","Y. Lian; T. Feng; J. Zhou","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","5061","5064","3D point cloud data has been widely used in remote sensing mapping because it is not affected by lighting, shadows and other factors. How to improve the performance of semantic segmentation of 3D point cloud data has attracted more and more attention. Previous works connected shallow features in encoders directly with deep features in decoders, which will lead to semantic gap. In this paper, we propose a Dense PointNet++ architecture, called DPNet, for semantic segmentation of 3D point cloud data. In order to weaken the semantic gap, multiple nested up-sampling layers and a series of cumulative, short and long skip link concatenation are introduced in the network to obtain more abundant point cloud features. Grid map and model fusion are used to further correct the results of network segmentation. The experimental results on US3D data set show that DPNet is superior to existing advanced architectures, especially for the categories with small samples. Moreover, DPNet with grid map and model fusion ranks the first place in 2019 IEEE GRSS Data fusion contest 3D point cloud classification challenge.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898177","3D point cloud semantic segmentation;DPNet;short and long skip link concatenation;grid map","Three-dimensional displays;Semantics;Buildings;Solid modeling;Feature extraction;Decoding;Data models","feature extraction;geophysical image processing;geophysical signal processing;image classification;image fusion;image segmentation;object detection;remote sensing;terrain mapping","Dense pointnet++ architecture;3D point cloud semantic segmentation;3D point cloud data;semantic gap;Dense PointNet++ architecture;point cloud features;model fusion;US3D data;grid map;IEEE GRSS Data fusion contest;3D point cloud classification challenge;DPNet","","9","","9","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Multi-Feature-Based Decision Fusion Framework for Hyperspectral Imagery Classification","S. Jia; J. Xian","College of Computer Science and Software Engineering, Shenzhen University, China; College of Computer Science and Software Engineering, Shenzhen University, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5","8","Classification of a hyperspectral image (HSI) is a very active topic in remote sensing, which has practical applications in many fields, In this paper, a multitask sparse logistic regression method based on multi-feature and decision fusion approach (MTSLR-MPGF) is proposed for cross-scene hyperspectral image classification. Specifically, the Gabor Features with certain orientations and morphology feature are utilized and used for hyperspectral imagery classification. Next, we used feature fusion methods in order to produce an accurate thematic map based on the remote sensed hyperspectral image classification. The extensive experiments on two real hyperspectral data sets have demonstrated superior performance of the proposed MTSLR-MPGF approach over the state-of-the-art methods in terms of overall accuracy and average accuracy.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518355","hyperspectral image;classification;multi-features;decision fusion","Training;Hyperspectral imaging;Feature extraction;Geoscience;Logistics","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;regression analysis;remote sensing","multifeature-based decision fusion framework;hyperspectral imagery classification;multitask sparse logistic regression method;decision fusion approach;cross-scene hyperspectral image classification;Gabor Features;morphology feature;remote sensed hyperspectral image classification;hyperspectral data sets;MTSLR-MPGF approach","","2","","11","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Improved Search and Detection of Surface-to-Air Missile Sites Using Spatial Fusion of Component Object Detections from Deep Neural Networks","A. B. Cannaday; C. H. Davis; G. J. Scott","Center for Geospatial Intelligence, University of Missouri, Columbia, MO, United States; Center for Geospatial Intelligence, University of Missouri, Columbia, MO, United States; Center for Geospatial Intelligence, University of Missouri, Columbia, MO, United States","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","9811","9814","Here we demonstrate how Deep Neural Network (DNN) detections of multiple constitutive or component objects that are part of a larger, more complex, and encompassing feature can be spatially fused to improve the search, detection, and retrieval (ranking) of the larger complex feature. Scores computed from a spatial clustering algorithm are normalized to a reference space so that they are independent of image resolution and DNN input chip size. DNN detections from multiple component objects can then be fused with or without human-expert provided weights to improve the retrieval (ranking) of DNN detections of a larger complex feature. We demonstrate the utility of this approach for broad area search and detection of Surface-to-Air Missile (SAM) sites that have a very low occurrence rate (only 16 sites) over a ~90,000 km2 study area in SE China. Our spatial fusion approach can be easily extended to a wide variety of other challenging object search and detection problems in large-scale remote sensing image datasets.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898397","Deep Neural Networks;Spatial Clustering;Broad Area Search;Information Retrieval","Detectors;Missiles;Training;Training data;Neural networks;Search problems;Feature extraction","geophysical image processing;image fusion;image resolution;image retrieval;learning (artificial intelligence);military computing;missiles;neural nets;object detection;pattern clustering;remote sensing","DNN detections;broad area search;Surface-to-Air Missile sites;spatial fusion;object search;component object detections;Deep Neural Network detections;object retrieval;spatial clustering algorithm;SAM sites;image resolution","","1","","4","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Feature-Level Loss for Multispectral Pan-Sharpening with Machine Learning","X. Liu; C. Deng; B. Zhao; J. Chanussot","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; GIPSA-Lab, Univ. Grenoble Alpes, Grenoble, France","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","8062","8065","Multispectral pan-sharpening plays an important role in providing earth observation with both high-spatial and high-spectral resolutions, and recently pan-sharpening with machine learning has been attracting broad interest. However, these algorithms minimizing the pixel-wise mean squared error, generally suffer from over-smoothed results that lack of high-frequency details in both spatial and spectral dimensions. In this paper, we propose to tackle this problem by shifting the learning loss from pixel-wise error to a higher-level feature loss. The new loss function, formulated by spatial structure similarity and spectral angle mapping, pushes the model to generate results that have similar feature representations with ground truth, rather than match with pixel-wise accuracy. Consequently, more realistic fusion results can be produced. Visual and quantitative analysis both demonstrate that our approach achieves better performance in comparison with state-of-the-art algorithms. Furthermore, experiments on high-level remote sensing task further confirm the superiority of the proposed method in real applications.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518692","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518692","Multispectral Pan-sharpening;Machine Learning;Feature-Level Loss;Spatial Structure Similarity;Spectral Angle Mapping","Task analysis;Spatial resolution;Loss measurement;Feature extraction;Machine learning;Machine learning algorithms","computer vision;feature extraction;geophysical image processing;image fusion;image representation;image resolution;learning (artificial intelligence);mean square error methods;minimisation;remote sensing;spectral analysis","feature-level loss;multispectral pan-sharpening;machine learning;earth observation;high-spectral resolutions;spatial dimensions;spectral dimensions;learning loss;pixel-wise error;loss function;spatial structure similarity;spectral angle mapping;feature representations;pixel-wise mean squared error minimization;visual analysis;quantitative analysis","","1","","8","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"An Interpretable Deep Neural Network for Panchromatic and Multispectral Image Fusion","D. Lei; X. Luo; L. Zhang; X. Li; Q. Liu; W. Li","Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; College of Computer, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China","2021 7th International Conference on Big Data and Information Analytics (BigDIA)","6 Dec 2021","2021","","","71","78","The fusion of panchromatic (PAN) and multispectral (MS) images aims to generate high-resolution multispectral images, also known as pansharpening. Among the pansharpening methods, deep learning-based methods have become the most popular solution. However, most deep learning-based methods have difficulty in making trade-offs between spectral and spatial information preservation as well as lack of interpretability. In this paper, we propose an interpretable deep neural network approach based on one observation model. Specifically, a hypothesis is proposed, which assumes that the high-resolution multispectral (HRMS) image is composed of the addition of the structural part and the spectral part, in which the spectral part is obtained by upsampling MS image. According to the observation model, we construct two variational models describing two type linear mapping relationship, one between HRMS and MS image, and another between HRMS and PAN image. The structural part is obtained by alternately solving the two variational models with the proximal gradient descent algorithm. Finally, We construct panchromatic and multispectral image fusion network (PMFNet) using deep unfolding method. PMFNet is interpretable with the iterative sovling steps of our proposed algorithm. Extensive experiments are conducted on GaoFen-2 and WorldView-2 datasets, which show that our proposed method is superior to the state of the art methods.","","978-1-6654-2466-0","10.1109/BigDIA53151.2021.9619729","National Natural Science Foundation of China(grant numbers:61936001,61972060,U1713213,62027827); National Key Research and Development Program of China(grant numbers:2019YFE0110800); Natural Science Foundation of Chongqing(grant numbers:cstc2020jcyj-zdxmX0025,cstc2019cxcyljrc-td0270); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9619729","interpretable;deep neural network;variational model;pansharpening","Deep learning;Learning systems;Pansharpening;Big Data;Visual effects;Data models;Iterative algorithms","geophysical image processing;gradient methods;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","upsampling MS image;observation model;variational models;structural part;multispectral image fusion network;panchromatic;high-resolution multispectral image;pansharpening methods;deep learning-based methods;spectral information preservation;spatial information preservation;interpretable deep neural network approach;spectral part","","","","28","IEEE","6 Dec 2021","","","IEEE","IEEE Conferences"
"Combining SAR-Based and Multispectral-Based Extractions to Map Urban Areas at Multiple Spatial Resolutions","A. Salentinig; P. Gamba","Dipartimento di Ingegneria Industriale e dell'Informazione, Universitá di Pavia, Pavia, Italy; Dipartimento di Ingegneria Industriale e dell'Informazione, Universitá di Pavia, Pavia, Italy","IEEE Geoscience and Remote Sensing Magazine","30 Sep 2015","2015","3","3","100","112","Urban remote sensing and data fusion are intimately connected, and this paper discusses recent developments in urban extent extraction using remotely sensed data with data sets or algorithms working at the global level. To achieve the results presented in this paper, several data fusion techniques at the raw data level, the feature level, and the decision level are designed and exploited. Specifically, multi-resolution data fusion techniques to exploit SAR data acquired in different modes, as well as fusion of information extracted from SAR and multispectral data are considered and evaluated.","2168-6831","","10.1109/MGRS.2015.2430874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284788","","Urban areas;Spatial resolution;Synthetic aperture radar;Data mining;Remote sensing;Feature extraction","feature extraction;image fusion;image resolution;radar imaging;remote sensing by radar;synthetic aperture radar;terrain mapping","multispectral-based extraction;urban area mapping;multiple spatial resolution;urban remote sensing;urban extent extraction;multiresolution data fusion technique;SAR","","22","","53","IEEE","30 Sep 2015","","","IEEE","IEEE Magazines"
"Fusionndvi: A Novel Fusion Method for NDVI in Remote Sensing","M. Zhang; Z. Zhao; Y. Chen; Z. Wang; X. Tian","Electronic Information School, Wuhan University, China; Department of Electronic and Computer Engineering, HKUST, Hong Kong; Electronic Information School, Wuhan University, China; School of Computer Science, Wuhan University, China; Electronic Information School, Wuhan University, China","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","4816","4820","Normalized difference vegetation index (NDVI) is widely utilized to examine vegetation coverage and estimate crop yield. To obtain a high-resolution (HR) NDVI, fusion techniques, which first generates a HR multispectral (MS) image by fusing a low-resolution (LR) MS image and a HR panchromatic image, and then calculates the HR NDVI based on the fused HR MS image, are utilized in previous studies. A HR vegetation index calculated on the basis of HR panchromatic image could provide HR spatial resolution, and this vegetation index has a spatial structure that is similar to that of NDVI. Therefore, this similarity is investigated to construct a novel method called FusionNDVI to improve the fusion performance in this study. The fusion problem is formulated to minimize a least square fitting error term and a nonlocal gradient sparsity regularization term. The fitting term is used to limit the difference between the fused HR NDVI and the LR NDVI, whereas the regularizer enforces a similar nonlocal spatial structure in the fused NDVI and the HR vegetation index. An efficient solving algorithm based on the augmented Lagrangian method of multipliers is derived. The superiority of the proposed FusionNDVI method over the state-of-the-art ones is verified via simulations.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9053062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9053062","NDVI;fusion;nonlocal gradient sparsity;ADMM","Visualization;Fitting;Vegetation mapping;Signal processing algorithms;Indexes;Speech processing;Spatial resolution","crops;geophysical image processing;image fusion;image resolution;vegetation;vegetation mapping","nonlocal gradient sparsity regularization term;LR NDVI;fused NDVI;HR vegetation index;FusionNDVI method;remote sensing;normalized difference vegetation index;vegetation coverage;estimate crop yield;high-resolution NDVI;fusion techniques;HR multispectral image;low-resolution MS image;HR panchromatic image;HR NDVI;fused HR MS image;HR spatial resolution;fusion problem;least square fitting error term;nonlocal spatial structure","","2","","21","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Improvement of spectral and spatial information using modified WAMM and modified bi-cubic interpolation method in non-subsampled contourlet transform domain","R. James; M. Vadivel","ETCE Department, Sathyabama University, Chennai, Tamil Nadu, India; ETCE Department, Sathyabama University, Chennai, Tamil Nadu, India","2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015]","16 Jul 2015","2015","","","1","5","Pan-sharpening is widely used as a tool to construct remotely sensed image, which is rich in both spectral and spatial contents. Panchromatic sharpening is used to improve the spatial resolution and provide an enhanced visualization of a multiband image using the high-resolutions, single-band image. The main scope of pan-sharpening is to maximize the spatial content simultaneously and minimize the spectral distortions which occur during processing. There are many approaches to implement pan-sharpening like principal component analysis (PCA), Intensity, hue and saturation (HIS), and WAVELET. The main drawback of these approaches is the distortion of spectral resolutions. However, Non-subsampled contourlet-transform (NSCT) approach is proved to be efficient. It is also very efficient in representing directional information and to capture intrinsic geometrical structure of the object. It combines features like high resolutions, shift-invariance and high-directionality. In this paper modified bi-cubic interpolation method and weighted average merging method (WAMM) are used. By using these methods the image issharper and there is no softening of details in the image, thus improving the quality. In the proposed paper it is possible to improve the efficiency of an image both spectrally and spatially with better efficiency compared to existing methods. The evaluation of efficiency of these methods is done by simulation, using performance metrics like RGE and Q4.","","978-1-4799-7075-9","10.1109/ICCPCT.2015.7159362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7159362","WAMM;NSCT;CT;HIS;GIS","Interpolation;Spatial resolution;Transforms;Remote sensing;Merging;Image fusion","geophysical image processing;image resolution;interpolation;principal component analysis;remote sensing;transforms;wavelet transforms","spectral information improvement;spatial information improvement;modified WAMM;modified bicubic interpolation method;nonsubsampled contourlet transform domain;remotely sensed image;panchromatic sharpening;spatial resolution improvement;multiband image;high-resolution-single-band image;spatial content maximization;spectral distortion minimization;principal component analysis;PCA;intensity-hue-and-saturation;HIS;wavelets;spectral resolutions;NSCT approach;directional information representation;intrinsic geometrical structure;shift-invariance;high-directionality;weighted average merging method;image issharper;image softening;image quality improvement;image efficiency improve;spectral analysis;spatial analysis;RGE performance metrics;Q4 performance metrics","","2","","12","IEEE","16 Jul 2015","","","IEEE","IEEE Conferences"
"Performance analysis of pansharpening algorithms for RASAT images","S. Kahraman; A. Ertürk","Kocaeli Üniversitesi İşaret ve Görüntü İşleme Laboratuvarı (KULIS), Kocaeli, Türkiye; Kocaeli Üniversitesi İşaret ve Görüntü İşleme Laboratuvarı (KULIS), Kocaeli, Türkiye","2017 25th Signal Processing and Communications Applications Conference (SIU)","29 Jun 2017","2017","","","1","4","Pansharpening approaches provide high performance for many image processing applications by combining the spectral information in multispectral or hyperspectral data with the high spatial resolution in PAN images. For this reason, there is a large number of approaches developed for pansharpening up until today. In this study, the most comprehensive performance comparison in terms of the number of pansharpening methods applied on images obtained from RASAT, which is the first imaging satellite designed and produced in our country and has been in use since 2011, is conducted.","","978-1-5090-6494-6","10.1109/SIU.2017.7960634","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960634","RASAT;pansharpening;performance analysis","Remote sensing;Spatial resolution;Principal component analysis;Image fusion;Satellites;Histograms;Additives","geophysical image processing;image resolution;remote sensing;spectral analysis","performance analysis;pansharpening algorithms;RASAT images;image processing applications;spectral information;multispectral data;hyperspectral data;high spatial resolution;PAN images;imaging satellite","","1","","24","IEEE","29 Jun 2017","","","IEEE","IEEE Conferences"
"Application of Pansharpening Algorithms for the Fusion of Raman and Conventional Brightfield Microscopy Images","C. Pomrehn; D. Klein; A. Kolb; P. Kaul; R. Herpers","Institute of Visual Computing, Bonn-Rhein-Sieg University of Applied Sciences, Germany; Institute of Safety and Security Research, Bonn-Rhein-Sieg University of Applied Sciences, Germany; Institute for Vision and Graphics, University of Siegen, Germany; Institute of Safety and Security Research, Bonn-Rhein-Sieg University of Applied Sciences, Germany; Faculty of Computer Science, University of New Brunswick, Canada","2018 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","27 Jun 2019","2018","","","1","5","This contribution investigates the application of established pansharpening algorithms for the fusion of hyperspectral images from Raman microspectroscopy and panchromatic images from conventional brightfield microscopy. Seven different methods based on multiresolution analysis and component substitution were applied and evaluated through visual assessment and quantitative quality measures at full and reduced resolution. The results indicate that, among the considered concepts, multiresolution methods are the more promising approaches for a physically and chemically meaningful fusion of the considered modalities. Here, pansharpening based on high-pass filtering led to the best results.","2158-6276","978-1-7281-1581-8","10.1109/WHISPERS.2018.8747082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747082","Hyperspectral image;pansharpening;image fusion;Raman microscopy;brightfield microscopy","Spatial resolution;Microscopy;Visualization;Distortion;Indexes;Principal component analysis","geophysical image processing;hyperspectral imaging;image fusion;image resolution;optical microscopy","hyperspectral images;panchromatic images;multiresolution analysis;component substitution;visual assessment;quantitative quality measures;multiresolution methods;chemical fusion;physical fusion;Raman microspectroscopy;high-pass filtering;pansharpening algorithms","","1","","14","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"Fuzzy-fusion approach for land cover classification","T. M. A. Santos; A. Mora; R. A. Ribeiro; J. M. N. Silva","Computational Intelligence Group of CTS/UNINOVA, FCT / NOVA University of Lisbon, Portugal; Computational Intelligence Group of CTS/UNINOVA, FCT / NOVA University of Lisbon, Portugal; Computational Intelligence Group of CTS/UNINOVA, FCT / NOVA University of Lisbon, Portugal; CCIAM - Climate Change Impacts, Adaptation & Modelling research group, CE3C, FCUL, Lisboa, Portugal","2016 IEEE 20th Jubilee International Conference on Intelligent Engineering Systems (INES)","29 Aug 2016","2016","","","177","182","The use of computational intelligent techniques for feature extraction and classification from earth observation satellite images, like Landsat multispectral images, can contribute to improve remote sensing analysis. Image fusion techniques are applied to fuse the spectral images into a higher-level image of the land cover distribution. In this paper we propose a fuzzy-fusion inference approach for satellite image classification based on a fuzzy process, which uses both a hybrid method to train the classifier and reinforcement aggregation operators in the inference scheme. The approach was tested with land cover maps for the district of Mandimba of the Niassa province, Mozambique and was validated against an expert classification and then with Decision trees and Artificial Neural Networks.","","978-1-5090-1216-9","10.1109/INES.2016.7555116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7555116","","Satellites;Histograms;Training;Remote sensing;Earth;Fitting;Vegetation mapping","decision trees;feature extraction;fuzzy reasoning;geophysical image processing;image classification;image fusion;neural nets;remote sensing","fuzzy-fusion approach;land cover classification;computational intelligent techniques;feature extraction;feature classification;earth observation satellite images;Landsat multispectral images;artificial neural networks;decision trees;Mozambique;Niassa province;Mandimba;inference scheme;reinforcement aggregation operators;fuzzy process;satellite image classification;fuzzy-fusion inference approach;land cover distribution;Image fusion techniques;remote sensing analysis","","5","","30","IEEE","29 Aug 2016","","","IEEE","IEEE Conferences"
"Improving spatial resolution of LANDSAT spectral bands from a single RGB image using artificial neural network","A. Marques; P. Rossa; R. K. Horota; D. Brum; E. M. de Souza; A. S. Aires; L. Kupssinskü; M. R. Veronez; L. Gonzaga; C. L. Cazarin","Advanced Vizualization & Geoinformatics Laboratory, Unisinos University, São Leopoldo, Brazil; Advanced Vizualization & Geoinformatics Laboratory, Unisinos University, São Leopoldo, Brazil; Advanced Vizualization & Geoinformatics Laboratory, Unisinos University, São Leopoldo, Brazil; Advanced Vizualization & Geoinformatics Laboratory, Unisinos University, São Leopoldo, Brazil; Advanced Vizualization & Geoinformatics Laboratory, Unisinos University, São Leopoldo, Brazil; Advanced Vizualization & Geoinformatics Laboratory, Unisinos University, São Leopoldo, Brazil; Advanced Vizualization & Geoinformatics Laboratory, Unisinos University, São Leopoldo, Brazil; Advanced Vizualization & Geoinformatics Laboratory, Unisinos University, São Leopoldo, Brazil; Advanced Vizualization & Geoinformatics Laboratory, Unisinos University, São Leopoldo, Brazil; CENPES Petrobras S.A, Rio de Janeiro, Brazil","2019 13th International Conference on Sensing Technology (ICST)","26 Mar 2020","2019","","","1","6","Spectral information provided by multispectral and hyperspectral sensors has a great impact on remote sensing studies. These sensors are embedded in aircrafts and satellites like the Landsat, which has more data freely available but lack the spatial resolution that suborbital sensors have. To increase the spatial resolution, a series of techniques have been developed like pansharpenning data fusion and more advanced convolutional neural networks for super-resolution, however, the later requires large datasets. To overcome this requirement, this work aims to increase the spatial resolution of Landsat spectral bands using artificial neural networks that uses pixel kernels of a single high-resolution image from Google Earth. Using this method, the high-resolution spectral bands were generated with pixel size of 1m in contrast to the 15m of pansharpenned Landsat bands. The evaluate the predicted spectral bands the validation measures Universal Quality Index (UQI) and Spectral Angle Mapper (SAM) were used, showing values of 0.98 and 0.16 respectively, presenting good results.","2156-8073","978-1-7281-4807-6","10.1109/ICST46873.2019.9047670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9047670","Landsat;high resolution;prediction;multispectral;Artificial Neural Networks;Spatial resolution","Earth;Remote sensing;Spatial resolution;Artificial satellites;Satellites;Google","convolutional neural nets;geophysical image processing;image colour analysis;image fusion;image resolution;remote sensing;spectral analysis","high-resolution spectral bands;pansharpenned Landsat bands;Spectral Angle Mapper;spatial resolution;LANDSAT spectral bands;single RGB image;artificial neural network;Spectral information;multispectral sensors;hyperspectral sensors;suborbital sensors;advanced convolutional neural networks;super-resolution;Landsat spectral bands;single high-resolution image;Universal Quality Index","","","","41","IEEE","26 Mar 2020","","","IEEE","IEEE Conferences"
"Hyperspectral image denoising with multiscale low-rank matrix recovery","Z. Huang; S. Li; F. Hu","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","5442","5445","Recently, low-rank matrix recovery has been demonstrated to be an effective tool in hyperspectral images (HSIs) denoising. However, the previous low-rank matrix recovery method with a window of the fixed-shape cannot adaptively exploit spatial structure information and nonlocal similarity. In this paper, multiscale low-rank matrix recovery (MC-LRMR) is proposed to recover HSI corrupted by different kinds of noise. The proposed method contains three main steps. First, HSI is transformed by the principal component analysis (PCA) algorithm and multiscale superpixel segmentation is applied to the first principal component, to segment the HSI into non-overlapping homogeneous regions. Then, the mixed noises, including Gaussian noise, impulse noise, dead lines noise, stripes noise, are removed by the low-rank matrix recovery (LRMR) in a superpixel-by-superpixel manner. Finally, a fusion rule, i.e., average operator, is adopted to combine denoising results of various scales, to acquire a fused noise-free estimation. Experiments on simulated and real HSI data sets can demonstrate the effectiveness of the proposed method.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128235","hyperspectral image;denoising;low-rank matrix recovery;image fusion","Noise reduction;Hyperspectral imaging;Image segmentation;Image denoising;Principal component analysis;Gaussian noise","Gaussian noise;hyperspectral imaging;image denoising;image fusion;image segmentation;impulse noise;matrix algebra;principal component analysis","hyperspectral image denoising;MC-LRMR;principal component analysis;PCA;mixed noises;Gaussian noise;impulse noise;dead lines noise;stripes noise;fused noise-free estimation;multiscale superpixel segmentation;low-rank matrix recovery method;HSI","","3","","11","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Joint Detection of Airplane Targets Based on Sar Images and Optical Images","J. Qin; H. Qu; H. Chen; W. Chen","Institute of Software, Liaoning Technical University, Huludao, China; Institute of Software, Liaoning Technical University, Huludao, China; Dept. of Information Engineering, Harbin Institute of Technology, Harbin, China; Dept. of Information Engineering, Harbin Institute of Technology, Harbin, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","1366","1369","In airplane target detection, there was the drawback of weak recognition ability for dark targets and high false alarm rate for detected targets. In order to address the problem, we proposed a detection method based on SAR and optical image feature fusion. It extracted texture, moment and backscattering characteristics from SAR images and combined with optical features. Moreover, the novel airplane edge templates incorporating SAR and optical images were created to acquire saliency map. During the process of detection, first, the saliency map and the One-Class-SVM (OCSVM) classifier were used to initially recognize the suspected airplane targets. Then, the combination features were adopted to further identify the misidentified airplane target. The experimental results showed that the Precision of the proposed method was 61.82% and the False Alarm Rate was 20%, which was better than the HIS-based detection method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900167","SAR Image;Optical Image;Airplane Object Detection;Image fusion","","feature extraction;image fusion;image texture;object detection;radar imaging;support vector machines;synthetic aperture radar","sar images;optical images;airplane target detection;weak recognition ability;dark targets;high false alarm rate;detected targets;detection method;optical image feature fusion;backscattering characteristics;saliency map;One-Class-SVM classifier;suspected airplane targets;combination features;misidentified airplane target;novel airplane edge templates;OCSVM classifier","","1","","8","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Fusion of Sentinel-2 Data with High Resolution Open Access Planet Basemaps for Grazing Lawn Detection in Southern African Savannahs","K. T. Awuah; P. Aplin","Department of Geography and Geology, Edge Hill University, Ormskirk, UK; Department of Geography and Geology, Edge Hill University, Ormskirk, UK","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","1409","1412","Short grass grazing lawn patches are significant components of habitat heterogeneity in southern African savannah ecosystems. Accurate maps of grazing lawn distribution is essential to enhance understanding of important ecosystem processes such as mega-herbivore population dynamics, nutrient cycling and plant community composition. The inherent heterogeneity of savannah landscapes however creates significant challenges for accurate discrimination of vegetation components and thus grazing lawn detection. Recent studies favour very high spatial resolution (VHR) multi-spectral imagery for dealing with this challenge. However, such data are costly for use in operational management. Planet Labs, through Norway's International Climate and Forests Initiative (NICFI), now grant free access to high-resolution, analysis-ready mosaics over the tropics, with great potential for fine-scale vegetation mapping. However, the spectral characteristics of these data are limited and fail to resolve the spectral similarity of different savannah vegetation components. We address these issues using Gram-Schmidt transformation to fuse Planet Basemaps and Sentinel-2A images for grazing lawn detection within the Lower Sabie region of Kruger National Park, South Africa. The original and fused images were classified using a random forest approach. Overall, the fused image achieved the best grazing lawn detection accuracy (0.85) and general map accuracy (0.72) results compared to Sentinel-2 (0.67 and 0.62) and Planet basemap (0.64 and 0.62 respectively). Our findings provide a foundation for cost-effective and accurate high spatial resolution vegetation mapping in heterogenous savannah landscapes. Further studies will investigate the potential of multi-temporal fused data and object-based approaches for enhanced savannah vegetation mapping","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554156","Edge Hill University; Royal Geographical Society; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554156","Image fusion;Sentinel-2;Planet basemaps;Random forest;savannah;grazing lawn","Planets;Open Access;Fuses;Ecosystems;Sociology;Vegetation mapping;Forestry","ecology;geophysical image processing;image fusion;image resolution;vegetation;vegetation mapping","Sentinel-2 data;high resolution open access Planet Basemaps;southern African savannahs;lawn patches;southern African savannah ecosystems;grazing lawn distribution;ecosystem processes;nutrient cycling;plant community composition;high spatial resolution multispectral imagery;fine-scale vegetation mapping;grazing lawn detection accuracy;Planet basemap;heterogenous savannah landscapes;multitemporal fused data;enhanced savannah vegetation mapping;high spatial resolution vegetation mapping;savannah vegetation components;International Climate and Forests Initiative;Kruger National Park","","1","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"GAF-Net: Improving the Performance of Remote Sensing Image Fusion using Novel Global Self and Cross Attention Learning","A. Jha; S. Bose; B. Banerjee","Indian Institute of Technology Bombay, India; Technical University of Munich, Germany; Indian Institute of Technology Bombay, India","2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)","6 Feb 2023","2023","","","6343","6352","The notion of self and cross-attention learning has been found to substantially boost the performance of remote sensing (RS) image fusion. However, while the self-attention models fail to incorporate the global context due to the limited size of the receptive fields, cross-attention learning may generate ambiguous features as the feature extractors for all the modalities are jointly trained. This results in the generation of redundant multi-modal features, thus limiting the fusion performance. To address these issues, we propose a novel fusion architecture called Global Attention based Fusion Network (GAF-Net), equipped with novel self and cross-attention learning techniques. We introduce the within-modality feature refinement module through global spectral-spatial attention learning using the query-key-value processing where both the global spatial and channel contexts are used to generate two channel attention masks. Since it is non-trivial to generate the cross-attention from within the fusion network, we propose to leverage two auxiliary tasks of modality-specific classification to produce highly discriminative cross-attention masks. Finally, to ensure non-redundancy, we propose to penalize the high correlation between attended modality-specific features. Our extensive experiments on five benchmark datasets, including optical, multispectral (MS), hyperspectral (HSI), light detection and ranging (LiDAR), synthetic aperture radar (SAR), and audio modalities establish the superiority of GAF-Net concerning the literature.","2642-9381","978-1-6654-9346-8","10.1109/WACV56688.2023.00629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10030780","Applications: Remote Sensing;Image recognition and understanding (object detection;categorization;segmentation;scene modeling;visual reasoning)","Representation learning;Laser radar;Limiting;Benchmark testing;Feature extraction;Optical imaging;Optical sensors","","","","","","54","IEEE","6 Feb 2023","","","IEEE","IEEE Conferences"
"GPU-Accelerated Adaptive Dictionary Learning and Sparse Representations for Multispectral Image Super-resolution","T. Barman; B. Deka; A. V. V. Prasad","Department of Electronics and Communication Engineering, Tezpur University, Tezpur, India; Department of Electronics and Communication Engineering, Tezpur University, Tezpur, India; Natational Remote Sensing Center (NRSC) ISRO Hyderabad, Telengana, India","2021 IEEE 18th India Council International Conference (INDICON)","1 Feb 2022","2021","","","1","7","Recently, single image super-resolution (SISR) based on sparse representations has been gaining much attention from the research community in the field of remote sensing. In this paper, a fast SISR reconstruction framework is developed for multispectral remote sensing (MSRS) images based on adaptive dictionary learning and sparse representations. It consists of two major parts: first, a novel super-resolution approach is developed for MSRS using sparse coding and adaptive dictionary learning. High-frequency features present in the input low-resolution MS image are extracted by using Butterworth low-pass, difference of Gaussian (DoG), and Sobel filters in horizontal and vertical directions. The proposed feature extraction method reveals the edges and other detailed information present in the MS image effectively. Secondly, massively parallel algorithms are designed for adaptive dictionary learning and sparse reconstruction using the Compute Unified Device Architecture (CUDA)-enabled General Purpose-Graphics Processing Unit (GP-GPU) programming model. The proposed method GP-GPU implementation not only gives better results in terms of visual quality and objective fidelity criteria, but also significantly reduces the computation time compared to its CPU counterparts to achieve near-real time operating speed.","2325-9418","978-1-6654-4175-9","10.1109/INDICON52576.2021.9691521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9691521","Super-resolution;Sparse representations;Adaptive dictionary learning;Multispectral remote sensing;CUDA-enabled GP-GPU","Visualization;Image edge detection;Superresolution;Machine learning;Programming;Feature extraction;Real-time systems","coprocessors;feature extraction;geophysical image processing;graphics processing units;image fusion;image reconstruction;image resolution;parallel algorithms;remote sensing","GPU-accelerated adaptive dictionary learning;sparse representations;multispectral image super-resolution;single image super-resolution;fast SISR reconstruction framework;multispectral remote sensing images;MSRS;sparse coding;input low-resolution MS image;feature extraction;sparse reconstruction;General Purpose-Graphics Processing Unit;Compute Unified Device Architecture;objective fidelity criteria;GP-GPU","","2","","32","IEEE","1 Feb 2022","","","IEEE","IEEE Conferences"
"Kohonen-Based Credal Fusion of Optical and Radar Images for Land Cover Classification","I. Hammami; J. Dezert; G. Mercier","Tunis Sciences Faculty, University of Tunis el Manar, Tunis, Tunisia; ONERA/DTIM/MSDA, The French Aerospace Lab, Palaiseau, France; CTO, eXo maKina, Digital Technologies, Paris, France","2018 21st International Conference on Information Fusion (FUSION)","6 Sep 2018","2018","","","1623","1630","This paper presents a Credal algorithm to perform land cover classification from a pair of optical and radar remote sensing images. SAR (Synthetic Aperture Radar) /optical multispectral information fusion is investigated in this study for making the joint classification. The approach consists of two main steps: 1) relevant features extraction applied to each sensor in order to model the sources of information and 2) a Kohonen map-based estimation of Basic Belief Assignments (BBA) dedicated to heterogeneous data. This framework deals with co-registered images and is able to handle complete optical data as well as optical data affected by missing value due to the presence of clouds and shadows during observation. A pair of SPOT-5 and RADARSAT-2 real images is used in the evaluation, and the proposed experiment in a farming area shows very promising results in terms of classification accuracy and missing optical data reconstruction when some data are hidden by clouds.","","978-0-9964527-6-2","10.23919/ICIF.2018.8455272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8455272","Land cover classification;remote sensing;image fusion;Kohonen map;belief functions","Optical imaging;Optical sensors;Adaptive optics;Self-organizing feature maps;Synthetic aperture radar;Laser radar;Feature extraction","feature extraction;geophysical image processing;image classification;image fusion;image reconstruction;image registration;land cover;remote sensing by radar;self-organising feature maps;synthetic aperture radar","Basic Belief Assignments;co-registered images;Kohonen-based Credal fusion;optical data;optical data reconstruction;SAR-optical multispectral information fusion;features extraction;SPOT-5 images;RADARSAT-2 images;Kohonen map-based estimation;Synthetic Aperture Radar;optical radar remote sensing images;Credal algorithm;land cover classification","","2","","29","","6 Sep 2018","","","IEEE","IEEE Conferences"
"A Probabilistic Framework for Fusing Classifications Derived From Multi-Temporal Hyperspectral Imagery","S. Schneider; R. J. Murphy; A. Melkumyan","Australian Centre for Field Robotics, University of Sydney, Sydney, NSW, Australia; Australian Centre for Field Robotics, University of Sydney, Sydney, NSW, Australia; Australian Centre for Field Robotics, University of Sydney, Sydney, NSW, Australia","2018 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","27 Jun 2019","2018","","","1","5","A new framework to fuse probabilistic classifications from replicate hyperspectral imagery of the same scene is presented. To improve scene classification accuracy, probabilistic outputs from a classifier, such as a Gaussian Process (GP), are fused. The framework allows fusion of several (n ≥ 2) images simultaneously or sequentially. The framework has been tested using hyperspectral imagery acquired from field-based platforms from a mine face at four different times during the day under different illumination conditions. Classification results of individual images showed large variations, however, using the fusion framework, the fused map showed a better agreement with the geology mapped in the field.","2158-6276","978-1-7281-1581-8","10.1109/WHISPERS.2018.8747266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747266","Gaussian Processes;Imaging Spectroscopy;Geology;Mining;Classification","Remote sensing;Indexes;Gaussian processes;Machine learning;Computer science;Data mining","Gaussian processes;geophysical image processing;image classification;image fusion","multitemporal hyperspectral imagery;probabilistic classifications;scene classification accuracy;probabilistic outputs;Gaussian Process;field-based platforms;mine face;fusion framework;fused map;hyperspectral imagery;illumination conditions","","","","11","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"Pan-Sharpening Based on Multilevel Coupled Deep Network","W. Cai; Y. Xu; Z. Wu; H. Liu; L. Qian; Z. Wei","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Science, Nanjing University of Science and Technology, Nanjing, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","7046","7049","Pan-sharpening is a common image-fusion method. To improve the quality of fused images, a multilevel deep learning Pan-sharpening method is proposed in this paper. In the training phase, we introduce Coupled Sparse Denoising Autoencorder (CSDA) to reconstruct high-Resolution (HR) multispectral (MS) image from low-Resolution (LR) MS image and HR Panchromatic (Pan) image. CSDA has four networks including LM-HP network, HR-MS network, feature mapping network and fine-tuning network. The hidden features in LM-HP network and HR-MS network as well as the mapping function between the two features are learned through joint optimization. In LM-HP and HR-MS networks, the hidden features of image patch pairs are extracted by the sparse autoencoder. A sparse denoising autoencoder is used to build the nonlinear mapping between the extracted features. In the testing phase, the LR-MS and HR-Pan images patches are fed to the CSDA network to reconstruct the fused HR-MS image. The experimental results show that the proposed method is better than the traditional pans-sharpening methods.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518121","Pan-sharpening;deep learning;sparse autoencoder;multilevel coupled networks","Feature extraction;Image reconstruction;Training;Neural networks;Noise reduction;Testing","feature extraction;image fusion;image representation;image resolution;learning (artificial intelligence);neural nets","LM-HP network;HR-MS network;feature mapping network;fine-tuning network;CSDA network;HR panchromatic image;multilevel coupled deep network;image-fusion method;multilevel deep learning pan-sharpening method;coupled sparse denoising autoencoder;high-resolution multispectral image;low-resolution multispectral image;HR-pan images patches;HR-MS image reconstruction;LR-MS image reconstruction;image quality;feature extraction","","4","","9","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"A Real-Time Imaging Processing Method Based on Modified RMA with Sub-Aperture Images Fusion for Spaceborne Spotlight SAR","F. Zhou; J. Yang; G. Sun; J. Zhang","School of Computer and Information, Hefei University of Technology, Hefei, China; School of Computer and Information, Hefei University of Technology, Hefei, China; National Key Lab of Radar Signal Processing, Xidian University, Xi'an, China; Key Laboratory of Aperture Array and Space Application, East China Research Institute of Electronic Engineering, Hefei, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1905","1908","The small satellite SAR has received increasing attention due to its flexibility and low cost. But limited by the data transmission technology, real-time transmission of a large amount of raw data generated by the spaceborne spotlight SAR can hardly be achieved. Meanwhile, the azimuth bandwidth of the spotlight mode is larger than the PRF, resulting in aliasing of the azimuth spectrum. Based on these, this paper proposes a real-time scheme for small satellite SAR with spotlight mode. The method can solve the problem of data transmission and eliminate spectrum overlap in Doppler domain by means of sub-aperture processing. The modified range migration algorithm (RMA) is used to perform range compression and range cell migration compensation (RCMC) on sub-aperture data. Then dechirp in the azimuth time domain is applied to obtain the low-resolution complex image focused in the range time-azimuth frequency domain. Finally, all theected onto a grid image with azimuth interval matching the azimuth full-resolution to complete image fusion.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324118","National Natural Science Foundation of China(grant numbers:61701156,2016M592045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324118","spaceborne spotlight SAR;real-time transmission;sub-aperture;real-time scheme","Azimuth;Spaceborne radar;Real-time systems;Synthetic aperture radar;Doppler effect;Imaging;Bandwidth","image fusion;image processing;image resolution;radar imaging;spaceborne radar;synthetic aperture radar","satellite SAR;flexibility;data transmission technology;real-time transmission;raw data;spaceborne spotlight SAR;azimuth bandwidth;spotlight mode;azimuth spectrum;real-time scheme;sub-aperture processing;modified range migration algorithm;range compression;range cell migration compensation;sub-aperture data;azimuth time domain;low-resolution complex image;range time-azimuth frequency domain;grid image;azimuth interval;azimuth full-resolution;complete image fusion;real-time imaging processing method;modified RMA;sub-aperture images fusion","","4","","12","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Robust Mutual Information-Based Multi-Image Registration","D. Liu; H. Mansour; P. T. Boufounos","Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","915","918","Image registration is of crucial importance in image fusion such as pan-sharpening. Mutual information (MI)-based methods have been widely used and demonstrated effectiveness in registering multi-spectral or multi-modal images. However, MI-based methods may fail to converge in searching registration parameters, resulting mis-registration. In this paper, we propose an outlier robust method to improve the robustness of MI-based registration for multiple rigid transformed images. In particular, we first generate registration parameter matrices using a MI-based approach, then we decompose each parameter matrix into a low-rank matrix of inlier registration parameters and a sparse matrix corresponding to outlier parameter errors. Results of registering multi-spectral images with random rigid transformations show significant improvement and robustness of our method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898834","Image registration;multi-spectral image;mutual information;sparsity constraint","Image registration;Sparse matrices;Mutual information;Registers;Robustness;Image resolution;Matrix decomposition","image fusion;image registration;matrix algebra","robust mutual information-based multiimage registration;image fusion;multimodal images;MI-based methods;outlier robust method;MI-based registration;inlier registration parameters;pan-sharpening;multiple rigid images transformation;parameter matrices registration;low-rank matrix;multispectral images registration","","3","","10","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"An image registration method based on the combination of multiple image features","G. -k. Wang; H. -p. Xu; H. Zhang","School of Electronics & Information Engineering, Beihang University, China; School of Electronics & Information Engineering, Beihang University, China; School of Electronics & Information Engineering, Beihang University, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","2803","2806","The image registration is one of the key steps to achieve three-dimensional (3D) localization and the other image fusion processes. This article presents a registration method based on the combination of edge feature and corner feature. The processing steps include image segmentation, corner detection, edge detection, extraction of interested region, and correspondence points matching. The algorithm flowchart and implementation of every step are presented. The proposed method makes the image registration with the multiple features of two images and avoids the inaccuracy of the SAR image information. Therefore, it has a better robustness and registration precision. A TerraSAR-X SAR image and a GeoEye-1 optical image of the Water Cube in Beijing are processed and the results validate the effectiveness of the proposed method.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729724","Image registration;combination;multiple image features;corner matching;edge matching","Image edge detection;Optical imaging;Image registration;Image segmentation;Transforms;Feature extraction;Image matching","geophysical techniques;image fusion;image registration;image segmentation;synthetic aperture radar","image registration method;multiple image features;three-dimensional localization;image fusion processes;edge feature;corner feature;image segmentation;corner detection;edge detection;correspondence point matching;algorithm flowchart;SAR image information;synthetic aperture radar;TerraSAR-X SAR image;GeoEye-1 optical image;water cube;Beijing;China","","2","","19","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Cloud removal with fusion of SAR and Optical Images by Deep Learning","J. Gao; H. Zhang; Q. Yuan","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China","2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp)","14 Oct 2019","2019","","","1","3","Due to the different imaging methods of SAR image and optical image, it is difficult to establish the corresponding relationship between them by traditional methods. However, with the development of deep learning, there are many researches on the transformation from SAR image to optical image based on GAN, which prove that the mapping between SAR image and optical image can be achieved. Based on this, this work will transform the SAR image into optical image and fuse to fill the cloud area of the optical image. This work will provide a method of heterogeneous image fusion to remove cloud, and get a good effect.","","978-1-7281-4615-7","10.1109/Multi-Temp.2019.8866939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8866939","Deep Learning;GAN;Cloud removal","Optical imaging;Optical sensors;Optical distortion;Radar polarimetry;Optical reflection;Adaptive optics;Gallium nitride","clouds;geophysical image processing;image fusion;optical images;radar imaging;synthetic aperture radar","optical image;deep learning;imaging methods;SAR image;heterogeneous image fusion;GAN","","1","","7","IEEE","14 Oct 2019","","","IEEE","IEEE Conferences"
"Banana Disease Detection by Fusion of Close Range Hyperspectral Image and High-Resolution Rgb Image","W. Liao; D. Ochoa; Y. Zhao; G. M. Villegas Rugel; W. Philips","Ghent University-IMEC, Ghent, Belgium; Escuela Superior Politecnica del Litoral, ESPOL, Guayaquil, Ecuador; School of Automation, Northwestern Polytechnical University, Xi’ An, China; Ghent University-FORSIT, Ghent, Belgium; Ghent University-IMEC, Ghent, Belgium","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1744","1747","Early detection of banana disease can limit the spread of disease, as well as reduce the treatment costs. Current methods focus on either manually interpretation or calculation of spectral indices (e.g., the normalized difference vegetation index). In this paper, we exploit the fusion of close range hyperspectral (HS) image and high-resolution (HR) visible RGB image for potential disease detection in banana leaves. Our approach applies the joint bilateral filter to transfer the textural structures of HR RGB image to low-resolution HS image and obtain an enhanced HS image. Initial experimental results on Musa acuminata (banana) leaf images demonstrate the efficiency of our fusion approach, with significant improvements over either single data source or some conventional methods.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519115","Close range hyperspectral image;data fusion;morphology;banana diseases","Diseases;Color;Hyperspectral imaging;Spatial resolution;Cameras","botany;diseases;filtering theory;image colour analysis;image enhancement;image fusion;image texture","treatment cost reduction;close range hyperspectral image fusion;early banana disease detection;high-resolution visible RGB image fusion;textural structures;enhanced HS image;low-resolution HS image;HR RGB image;joint bilateral filter;banana leaves;potential disease detection;normalized difference vegetation index;spectral indices;Musa acuminata leaf images","","6","","12","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Fusion of panchromatic and multispectral images for classification using the Chinese restaurant franchise with shaped tables","Ting Mao; Hong Tang; Shi He; Yang Shu; Jianjun Wu","Key Laboratory of Environment Change and Natural Disaster, Beijing Normal University, Beijing, China; Key Laboratory of Environment Change and Natural Disaster, Beijing Normal University, Beijing, China; Key Laboratory of Environment Change and Natural Disaster, Beijing Normal University, Beijing, China; Key Laboratory of Environment Change and Natural Disaster, Beijing Normal University, Beijing, China; Key Laboratory of Environment Change and Natural Disaster, Beijing Normal University, Beijing, China","2015 Joint Urban Remote Sensing Event (JURSE)","11 Jun 2015","2015","","","1","4","In this paper, we propose a novel framework to fuse both panchromatic (PAN) and multispectral (MS) images for classification under a Chinese restaurant franchise metaphor (CRF). In the metaphor of CRF, one kind of observations would be interpreted by using two sequent random processes: a customer randomly selects a table in a restaurant to sit and randomly selects a dish to eat for a newly occupied table. In our method, shaped tables (i.e., local semantic segments) are discovered from panchromatic images in the process of table selection. In the other process, a dish (i.e., an unsupervised class label) is allocated based on multispectral images for each table discovered in panchromatic images. This approach takes advantage of the rich spatial and spectral information in panchromatic and multispectral image respectively. The result indicates that the proposed algorithm outperforms these exiting state-of-art methods in all of the experiments.","2334-0932","978-1-4799-6652-3","10.1109/JURSE.2015.7120534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120534","","Image resolution","image classification;image fusion;random processes","panchromatic image fusion;multispectral image fusion;image classification;PAN imaging;MS imaging;CRF;Chinese restaurant franchise metaphor;random processing;local semantic segmentation;unsupervised class label","","","","8","IEEE","11 Jun 2015","","","IEEE","IEEE Conferences"
"Effective Fusion of Multi-Modal Data with Group Convolutions for Semantic Segmentation of Aerial Imagery","K. Chen; K. Fu; X. Gao; M. Yan; W. Zhang; Y. Zhang; X. Sun","Key Laboratory of Network Information System Technology, Institute of Electronics, Chinese Academy of Sciences, Beijing, P.R. China; Key Laboratory of Network Information System Technology, Institute of Electronics, Chinese Academy of Sciences, Beijing, P.R. China; Key Laboratory of Network Information System Technology, Institute of Electronics, Chinese Academy of Sciences, Beijing, P.R. China; Key Laboratory of Network Information System Technology, Institute of Electronics, Chinese Academy of Sciences, Beijing, P.R. China; Key Laboratory of Network Information System Technology, Institute of Electronics, Chinese Academy of Sciences, Beijing, P.R. China; Key Laboratory of Network Information System Technology, Institute of Electronics, Chinese Academy of Sciences, Beijing, P.R. China; Key Laboratory of Network Information System Technology, Institute of Electronics, Chinese Academy of Sciences, Beijing, P.R. China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3911","3914","In this paper, we achieve a semantic segmentation of aerial imagery based on the fusion of multi-modal data in an effective way. The multi-modal data contains a true orthophoto and the corresponding normalized Digital Surface Model (nDSM), which are stacked together before they are fed into a Convolutional Neural Network (CNN). Though the two modalities are fused at the early stage, their features are learned independently with group convolutions firstly and then the learned features of different modalities are fused at multiple scales with standard convolutions. Therefore, the multi-scale fusion of multi-modal features is completed in a single-branch convolutional network. In this way, the computational cost is reduced while the experimental results reveal that we can still get promising results.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899217","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899217","Semantic segmentation;aerial imagery;multi-modal data;deep learning;group convolution","Standards;Semantics;Image segmentation;Remote sensing;Convolutional neural networks;Computational efficiency;Vegetation","convolutional neural nets;feature extraction;image fusion;image segmentation","convolutional neural network;multiscale fusion;multimodal data fusion;single-branch convolutional network;semantic segmentation;aerial imagery;normalized digital surface model","","5","","17","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"A Markov Random Field model for decision level fusion of multi-source image segments","W. C. Olding; J. C. Olivier; B. P. Salmon","School of Engineering and ICT University of Tasmania, Australia; School of Engineering and ICT University of Tasmania, Australia; University of Tasmania, Hobart, TAS, AU","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","2385","2388","We present a method based on Markov Random Fields (MRFs) for conducting decision level fusion of segments derived from multiple images of the same region. These images are not required to share the same resolution or sensor characteristics. By working at the segment level we preserve the advantages of segment based image classification while also incorporating the benefits of using multiple image sources. Segment fusion is achieved by constructing a MRF graph over the segments with an edges connecting overlapping segments from different images. These edges penalize connected segments for taking different labels as a function of the degree of overlap. Experimentation on the fusion of Land-sat and SPOT5 imagery for classification of different forest types shows the ability of this method to deliver substantial improvements in classification accuracy.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326289","Graphical models;Image classification;Markov random fields;Sensor fusion;Vegetation mapping","Image segmentation;Remote sensing;Satellites;Earth;Accuracy;Noise measurement;Image resolution","geophysical image processing;image fusion;image resolution;image segmentation;Markov processes;random processes","Markov random field model;decision level fusion;multisource image segments;resolution characteristic;sensor characteristic;segment level;segment based image classification;multiple image sources;MRF graph;Land-sat imagery;SPOT5 imagery;classification accuracy","","4","","13","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Fusing Multi-Seasonal Sentinel-2 Images with Residual Convolutional Neural Networks for Local Climate Zone-Derived Urban Land Cover Classification","C. Qiu; M. Schmitt; X. X. Zhu","Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","5037","5040","This paper proposes a framework to fuse multi-seasonal Sentinel-2 images, with application on LCZ-derived urban land cover classification. Cross-validation over a seven-city study area in central Europe demonstrates its consistently better performance over several previous approaches, with the same experimental setup. Based on our previous work, we can conclude that decision-level fusion is better than feature-level fusion for similar tasks at similar scale with multi-seasonal Sentinel-2 images. With the framework, urban land cover maps of several cities are produced. The visualization of two exemplary areas shows urban structures that are consistent with existing datasets. This framework can be also generally beneficial for other types of urban mapping.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898223","Sentinel-2;Classification;residual convolutional neural network (ResNet);urban land cover;long short-term memory (LSTM)","Meteorology;Earth;Urban areas;Google;Remote sensing;Europe;Meters","geophysical image processing;image classification;image fusion;land cover;neural nets;terrain mapping","local climate zone-derived urban land cover classification;multiseasonal Sentinel-2 images;LCZ-derived urban land cover classification;urban land cover maps;urban structures;residual convolutional neural networks;central Europe;decision-level fusion","","4","","11","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Classification of fusing SAR and multispectral image via deep bimodal autoencoders","J. Geng; H. Wang; J. Fan; X. Ma","School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; National Marine Environmental Monitoring Center, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","823","826","Classification of multisensor data provides potential advantages over a single sensor in accuracy. In this paper, deep bimodal autoencoders are proposed for classification of fusing synthetic aperture radar (SAR) and multispectral images. The proposed deep network based on autoencoders is trained to discover both independencies of each modality and correlations across the modalities. Specifically, the sparse encoding layers in the front are applied to learn features of each modality, then shared representation layers in the middle are developed to learn fused features of two modalities, finally softmax classifier in the top is adopted for classification. Experimental results demonstrate that the proposed network is able to yield superior classification performance compared with some related networks.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127079","Data fusion;image classification;deep learning;synthetic aperture radar (SAR) image;multispectral image","Synthetic aperture radar;Feature extraction;Remote sensing;Data integration;Encoding;Kernel;Training","feature extraction;image classification;image fusion;neural nets;pattern classification;radar computing;radar imaging;sensor fusion;synthetic aperture radar","multispectral image classification;fusing SAR image classification;synthetic aperture radar;softmax classifier;multisensor data;fused features;sparse encoding layers;modality;deep network;multispectral images;deep bimodal autoencoders;single sensor","","4","","11","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Robustified smoothing for enhancement of thermal image sequences affected by clouds","P. Addesso; M. Longo; A. Maltese; R. Montone; R. Restaino; G. Vivone","Dipartimento di Ingegneria dell’ Informazione, Ingegneria Elettrica e Matematica Applicata, Università degli Studi di Salerno, Fisciano, SA, Italy; Dipartimento di Ingegneria dell’ Informazione, Ingegneria Elettrica e Matematica Applicata, Università degli Studi di Salerno, Fisciano, SA, Italy; Dipartimento di Ingegneria Civile Ambientale, Aerospaziale e dei Materiali, Università degli Studi di Palermo, Palermo, PA; Dipartimento di Ingegneria dell’ Informazione, Ingegneria Elettrica e Matematica Applicata, Università degli Studi di Salerno, Fisciano, SA, Italy; Dipartimento di Ingegneria dell’ Informazione, Ingegneria Elettrica e Matematica Applicata, Università degli Studi di Salerno, Fisciano, SA, Italy; North Atlantic Treaty Organization (NATO) Science and Technology Organization (STO) Centre for Maritime Research and Experimentation, La Spezia, Italy","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","1076","1079","Obtaining radiometric surface temperature information with both high acquisition rate and high spatial resolution is still not possible through a single sensor. However, in several earth observation applications, the fusion of data acquired by different sensors is a viable solution for so called image sharpening. A related issue is the presence of clouds, which may impair the performance of the data fusion algorithms. In this paper we propose a robustified setup for the sharpening of thermal images in a non real-time scenario, capable to deal with missing thermal data due to cloudy pixels, and robust with respect to cloud mask misclassifications. The effectiveness of the presented technique is assessed via numerical simulations based on SEVIRI data.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325956","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325956","Thermal Sharpening;Cloud Masking;Multitemporal Analysis;Bayesian Smoothing;Robustness","Clouds;Spatial resolution;Interpolation;Remote sensing;Robustness;Smoothing methods","data acquisition;geophysical image processing;image classification;image fusion;image resolution;image sequences;infrared imaging;land surface temperature;numerical analysis;radiometry","radiometric surface temperature information;high acquisition rate;high spatial resolution;single sensor;earth observation applications;data fusion algorithms;thermal image sharpening;nonreal-time scenario;thermal data;cloudy pixels;cloud mask misclassifications;numerical simulations;SEVIRI data;thermal image sequences","","3","","9","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Multiscale multisensor decision level data fusion for urban mapping","A. Salentinig; P. Gamba","Department of Industrial and Information Engineering, University of Pavia, Pavia, Italy; Department of Industrial and Information Engineering, University of Pavia, Pavia, Italy","2016 4th International Workshop on Earth Observation and Remote Sensing Applications (EORSA)","29 Aug 2016","2016","","","67","71","The combination of multiple maps of urban area extents is becoming more and more common, as the availability of multiple EO data sets is increasing constantly, and different teams develop efficient techniques to extract human settlements from SAR and optical data. The next challenge is the need to combine all of these maps using an approach suitable to multi-source but also multi-resolution data sets. This work provides a first step towards this aim, by considering a “pixel aggregation” technique which proves to be more effective than standard techniques to resample all data sets on a common grid at the intended spatial resolution. Results on Beijing using SAR data from three different sensors and multispectral data from Landsat are offered to prove the effectiveness of the proposed procedure.","","978-1-5090-1479-8","10.1109/EORSA.2016.7552768","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552768","urban mapping;multiscale data fusion;SAR;multispectral data","Spatial resolution;Synthetic aperture radar;Remote sensing;Urban areas;Earth;Satellites","geophysical image processing;image fusion;image resolution;image sampling;radar imaging;synthetic aperture radar;terrain mapping","multiscale multisensor decision level data fusion;urban mapping;urban area extent;human settlement extraction;SAR data;optical data;multiresolution data set;pixel aggregation technique;data set resampling;spatial resolution;Beijing;China;multispectral data;Landsat","","3","","12","IEEE","29 Aug 2016","","","IEEE","IEEE Conferences"
"MRTA: Multi-Resolution Training Algorithm for Multitemporal Semantic Change Detection","Q. Bao; Y. Liu; Z. Zhang; D. Chen; Y. Yang; L. Jiao; F. Liu","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian, Shaanxi Province, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2062","2065","The multitemporal semantic change detection challenge track (Track MSD) in the 2021 Data Fusion Contest is to extract the land cover changes of the US state of Maryland from 2013 to 2017, but only low-resolution label data is provided. We present a multi-resolution training algorithm (MRTA) to alleviate the overfitting of the model on the coarse labels. First, using low-resolution coarse labels to train FCN, the average IoU can reach 0.5253. Generating pseudo-labels using this network, they are combined with coarse labels to form a multi-resolution label combination, and perform iterative fine-tuning and retrain. After that, by analyzing the loss and gain indicators of specific categories, it was found that the discrimination effect of the water area was poor, so a strong classifier was trained for the water area. We also implemented strategies such as model voting and weighted training to improve model performance. Finally, our method achieves 0.6445 mIoU on the test set, ranking 3rd in Track 2 of the IEEE Data Fusion Contest of 2021.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554425","semantic change detection;pseudo-labels;multi-resolution label","Training;Change detection algorithms;Semantics;Data integration;Geoscience and remote sensing;Classification algorithms;Data mining","image classification;image fusion;image resolution;iterative methods","MRTA;multiresolution training algorithm;multitemporal semantic change detection;Track MSD;data fusion;land cover;low-resolution label data;low-resolution coarse labels;multiresolution label combination","","2","","17","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A Cross-Scale Loss for CNN-Based Pansharpening","S. Vitale; G. Scarpa","Dipartimento di Ingegneria, Università degli Studi di Napoli Parthenope; Dipartimento di Ing. Elettrica e delle Tecnologie dell'informazione, Università Federico II di Napoli","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","645","648","To cope with the lack of input-output training samples, deep learning (DL) methods for pansharpening usually resort to Wald's protocol or other similar downscaling processes. By doing so, the scaled versions of the multispectral (MS) and panchromatic (PAN) components serve as input while the original MS plays as output during the training phase. As a side effect, the informational gap between reduced and full scales causes a mismatch between the training and test phases. In fact, DL methods typically provide a pretty good performance at reduced scale, with a good margin over traditional solutions that tends to vanish in the full-resolution framework. In this work, we propose a training framework that involves both the reduced and the full scale versions of the multiresolution image samples. This is achieved thanks to a suitably defined loss which comprises costs for both scales. Our numerical and visual experimental results confirm that the proposed approach provides an improved performance in the full-resolution case.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324219","Pansharpening;super-resolution;convolutional neural network;data fusion;machine learning","Training;Pansharpening;Image resolution;Spatial resolution;Remote sensing;Sensors;Noise measurement","geophysical image processing;image fusion;image resolution;image sampling;learning (artificial intelligence)","suitably defined loss;multiresolution image samples;scale versions;training framework;full-resolution framework;good margin;reduced scale;pretty good performance;DL methods;test phases;informational gap;training phase;original MS;scaled versions;similar downscaling processes;Wald's protocol;resort;deep learning methods;input-output training samples;CNN-based;cross-scale loss","","2","","17","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"PTGAN: A Proposal-Weighted Two-Stage GAN with Attention for Hyperspectral Target Detection","H. Qin; W. Xie; Y. Li; K. Jiang; J. Lei; Q. Du","State Key Lab. of Integrated Services Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Services Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Services Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Services Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Services Networks, Xidian University, Xi'an, China; The Department of Electrical and Computer Engineering, Mississippi State University, USA","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4428","4431","In this paper, a proposal-weighted two-stage generative adversarial network (GAN) with attention mechanism is proposed for hyperspectral target detection (HTD). PTGAN leverages GAN to estimate spectral background distribution and realize mapping from the latent space to the spectral space. Meanwhile, PTGAN conducts the reversed mapping through latent-spectral-latent and spectral-latent-spectral learning. On this basis, PTGAN implements accurate reconstruction of background spectrum via latent space. Therefore, targets of interest can be detected through larger pixel-level reconstruction error. In particular, the variance attention module is designed to make full use of global information among spectral bands to selectively emphasize channel-wise spectral features. Furthermore, a proposal-weighted strategy in a two-stage manner reduces the false alarm of detection by refining the previous detection proposal. Finally, exponential nonlinear fusion combines the discriminative feature from two stages to suppress the background. Extensive experiments on two real hyperspectral images (HSIs) verify the effectiveness of PTGAN.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553721","National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); China Postdoctoral Science Foundation(grant numbers:2019T120878,2017M620440); Fundamental Research Funds for the Central Universities(grant numbers:JB180104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553721","Hyperspectral target detection;generative adversarial network;background reconstruction;two-stage","Refining;Geoscience and remote sensing;Object detection;Generative adversarial networks;Feature extraction;Proposals;Data mining","feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;learning (artificial intelligence);neural nets;object detection","reversed mapping;spectral-latent-spectral learning;latent space;pixel-level reconstruction error;variance attention module;spectral bands;channel-wise spectral features;detection proposal;hyperspectral images;proposal-weighted two-stage GAN;hyperspectral target detection;attention mechanism;PTGAN;spectral background distribution;spectral space;proposal-weighted two-stage generative adversarial network;HTD;latent-spectral-latent learning;background spectrum reconstruction;exponential nonlinear fusion;discriminative feature;HSIs","","2","","8","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Feature Correlation Analysis of Two-Branch Convolutional Networks for Multi-Source Image Classification","X. Liu; L. Jiao; F. Liu; X. Hou; D. Zhang","WeBank, ShenZhen, Shaanxi Guangdong, China; WeBank, ShenZhen, Shaanxi Guangdong, China; WeBank, ShenZhen, Shaanxi Guangdong, China; WeBank, ShenZhen, Shaanxi Guangdong, China; WeBank, ShenZhen, Shaanxi Guangdong, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","936","939","With the development of multi-sensor imaging technology, constructing a multi-branch network model has become an important requirement for fusion decision. In the literature, many two-branch networks are proposed to interpret multisensor data and also get the satisfactory results. In this paper, we divide these models into two types and study them by mining changes in data and features. The main method used is analysis the correlation between the features of the same layer from the first branch and the second branch. The task of dual-source image classification serves as a means of experimentation. When the classification network is trained, the features of each layer are extracted for experimental analysis. Extensive experiments and analysis on the dataset IEEE_grss_dfc_2017 show that the analysis is meaningful. It is found that the correlation of the features from the low level to the high level is more and more consistent, a quantitative analysis is given in this paper.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324476","State Key Program of National Natural Science of China(grant numbers:61836009,91438201,91438103); National Natural Science Foundation of China(grant numbers:61801351); National Science Basic Research Plan in Shaanxi Province of China(grant numbers:2018JQ6018); Fundamental Research Funds for the Central Universities(grant numbers:XJS17108); China Postdoctoral Fund(grant numbers:2019M663641); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324476","multi-source;analyzing;two-branch convolutional networks;feature correlation","Correlation;Remote sensing;Laser radar;Feature extraction;Earth;Vegetation mapping;Spatial resolution","data mining;feature extraction;geophysical image processing;image classification;image fusion;sensor fusion","multisource image classification;multisensor imaging technology;multibranch network model;fusion decision;two-branch networks;multisensor data;mining changes;dual-source image classification;classification network;quantitative analysis;feature correlation analysis;two-branch convolutional networks","","1","","11","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Detection of Changes in Built-Up Areas with a Fully Convolutional Network in the Context of the European Settlement Map","C. Corbane; F. Sabo; P. Politis; V. Syrris","European Commission, Joint Research Centre (JRC), Ispra, Italy; Arhs Developments S.A, Belvaux, Luxembourg; Arhs Developments S.A, Belvaux, Luxembourg; European Commission, Joint Research Centre (JRC), Ispra, Italy","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","6793","6796","This work presents a novel method for detecting newly built-up areas in Very High Resolution satellite imagery using Fully Convolutional Neural Networks. The architecture builds on Early Fusion concept where the pairs of image patches are concatenated before inputting into the network as different color channels. The model was trained on different sensors (Pléiades, Kompsat and Superview) using the labels from the Urban Atlas change maps (2012-2018). The results demonstrate the potential of the proposed method for a pan-European mapping of changes in built-up areas at a spatial resolution of 2 meters in the framework of the European Settlement Map of the Joint Research Centre.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553247","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553247","Change detection;Fully Convolutional Neural Network;European Settlement Map;Copernicus;Built-up areas","Meters;Image sensors;Satellites;Image color analysis;Europe;Training data;Geoscience and remote sensing","cartography;convolutional neural nets;geophysical image processing;image colour analysis;image fusion;image resolution","very high resolution satellite imagery;fully convolutional neural networks;image patches;color channels;Urban Atlas change maps;spatial resolution;European Settlement Map;change detection;early fusion concept;Pleiades sensor;Kompsat sensor;Superview sensor","","1","","11","EU","12 Oct 2021","","","IEEE","IEEE Conferences"
"Multi Scale Ship Detection Based on Attention and Weighted Fusion Model for High Resolution SAR Images","L. Zhang; Z. Chu; B. Zou","Dept. of Information Engineering, Harbin Institute of Technology, Harbin, China; Dept. of Information Engineering, Harbin Institute of Technology, Harbin, China; Dept. of Information Engineering, Harbin Institute of Technology, Harbin, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","631","634","Ship detection in SAR images is a challenging problem. CNN-based ship detection method in SAR images has achieved remarkable results. Due to the multi scale of the ships and interference from complex sea conditions or nearshore background in SAR images, many false alarms and missed detections can occur in ship detection. To solve these problems, a multi-scale ship detection network in SAR images based on attention and weighted fusion is proposed in this paper. First, a higher-resolution detect head is added based on the YOLOv5 framework for detecting tiny-scale ships in SAR images. Then, the coordinate attention block is introduced to refine the location features of ship targets and suppress the interference of complex background. Finally, in the feature fusion stage, adaptive weighted feature fusion is used to reduce feature redundancy. Experiments on the SSDD dataset show the effectiveness of the proposed method.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883844","National Natural Science Foundation of China(grant numbers:61871158); Aeronautical Science Foundation of China(grant numbers:20182077008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883844","Synthetic Aperture Radar (SAR);ship detection;multi -scale;coordinate attention;weighted feature fusion","Head;Image resolution;Redundancy;Semantics;Geoscience and remote sensing;Interference;Detectors","feature extraction;image fusion;object detection;radar imaging;ships;synthetic aperture radar","weighted fusion model;high resolution sar images;CNN-based ship detection method;missed detections;multiscale ship detection network;higher-resolution detect head;tiny-scale ships;ship targets","","1","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Hyperspectral Image Super-Resolution Based on Multiscale Residual Block and Multilevel Feature Fusion","G. Yu; F. Zhang; T. Hu; W. Li; R. Tao","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2170","2173","Hyperspectral images have high spectral resolution, but this is often at the expense of spatial resolution. Although deep learning-based super-resolution (SR) algorithms have shown comparative performance for spatial resolution enhancement, most of them cannot effectively extract features of different size objects because of single scale convolution. In deep architectures, low level features also tend to disappear during transmission. In this paper, an efficient network (MRBMFF) for enhancing the spatial resolution of hyperspectral image is proposed. Based on the multiscale residual block (MRB), features at different scales can be effectively extracted and fused. Meanwhile, the multilevel feature fusion (MFF) is introduced to concatenate the low and high level features. Effective SR images could be recovered after inputting their low-resolution counterparts to the proposed network. Experimental results show that the proposed network achieves superior reconstruction performance compared with the state-of-the-art approaches.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553552","Beijing Natural Science Foundation(grant numbers:L191004); National Natural Science Foundation of China(grant numbers:61731023,61922013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553552","hyperspectral imagery;super-resolution;multiscale residual block;feature fusion","Convolution;Superresolution;Geoscience and remote sensing;Deep architecture;Feature extraction;Spatial resolution;Image reconstruction","deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;image reconstruction;image resolution","SR images;hyperspectral image super-resolution;multiscale residual block;multilevel feature fusion;high spectral resolution;deep learning-based super-resolution algorithms;spatial resolution enhancement;size objects;single scale convolution;deep architectures","","1","","11","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Multi-Scale Feature Fusion for Hyperspectral and Lidar Data Joint Classification","M. Zhang; F. Gao; J. Dong; L. Qi","Institute of Marine Development, Ocean University of China; Institute of Marine Development, Ocean University of China; Institute of Marine Development, Ocean University of China; Institute of Marine Development, Ocean University of China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2856","2859","To exploit the multi-scale information to improve the feature representation, we propose a multi-scale feature fusion network for hyperspectral image (HSI) and LiDAR data joint classification. The model is comprised of three parts: feature extraction module, feature fusion module, and MSF(Multi-Scale Fusion) module. The feature fusion module integrates the multi-modal information into the two inputs by fusing attention. MSF module integrates richer semantic information by integrating multi-scale information to improve performance. Experimental results show that the proposed method is effective in multi-modal data joint classification.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884168","National Key Research and Development Program of China(grant numbers:2018AAA0 100602); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884168","hyperspectral image;LiDAR;cross-modal data fusion;classification","Laser radar;Semantics;Geoscience and remote sensing;Feature extraction;Hyperspectral imaging","feature extraction;hyperspectral imaging;image classification;image fusion;image representation","hyperspectral data joint classification;multiscale information;feature representation;multiscale feature fusion network;feature extraction module;feature fusion module;MultiScale Fusion;multimodal information;MSF module;semantic information;multimodal data joint classification;Lidar data joint classification","","1","","12","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Multiple feature fusion using a multiset aggregated canonical correlation analysis for high spatial resolution satellite image scene classification","D. Lin; X. Xu; F. Pu","Signal Processing Laboratory, Wuhan University, Wuhan, China; Signal Processing Laboratory, Wuhan University, Wuhan, China; Wireless Communication and Sensor Network Laboratory, Wuhan University, Wuhan, China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","481","484","This paper presents a novel classification method for high-spatial-resolution satellite scene classification introducing multiset aggregated canonical correlation analysis (MACCA)-based feature fusion to fuse and combine multiple features. Firstly, a superpixel representation of the scene is constructed by employing a high-efficiency linear iterative clustering algorithm. After that, three diverse and complementary visual descriptors are extracted to characterize each superpixel. For taking full advantage of multiset features to yield the effective discriminant information and eliminating the redundancy between multiset features to some extent, MACCA is performed on three different feature sets to acquire fused feature for classification. Experimental analysis on high-spatial-resolution satellite scenes reveals that the suggested method achieves exceedingly promising performance and surpasses other off-the-shelf methods in classification accuracy.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325805","Feature fusion;scene classification;canonical correlation analysis (CCA)","Correlation;Satellites;Feature extraction;Remote sensing;Spatial resolution;Accuracy;Image color analysis","image classification;image fusion;pattern clustering","high-spatial-resolution satellite image scene classification accuracy;multiset aggregated canonical correlation analysis-based feature fusion;superpixel representation;high-efficiency linear iterative clustering algorithm;visual descriptors;multiset features;off-the-shelf methods;multiple feature fusion","","","","12","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"A Graph-Based Textural Superpixel Segmentation Method for Pansharpening Application","H. Hallabia; H. Hamam","LIS, GMOD, Aix Marseille Université, France; Faculty of Engineering, Université de Moncton, NB, Canada","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2640","2643","In this paper, a graph-based pan-sharpening technique is proposed which is processed on multiple regions defined as a graph-based superpixels generated from texture descriptor of PAN image. To this end, the PAN texture is to over-segmented into a set of superpixels, which are considered as initialization for the Region Adjacency Graph (RAG). Then, the fusion process is accomplished by inferring details into their corresponding up-sampled MS at feature level guided by the graph-based textural segmentation map. Our proposal has been evaluated using two datasets acquired by the WorldView-2 and WorldView-4 sensors. Its performance is clearly demonstrated in comparison with several state-of-the-art pansharpening methods both at pixel and region levels.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553304","Texture;Superpixels;Region Adjacency Graph (RAG);Pansharpening;Graphical representation","Image segmentation;Geoscience and remote sensing;Pansharpening;Sensors;Proposals","geophysical image processing;graph theory;image colour analysis;image fusion;image resolution;image segmentation;image texture","texture descriptor;PAN image;PAN texture;Region Adjacency Graph;fusion process;graph-based textural segmentation map;state-of-the-art pansharpening methods;region levels;Graph-based textural superpixel segmentation method;pansharpening application;graph-based pan-sharpening technique;multiple regions;graph-based superpixels","","","","14","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A Temporal-Spectral Generative Adversarial Fusion Network for Improving Satellite Hyperspectral Temporal Resolution","K. Ren; W. Sun; J. Zhou; X. Meng; G. Yang; J. Peng","Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Faculty of Mathematics and Statistics, Hubei University, Wuhan, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","899","902","The improvement of temporal resolution of hyperspectral (HS) data is a fundamental and challenging problem. In this paper, we propose a Temporal-Spectral fusion method based on Generative Adversarial Network (TSF-GAN). First, the generator is used to train the nonlinear relationship between multispectral (MS) and HS data pairs at time T1 and T3, and we map the relationship to the MS data at T2 to obtain the HS data. Second, the discriminator is used to identify whether the differential image of HS data at different times is consistent with that of MS data, and whether the HS data at time T2 after spectral down-sampling is consistent with that of MS data at time T2. Preliminary experimental results demonstrate that the proposed TSF-GAN achieves comparative fidelity and has strong practicability.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884778","National Natural Science Foundation of China(grant numbers:42122009,42171351,41971296,61871177,42171326,41801256,41801252); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884778","Temporal-Spectral fusion;TSF-GAN;hyperspectral;multispectral","Satellites;Image resolution;Geoscience and remote sensing;Feature extraction;Generative adversarial networks;Generators;Hyperspectral imaging","hyperspectral imaging;image fusion;image resolution;neural nets","TSF-GAN;data pairs;MS data;HS data;spectral down-sampling;hyperspectral data;temporal-spectral generative adversarial fusion network;satellite hyperspectral temporal resolution;differential image","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Location Aware Super-Resolution for Satellite Data Fusion","O. Adigun; P. A. Olsen; R. Chandra","Dept. of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA; Microsoft Research Research For Industry, Redmond, WA; Microsoft Research Research For Industry, Redmond, WA","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","3758","3761","Satellite data fusion involves images with different spatial, temporal, and spectral resolution. These images are taken under different illumination conditions, with different sensors and atmospheric noise. We use classic super-resolution algorithms to synthesize commercial satellite images (Pléiades) from a public satellite source (Sentinel-2). Each super-resolution method is then further improved by adaptive sharpening to the location by use of matrix completion (regression with missing pixels). Finally, we consider ensemble systems and a residual channel attention dual network with stochastic dropout. The resulting systems are visibly less blurry with higher fidelity and yield improved performance.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884391","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884391","Super-resolution;matrix completion;cloud removal","Location awareness;Image sensors;Satellites;Superresolution;Data integration;Lighting;Geoscience and remote sensing","artificial satellites;geophysical image processing;image fusion;image resolution;image restoration;sensor fusion","satellite data fusion;different spatial;spectral resolution;different illumination conditions;atmospheric noise;super-resolution algorithms;commercial satellite images;public satellite source;super-resolution method;location aware super-resolution","","","","12","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"SOF-UNet: SAR and Optical Fusion Unet for Land Cover Classification","D. Zhang; M. Gade; J. Zhang","Fachbereich Informatik, Universität Hamburg, Hamburg, Germany; Universität Hamburg, Institut für Meereskunde, Hamburg, Germany; Fachbereich Informatik, Universität Hamburg, Hamburg, Germany","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","907","910","We propose a SAR and Optical Fusion Network based on the UNet framework (SOF-UNet) for multi-modal land cover classification. The two-stream SOF-UNet consists of three parts: two encoders to extract features, a sharing decoder to upsample the feature maps and specially designed skip connections to fuse multi-modal features. The qualitative and quantitative experimental results show that SOF-UNet has a promising capability to identify different land cover classes and can retain fine details in the prediction maps. Symmetric Cross Entropy (SCE) loss is also verified useful in this framework.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884504","National Science Foundation of China (NSFC)(grant numbers:61621136008/DFG TRR-169); German Research Foundation (DFG); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884504","land cover classification;SAR;multi-spectral;multi-modal fusion;UNet","Optical losses;Fuses;Geoscience and remote sensing;Optical fiber networks;Feature extraction;Entropy;Decoding","decoding;entropy;feature extraction;image classification;image fusion;pattern classification;sensor fusion","SAR;Optical Fusion unet;Optical Fusion Network;UNet framework;multimodal land cover classification;sharing decoder;feature maps;skip connections;multimodal features;qualitative results;quantitative experimental results;SOF-UNet;different land cover classes","","","","9","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"RGB-Thermal based Pedestrian Detection with Single-Modal Augmentation and ROI Pooling Multiscale Fusion","J. Xiang; S. Gou; R. Li; Z. Zheng","School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Academy of Advanced Interdisciplinary Research, Xidian University, Xi'an, Shaanxi Province, China; Beijing Aerospace Automatic Control Institute, Beijing, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","3532","3535","RGB-Thermal based pedestrian detection has received more extensive attention due to the provided detailed information and thermal sensitivity of pedestrians. In this paper, a single-modal feature augmentation network (SMA-Net) is proposed. Firstly, two single-modal branches are trained separately to optimize the feature extraction of each branch in addition to the training of pedestrian detection based on fused features. Secondly, a lightweight ROI pooling multiscale fusion module (PMSF) is proposed to obtain more fine-grained and abundant features, in which pooling features of different scales are integrated by adaptively weighting. Finally, a generative constraint strategy is designed to constrain fusion by minimizing the loss function between the generated fusion image and RGB-Thermal pairs. Experimental result on the challenging dataset KAIST demonstrates that the proposed SMA-Net achieves great performance in terms of accuracy and computational efficiency.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883131","Pedestrian detection;single-modal augmentation;ROI pooling multiscale fusion;generative constraint;multispectral datasets","Training;Sensitivity;Geoscience and remote sensing;Feature extraction;Computational efficiency","feature extraction;image colour analysis;image fusion;image segmentation;object detection;pedestrians;sensor fusion;traffic engineering computing;video signal processing","RGB-Thermal based pedestrian detection;single-modal augmentation;single-modal feature augmentation network;single-modal branches;feature extraction;fused features;lightweight ROI pooling multiscale fusion module;fine-grained features;abundant features;RGB-Thermal pairs","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Hyperspectral and Multispectral Data Fusion with 1D-Convolution on Spectrum","J. Xie; Y. Wang; J. Li","School of Electronic Engineering, Xidian University, Xi'an, China; School of Electronic Engineering, Xidian University, Xi'an, China; School of Electronic Engineering, Xidian University, Xi'an, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2394","2397","Fusion of hyperspectral and multispectral data to obtain hyperspectral data with high-spatial-resolution has been an important topic in recent years. The fusion methods that from model-driven to data-driven have been proposed constantly. Data driven models, especially the deep neural networks, are widely used owing to their excellent performance. And the design of its structure is highly related to the task. It is noteworthy that the hyperspectral and multispectral spectrum contains abundant information. However, most neural net-work based approaches mainly focus on spatial features and ignore the spectral information, which is also very important for fusion task. In this paper, we propose a ID-convolutional neural network, it can extract hyperspectral and multispectral features, simultaneously, and also can capture the spectral correlation and cross-correlation between hyperspectral and multispectral spectrum. Groups of simulation experiments demonstrate that compared with the state-of-the-art methods, paying more attention to the spectrum can obtain better performance.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884077","Hyperspectral data;multispectral data;data fusion;data-driven;1D-convolution","Deep learning;Correlation;Neural networks;Geoscience and remote sensing;Data integration;Feature extraction;Data models","feature extraction;geophysical image processing;image fusion;neural nets;sensor fusion","1d-convolution;hyperspectral data;multispectral data;high-spatial-resolution;fusion methods;model-driven;data driven models;deep neural networks;hyperspectral spectrum;multispectral spectrum;neural net-work;spatial features;fusion task;ID-convolutional neural network;hyperspectral features;multispectral features","","","","12","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Context-Aware Element Filter for Hyperspectral Image Super-Resolution","R. Ran; L. -J. Deng; C. -Y. Zhao","School of Mathematical Sciences; School of Mathematical Sciences; Yingcai Honors College, University of Electronic Science and Technology of China, Chengdu, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2378","2381","Hyperspectral image super-resolution (HISR) aims to fuse a low-resolution image (LR-HSI) and a high-resolution multispectral image (HR-MSI), generating a high-resolution hyperspectral image (HR-HSI). Previous attempts to apply convolutional neural networks (CNNs) with spatial-variant adaptive filters for HISR tasks. Such filters overcome the spatial invariance and content-agnostic property of standard convolution. However, the current adaptive filters only consider pixellevel specificity, ignoring that each element of the features has unique close relationships with their neighbourhoods. To address the issue, we propose a context-aware element filter (CEF) operation, which generates adaptive filters for each element with sufficient perception of the specificity of each element to improve the representation capability. CEF can generate a single-channel filter to trade off the computational resource consumption for each element and is appropriate for HISR tasks with element-level dependencies. Specifically, we design a new network structure for HISR, which utilizes CEF to replace the standard convolution in the residual block. Extensive experiments demonstrate the superiority of the proposed CEF both visually and quantitatively compared with state-of-the-art (SOTA) methods.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884944","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884944","Hyperspectral Image Super-resolution;Context-aware Filter;Convolutional Neural Network","Convolution;Fuses;Superresolution;Neural networks;Adaptive filters;Geoscience and remote sensing;Task analysis","adaptive filters;hyperspectral imaging;image filtering;image fusion;image recognition;image resolution;learning (artificial intelligence);neural nets","hyperspectral image super-resolution;low-resolution image;high-resolution multispectral image;high-resolution hyperspectral image;convolutional neural networks;spatial-variant adaptive filters;HISR tasks;standard convolution;current adaptive filters;context-aware element filter operation;CEF;single-channel filter;element-level dependencies","","","","13","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Foreground-Background Segmentation of Sequential Point Clouds","C. Yang; Y. Chen; C. Wang; J. Li","School of Informatics, Xiamen University, Xiamen, Fujian; School of Geospatial Engineering and Science, Sun Yat-sen University, Zhuhai, China; School of Informatics, Xiamen University, Xiamen, Fujian; Department of Geography and Environmental Management and Department of Systems Design Engineering, University of Waterloo, Waterloo, ON, Canada","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","7503","7506","Point clouds are receiving increasing attention in the field of computer vision. Hitherto, segmentation tasks for point clouds have dealt with single-frame data without considering the temporal information. In this paper, we propose a point cloud segmentation network based on a neuron-like model that can exploit the time information of point cloud data to improve network performance. The network's encoder adopts the structure of the U-Net network and sparse convolution. The features extracted by the encoder and the last moment are used as input to the time integration module. The time integration module is a timing-based neuron model which can fuse features from two moments and use them to enhance the features of the current moment. Experiments on the public autopilot dataset NuScenes validate that our proposed point cloud segmentation network can achieve a 97.7 % Intersection over Union (IoU) for segmenting static scenes and dynamic objects. The experiments further validate that fusing temporal information improves the performance of point cloud foreground-background segmentation compared to considering only data from a single frame point cloud data.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883255","National Natural Science Foundation of China(grant numbers:41871380); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883255","3D Point Cloud;Semantic Segmentation;Fusion Time;Neuronal model;Temporal-LiDAR Point Clouds","Point cloud compression;Solid modeling;Three-dimensional displays;Fuses;Convolution;Neurons;Geoscience and remote sensing","computer vision;convolutional neural nets;feature extraction;image fusion;image segmentation","sequential point clouds;temporal information fusion;time information;network performance;U-Net network;time integration module;timing-based neuron model;computer vision;network encoder;Intersection over Union;sparse convolution;feature extraction;point cloud foreground-background segmentation network","","","","15","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Data fusion approach for Urban area identification using multisensor information","A. A. López-Caloca","“Ing. Jorge L. Tamayo, A. C. CentroGeo CONACYT, Centro de Investigación en Geografia y Geomática, México City, México","2015 8th International Workshop on the Analysis of Multitemporal Remote Sensing Images (Multi-Temp)","10 Sep 2015","2015","","","1","4","This article presents a procedure to identify and extract urban areas in medium resolution satellite images. At present, we have and continue to study various methodologies to process and extract information on urban surfaces, since urban growth is having environmental impacts on the involved ecological systems. The proposed method takes advantage of the fact that data fusion allows us to combine in an optimal manner, multiple sources of classifiers and to generate a single source of information. In this context, we propose the use of data fusion algorithms, by multiple classifiers, taking into account the spectral and spatial characteristics of the satellite data, which in our case are the Landsat ETM+ and the ENVISAT-ASAR. The developed system includes an ensemble fusion architecture and the use of algorithms such as Fuzzy K-mean and Markov Random Field (MRF). The study case is the Guadalajara metropolitan area, in Jalisco, Mexico, which has great growth and sprawl; in its surrounding areas there are regions which are interesting in terms of geothermal exploitation and with great ecological value. The experimental results, using the multiple classifier system (MCS) show the urban characteristics at the regional scale, offering results that are potentially significant at this scale and the direction of changes in urban growth.","","978-1-4673-7119-3","10.1109/Multi-Temp.2015.7245783","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7245783","Data fusion;multiple classifier system","Indexes;Satellites;Remote sensing;Urban areas;Earth;Data integration;Classification algorithms","ecology;geophysical image processing;image classification;image fusion;Markov processes;terrain mapping","data fusion approach;urban area identification;multisensor information;medium resolution satellite images;extract urban areas;urban surface information;ecological systems;data fusion algorithms;multiple classifiers;satellite data;spatial characteristics;spectral characteristics;ENVISAT-ASAR;Landsat ETM+;fusion architecture developed system;Fuzzy K-mean algorithms;Markov random field algorithms;Jalisco;Mexico;MRF;geothermal exploitation;multiple classifier system;urban characteristics;regional scale;Guadalajara metropolitan area","","","","11","IEEE","10 Sep 2015","","","IEEE","IEEE Conferences"
"Deep Learning Integrated with Multiscale Pixel and Object Features for Hyperspectral Image Classification","M. Zhang; L. Hong","Engineering research center of GIS technology in western China, Kunming, P. R. China; Engineering research center of GIS technology in western China, Kunming, P. R. China","2018 10th IAPR Workshop on Pattern Recognition in Remote Sensing (PRRS)","11 Oct 2018","2018","","","1","8","The spectral resolution and spatial resolution of hyperspectral images are continuously improving, providing rich information for interpreting remote sensing image. How to improve the image classification accuracy has become the focus of many studies. Recently, Deep learning is capable to extract discriminating high-level abstract features for image classification task, and some interesting results have been acquired in image processing. However, when deep learning is applied to the classification of hyperspectral remote sensing images, the spectral-based classification method is short of spatial and scale information; the image patch-based classification method ignores the rich spectral information provided by hyperspectral images. In this study, a multi-scale feature fusion hyperspectral image classification method based on deep learning was proposed. Firstly, multiscale features were obtained by multi-scale segmentation. Then multiscale features were input into the convolution neural network to extract high-level features. Finally, the high-level features were used for classification. Experimental results show that the classification results of the fusion multi-scale features are better than the single-scale features and regional feature classification results.","2377-0198","978-1-5386-8479-5","10.1109/PRRS.2018.8486304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8486304","Deep learning;CNN;multiscale feature fusion;object based image analysis","Feature extraction;Convolution;Hyperspectral imaging;Machine learning;Kernel;Image classification","convolution;feature extraction;feedforward neural nets;geophysical image processing;hyperspectral imaging;image classification;image fusion;image resolution;image segmentation;learning (artificial intelligence);remote sensing","multiscale pixel;object features;spectral resolution;spatial resolution;hyperspectral images;remote sensing image;image classification accuracy;deep learning;image processing;hyperspectral remote sensing images;spectral-based classification method;spatial scale information;image patch-based classification method;multiscale feature fusion hyperspectral image classification method;multiscale segmentation;fusion multiscale features;feature classification;convolution neural network","","","","28","IEEE","11 Oct 2018","","","IEEE","IEEE Conferences"
"Big Data Fusion Challenge: Unmanned Aerial System Based Precision Agriculture","B. Gokaraju; S. Samiappan; Y. S. Kale","Computational Data Science and Engineering, ViCAR Center & North Carolina A&T State University, Greensboro, NC, USA; Gosytems Research Institute, Mississippi State University, Starkville, MS, USA; Computational Data Science and Engineering, ViCAR Center & North Carolina A&T State University, Greensboro, NC, USA","2021 International Conference on Emerging Techniques in Computational Intelligence (ICETCI)","27 Oct 2021","2021","","","77","81","The International Conference of Computational Intelligence 2021 organized multi Remote Sensing Data Competitions, and a Big Data Fusion Challenge using UAS based precision Agriculture is coordinated by North Carolina A&T State University and Mississippi State University. The Big Data Challenge focused on Multi-sensor and Multitemporal datasets including the hyper spectral imagery with 251 bands. A multi-class problem is given to classify three different crops and at three yield levels. The challenge received multiple submissions and the winning approach is based on inference learning using residual neural networks with 150 layers with a performance of Kappa accuracy as 0.86.","","978-1-6654-1559-0","10.1109/ICETCI51973.2021.9574054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9574054","Big Data Challenge;Remote Sensing;Deep Learning;Convolution Neural Networks","Deep learning;Big Data;Speckle;Agriculture;Sensors;Spatial resolution;Task analysis","agriculture;Big Data;crops;geophysical image processing;image fusion;inference mechanisms;learning (artificial intelligence);neural nets;remote sensing","Mississippi State University;Big Data Challenge;multiclass problem;Big Data Fusion Challenge;unmanned aerial system;multiRemote Sensing Data Competitions;UAS based precision Agriculture;North Carolina A&T State University;International Conference of Computational Intelligence 2021;Multisensor datasets;winning approach;inference learning;residual neural networks","","1","","4","IEEE","27 Oct 2021","","","IEEE","IEEE Conferences"
"Application of radon transform for fast image registration","M. I. Patel; V. K. Thakar","Department of Electronics & Communication Engineering, Sankalchand Patel College of Engineering, Visnagar, Gujarat, India; Department of Electronics & communication Engineering, A. D. Patel Institute of Technology, Gujarat, India","2015 International Conference on Advanced Computing and Communication Systems","12 Nov 2015","2015","","","1","4","One of the important steps in image fusion is image registration. The process of determining the spatial transformation that maps the points in the target image to the points in the source image is known as image registration. Various image registration approaches can be classified as area, feature and transform domain based. Choice of approach depends on image contents and application. Area based approach requires more computation time, specially for large images such as satellite images, while feature based approach may not be accurate if significant features are not available in the images. In this paper authors have used the rotation and translation invariant properties of radon transform to find the amount of rotation and translation required to perform registration, i.e. to align the images. Simulation results are shown for different images, with different amount of rotation and translations, to show the accuracy and reliability of the method. Again noise level is also varied, to observe the robustness of the method to noise. The required average computation time is in seconds, depending on the size of images.","","978-1-4799-6438-3","10.1109/ICACCS.2015.7324141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324141","image registration;radon transform;rotational property;translational property","Radon;Transforms;Image registration;Remote sensing;Estimation;Communication systems;Accuracy","computational complexity;image fusion;image registration;Radon transforms","Radon transform;image registration;image fusion;spatial transformation;target image;source image;transform domain;translation invariant property;computation time","","2","","16","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Improved ORB Algorithm used in Image Mosaic","H. Yu; Y. Dai","Electrical Research Institute of Yunnan Electric Power Grid Co,Ltd, Kunming, China; Graduate workstation Yunnan Electric Power Grid (Group) Co,Ltd, Kunming, China","2019 IEEE International Conference on Mechatronics and Automation (ICMA)","29 Aug 2019","2019","","","621","625","The classical SIFT, SURF and other algorithms based on linear scale decomposition are robust, but they lose local precision, which is easy to cause boundary blur and detail loss, and the real-time performance is generally not good. To overcome the above problems, and considering that ORB is not sensitive to the scale transformation of feature points, an improved algorithm is proposed. After the image is preprocessed, the feature points are first proposed by ORB algorithm, Then use the FREAK algorithm to obtain the binary code descriptor of the feature point for matching, and then adopts KNN algorithm to rough match these points. Then it uses the Random Sample Consensus Algorithm to remove the fault matching and to realize fine match. Finally, it uses the improved weighted average method for image fusion and stitching. Experimental results show that the proposed algorithm is effective for image stitching, moreover the stitching quality and stitching speed are much better than other image stitching algorithms.","2152-744X","978-1-7281-1699-0","10.1109/ICMA.2019.8816329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816329","image stitching;ORB;FREAK;RANSAC","Feature extraction;Lighting;Image matching;Remote sensing;Drones;Data mining","feature extraction;image fusion;image matching;image segmentation;nearest neighbour methods","image fusion;image stitching algorithms;image mosaic;linear scale decomposition;local precision;boundary blur;detail loss;real-time performance;scale transformation;feature point;ORB algorithm;FREAK algorithm;binary code descriptor;KNN algorithm;Random Sample Consensus Algorithm;fault matching;fine match;improved weighted average method","","1","","6","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Zero-Shot Sentinel-2 Sharpening Using a Symmetric Skipped Connection Convolutional Neural Network","H. V. Nguyen; M. O. Ulfarsson; J. R. Sveinsson; J. Sigurdsson","Faculty of Electrical and Computer Engineering, University of Iceland; Faculty of Electrical and Computer Engineering, University of Iceland; Faculty of Electrical and Computer Engineering, University of Iceland; Faculty of Electrical and Computer Engineering, University of Iceland","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","613","616","Sentinel-2 (S2) satellite constellations can provide multispectral images of 10 m, 20 m, and 60 m resolution for visible, near-infrared (NIR) and short-wave infrared (SWIR) in the electromagnetic spectrum. In this paper, we present a sharpening method based on a symmetric skipped connection convolutional neural network, called SSC-CNN, to sharpen 20 m bands using 10 m bands. The main advantage of SSC-CNN architecture is that it brings the features of the input branch to the output, thus improving convergence without using too many deep layers. The proposed method uses the reduced-scale combination of 10 m bands and 20 m bands, and the observed 20 m bands as the training pairs. The experimental results using two Sentinel-2 datasets show that our method outperforms competitive methods in quantitative metrics and visualization.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323614","Sentinel-2;image fusion;image sharpening;super resolution;convolutional neural network","Image resolution;Spatial resolution;Training;Sea measurements;Remote sensing;Pansharpening;Measurement","convolutional neural nets;image classification;image resolution;image sensors;learning (artificial intelligence)","SSC-CNN architecture;Sentinel-2 datasets;zero-shot Sentinel-2 sharpening;symmetric skipped connection convolutional neural network;Sentinel-2 satellite constellations;near-infrared;sharpening method;size 20.0 m;size 60.0 m;size 10.0 m","","5","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Tuning Parameter Selection for Sentinel-2 Sharpening Using Wald's Protocol","S. E. Armannsson; J. Sigurdsson; J. R. Sveinsson; M. O. Ulfarsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2871","2874","In recent years numerous model-based methods for super-resolution of Sentinel-2 (S2) multispectral images have been suggested. Super-resolution aims to enhance the resolution of a captured image by upscaling and enhancing the details. The performance of model-based methods relies on carefully selecting regularizers and tuning parameters. This paper investigates whether using Wald's protocol, i.e., selecting tuning parameters at reduced-resolution, translates to a good performance at a full-scale. To investigate this, we use the recently proposed S2Sharp method and show that selecting its tuning parameters using Wald's protocol improves its performance.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553346","University of Iceland Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553346","Image fusion;image sharpening;multispectral (MS) multiresolution images;parameter selection;scale invariance;Sentinel-2 constellation;super-resolution","Measurement;Image quality;Protocols;Parameter estimation;Superresolution;Geoscience and remote sensing;Bayes methods","geophysical image processing;image enhancement;image resolution","Wald's protocol;Sentinel-2 multispectral images;super-resolution;captured image;reduced-resolution;S2Sharp;parameter selection tuning;Sentinel-2 sharpening;image upscaling;image enhancement;model-based methods","","1","","13","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Hyperspectral Super-Resolution by Unsupervised Convolutional Neural Network and Sure","H. V. Nguyen; M. O. Ulfarsson; J. R. Sveinsson; M. D. Mura","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; GIPSA-Lab, Grenoble Institute of Technology, Saint Martin d'Hères, France","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","903","906","Recent advances in deep learning (DL) reveal that the structure of a convolutional neural network (CNN) is a good image prior (called deep image prior (DIP)), bridging the model-based and DL-based methods in image restoration. However, optimizing a DIP-based CNN is prone to over-fitting leading to a poorly reconstructed image. This paper derives a loss function based on Stein's unbiased risk estimate (SURE) for unsupervised training of a DIP-based CNN applied to the hyperspectral image (HSI) super-resolution. The SURE loss function is an unbiased estimate of the mean-square-error (MSE) between the clean low-resolution image and the low-resolution estimated image, which relies only on the observed low-resolution image. Experimental results on HSI show that the proposed method not only improves the performance, but also avoids overfitting. Codes are available at https://github.com/hvn2/SURE-MS-HS","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883576","University of Iceland(grant numbers:1547-15430); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883576","Hyperspectral image;image fusion;Stein's unbiased risk estimate (SURE);unsupervised CNN","Training;Filtering;Superresolution;Noise reduction;Geoscience and remote sensing;Pansharpening;Image restoration","convolutional neural nets;deep learning (artificial intelligence);hyperspectral imaging;image reconstruction;image resolution;image restoration;mean square error methods;unsupervised learning","unsupervised convolutional neural network;deep learning;image restoration;DIP-based CNN;poorly reconstructed image;Stein's unbiased risk estimate;unsupervised training;hyperspectral image super-resolution;SURE loss function;mean-square-error","","","","13","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Hyperspectral pansharpening using convex optimization and collaborative total variation regularization","P. Addesso; M. Dalla Mura; L. Condat; R. Restaino; G. Vivone; D. Picone; J. Chanussot","DIEM, University of Salerno, Italy; GIPSA-Lab, Grenoble Institute of Technology, France; GIPSA-Lab, Grenoble Institute of Technology, France; DIEM, University of Salerno, Italy; NATO STO Centre for Maritime Research and Experimentation, La Spezia, Italy; DIEM, University of Salerno, Italy; DIEM, University of Salerno, Italy","2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Oct 2017","2016","","","1","5","Hyperspectral pansharpening is a challenging research area and several methods have been recently developed to fuse low resolution hyperspectral and high resolution panchromatic images. In this paper we focus on a recent regularization method, called Collaborative Total Variation, exploiting a convex optimization algorithm. We evaluate the effectiveness of this novel approach in comparison to existing methods, and assess the performances on two datasets: a synthetic scene mimicking the characteristics of the Hyperion and ALI sensors and the Pavia University dataset.","2158-6276","978-1-5090-0608-3","10.1109/WHISPERS.2016.8071736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071736","Data fusion;Hyperspectral pansharpening;Convex optimization;Total variation;Deconvolution","Spatial resolution;Hyperspectral imaging;Sensors;Convex functions;Collaboration","hyperspectral imaging;image fusion;remote sensing","hyperspectral pansharpening;collaborative total variation regularization;convex optimization algorithm;regularization method;low-resolution hyperspectral image;high-resolution panchromatic images;ALI sensor;Hyperion sensor;Pavia University dataset","","1","","12","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"Comparative analysis of the hyperspectral vegetatation index and radar vegetation index: A novel fusion vegetation index","Y. -H. Kim; J. -H. Oh; J. -W. Choi; Y. -I. Kim","Department of Civil and Environmental Engineering, Seoul National University, Republic of Korea; Department of Civil Engineering, Chonnam National University, Republic of Korea; Chungbuk National University, Cheongju, Chungcheongbuk-do, KR; Department of Civil and Environmental Engineering, Seoul National University, Republic of Korea","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","23 Oct 2017","2015","","","1","4","The hyperspectral vegetation index (HVI) has shown promise in vegetation fields, but its relationship to the radar vegetation index (RVI) is not known in the context of various land covers. This work presents a comparative analysis of the HVI data derived from the AISA sensor and RVI values originating from the RADARSAT-2 quad-polarimetric synthetic aperture radar (SAR) data. Six types of land cover (buildings, forest, salt pond, tidal flat, ocean, and paddy field) were compared, and the patterns were investigated. In the RVI, forest areas show higher separability than the other types of land cover without exception. Also, in the HVIs, the forest areas indicate high values, without exception. The statistics of the region of interest (ROI) demonstrate that the RVI patterns of the six land-cover types are highly similar to those of the HVI. Thus, during bad weather conditions and at night, the RVI data could serve as an alternative to the HVI data. In addition to comparative analysis, we propose a novel fusion vegetation index (FVI) using the RVI and normalized difference vegetation index (NDVI). The proposed FVI creates obvious vegetation separation, more so than other land covers. Using the FVI, more effective vegetation monitoring could be possible in various vegetation monitoring fields.","2158-6276","978-1-4673-9015-6","10.1109/WHISPERS.2015.8075434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075434","Hyperspectral vegetation index;radar vegetation index;comparative analysis;fusion vegetation index","Vegetation mapping;Indexes;Hyperspectral imaging;Radar;Monitoring","geophysical image processing;hyperspectral imaging;image fusion;radar polarimetry;remote sensing by radar;synthetic aperture radar;vegetation","hyperspectral vegetation index;radar vegetation index;fusion vegetation index;vegetation fields;land cover;forest areas;land-cover types;normalized difference vegetation index;vegetation monitoring;RADARSAT-2 quadpolarimetric SAR data;synthetic aperture radar;vegetation separation;buildings;forest;salt pond;tidal flat;paddy field;ocean","","1","","16","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Optimized feature fusion of LiDAR and hyperspectral data for tree species mapping in closed forest canopies","F. M. B. Van Coillie; W. Liao; P. Kempeneers; K. Vandekerkhove; S. Gautama; W. Philips; R. R. De Wulf","Faculty of Bioscience Engineering, Ghent University, Gent, Belgium; Department of Telecommunications and Information Processing, Ghent University - iMinds - Image Processing and Interpretation, Ghent, Belgium; Flemish Institute for Technological Research - VITO, Mol, Belgium; Research Institute for Nature and Forest - INBO, Geraardsbergen, Belgium; Department of Telecommunications and Information Processing, Ghent University - iMinds - Image Processing and Interpretation, Ghent, Belgium; Department of Telecommunications and Information Processing, Ghent University - iMinds - Image Processing and Interpretation, Ghent, Belgium; Faculty of Bioscience Engineering, Ghent University, Gent, Belgium","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","23 Oct 2017","2015","","","1","4","This study deals with data fusion of hyperspectral and LiDAR sensors for tree species mapping in complex, closed forest canopies in Belgium. In particular, seven tree species were mapped: Beech, Ash, Larch, Poplar, Copper beech, Chestnut and Oak. The added value of LiDAR height profile data on tree species mapping was assessed. Sensor data were fused in the PCA domain, while optimal feature combination was derived from the best classification performance (in terms of Kappa and producer's accuracy) based on 5-fold cross-validation. Besides, varying training set sizes were tested (resp. 10%, 30% and 50% number of samples per tree species class). Feature fusion of PCA-transformed HS and LiDAR data was most effective for small sample set sizes reaching a Kappa accuracy improvement of 10.51%.","2158-6276","978-1-4673-9015-6","10.1109/WHISPERS.2015.8075403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075403","Feature fusion;LiDAR;hyperspectral;tree species mapping;closed forest canopy","Laser radar;Vegetation;Hyperspectral imaging;Training;Atmospheric modeling","forestry;geophysical image processing;hyperspectral imaging;image classification;image fusion;optical radar;principal component analysis;remote sensing by laser beam;vegetation;vegetation mapping","tree species mapping;complex forest canopies;closed forest canopies;LiDAR height profile data;sensor data;hyperspectral data;data fusion;hyperspectral sensor;LiDAR sensor;Belgium;ash;larch;poplar;copper beech;chestnut;oak","","1","","11","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Jointly spatial-spectral resolution enhancement of hyperspectral imagery","Y. Zhao; C. Yi; J. Yang","School of Automation, Northwestern Polytechnical University, Xi'An, China; School of Automation, Northwestern Polytechnical University, Xi'An, China; School of Automation, Northwestern Polytechnical University, Xi'An, China","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","23 Oct 2017","2015","","","1","4","For hyperspectral image, low spatial and spectral resolution will cause inaccurate object detection and classification. In this paper, a novel jointly spatial-spectral resolution enhancement algorithm is proposed to promote the spatial resolution and spectral resolution simultaneously, high spatial resolution information from panchromatic image is used as constraint in spectral enhancement while the high spectral resolution details are benefited for spatial enhancement, which makes spatial SR and spectral SR promote each other. The experiments prove that our algorithm can provide joint spatial-spectral enhanced results, which can outperform the state-of-art methods both in spatial and spectral domains.","2158-6276","978-1-4673-9015-6","10.1109/WHISPERS.2015.8075455","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075455","Hyperspectral image;spatial-spectral super resolution;unmixing;sparse representation","Spatial resolution;Dictionaries;Sparse matrices;Hyperspectral imaging;Libraries;Matrices","feature extraction;geophysical image processing;geophysical signal processing;geophysical techniques;hyperspectral imaging;image enhancement;image fusion;image resolution;object detection;remote sensing","hyperspectral imagery;high spatial resolution information;panchromatic image;high spectral resolution details;spatial enhancement;object detection;object classification;joint spatial-spectral resolution enhancement algorithm;spatial SR;spectral SR","","","","16","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Spectrally Coarse-To-Fine Pansharpening For Hyperspectral Images","H. Lai; L. He; D. Xi","School of Automation Science and Engineering, South China University of Technology, China; School of Automation Science and Engineering, South China University of Technology, China; School of Automation Science and Engineering, South China University of Technology, China","2022 12th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","22 Nov 2022","2022","","","1","4","Hyperspectral (HS) pansharpening synthesizes low spatial resolution HS (LRHS) images and connected panchromatic (PAN) images to form high spatial resolution HS (HRHS) images. A major problem that such a task is faced with is the huge spectral gap between the two kinds of data. To handle that, in this work we propose a spectrally coarse-to-fine network (SCFNet) to recover the HRHS images, which combines the LRHS images and the PAN images to build multiple low spectral resolution HRHS images in several spectral resolution levels, and then fuse them into the final HRHS images. Experiments are conducted to compare the proposed method with some of representative techniques, where the superiority of our method is validated.","2158-6276","978-1-6654-7069-8","10.1109/WHISPERS56178.2022.9955078","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955078","Hyperspectral images;spectrally coarse-to-fine pansharpening;CNNs","Fuses;Conferences;Pansharpening;Spatial resolution;Task analysis;Signal resolution;Image reconstruction","geophysical image processing;hyperspectral imaging;image colour analysis;image fusion;image resolution;remote sensing","coarse-to-fine pansharpening;connected panchromatic images;high spatial resolution HS images;hyperspectral images;LRHS images;multiple low spectral resolution HRHS images;PAN images;spectral gap;spectral resolution levels;spectrally coarse-to-fine network","","","","7","IEEE","22 Nov 2022","","","IEEE","IEEE Conferences"
"Hyperspectral resolution enhancement using multisensor image data","J. Bieniarz; D. Cerra; X. X. Zhu; R. Müller; P. Reinartz","Earth Observation Center (EOC), German Aerospace Center (DLR), Wessling, Germany; Earth Observation Center (EOC), German Aerospace Center (DLR), Wessling, Germany; Earth Observation Center (EOC), German Aerospace Center (DLR), Wessling, Germany; Earth Observation Center (EOC), German Aerospace Center (DLR), Wessling, Germany; Earth Observation Center (EOC), German Aerospace Center (DLR), Wessling, Germany","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","23 Oct 2017","2015","","","1","4","In this paper we apply the Multi-Look Joint Sparsity Fusion algorithm to multisensor image data. Our algorithm at first performs sparse unmixing of the hyperspectral data and selects pixels for a second unmixing of the multispectral image. This is done by applying a joint sparsity model, which exploits similarities within neighbouring pixels. We test our resolution enhancement method using a hyperspectral and a multispectral image with a spatial resolution of 30 m and 3 m, respectively. To asses the results we evaluate the classification result of the resolution enhanced and original images.","2158-6276","978-1-4673-9015-6","10.1109/WHISPERS.2015.8075400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075400","Resolution enhancement;hyperspectral image sharpening;spectral unmixing;joint sparsity;overcomplete spectral dictionary","Spatial resolution;Hyperspectral imaging;Dictionaries;Image reconstruction;Soil","hyperspectral imaging;image classification;image enhancement;image fusion;image resolution;remote sensing","hyperspectral resolution enhancement;multisensor image data;hyperspectral data;multispectral image;joint sparsity model;sparse unmixing;multilook joint sparsity fusion algorithm;hyperspectral image","","","","11","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Multi-scale Features Fusion Network for Unsupervised Change Detection in Heterogeneous Optical and SAR Images","J. Shi; Z. Zhang; T. Wu; X. Li; D. Zhou; Y. Lei","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, Shaanxi, China","2021 IEEE 7th International Conference on Cloud Computing and Intelligent Systems (CCIS)","14 Apr 2022","2021","","","270","274","Change detection (CD) in heterogeneous remote sensing image applications has become an issue of increasing concern in, as they cannot be compared directly with traditional homogenous CD methods. To solve feature loss problem and generating better representations to accommodate regions of various sizes in heterogeneous images CD, a multi-scale features fusion network (MFFN) is proposed. Firstly, multi-scale representative deep features can be extracted to distinguish difference in high-dimension feature space. Then, hierarchical features from the original image pairs can be fuse to generate a difference image with more explicit semantic information owing to the strategy of multi-scale features fusion, which can better adapt different scale of changes in heterogeneous remote sensing images. It is noteworthy that the experimental results on both heterogeneous and homogeneous data set confirm the effectiveness of the proposed method.","","978-1-6654-4149-0","10.1109/CCIS53392.2021.9754667","National Natural Science Foundation of China; Natural Science Foundation of Shaanxi Province; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9754667","Change detection;Multi-scale feature fusion;Heterogeneous images;Neural network","Semantics;Radar detection;Radar;Radar imaging;Feature extraction;Optical imaging;Radar polarimetry","feature extraction;geophysical image processing;image fusion;image segmentation;radar imaging;remote sensing;synthetic aperture radar","multiscale features fusion network;unsupervised change detection;heterogeneous optical images;sar images;heterogeneous remote sensing image applications;traditional homogenous CD methods;feature loss problem;heterogeneous images CD;multiscale representative deep features;high-dimension feature space;hierarchical features;original image pairs;difference image;heterogeneous remote sensing images;heterogeneous data;homogeneous data","","","","15","IEEE","14 Apr 2022","","","IEEE","IEEE Conferences"
"Maximum entropy neural networks for feature enhanced imaging with collaborative microwave multi-sensor data fusion","Y. V. Shkvarko; J. A. Lopez; S. R. Santos","Centre for Advanced Research and Education (CINVESTAV), National Polytechnic Institute, Guadalajara, Jalisco, Mexico; Centre for Advanced Research and Education (CINVESTAV), National Polytechnic Institute, Guadalajara, Jalisco, Mexico; University Centre for Exact Sciences and Engineering (CUCEI), University of Guadalajara, Jalisco, Mexico","2016 IEEE MTT-S Latin America Microwave Conference (LAMC)","13 Feb 2017","2016","","","1","3","We present a collaborative neural network (NN) computing-oriented approach for feature enhanced reconstruction of microwave remote sensing (RS) imagery via sensor data fusion. Two reconstruction/fusion frameworks are proposed and featured. Both unify the maximum entropy and descriptive experiment design regularization (DEDR) paradigms but employ different NN-based fusion (NNF) strategies. The first one addressed as RS-NNF(1) aggregates the adaptively weighted DEDR-structured individual sensor image recovery objective functions, while the second one addressed as RS-NNF(2) performs cooperative multi-sensor statistical recovery performances enhancement-oriented fusion. The simulations corroborate superiority of both proposed technics over the conventional non-collaborative RS image fusion with RS-NNF(2) on top.","","978-1-5090-4287-6","10.1109/LAMC.2016.7851241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7851241","Experiment design;imaging radar;maximum entropy;neural network;regularization;sensor fusion","Artificial neural networks;Image reconstruction;Imaging;Collaboration;Radar imaging;Iron;Optimization","geophysical image processing;image fusion;image reconstruction;maximum entropy methods;microwave imaging;neural nets;remote sensing;statistical analysis","cooperative multisensor statistical recovery performance enhancement-oriented fusion;RS-NNF;sensor image recovery objective function;NN-based fusion strategy;descriptive experiment design regularization;reconstruction-fusion framework;RS imagery;microwave remote sensing imagery;NN computing-oriented approach;collaborative neural network computing-oriented approach;collaborative microwave multisensor data fusion;feature enhanced imaging;maximum entropy neural network","","","","7","IEEE","13 Feb 2017","","","IEEE","IEEE Conferences"
"Oil spill segmentation in fused Synthetic Aperture Radar images","F. S. Longman; L. Mihaylova; D. Coca","Department of Automatic Control and System Engineering, University of Sheffield, UK; Department of Automatic Control and System Engineering, University of Sheffield, UK; Department of Automatic Control and System Engineering, University of Sheffield, UK","2016 4th International Conference on Control Engineering & Information Technology (CEIT)","18 May 2017","2016","","","1","6","Synthetic Aperture Radar (SAR) satellite systems are very efficient in oil spill monitoring due to their capability to operate under all weather conditions. Systems such as the Envisat and RADARSAT have been used independently in many studies to detect oil spill. This paper presents an automatic feature based image registration and fusion algorithm for oil spill monitoring using SAR images. A range of metrics are used to evaluate the performance of the algorithm and to demonstrate the benefits of fusing SAR images of different modalities. The proposed framework has shown 45% improvement of the oil spill location when compared with the individual images before the fusion.","","978-1-5090-1055-4","10.1109/CEIT.2016.7929055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7929055","Oil Spill;Synthetic Aperture Radar (SAR);Registration;Image Fusion;Segmentation","Discrete wavelet transforms;Image resolution;Matched filters","image fusion;image segmentation;remote sensing;synthetic aperture radar","oil spill segmentation;fused synthetic aperture radar images;SAR satellite systems;oil spill monitoring;Envisat;RADARSAT;image registration;fusion algorithm;SAR images;oil spill location","","3","","25","IEEE","18 May 2017","","","IEEE","IEEE Conferences"
"Unsupervised Deep Hyperspectral Super-Resolution With Unregistered Images","J. Nie; L. Zhang; W. Wei; C. Ding; Y. Zhang","School of Computer Science, Northwestern Polytechnical University, Xi’an, Shaanxi, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, Shaanxi, China; Research & Development Institute of Northwestern Polytechnical University in Shenzhen, Shenzhen, China; School of Computer Science & Technology, Xi’an University of Posts and Telecommunications, Xi’an, Shaanxi, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, Shaanxi, China","2020 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2020","2020","","","1","6","Fusion based hyperspectral image (HSI) super-resolution has long been the research focus of hyperspectral image processing since it can generate a high-resolution (HR) HSI in both spatial and spectral domains. However, the success of the existing fusion based HSI super-resolution methods depends on the premise that the images utilized for fusion (i.e. the input low-spatial-resolution HSI and the low-spectral-resolution multispectral image) are exactly registered. Although such a premise is too idealistic to comply with in real cases, few efforts have considered this problem. To fill this gap, we propose to incorporate image registration into HSI super-resolution for joint unsupervised learning in this study. Specifically, a spatial transformer network (STN) is introduced to learn the parameters of the affine transformation between the input two images. In order to avoid over-fitting, we constrain the STN with a novel constraint during learning. By doing this, both the STN and super-resolution network can be cast into a weighted joint learning model without any supervision from the latent HR HSI. Experimental results demonstrate the effectiveness of the proposed method in coping with unregistered input images.","1945-788X","978-1-7281-1331-9","10.1109/ICME46284.2020.9102881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9102881","Hyperspectral image fusion;unregistered image pairs;unsupervised","Spatial resolution;Image reconstruction;Hyperspectral imaging;Spectral analysis;Image registration","geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image registration;image resolution;learning (artificial intelligence);remote sensing;unsupervised learning","input low-spatial-resolution HSI;low-spectral-resolution multispectral image;image registration;joint unsupervised learning;spatial transformer network;super-resolution network;latent HR HSI;unregistered input images;unsupervised deep hyperspectral super-resolution;unregistered images;fusion based hyperspectral image super-resolution;hyperspectral image processing;high-resolution HSI;spatial domains;spectral domains;existing fusion;HSI super-resolution methods","","2","","23","IEEE","9 Jun 2020","","","IEEE","IEEE Conferences"
"Change detection approach using evidential fusion of change indices","A. Bouakache; A. Tahraoui; R. Kheddam; A. Belhadj-Aissa","Faculty of Electronic and Computer Science, University of Science and Technology Houari Boumediene, Algiers, Algeria; Faculty of Electronic and Computer Science, University of Science and Technology Houari Boumediene, Algiers, Algeria; Faculty of Electronic and Computer Science, University of Science and Technology Houari Boumediene, Algiers, Algeria; Faculty of Electronic and Computer Science, University of Science and Technology Houari Boumediene, Algiers, Algeria","2017 International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)","23 Oct 2017","2017","","","1","6","In this paper, we present fusion and classification process of change indices using multitemporal satellites images in the aim to detect the change of surface states after a flood. This process is performed in the framework of Dempster Shafer Theory (DST), which takes into account the imprecision and the ignorance related to data. We apply this process to a study site located at south west of England, traversed by Severn river, which have undergone in October 2000 an important flood. For the detection of the flood damage, we have used two change indices: difference values and texture evolution. We find that change index fusion overcomes the limits of change mono-index classification.","","978-1-5386-0551-6","10.1109/ATSIP.2017.8075574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075574","change indices;DST;cluster shade;SAVI;classification;image fusion;change detection","Indexes;Estimation;Image classification;Uncertainty;Classification algorithms;Floods;Laser radar","floods;image classification;image fusion;image texture;inference mechanisms;object detection;remote sensing","change detection approach;classification process;multitemporal satellites images;Dempster Shafer Theory;change index fusion;change mono-index classification;flood;change indices evidential fusion;surface state change detection;England;Severn river;flood damage detection;difference values;texture evolution","","1","","23","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Fast Weighted Least Squares Based Pansharpening","N. H. Kaplan; I. Erer","Electrical and Electronics Engineering Department, Erzurum Technical University, Erzurum, Turkey; Electrical and Electronics Engineering Department, Erzurum Technical University, Erzurum, Turkey","2019 9th International Conference on Recent Advances in Space Technologies (RAST)","22 Jul 2019","2019","","","441","445","The fusion of high spatial resolution panchromatic (PAN) images with high spectral resolution multispectral (MS) images provides high resolution images in both spatial and spectral domains. In this work, a pansharpening method based on the fast weighted least squares (WLS) smoothing filter has been proposed. In this method, input PAN image has been decomposed into its approximation and detail sub bands by a multiscale scheme based on fast WLS filtering. The details extracted from the PAN image has been added proportionally the expanded MS image. The resulting images as well as evaluation metrics demonstrate that the proposed injection approach shows better performance than the traditional methods.","","978-1-5386-9448-0","10.1109/RAST.2019.8767865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8767865","pansharpening;image fusion;weighted least squares","","geophysical image processing;image fusion;image resolution;least squares approximations;remote sensing","high resolution images;spatial domains;spectral domains;pansharpening method;fast weighted least squares smoothing filter;input PAN image;fast WLS filtering;expanded MS image;resulting images;high spatial resolution panchromatic images;high spectral resolution multispectral images","","","","14","IEEE","22 Jul 2019","","","IEEE","IEEE Conferences"
"Multispectral Pansharpening by Extracting Local Coefficients Based on WT Concept and MTF of Multispectral Sensors","S. Z. Azarnivar; H. Ghassemian","Department of Electrical and Computer Engineering, Science and Research Branch, Islamic Azad University, Tehran, Iran; Image Processing and Information Analysis Lab., Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2020 28th Iranian Conference on Electrical Engineering (ICEE)","26 Nov 2020","2020","","","1","5","In order to increase the spatial resolution of the multispectral images, the fusion process is used that the high frequency spatial information of the panchromatic image is added to the multispectral image. In this paper, first, the panchromatic image of the histogram matching each multispectral image band and each multispectral image band interpolated to the panchromatic image scale was divided. A significant and soluble linear equation was obtained between the high-frequency spatial information of the multispectral image and the panchromatic image, and this model was applied to each local window. The scale invariance assumption was used to obtain the coefficients of fusion and for this purpose, the concept of wavelet transform was used, with its low pass filter frequency response being identical to the MTF multispectral image sensor. The proposed method was applied to two worldview2 and IKONOS datasets. The results were competitive with some popular fusion algorithms.","2642-9527","978-1-7281-7296-5","10.1109/ICEE50131.2020.9260934","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9260934","Image Fusion;Pansharpening;MTF of MS Sensors;Spectral Information;Spatial Information","Spatial resolution;Sensors;Image sensors;Visualization;Low-pass filters;Nonlinear distortion;Sensor fusion","feature extraction;frequency response;geophysical image processing;image classification;image colour analysis;image fusion;image resolution;image sensors;interpolation;low-pass filters;remote sensing;wavelet transforms","MTF multispectral image sensor;high-frequency spatial information;panchromatic image scale;multispectral image band;high frequency spatial information;multispectral sensors;multispectral pansharpening","","","","21","IEEE","26 Nov 2020","","","IEEE","IEEE Conferences"
"A comparison analysis of pan-sharpening methods on Alsat-2A images","A. OUAHAB; M. F. BELBACHIR","Department of mathematics and computer science, University of Adrar, Algeria; Laboratoire Signaux, Systèmes et Données (LSSD), University of Science and Technonlogy Mohamed Boudiaf, El M’naouer, 31000, Oran, Algerie","2020 2nd International Conference on Mathematics and Information Technology (ICMIT)","26 Mar 2020","2020","","","138","141","The aim of pan-sharpening is to extract the spatial information from the panchromatic image and to inject them into the multispectral images. In this study, seven pan-sharpening techniques are tested on Alsat-2A images. The spectral and spatial qualities of fused images are tested using the Correlation Coefficient (CC), the Structural Similarity (SSIM), spatial correlation coefficient, and the quality with no reference (QNR). A comparative performance analysis of the CC, the SSIM, SCC and the QNR shows that the Brovey and the generalized intensity-hue-saturation (GIHS) perform the best among all the techniques in terms of spatial information. The Additive Trous Wavelet Transform with unitary injection model (ATWT) perform the best among all the techniques in terms of spectral information. The Additive Wavelet Luminance Proportional (AWLP) gives the best balance between the spatial and spectral information.","","978-1-7281-2580-0","10.1109/ICMIT47780.2020.9046990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9046990","pan-sharpening;Alsat-2A;image fusion;panchromatic","","geophysical image processing;image fusion;image resolution;image sensors;remote sensing;wavelet transforms","multispectral images;seven pan-sharpening techniques;Alsat-2A;spectral qualities;spatial qualities;SSIM;spatial correlation coefficient;QNR;comparative performance analysis;generalized intensity-hue-saturation;spatial information;unitary injection model;spectral information;additive wavelet luminance proportional;pan-sharpening methods;panchromatic image;additive Trous wavelet transform","","","","11","IEEE","26 Mar 2020","","","IEEE","IEEE Conferences"
"A CNN-Based Fusion Method for Super-Resolution of Sentinel-2 Data","M. Gargiulo; A. Mazza; R. Gaetano; G. Ruello; G. Scarpa","DIETI, University Federico II, Naples, Italy; DIETI, University Federico II, Naples, Italy; CIRAD, UMR-TETIS Laboratory, Montpellier, France; DIETI, University Federico II, Naples, Italy; DIETI, University Federico II, Naples, Italy","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","4713","4716","Sentinel-2 data represent a rich source of information for the community due to the free access and to the temporal-spatial coverage assured. However, some of the spectral bands are sensed at reduced resolution due to a compromise between technological limitations and Copernicus program's objectives. For this reason in this work we present a new super-resolution method based on Convolutional Neural Networks (CNNs) to rise the resolution of the short wave infra-red (SWIR) band from 20 to 10 meters, that is the highest resolution provided. This is accomplished by fusing the target band with the finer-resolution ones. The proposed solution compares favourably against several alternative methods according to different quality indexes. In addition we have also tested the use of the super-resolved band from an applicative perspective by detecting water basins through the Modified Normalized Difference Water Index (MNDWI).","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518447","Deep learning;convolutional neural network;normalized difference water index;Sentinel-2;pansharpening","Spatial resolution;Training;Indexes;Convolutional neural networks;Meters","geophysical image processing;image fusion;image resolution;neural nets;remote sensing;water resources","short wave infrared band;Sentinel-2 data;water basins;Modified Normalized Difference Water Index;MNDWI;Convolutional Neural Networks;Copernicus program;spectral bands;CNN-based fusion method;super-resolved band","","12","","17","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"DIM Moving Target Detection using Spatio-Temporal Anomaly Detection for Hyperspectral Image Sequences","Y. Li; J. Wang; X. Liu; N. Xian; C. Xie","Image Processing Center, Beihang University, Beijing, China; Image Processing Center, Beihang University, Beijing, China; Image Processing Center, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University; Infrared Detection Technology Research & Development Center, CASC","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","7086","7089","Dim moving target detection from hyperspectral image sequences, which contains temporal information as well as spectral information, has attracted researchers' interest for its crucial role in civil and military application. In this paper, we propose a novel spatio-temporal anomaly approach to solve the dim moving target detection problem. This approach calculates spatial anomaly map, temporal anomaly map using anomaly detection algorithm from spatial domain and temporal domain, respectively. To achieve motion consistency characteristic, this approach manages to generate the trajectory prediction map. After fusing the spatial anomaly map, the temporal anomaly map and the trajectory prediction map, target of interest can be easily detected from background. The proposed approach is applied to a test dataset of airborne target in the cloud clutter background. Experimental results confirm that the proposed approach can achieve a low false alarm rate as well as a high probability of detection.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517601","Dim target detection;Hyperspectral imagery sequences;Anomaly detection;Spatial and temporal processing","Hyperspectral imaging;Object detection;Image sequences;Trajectory;Anomaly detection;Principal component analysis","clutter;geophysical image processing;hyperspectral imaging;image fusion;image sequences;object detection;probability;remote sensing;target tracking","hyperspectral image sequences;dim moving target detection problem;spatial anomaly map;temporal anomaly map;anomaly detection algorithm;temporal domain;trajectory prediction map;airborne target;spatiotemporal anomaly detection;spatio-temporal anomaly approach","","9","","5","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Ship Detection Based on Deep Convolutional Neural Networks for Polsar Images","F. Zhou; W. Fan; Q. Sheng; M. Tao","Key Laboratory of Electronic Information Counter Measure and Simulation Technology of Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Electronic Information Counter Measure and Simulation Technology of Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Electronic Information Counter Measure and Simulation Technology of Ministry of Education, Xidian University, Xi'an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","681","684","In this paper, we proposed a ship detection method based on deep convolutional neural networks for PolSAR images. The proposed ship detector firstly segments PolSAR images into sub-samples using a sliding window of fixed size to effectively extract translational-invariant spatial features. Further, the modified faster region based convolutional neural network (Faster-RCNN) method is utilized to realize ship detection for ships with different sizes and fusion the detection result. Finally, the proposed method was validated using real measured NASAlJPL AIRSAR datasets by comparing the performance with the modified constant false alarm rate (CFAR) detector. The comparison results demonstrate the validity and generality of the proposed detection algorithm.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518589","Deep convolutional neural networks;polarimetric synthetic aperture radar (PolSAR);ship detection","Marine vehicles;Proposals;Detectors;Feature extraction;Convolutional neural networks;Image segmentation;Object detection","feature extraction;feedforward neural nets;geophysical image processing;image fusion;image segmentation;object detection;radar detection;radar imaging;radar polarimetry;remote sensing by radar;ships;synthetic aperture radar","modified constant false alarm rate detector;detection algorithm;deep convolutional neural networks;ship detection method;PolSAR image segmentation;translational-invariant spatial feature extraction;faster region based convolutional neural network method;faster-RCNN method;NASAlJPL AIRSAR datasets;constant false alarm rate detector;CFAR detector","","8","","8","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Hyperspectral Image Super-Resolution via Local Low-Rank and Sparse Representations","R. Dian; S. Li; L. Fang; J. Bioucas-Dias","Instituto de Telecomunicacões, Universidade de Lisboa, Lisbon, Portugal; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Instituto de Telecomunicacões, Universidade de Lisboa, Lisbon, Portugal","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","4003","4006","Remotely sensed hyperspectral images (HSIs) usually have high spectral resolution but low spatial resolution. A way to increase the spatial resolution of HSIs is to solve a fusion inverse problem, which fuses a low spatial resolution HSI (LR-HSI) with a high spatial resolution multispectral image (HR-MSI) of the same scene. In this paper, we propose a novel HSI super-resolution approach (called LRSR), which formulates the fusion problem as the estimation of a spectral dictionary from the LR-HSI and the respective regression coefficients from both images. The regression coefficients are estimated by formulating a variational regularization problem which promotes local (in the spatial sense) low-rank and sparse regression coefficients. The local regions, where the spectral vectors are low-rank, are estimated by segmenting the HR-MSI. The formulated convex optimization is solved with SALSA. Experiments provide evidence that LRSR is competitive with respect to the state-of-the-art methods.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519213","Hyperspectral image super-resolution;low rank;superpixels","Spatial resolution;Dictionaries;Hyperspectral imaging;Signal resolution","geophysical image processing;geophysical techniques;hyperspectral imaging;image fusion;image reconstruction;image representation;image resolution;remote sensing","state-of-the-art methods;SALSA;convex optimization;remotely sensed hyperspectral images;spectral vectors;local regions;spatial sense;variational regularization problem;respective regression coefficients;spectral dictionary;fusion problem;novel HSI super-resolution approach;high spatial resolution multispectral image;LR-HSI;low spatial resolution HSI;fusion inverse problem;high spectral resolution;HSIs;sparse representations;local low-rank;hyperspectral image super-resolution","","8","","15","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"U-Net Ensemble for Semantic and Height Estimation Using Coarse-Map Initialization","S. Kunwar","NestAI, Nepal","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","4959","4962","This is our submission to the IEEE GRSS Data Fusion Single-view Semantic 3D Challenge. The task of this track was to predict semantic labels and normalized DSM (nDSM) aboveground heights from a single-view imagery. RGB and Multi-Spectral Satellite data, along with semantic labels and corresponding height ground-truth were provided for training. We show, in this paper, that an ensemble of a few varied backbones employed in a U-Net architecture, is best at the semantic segmentation and height prediction tasks. Specific band combination from the Multi-Spectral Imagery and the use of hybrid of binary cross-entropy and Jaccard loss proved key in higher semantic accuracy. For height prediction, we show that the addition of a coarse-map initialized from either the global mean or median heights, specific to that particular class label to be valuable for fast convergence and accuracy.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899861","Semantic Segmentation;IEEE GRSS Data Fusion;normalized DSM;Semantic 3D","Semantics;Bridges;Training;Task analysis;Buildings;Vegetation;Data integration","entropy;geophysical image processing;image classification;image colour analysis;image fusion;image segmentation;learning (artificial intelligence);neural net architecture;remote sensing;terrain mapping","U-Net ensemble;height estimation;coarse-map initialization;IEEE GRSS Data Fusion Single-view Semantic 3D Challenge;semantic labels;normalized DSM;single-view imagery;MultiSpectral Satellite data;U-Net architecture;semantic segmentation;MultiSpectral Imagery;semantic estimation;nDSM;RGB data;binary cross-entropy;Jaccard loss","","6","","12","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Fishing forecasting system in Adriatic sea — A model approach based on a normalized scalar product of the SST gradient and CHL gradient vectors","K. Tijani; M. T. Chiaradia; A. Morea; R. Nutricato; L. Guerriero; G. Pasquariello","Politecnico di Bari, Bari, Puglia, IT; Politecnico di Bari, Bari, Puglia, IT; Politecnico di Bari, Bari, Puglia, IT; Geophysical Applications Processing, GAP srl, Bari, Italy; Politecnico di Bari, Bari, Puglia, IT; Consiglio Nazionale delle Ricerche, Roma, Lazio, IT","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","2257","2260","By mapping the concentration of chlorophyll-a (CHL) and the temperature of the sea surface (SST), satellite images reveal the complex dynamics of marine waters and prove to be a very powerful tool when used to detect potential fishing areas, significantly reducing the time of the search, the fuel consumption and the human effort, and simultaneously increasing the CPUE (catch per unit effort). In the present work, various techniques of multi-sensor, multi-resolution and multi-temporal data fusion are applied to multi-spectral satellite image data of MODIS-AQUA, MODIS-TERRA and VIIRS sensors, in order to detect ""fronts"" of chlorophyll concentration and temperature on the sea surface. According to the physical model of the phenomena, these fronts are generated by the upwelling of cold waters rich of nutrients (phytoplankton) which correspond to areas with a high concentration of pelagic fish and are characterized by high values of local gradients of SST and CHL with anti-parallel orientation. An automatic procedure has been developed to calibrate and validate the production in near-real time of daily maps of expected good fishing grounds to be provided to the FEDERPESCA fleet. The same procedure could be optimized also for other seas.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326256","Potential Fishing Zones (PFZ);Upwelling;Ocean Color Analysis;Multi-sensor data fusion","Ocean temperature;MODIS;Sea surface;Forecasting;Satellites;Data integration","geophysical image processing;gradient methods;image fusion;microorganisms;ocean temperature;oceanographic regions;oceanographic techniques;remote sensing by radar;seawater;sensor fusion","fishing forecasting system;Adriatic sea;normalized scalar product;SST gradient vector;CHL gradient vector;chlorophyll-a concentration;surface temperature;marine water;complex dynamics;potential fishing area;CPUE;multitemporal data fusion;MODIS-TERRA sensor;MODIS-AQUA sensor;VIIRS sensor;sea water upwelling;phytoplankton;antiparallel orientation;near-real daily map time production;FEDERPESCA;high pelagic fish concentration;fishing grounds","","4","","9","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"GEPATAR: A geotechnical based PS-InSAR toolbox for architectural conservation in Belgium","M. Shimoni; J. Lopez; J. Walstra; P. . -Y. Declercq; L. Bejarano-Urrego; E. Verstrynge; D. Derauw; R. Hayen; K. Van Balen","Signal and Image Centre, Belgian Royal military Academy (SIC-RMA), Brussels, Belgium; Signal and Image Centre, Belgian Royal military Academy (SIC-RMA), Brussels, Belgium; Geological Survey of Belgium, Royal Belgian Institute of Natural Sciences (RBINS), Brussels, Belgium; Geological Survey of Belgium, Royal Belgian Institute of Natural Sciences (RBINS), Brussels, Belgium; Building Materials and Building Technology Division, KU Leuven, Belgium; Building Materials and Building Technology Division, KU Leuven, Belgium; Central Spatial of Liege (CSL), Angleur, Belgium; Royal Institute for Cultural Heritage (KIK-IRPA), Brussels, Belgium; Building Materials and Building Technology Division, KU Leuven, Belgium","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","5555","5558","Ground displacements that cause structural damage to heritage buildings are precipitating cultural and economic value losses. The GEPATAR project (GEotechnical and Patrimonial Archives Toolbox for ARchitectural conservation in Belgium) aims creating an online interactive geoinformation tool that allows the user to view and to be informed about the Belgian heritage buildings at risk due to differential ground movements. In the last decade, Persistent Scatterer SAR interferometry (PS-InSAR) has proven to be a powerful technique for analysing earth surface deformation. In order to identify the level of risk at national and local scales, this information is integrated with the Belgian heritage data by means of a GIS environment interactive toolbox and fusion modules. This paper presents a description of the methodology implemented in the project together with the case study of Saint-Vincent church, located in Zolder in a former colliery zone, for which damage is assessed.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128263","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128263","PS-InSAR;differential ground movements;stability damage;historical buildings;GeoWeb toolbox","Strain;Buildings;Cultural differences;Interferometry;Monitoring;Time series analysis;Geology","buildings (structures);deformation;geographic information systems;geomorphology;geophysical image processing;image fusion;radar interferometry;remote sensing by radar;synthetic aperture radar","Belgium;ground displacements;structural damage;cultural value losses;economic value losses;GEPATAR project;online interactive geoinformation tool;Belgian heritage buildings;fusion modules;ground movements;Earth surface deformation;geotechnical-based PS-InSAR toolbox;Geotechnical and Patrimonial Archives Toolbox for Architectural Conservation;persistent scatterer SAR interferometry;Zolder;colliery zone","","4","","9","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Intrinsic Image Decomposition-Based Resolution Enhancement for Mineral Mapping","P. Duan; P. Ghamisi; R. Jackisch; X. Kang; R. Gloaguen; S. Li","College of Electrical and Information Engineering, Hunan University, China; Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany; Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany; College of Electrical and Information Engineering, Hunan University, China; Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany; College of Electrical and Information Engineering, Hunan University, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","4112","4115","Hyperspectral imaging plays an important role for mineral mapping in a nondestructive and noninvasive way. In this paper, a novel resolution enhancement method is proposed based on the principle of intrinsic image decomposition for mineral mapping. This method is based on an assumption that hyperspectral image (HSI) can be decomposed into a reflectance component and an illumination component. Based on this idea, the RGB image is first transformed into Intensity-Hue-Saturation (IHS) space, and the intensity channel is considered as the illumination component of the HSI with an ideal high spatial resolution. Then, the reflectance component of the ideal HSI is estimated with the downsampled HSI image and the downsampled intensity channel. Finally, the HSI with high resolution can be reconstructed by utilizing the estimated illumination and the reflectance components. Experimental results validate the effectiveness of the proposed method qualitatively and quantitatively by outperforming several state-of-the-art approaches.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323470","National Natural Science Foundation of China(grant numbers:61890962,61871179); China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323470","Hyperspectral image;mineral mapping;resolution enhancement;intrinsic image decomposition","Image resolution;Spatial resolution;Hyperspectral imaging;Lighting;Minerals;Image reconstruction;Image decomposition","geophysical image processing;image classification;image colour analysis;image enhancement;image fusion;image resolution;remote sensing","novel resolution enhancement method;mineral mapping;hyperspectral image;reflectance component;illumination component;RGB image;Intensity-Hue-Saturation space;ideal high spatial resolution;ideal HSI;downsampled HSI image;downsampled intensity channel;estimated illumination;intrinsic image decomposition-based resolution enhancement;hyperspectral imaging","","3","","11","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Deep Residual Spatial Attention Network for Hyperspectral Pansharpening","Y. Zheng; J. Li; Y. Li; Y. Shi; J. Qu","State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2671","2674","In this paper, we propose a deep residual spatial attention network (DRSAN) for hyperspectral (HS) pansharpening. Different from the existing methods, our newly proposed method not only considers the spatial information of both the panchromatic (PAN) and the HS image simultaneously, but also adaptively learns more informative features of spatial locations for details enhancement, which mainly includes four steps. Firstly, the spatial details of the enhanced PAN image are obtained through the structure tensor. Then we extract the spatial information of the upsampled HSI by using the guided filter. The integrated spatial information of both PAN and HS images is subsequently fed into the DRSAN to map the residual HSI between the upsampled HSI and the reference HSI, where several residual spatial attention blocks (RSABs) are cascaded to exploit more useful details information. Finally, the fused HSI is generated by the summation of the upsampled HSI and the reconstructed residual HSI. Extensive visual and quantitative assessments validate the superiority of our proposed DRSAN over the state-of-the-art HS pansharpening methods.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323620","National Nature Science Foundation of China(grant numbers:61901343); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323620","Hyperspectral pansharpening;structure tensor;guided filter;deep residual network;spatial attention","Pansharpening;Feature extraction;Data mining;Tensors;Hyperspectral imaging;Visualization;Training","feature extraction;geophysical image processing;image enhancement;image fusion;image resolution;image sensors;remote sensing;sensor fusion","deep residual spatial attention network;hyperspectral pansharpening;DRSAN;HS image;informative features;spatial locations;spatial details;enhanced PAN image;upsampled HSI;integrated spatial information;residual spatial attention blocks;useful details information;reconstructed residual HSI;details enhancement;structure tensor;RSAB","","3","","7","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A Simple Fusion Approach of Chlorophyll Images and Sea Surface Temperature Images for Improving the Detection of Moroccan Coastal Upwelling","Z. Elabidi; K. Minaoui; A. Tamim; H. Laanaya","Faculty of sciences In Rabat, Mohammed V university, Morocco; Faculty of sciences In Rabat, Mohammed V university, Morocco; Department of Marine Fisheries, Higher Institute of Marine Fisheries (ISPM), Agadir, Morocco; Faculty of sciences In Rabat, Mohammed V university, Morocco","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","7208","7211","In order to improve the decision-making on the Moroccan upwelling region detection, we present in this paper a simple and reliable fusion approach. In this context, we started by applying Fuzzy C-means algorithm on each 46 Sea Surface Chlorophyll images and on each 46 Sea Surface Temperature images during the year of 2014. After that, we implement post classification fusion by using logical AND operator set to combine FCM result of the both types and consequently having single image more informative and suitable for visual perception. The oceanographer validation indicate that the proposed methodology detect automatically and effectively the different Moroccan coastal upwelling scenarios of our database.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518889","Sea Surface Chlorophyll Image;Sea Surface Temperature Image;Fuzzy C-means;Post Classification Fusion;Moroccan Coastal Upwelling","Ocean temperature;Sea surface;Sea measurements;Databases;Satellites;Clustering algorithms","fuzzy set theory;geophysical image processing;image classification;image fusion;object detection;ocean temperature;oceanographic regions;oceanographic techniques;pattern clustering;remote sensing;visual perception","post classification fusion;oceanographer validation;visual perception;logical AND operator set;fuzzy c-means algorithm;Moroccan upwelling region detection;Moroccan coastal upwelling scenarios;sea surface temperature images;sea surface chlorophyll images;fusion approach","","3","","13","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Fusion of intensity/coherent information using region covariance features for unsupervised classification of SAR imagery","X. Yang; S. Tu; Y. Bai; W. Yang","School of Electronic Information, Wuhan University, Wuhan, China; Shanghai Institute of Satellite Engineering, Shanghai, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","941","944","Unsupervised classification of synthetic aperture radar (SAR) imagery is an essential step in SAR image interpretation. There is a growing demand for an efficient way to fuse multi-information of SAR imagery. This paper presents an intensity/coherent information fusion algorithm by using region covariance features for unsupervised classification. More precisely, we firstly extract the intensity properties and coherent characteristics from each pixel of SAR imagery, then use the region covariance descriptor to fuse the intensity and coherent features, and finally exploit the K-means algorithm to obtain the final unsupervised classification map. Experimental results on SAR imagery demonstrate the effectiveness of the proposed fusion scheme.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729238","unsupervised classification;synthetic aperture radar (SAR);feature fusion;region covariance","Synthetic aperture radar;Coherence;Feature extraction;Covariance matrices;Rivers;Buildings;Classification algorithms","geophysical image processing;image classification;image fusion;image resolution;remote sensing by radar;synthetic aperture radar","region covariance features;synthetic aperture radar imagery unsupervised classification;SAR image interpretation;SAR imagery multiinformation;intensity information fusion algorithm;intensity properties;coherent characteristics;region covariance descriptor;K-means algorithm;final unsupervised classification map;fusion scheme;coherent information fusion algorithm","","3","","15","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Multiresolution and Multimodality Sar Data Fusion Based on Markov and Conditional Random Fields for Unsupervised Change Detection","D. Solarna; G. Moser; S. B. Serpico","University of Genoa, Dept. of Electrical, Electronic, Telecommunication Eng. and Naval Architecture, Genoa, Italy; University of Genoa, Dept. of Electrical, Electronic, Telecommunication Eng. and Naval Architecture, Genoa, Italy; University of Genoa, Dept. of Electrical, Electronic, Telecommunication Eng. and Naval Architecture, Genoa, Italy","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","29","32","Current satellite missions (e.g., COSMO-SkyMed, Sentinel-1) collect single- or multipolarimetric synthetic aperture radar (SAR) images with multiple spatial resolutions and possibly short revisit times. The availability of heterogeneous data requires effective methods able to exploit all the available information. In the context of environmental monitoring and natural disaster recovery, this paper proposes an unsupervised change detection method able to properly fuse and exploit multiresolution and multimodality SAR data. The data fusion process is based on the estimation of the virtual images that would have been collected in case all the sensors worked at the same spatial resolution and on the definition of a probabilistic model based on generalized Gaussian distributions and Gram-Charlier approximations. The detection of changes is addressed in a probabilistic graphical framework through a novel conditional random field, by defining an energy function that is minimized through graph-cuts or belief propagation methods.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898122","synthetic aperture radar (SAR);multiresolution fusion;multimodality fusion;Markov random fields (MRF);conditional random fields (CRF)","Synthetic aperture radar;Spatial resolution;Lattices;Belief propagation;Maximum likelihood estimation;Markov processes","environmental monitoring (geophysics);Gaussian distribution;geophysical image processing;image fusion;image resolution;radar detection;radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","multiple spatial resolutions;heterogeneous data;environmental monitoring;natural disaster recovery;unsupervised change detection method;multiresolution;data fusion process;virtual images;spatial resolution;conditional random field;belief propagation methods;conditional random fields;current satellite missions;COSMO-SkyMed;Sentinel-1;multipolarimetric synthetic aperture radar images;SAR","","3","","14","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Robust Deep Hyperspectral Imagery Super-Resolution","J. Nie; L. Zhang; C. Wang; W. Wei; Y. Zhang","School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","847","850","Fusing a low spatial resolution (LR) hyperspectral image (HSI) with a high spatial resolution (HR) multi-spectral image (MSI) is an effective way for HSI super-resolution. When the input LR HSI and the HR MSI are clean, most of existing fusion based methods can produce pleasing results. However, the input HSI and MSI are often corrupted with random noise in practice, which can greatly degrade the performance of these methods. To address this problem, we present a robust deep HSI super-resolution method in this study. In contrast to leveraging a heuristic shallow sparsity or low-rank prior in previous methods, we propose to employ a deep convolution neural network as the prior of the latent HR HSI. With such a prior, the fusion based HSI super-resolution can be formulated as an end-to-end deep learning problem, which can be effectively solved with the back-propagation algorithm. Due to the deep structure, the proposed image prior is able to capture more powerful statistics of the latent HR HSI, and thus can still produce pleasing results with noisy input images. Experimental results on two benchmark datasets demonstrate the effectiveness of the proposed method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900117","Hyperspectral image super-resolution;deep convolution neural networks;unsupervised learning","Noise measurement;Image reconstruction;Spatial resolution;Convolution;Neural networks","convolutional neural nets;geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image resolution;learning (artificial intelligence);remote sensing","robust deep hyperspectral imagery super-resolution;low spatial resolution hyperspectral image;high spatial resolution multispectral image;MSI;input LR HSI;fusion based methods;input HSI;robust deep HSI super-resolution method;heuristic shallow sparsity;deep convolution neural network;end-to-end deep learning problem;deep structure;noisy input images","","3","","12","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Downscaling SMAP soil moisture images with wavelets transform and PCA method","G. Gabriel; V. Virginia","Facultad de Ingeniería y Ciencias Hídricas (FICH), Ciudad Universitaria, Santa Fe, Argentina; Facultad de Ingeniería y Ciencias Hídricas (FICH), Ciudad Universitaria, Santa Fe, Argentina","2017 First IEEE International Symposium of Geoscience and Remote Sensing (GRSS-CHILE)","31 Jul 2017","2017","","","1","4","The soil moisture estimation is important for the management of natural and agricultural-livestock resources. Passive and active microwave satellite sensors are suitable for quantifying the moisture in the first centimeters of the soil. Passive sensors allow precise soil moisture estimation, but with low spatial resolution. Therefore, it is necessary to fuse them with products with better spatial resolution to enhance their applications. The main problem for re-scaling soil moisture is to find a variable whose spatial variability were similar to that of soil moisture. In this paper the improvement of the pixel size of SMAP images are proposed by their fusion with precipitation maps of the GPM mission. Although there are different fusion techniques, the wavelets and principal component methods are selected for this work, since the literature indicates that they are the most used. The results suggest that GPM precipitation maps are suitable for introducing variability in soil moisture maps and that wavelets results in lower errors than the principal component method, when compared with in situ data. The spatial distribution of soil moisture observations is not good enough to verify the detail of the spatial variability.","","978-1-5386-0740-4","10.1109/GRSS-CHILE.2017.7996007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7996007","Soil moisture;SMAP;GPM;spatial resolutions;wavelets;PCA","Soil moisture;Principal component analysis;Media;Biomedical imaging;Image resolution","geophysical image processing;hydrological techniques;image fusion;moisture;remote sensing;soil","SMAP soil moisture images;wavelets transform;PCA method;soil moisture estimation;agricultural-livestock resources;natural resource management;passive microwave satellite sensors;active microwave satellite sensors;GPM mission;principal component methods;GPM precipitation maps","","3","","16","IEEE","31 Jul 2017","","","IEEE","IEEE Conferences"
"A Multi-Scale Densely Deep Learning Method for Pansharpening","Z. Xiang; L. Xiao; P. Liu; Y. Zhang","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","2786","2789","Pansharpening aims to produce a higher resolution multi-spectral (HRMS) image by fusing the spectral information in lower resolution multispectral (LRMS) image and the spatial information in corresponding high resolution panchromatic (PAN) image. In this work, we propose a multi-scale densely deep learning based pansharpening method. Following an end-to-end learning architecture, the proposed deep neural network contains three modules: 1) a parallel multi-scale convolutional layer is used to extract multiscale features of PAN image; 2) a global identity branch structure is adopted to preserve spectral structures; and 3) a dense learning block is integrated to improve the spectral-spatial expressive power. Compared with other state-of-the-art methods, experimental results obtained with our proposed method achieve high pansharpening quality in visualization and quantification.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898095","Pansharpening;Multiscale feature extraction;dense connection","Spatial resolution;Feature extraction;Indexes;Distortion;Kernel;Deep learning","convolutional neural nets;feature extraction;geophysical image processing;geophysical techniques;image fusion;image resolution;learning (artificial intelligence);neural net architecture;remote sensing","multiscale densely deep learning method;spectral information fusion;lower resolution multispectral image;spatial information;high resolution panchromatic image;multiscale densely deep learning based pansharpening method;end-to-end learning architecture;deep neural network;PAN image;higher resolution multispectral image;parallel multiscale convolutional layer;multiscale feature extraction;HRMS image;LRMS image","","3","","11","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Very High Resolution Optical Image Classification Using Watershed Segmentation and a Region-Based Kernel","A. De Giorgi; G. Moser; G. Poggi; G. Scarpa; S. B. Serpico","DITEN Dept., University of Genoa, Genoa, Italy; DITEN Dept., University of Genoa, Genoa, Italy; DIETI Dept., University of Naples “Federico II,”, Napoli; DIETI Dept., University of Naples “Federico II,”, Napoli; DITEN Dept., University of Genoa, Genoa, Italy","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1312","1315","In this paper, the problem of the spatial-spectral classification of very high-resolution optical images is addressed using a kernel- and region-based approach. A novel method based on integrating region-based or object-based information into a kernel machine is developed. A Gaussian process model is used to characterize each segment in a segmentation map and to define a region-based admissible kernel accordingly. This kernel is combined with a marker-controlled watershed segmentation that incorporates scale adaptivity. Spatial-spectral fusion capabilities are also ensured by combining the resulting classification method with composite kernels.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518526","Kernel machines;watershed segmentation;region-based classification;geospatial object-based image analysis (GEOBIA)","Kernel;Image segmentation;Support vector machines;Optical imaging;Gaussian processes;Training;Classification algorithms","Gaussian processes;geophysical image processing;hydrological techniques;image classification;image fusion;image segmentation;optical images;remote sensing","spatial-spectral fusion capabilities;resulting classification method;composite kernels;high resolution optical image classification;region-based kernel;spatial-spectral classification;high-resolution optical images;region-based approach;kernel machine;Gaussian process model;segmentation map;region-based admissible kernel;marker-controlled watershed segmentation;region-based information;object-based information","","2","","15","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"A New Spatiotemporal Data Fusion Method to Reconstruct High-Quality Landsat Ndvi Time-Series Data","X. Ling; R. Cao","The Yangtze Delta Region Institute (Huzhou), University of Electronic Science and Technology of China, Huzhou, P. R. China; The Yangtze Delta Region Institute (Huzhou), University of Electronic Science and Technology of China, Huzhou, P. R. China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2564","2567","Landsat NDVI time-series data have great potential in global change research. However, Landsat NDVI time-series data are quite discontinuous due to frequent cloud contamination. Although spatiotemporal data fusion technology has been widely used to reconstruct Landsat NDVI time-series data, the existing spatiotemporal fusion methods usually ignore the effective use of partially cloud-contaminated images. In this study, we presented a new spatiotemporal fusion method, which employed the cloud-free pixels in the partially cloud-contaminated images to Correct the inconsistency between MODIS and Landsat data in Spatiotemporal DAta Fusion (called CSDAF). By comparing with recently developed advanced algorithm IFSDAF, we found that CSDAF performed better in terms of both visual inspections and quantitative evaluations. Besides, CSDAF is simple and its running time is about 1/4 of that of IFSDAF. Because of high accuracy and running efficiency, we expect that CSDAF has the potential to be widely used in forestry investigations, ecological evaluations and other relevant fields.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554810","Landsat NDVI;Spatiotemporal fusion;Clear pixels;MODIS-Landsat;Time series data","Earth;Visualization;Artificial satellites;Time series analysis;Data integration;Forestry;Spatiotemporal phenomena","clouds;forestry;geophysical image processing;image fusion;remote sensing;sensor fusion;spatiotemporal phenomena;time series;vegetation mapping","new Spatiotemporal data Fusion method;high-quality Landsat ndvi time-series data;spatiotemporal data fusion technology;existing spatiotemporal fusion methods;partially cloud-contaminated images;spatiotemporal fusion method","","2","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Vecnet: A Spectral and Multi-Scale Spatial Fusion Deep Network for Pixel-Level Cloud Type Classification in Himawari-8 Imagery","Z. Wang; X. Kong; Z. Cui; M. Wu; C. Zhang; M. Gong; T. Liu","School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; School of Mathematics and Statistics, University of Melbourne, Melbourne, Australia; School of Computer Science, Faculty of Engineering, The University of Sydney, Sydney, Australia","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4083","4086","Classifying cloud type is essential for analyzing the Earth's radiation budget and global climate change. However, cloud type classification is a challenging task as it requires domain knowledge while extracting features, various sets of parameters obtained from satellites, thresholds determined, and human intervention for analysis. This study proposes a deep learning-based pixel-level cloud type classification method, VecNet, to alleviate the dependency of the domain knowledge. VecNet mainly utilizes 1 × 1 convolution to learn robust local spectral information to generate fine-grained predictions, and designs a semantic pyramid module to capture multi-scale features to accurately classify different-morphological clouds. We conduct extensive experiments on all year Himawari-8 satellite images (6622 images), and VectNet achieved a competitive result of 67.9% mIOU with real-time inference speed.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554737","cloud type classification;deep learning;convolutional neural network;Himawari-8 geostationary satellite","Convolutional  neural networks;Image color analysis;Clouds;Semantics;Feature extraction;Prediction algorithms;Real-time systems;Climate change","clouds;computer vision;feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing","vecnet;Himawari-8 imagery;Earth's radiation budget;global climate change;domain knowledge;deep learning-based pixel-level cloud type classification method;robust local spectral information;multiscale features;different-morphological clouds;year Himawari-8 satellite images","","2","","14","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A novel fusion method of SAR and optical sensors to reconstruct 3-D buildings","H. Zhang; H. -p. Xu; Z. -f. Wu","School of Electronics & Information Engineering, Beihang University, Beijing, China; School of Electronics & Information Engineering, Beihang University, Beijing, China; School of Electronics & Information Engineering, Beihang University, Beijing, China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","609","612","This paper investigates the method for 3-D buildings reconstruction using the fusion of SAR and optical sensors. First, the joint positioning equation is described as the foundation of this method. Then, the principal steps of this method are recommended. Image registration is employed to obtain a mapping function between the two images as the first step. And the 3-D coordinates are calculated with the joint positioning equation. Afterwards, the 3-D buildings are reconstructed using the 3-D coordinates obtained above. Finally, an experiment is conducted to show the reconstruction result of this method.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325837","3-D building;reconstruction;fusion;synthetic aperture radar;optical image","Synthetic aperture radar;Optical sensors;Optical imaging;Buildings;Image reconstruction;Adaptive optics;Mathematical model","buildings (structures);geophysical image processing;image fusion;image reconstruction;image registration;optical sensors;remote sensing by radar;synthetic aperture radar","fusion method;SAR sensor;optical sensor;3D building reconstruction;joint positioning equation;image registration;3D coordinate","","2","","8","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Multivariate Regression-Based Pan-Sharpening With Low Rank Regularization","Y. Zhang; H. Li; L. Xiao","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","7188","7191","Pan-sharpening is a fusion task of exploiting the spectral information in low resolution multispectral images (LRMS) with spatial information in a corresponding high resolution panchromatic image (PAN). Under the component substitution framework, a multivariate regression based fidelity term is presented to enforce high resolution spatial detail injection, while a low rank regularization term is proposed to capture intrinsic structures of latent high-resolution multispectral images (HRMS). To this end, a joint optimizing pan-sharpening model is proposed to establish a trade-off mechanism between the spatial detail injection and spectral-spatial preserving capacity. Finally, the Augmented Lagrangian Multiplier (ALM) method is used to develop a pan-sharpening algorithm. Experiments demonstrate that the proposed algorithm can achieve higher spatial and spectral resolution than several state-of-the-art methods.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518822","Pan-sharpening;low-rank regularization;detail injection;Augmented Lagrangian Multiplier","Spatial resolution;Indexes;Distortion;Multivariate regression;Optimization;Correlation","geophysical image processing;image fusion;image resolution;regression analysis;remote sensing","spatial information;corresponding high resolution panchromatic image;component substitution framework;multivariate regression based fidelity term;high resolution spatial detail injection;low rank regularization term;capture intrinsic structures;high-resolution multispectral images;joint optimizing pan-sharpening model;pan-sharpening algorithm;higher spatial resolution;spectral resolution;multivariate regression-based pan-sharpening;fusion task;spectral information;low resolution multispectral images;augmented Lagrangian multiplier method;ALM","","2","","10","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"A Novel Approach for Abundance Estimation Using Discontinuity Preserving Prior","J. R. Patel; M. V. Joshi; J. S. Bhatt","Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar, India; Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar, India; Indian Institute of Information Technology, Vadodara, India","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","6167","6170","In this paper, we propose an algorithmic approach to estimate fractions (abundances) of materials (endmembers) in a pixel by considering linear mixture model (LMM) and prior knowledge of endmembers. We propose the use of inhomogeneous Gaussian Markov random field (IGMRF) that captures the smoothness as well as preserves the discontinuities among abundance values. We obtain the IGMRF parameters for the proposed prior using the initial estimate of abundances and both the abundances and IGMRF parameters are refined by optimizing an objective function. A two-step iterative approach is proposed to obtain the final estimates of both the abundance maps and their prior parameters. In order to demonstrate the efficiency of the proposed approach, we conduct experiments on the synthetic hyperspectral images (HSIs) with different noise levels as well as on the real HSIs and compare the result with other state-of-the-art approaches.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517950","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517950","Abundance estimation;Spectral unmixing;Fully constrained least squares (FCLS);Inhomogeneous Gaussian Markov random field (IGMRF)","Signal to noise ratio;Estimation;Iterative methods;Hyperspectral imaging;Optimization;Markov processes","Gaussian processes;geophysical image processing;geophysical techniques;geophysics computing;image fusion;image resolution;iterative methods;Markov processes;remote sensing","noise levels;synthetic hyperspectral images;linear mixture model;discontinuity preserving prior;state-of-the-art approaches;abundance maps;two-step iterative approach;IGMRF parameters;discontinuity;inhomogeneous Gaussian Markov random field;algorithmic approach;abundance estimation","","2","","18","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Collaborative total variation for hyperspectral pansharpening","P. Addesso; M. Dalla Mura; L. Condat; R. Restaino; G. Vivone; D. Picone; J. Chanussot","DIEM, University of Salerno, Italy; GIPSA-Lab, CNRS, University of Grenoble Alpes, Grenoble, France; GIPSA-Lab, University of Grenoble Alpes, Grenoble, France; DIEM, University of Salerno, Italy; DIEM, University of Salerno, Italy; DIEM, University of Salerno, Italy; GIPSA-Lab, University of Grenoble Alpes, Grenoble, France","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","2597","2600","Variational methods are widely used in image processing for problems ranging from denoising to data fusion. In this paper we focus on a recent regularization method, called Collaborative Total Variation, applied to the hyperspectral pansharpening, which deals with the fusion of low resolution hyperspectral and high resolution panchromatic images. The effectiveness of this novel approach is evaluated for different Collaborative Norms and the assessment is performed on the Pavia University dataset.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127527","Data fusion;Hyperspectral pansharpening;Convex optimization;Total variation;Deconvolution","Spatial resolution;Hyperspectral imaging;Collaboration;Data integration;Indexes","geophysical image processing;image classification;image fusion;image processing;image resolution;remote sensing","called Collaborative Total Variation;hyperspectral pansharpening;low resolution hyperspectral;high resolution panchromatic images;variational methods;image processing;data fusion;regularization method;Pavia University dataset","","2","","11","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"A Comparison of Hyper-Sharpening Algorithms for Fusing VNIR and SWIR Bands of WorldView-3 Satellite Imagery","H. Park; J. Choi","School of Civil Engineering, Chungbuk National University, Cheongju, Korea; School of Civil Engineering, Chungbuk National University, Cheongju, Korea","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5124","5127","In this study, visible and near-infrared (VNIR) and shortwave infrared (SWIR) bands of WorldView-3 imagery were sharpened to the spatial resolution of a panchromatic image. We performed experiments on three band schemes according to a method for generating an optimal panchromatic image. Various pan-sharpening algorithms based on component-substitution (CS) and multi-resolution analysis (MRA) were applied to each band scheme. Quantitative and qualitative assessments were performed using the quality indices and visual inspection.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517762","WorldView-3 sharpening;multiple linear regression;selected and synthesized band scheme","Spatial resolution;Satellites;Hyperspectral imaging;Classification algorithms;Linear regression","image fusion;image resolution;remote sensing","spatial resolution;band scheme;optimal panchromatic image;pan-sharpening algorithms;multiresolution analysis;hyper-sharpening algorithms;WorldView-3 satellite imagery","","1","","12","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Unsupervised Blur Kernel Learning for Pansharpening","A. Guo; R. Dian; S. Li","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","633","636","Deep learning (DL) for pansharpening has recently attracted considerable attentions. To construct training data, DL based pansharpening approaches often downsample the original multispectral image (MSI) and panchromatic image (PAN) with fixed blur kernel, which can be different from the real point spread functions (PSF) of the satellites. And a mismatched blur kernel will cause the pansharpening performance to drop dramatically. In this paper, we propose a novel blur kernel learning method for pansharpening, which can learn the spatial and spectral blur kernels between PAN and MSI in an unsupervised way. Specifically, we analyze the relationship between PAN and MSI, and then construct a mini net for blur kernel learning. Once the spatial blur kernel is found, a convolutional neural network (CNN) for pansharpening is trained on the downsampled dataset using the learned spatial blur kernel. Experimental results on GF-2 images demonstrate the superiority of the proposed method.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324543","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324543","Pansharpening;blur kernel learning;deep learning;hyperspectral image super-resolution","Kernel;Pansharpening;Image reconstruction;Feature extraction;Training;Spatial resolution;Satellites","geophysical image processing;image fusion;image resolution;image restoration;image sampling;image sensors;learning (artificial intelligence);neural nets;optical transfer function;remote sensing","MSI;panchromatic image;PAN;fixed blur kernel;point spread functions;mismatched blur kernel;pansharpening performance;novel blur kernel;spatial blur kernels;spectral blur kernels;learned spatial blur kernel;GF-2 images;unsupervised blur kernel learning;deep learning;considerable attentions;training data;pansharpening approaches","","1","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Robust Coupled Non-Negative Matrix Factorization for Hyperspectral and Multispectral Data Fusion","T. Ahmad; R. B. Lyngdoh; S. S. Anand; P. K. Gupta; A. Misra; S. Raha","Space Applications Centre, ISRO, Ahmedabad, India; Space Applications Centre, ISRO, Ahmedabad, India; Space Applications Centre, ISRO, Ahmedabad, India; Space Applications Centre, ISRO, Ahmedabad, India; Space Applications Centre, ISRO, Ahmedabad, India; Indian Institute of Science, Bangalore, India","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2456","2459","In recent time, Hyperspectral(HS) and multispectral(MS) data fusion based on spectral unmixing methods has become an active area of research. Coupled non-negative matrix factorization (CNMF), one among many unmixing-based data fusion approaches, performs alternating unmixing of the HS and MS data while connecting the results by point spread function and spectral response function of the sensors. However, CNMF operates exclusively on the spectral information of the HS and MS data and also disregards the spatial distribution of the data. In this paper, we propose an extended linear mixing model approach to enhance the spatial resolution of HS data. The proposed method extends the commonly used linear mixing model for data fusion by introducing an additional term that accounts for the non-linearity effects as well. The results of the simulation obtained from the analysis on various synthesized datasets suggest that the proposed method can significantly improve the Peak signal-to-noise ratio (PSNR), minimize the relative dimensionless global error (ERGAS) of fusion, and also competes with state-of-the-art approaches.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553681","Hyperspectral;Multispectral;Data fusion;Nonlinear unmixing;Non Negative Matrix Factorization","PSNR;Graphical models;Data integration;Peak to average power ratio;Sensor fusion;Data models;Sensors","geophysical image processing;image fusion;image resolution;matrix decomposition;remote sensing","data fusion approaches;point spread function;spectral response function;CNMF;spectral information;extended linear mixing model approach;HS data;nonlinearity effects;robust coupled nonnegative matrix factorization;multispectral data fusion;spectral unmixing methods","","1","","15","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Semantic 3D Reconstruction Using Multi-View High-Resolution Satellite Images Based on U-Net and Image-Guided Depth Fusion","R. Qin; X. Huang; W. Liu; C. Xiao","Department of Civil, The Ohio State University; Department of Electrical and Computer Engineering, The Ohio State University; Department of Electrical and Computer Engineering, The Ohio State University; Department of Electrical and Computer Engineering, The Ohio State University","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","5057","5060","This paper reports our workflow for multi-view semantic stereo reconstruction in the IEEE Data Fusion Contest (DFC) of 2019. The workflow mainly consists of two parts: semantic classification and multi-view stereo digital surface model (DSM) generation. Our strategy involves the interaction of these two components for better classification and DSM results. We firstly classify semantic objects using only original image information and then project them to the generated DSM for voting and post-refinement. An example-based pair selection strategy is used to pre-selected pairs for dense matching, and the fused DSM is performed through an image-guided median filter. Since the adjusted rational polynomial coefficient (RPC) parameters are not accurate in the dataset, these DSMs are co-registered after production. In our workflow, we only consider non-vegetation and non-water area for registration to obtain more robust co-registration result. These co-registered DSMs are fused and refined using the result from the semantic classification. Our method achieved 73.00% of mIoU-3 in the testing data in the contest.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900588","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900588","Semi-global Matching;Pair Selection;DSM Fusion;Semantic Classification","Semantics;Image segmentation;Buildings;Three-dimensional displays;Testing;Satellites;Training","feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;image matching;image reconstruction;image registration;image resolution;image segmentation;median filters;remote sensing;stereo image processing","semantic 3d reconstruction;multiview high-resolution satellite images;image-guided depth;multiview semantic stereo reconstruction;IEEE Data Fusion Contest;semantic classification;multiview stereo digital surface model generation;semantic objects;original image information;generated DSM;example-based pair selection strategy;pre-selected pairs;fused DSM;image-guided median filter;adjusted rational polynomial coefficient parameters;robust co-registration result","","1","","18","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Research on the incomplete point cloud data repairing of the large-scale scene buildings","Y. Li; L. Li; L. Niu; T. Huang; Y. Li","School of Surveying and Land Information Engineering, Henan Polytechnic University, Jiaozuo, China; School of Surveying and Land Information Engineering, Henan Polytechnic University, Jiaozuo, China; School of Surveying and Land Information Engineering, Henan Polytechnic University, Jiaozuo, China; School of Surveying and Land Information Engineering, Henan Polytechnic University, Jiaozuo, China; School of Surveying and Land Information Engineering, Henan Polytechnic University, Jiaozuo, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","6726","6729","Both airborne and mobile LiDAR are suitable for the fast information acquisition of large-scale urban buildings, which respectively express the rooftop and the facade information of buildings. The fusion of airborne and mobile LiDAR can provide a relatively complete building point cloud, which is beneficial to constructing precise building models. Due to the different data acquisition methods, airborne LiDAR can get the relatively complete point cloud data of the building rooftop, while the building facade point cloud from mobile LiDAR is often incomplete or even missing because of the tree occlusion or the limitation of operating conditions, which to some extent limits the application of the fused data. To solve the problems above, a method of building point cloud similarity matching based on airborne LiDAR and mobile LiDAR data fusion is proposed. Finally a test result is provided to illustrate the proposed algorithm.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730756","Airborne LiDAR;Mobile LiDAR;Building Reconstruction;Point Cloud Repairing","Buildings;Three-dimensional displays;Laser radar;Mobile communication;Atmospheric modeling;Data models;Data mining","buildings (structures);data acquisition;geophysical image processing;image fusion;optical radar;remote sensing by laser beam","point cloud data;large-scale scene building;airborne LiDAR;mobile LiDAR;large-scale urban building;building point cloud;data acquisition method;building rooftop;tree occlusion;LiDAR data fusion","","1","","13","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Critical Analysis of Fusion Algorithms for Digital Agriculture: An Efficient Application of PALSAR Data","V. Mittal; D. Singh","Electronics and Communication Engg. Deptt., National Institute of Technology Kurukshetra, Kurukshetra, India; Electronics and Communication Engg. Deptt., Indian Institute of Technology Roorkee, Roorkee, India","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","5917","5920","Satellite imagery is increasingly being used for the important application of digital agriculture. Efficient estimation of the areas of various land covers such as water, urban, wetland, bare soil, short vegetation and tall vegetation is needed for proper planning of various agriculture activities such as availability of resources, estimating cultivable area and grain yield etc. on a large scale in an unsupervised manner. It is further desirable that the developed techniques should rely on observations acquired using only one sensor that provides data 24x7x365 circumventing the problems of procuring data from multiple agencies, co-registering multiple sensor data etc. Hence, in this paper, three commonly used pixel based fusion techniques spatial frequency SF), principal component analysis PCA and expectation maximization EM algorithm are critically analyzed for fusing multi-polarized high resolution PALSAR data channels among themselves for the agriculture applications. K-means unsupervised algorithm has been applied for classifying various land covers. Unsupervised classification approach as an important step towards development of an autonomous land cover information system with minimum human intervention.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898580","PolSAR land cover classification;pixel level fusion;unsupervised classification","Principal component analysis;TV;Agriculture;Vegetation mapping;Classification algorithms;Optical imaging;Optical sensors","agriculture;expectation-maximisation algorithm;geophysical image processing;image classification;image fusion;image resolution;land cover;learning (artificial intelligence);principal component analysis;remote sensing by radar;terrain mapping;vegetation","critical analysis;fusion algorithms;digital agriculture;efficient application;satellite imagery;bare soil;short vegetation;tall vegetation;agriculture activities;cultivable area;grain yield;unsupervised manner;developed techniques;multiple agencies;multiple sensor data;multipolarized high resolution PALSAR data channels;agriculture applications;unsupervised classification approach;autonomous land cover information system;pixel based fusion techniques spatial frequency SF;expectation maximization EM algorithm","","1","","23","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Block-Based and Segmentation-Based Approaches for Component Substitution based Hyperspectral Pansharpening","S. Kahraman; G. Yeşilyurt; A. Ertürk; S. Ertürk","Kocaeli University Laboratory of Image and Signal Processing (KULIS), Kocaeli University, Turkey; Kocaeli University Laboratory of Image and Signal Processing (KULIS), Kocaeli University, Turkey; Kocaeli University Laboratory of Image and Signal Processing (KULIS), Kocaeli University, Turkey; Kocaeli University Laboratory of Image and Signal Processing (KULIS), Kocaeli University, Turkey","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","8054","8057","Pansharpening is the fusion of panchromatic (PAN) image and multispectral (MS) or hyperspectral (HS) images and provides high spatial and high spectral resolution MS or HS images. Pansharpening mainly extracs the high frequency details from the PAN image, and then injects these details to the MS or HS image. This detail injection procedure can be performed in a variety of ways: using global, block-based or clustering-based techniques. In this paper, block-based and clustering-based approaches are utilized for standart component substitution pansharpening approaches, namely Intensity Hue Saturation (IHS), Brovey Transform (BT), Gram Schmidt (GS) orthagonalization procedure and Principal Component Analysis (PCA) techniques. Both nonoverlapping and overlapping blocks are considered, along with various segmentation approaches such as k-means, Iterative Self Organizing Data Analysis Techniques Algorithm (ISODATA) and Simple Linear Iterative Clustering (SLIC). Two datasets with different characteristics are used in order to evaluate the approaches, and the block-based and segmentation-based approaches are shown to provide enhanced performance.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519556","Block-based;hyperspectral;pansharpening;segmentation;superpixels","Image segmentation;Principal component analysis;Hyperspectral imaging;Spatial resolution;Histograms;Image color analysis","data analysis;geophysical image processing;image colour analysis;image fusion;image resolution;image segmentation;iterative methods;pattern clustering;principal component analysis;remote sensing","nonoverlapping blocks;segmentation approaches;Self Organizing Data Analysis Techniques Algorithm;Principal Component Analysis techniques;Gram Schmidt orthagonalization procedure;standart component substitution pansharpening approaches;clustering-based approaches;global block-based;detail injection procedure;PAN image;high frequency details;panchromatic image;hyperspectral pansharpening;segmentation-based approaches","","1","","18","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"A Hybrid Model based on Fused Features for Detection of Natural Disasters from Satellite Images","T. Gupta; S. Roy","Centre of Excellence in Disaster Mitigation and Management (CoEDMM); Department of Computer Science and Engineering, Indian Institute of Technology Roorkee, India","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1699","1702","Earthquakes, floods, tsunami, and other natural disasters are appearing as worldwide threats because of their widespread destruction that results in thousands of human and economic losses. It is vital for first responders to know the root cause of damages in a region so that the emergency response activities can be planned accordingly and more effectively. We proposed a framework for the detection and recognition of natural disasters from satellite images. In this work, satellite images of six different types of disasters are considered, namely earthquake, volcanic eruption, flood, tsunami, and hurricane. The framework relies on the fusion of wavelet image scattering features and local binary pattern features for constructing the final feature vector. We also compared the accuracy of our framework with the existing state-of-the-art hand-crafted and machine learning models. Simulation results confirm that the proposed framework is able to recognize the type of natural disaster from the satellite images with an accuracy of 99.59% (Kappa coefficient 98.54% and F-Score 99.40%). The proposed approach results in less computational cost while achieving better accuracy compared to the deep convolutional neural network. We believe that the proposed model can be integrated with satellite imagery for locating the geographical regions affected by the multiple natural disaster events at the same time or at short intervals.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324611","natural disasters;satellite images;wavelet;LBP;deep features","Satellites;Feature extraction;Sensitivity;Scattering;Decision trees;Wavelet transforms;Earthquakes","deep learning (artificial intelligence);disasters;earthquakes;emergency services;feature extraction;floods;geophysical image processing;image fusion;image recognition;object detection;remote sensing;tsunami;wavelet transforms","flood;tsunami;wavelet image scattering features;local binary pattern features;feature vector;satellite images;satellite imagery;fused features;earthquake;multiple natural disaster event detection;economic losses;emergency response activities;natural disaster recognition;volcanic eruption;hurricane;machine learning models;hand-crafted models;deep convolutional neural network;geographical regions","","1","","25","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A novel two-stage guided filtering based pansharpening method","Y. Zhang; X. Li; A. Gao; L. Li; S. Yue","School of Electronics and Information, Northwestern Polytechnical University, P. R. China; School of Electronics and Information, Northwestern Polytechnical University, P. R. China; School of Electronics and Information, Northwestern Polytechnical University, P. R. China; School of Electronics and Information, Northwestern Polytechnical University, P. R. China; School of Computer Science, University of Lincoln, Lincoln, UK","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","2609","2612","Pansharpening methods generally inject the missing spatial details from a high spatial resolution panchromatic (PAN) image into the corresponding co-registered low spatial resolution multispectral (MS) images while preserving the spectral information. However, most of methods extract the details only from PAN image, which may lead to distortions in the sharpened results. Motivated by this, we present a novel two-stage guided filtering based pansharpening method. In the preliminary stage, an injection model based on multi-channel guidance filtering is designed to keep the spectral fidelity of MS imagery. Then a single channel guidance filtering based injection model is proposed to enhance the details in the second stage. The proposed method is tested and verified by GeoEye-1 satellite images. Qualitative and quantitative analyses demonstrate the superiority of the proposed method compared with some state-of-the-art guided filtering based pansharpening methods.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127529","Pansharpening;guided filter;panchromatic;multispectral","Histograms;Filtering;Indexes;Geoscience","geophysical image processing;image fusion;image registration;image resolution;remote sensing","two-stage guided filtering based pansharpening method;missing spatial details;high spatial resolution panchromatic image;PAN image;injection model;multichannel guidance filtering;GeoEye-1 satellite images;co-registered low spatial resolution multispectral images;state-of-the-art guided filtering based pansharpening methods.","","1","","12","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Exploring the Fusion of Sentinel-1 SAR and Sentinel-2 MSI Data for Built-Up Area Mapping Using Deep Learning","S. Hafner; Y. Ban; A. Nascetti","Division of Geoinformatics, KTH Royal Institute of Technology; Division of Geoinformatics, KTH Royal Institute of Technology; Division of Geoinformatics, KTH Royal Institute of Technology","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4720","4723","This research explores the potential of combining Sentinel-1 C-band Synthetic Aperture Radar (SAR) and Sentinel-2 MultiSpectral Instrument (MSI) data for Built-Up Area (BUA) mapping using deep learning. A lightweight U-Net model is trained using openly available building footprint reference data in North America and tested in four cities across three additional continents. The best test performance in terms of F1 score was achieved by the joint use of SAR and multispectral data (0.676), followed by multi-spectral (0.611) and SAR data (0.601). The developed fusion approach is particularly promising to distinguish BUA in low-density residential neighborhoods. Furthermore, our fusion approach compares favorably to the state-of-the-art in BUA mapping in the selected cities. However, associated with the diverse characteristics of human settlements around the world, considerable differences in accuracy among the test cities were observed. This indicates the need for more sophisticated fusion techniques to improve CNN model generalization and for adding more diverse training data.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553448","Swedish National Space Agency; ESA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553448","Sentinel-1;Sentinel-2;Built-up area mapping;data fusion;deep learning","Deep learning;Instruments;Urban areas;Training data;Optical imaging;Data models;Internet","convolutional neural nets;geophysical image processing;image fusion;learning (artificial intelligence);radar imaging;remote sensing by radar;synthetic aperture radar","Sentinel-1 SAR;Sentinel-2 MSI data;Built-Up Area mapping;deep learning;SAR data;BUA mapping;fusion techniques;diverse training data;building footprint reference data;Sentinel-1 C-band synthetic aperture radar;lightweight U-Net model;Sentinel-2 multispectral instrument data;F1 score","","1","","8","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Satellite Data Fusion of Multiple Observed XCO2 using Compressive Sensing and Deep Learning","P. Nguyen; S. Shivadekar; S. S. Laya Chukkapalli; M. Halem","Department of Computer Science and Electrical Engineering, University Of Maryland Baltimore County; Department of Computer Science and Electrical Engineering, University Of Maryland Baltimore County; Department of Computer Science and Electrical Engineering, University Of Maryland Baltimore County; Department of Computer Science and Electrical Engineering, University Of Maryland Baltimore County","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2073","2076","Providing climate data records to infer seasonal and interannual variations from multiple heterogeneous sources is a challenging data fusion. We combine the Compressive Sensing (CS) and Deep Learning (DL) into a single framework for fusing data from multiple sensors to improve spatial and temporal resolution for long term analysis. CS is used as an initial guess to combine data from multiple sources. DL models, using Long Short-Term Memory Neural Network (LSTM/RNN), Convolutional Neural Network (CNN), refine and further improve the data fusion from CS algorithm's output. The proposed framework has been tested the using daily global observations from two satellites the NASA Orbiting Carbon Observatory-2 (OCO-2) and the JAXA Greenhouse gases from Orbiting Satellites (GOSAT). Our framework achieves lower errors and high correlation compared with the original data. The quality of fused data is evaluated by comparing again AmeriFlux ground station's datasets. Long term trends using fused data over the United States indicate an increase of 8 parts per million (ppm) annually in XCO2 over four years with Root Mean Square Errors of 0.39 ppm and correlation of 0.98 compared with original data. Interannual variability of the seasonal cycle shows an increase in years 2015 - 2017, but a sharp decrease in 2018.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323861","NASA(grant numbers:NNH16ZDA001N-AIST16-0091); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323861","Compressive Sensing;Deep Learning;LSTM;Convolutional Neural Network;Data fusion;XCO2;OCO-2;GOSAT","Deep learning;Satellites;Data integration;Market research;Orbits;Sensors;Compressed sensing","air pollution measurement;atmospheric composition;atmospheric techniques;geophysical image processing;image fusion;learning (artificial intelligence);neural nets;remote sensing","fused data;satellite data fusion;multiple observed XCO2;Compressive Sensing;Deep Learning;climate data records;seasonal variations;interannual variations;multiple heterogeneous sources;data fusion;single framework;multiple sensors;Long Short-Term Memory Neural Network;CS algorithm;daily global observations;NASA Orbiting Carbon Observatory-2;JAXA GOSAT;Greenhouse gases from Orbiting Satellites;OCO-2;Convolutional Neural Network;AmeriFlux ground station datasets","","","","14","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Target Detection in Multispectral Images via Detail Enhanced Pansharpening","V. Tarverdiyev; I. Erer; N. H. Kaplan; N. Musaoğlu","Istanbul Technical University, Istanbul, Turkey; Istanbul Technical University, Istanbul, Turkey; Erzurum Technical University, Yakutiye, Erzurum, Turkey; Istanbul Technical University, Istanbul, Turkey","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1544","1547","Object detection in high resolution satellite images has recently become a major concern in new geospatial information methods. The higher spatial resolution with spectral information provides better detection results. Therefore, increasing the image resolution prior to the object detection is important. For this purpose, pansharpening, which uses complementary information from MS and PAN images, is gaining popularity as it helps to increase spatial resolution while preserving the spectral information. This study proposes a detailed enhanced scheme for pansharpening to improve the detection results. Several deep learning models are trained on raw dataset, as well as on the detail enhanced pansharpened images. It is shown that the training stage using proposed detail enhanced scheme provides better detection results compared to classical pansharpening or raw data based training for different deep networks.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884355","target detection;multispectral images;deep learning;pansharpening;image enhancement","Training;Satellites;Image edge detection;Pansharpening;Object detection;Optical imaging;Optical sensors","geophysical image processing;geophysical signal processing;image classification;image enhancement;image fusion;image resolution;learning (artificial intelligence);object detection;remote sensing","target detection;multispectral images;detail enhanced pansharpening;object detection;high resolution satellite images;geospatial information methods;higher spatial resolution;spectral information;image resolution;complementary information;detailed enhanced scheme;detail enhanced scheme;classical pansharpening","","","","9","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Improving the Spatiotemporal Resolution of Land Surface Temperature Data Using Disaggregation and Fusion Techniques: A Comparison","K. Sara; E. Rajasekaran","Department of Civil Engineering, Indian Institute of Technology Bombay, Mumbai, India; Interdisciplinary Program in Climate Studies, Indian Institute of Technology Bombay, Mumbai, India","2020 IEEE India Geoscience and Remote Sensing Symposium (InGARSS)","23 Feb 2021","2020","","","46","49","Land Surface Temperature (LST) and its diurnal variation are important parameters for several applications. Thermal sensors in polar orbiting and geostationary orbiting satellites can provide LST data at high spatial and temporal resolutions respectively. This study aims to generate high spatiotemporal LST by combining the coarse resolution geostationary satellite data (INSAT 3D) with the Moderate Resolution Imaging Spectroradiometer (MODIS) LST product using spatial disaggregation (DisTrad model) and spatiotemporal fusion (STITFM model) techniques. In addition, the ability of these two methods to properly represent the diurnal temperature cycle (DTC) is also examined. It was found that the spatial disaggregation method provided relatively better results than spatiotemporal fusion technique in improving the spatiotemporal resolution of LST.","","978-1-7281-3114-6","10.1109/InGARSS48198.2020.9358936","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9358936","Land surface temperature;disaggregation;spatiotemporal fusion","Solid modeling;Land surface;Data models;Orbits;Land surface temperature;Spatiotemporal phenomena;Spatial resolution","geophysical image processing;geophysical techniques;image fusion;image resolution;land surface temperature;remote sensing","Moderate Resolution Imaging Spectroradiometer LST product;DisTrad model;STITFM model;diurnal temperature cycle;spatial disaggregation method;spatiotemporal fusion technique;spatiotemporal resolution;land surface temperature data;fusion techniques;diurnal variation;thermal sensors;polar orbiting;geostationary orbiting satellites;LST data;high spatiotemporal LST;coarse resolution geostationary satellite data;INSAT 3D","","","","6","IEEE","23 Feb 2021","","","IEEE","IEEE Conferences"
"Multisensor Feature Fusion Using Low-Rank Modeling and Component Analysis","B. Rasti; P. Ghamisi; R. Gloaguen","Faculty of Electrical and Computer Engineering, University of Iceland; Helmholtz-Zentrum Dresden-Rossendorf (HZDR), Helmholtz Institute Freiberg for Resource Technology; Helmholtz-Zentrum Dresden-Rossendorf (HZDR), Helmholtz Institute Freiberg for Resource Technology","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","4811","4814","In this paper, we propose a framework to fuse features extracted from hyperspectral and Light Detection And Ranging (LiDAR)-derived data. Spatial and elevation features are extracted from multisensor data using extinction profiles (EP). All the features, including the spectral ones, are fused using sparse and smooth low-rank analysis (SSLRA). In terms of classification accuracy, the proposed framework outperforms other studied fusion techniques used in the experiments.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8897704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8897704","Extinction profile;hyperspectral image;LiDAR;low-rank modeling;multisensor fusion;sparse and smooth analysis","Feature extraction;Laser radar;Hyperspectral imaging;Radio frequency;Fuses","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;optical radar;remote sensing","low-rank modeling;component analysis;LiDAR;spatial elevation features;multisensor data;extinction profiles;feature extraction;light detection and ranging;sparse and smooth low-rank analysis;SSLRA;multisensor feature fusion techniques;hyperspectral imaging","","","","13","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Pan-Sharpening with Hessian Nuclear Norm Induced Spatial Consistency","P. Liu; L. Xiao; S. Tang","School of Computer Science, Nanjing University of Posts and Telecommunications, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, China; Department of Criminal Science and Technology, Nanjing Forest Police College, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5116","5119","In this paper, we propose a new variational pan-sharpening method with Hessian nuclear norm induced spatial consistency, which aims to fuse a low resolution (LR) multispectral (MS) image and a high resolution (HR) panchromatic (Pan) image into an HR MS image. In addition to using the fidelity based local spectral consistency term for preserving the spectral information, we particularly exploit the Hessian feature consistence between the HR MS image and Pan image, and propose a new Hessian nuclear norm induced spatial consistency term for preserving the spatial information. Then, the proposed model is solved by an efficient algorithm under the FISTA framework. Finally, the experimental results demonstrate that the proposed method outperforms various pan-sharpening methods in terms of higher spectral and spatial qualities.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517670","Pan-sharpening;variational methods;local spectral consistency;Hessian feature;spatial consistency","Spatial resolution;Distortion;Multiresolution analysis;Geometry;Computer science","feature extraction;geophysical image processing;image fusion;image resolution;remote sensing","high resolution panchromatic image;low resolution multispectral image;variational pan-sharpening method;spatial qualities;higher spectral qualities;pan-sharpening methods;spatial information;Hessian nuclear norm induced spatial consistency;Hessian feature consistence;local spectral consistency term;HR MS image","","","","22","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Quantitative Evaluation of Digital Orthophoto Map","F. Luo; C. Li; H. Hu","National Quality Inspection and Testing Center for Surveying and Mapping Products, Beijing, China; China Transport Telecommunications &Information Center, Beijing, China; Xinjiang Uygur Autonomous Region Surveying and Mapping Product Quality Supervision and Inspection Station, Urumqi, China","2022 3rd International Conference on Geology, Mapping and Remote Sensing (ICGMRS)","18 Aug 2022","2022","","","116","120","There is no comprehensive and scientific standard and system for the classification, selection and use of digital orthophoto map quality evaluation indexes. There are some problems in the evaluation, such as unclear selection of indexes and inconsistent evaluation results. In this paper, the quantitative evaluation indexes of position accuracy, edge quality, texture quality, fusion quality and other quality characteristics are selected, the calculation formula is sorted out, the program is written, and compared with the subjective evaluation of the inspector, the best evaluation scheme of digital orthophoto map is discussed. The results of the digital orthophoto map quality evaluation experiment in Lusong District show that the position accuracy detection based on speed up robust feature (SURF) dense matching algorithm can reflect the overall geometric accuracy of the image, and the radiation consistency, geometric consistency contrast and clarity indexes such as gray contrast coefficient, uniform brightness distribution, mean gradient, error relative global adimensionnelle synthese (ERGAS) index can effectively reflect the edge quality, texture quality and fusion quality of digital orthophoto map.","","978-1-6654-8595-1","10.1109/ICGMRS55602.2022.9849306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9849306","Orthophoto Map;Quality Characteristics;Evaluation Index;Position Accuracy;Texture Quality;Fusion Quality","Visualization;Image edge detection;Geology;Brightness;Inspection;Feature extraction;Indexes","feature extraction;geophysical image processing;geophysical signal processing;image colour analysis;image fusion;image matching;image texture;remote sensing","scientific standard;digital orthophoto map quality evaluation indexes;inconsistent evaluation results;quantitative evaluation indexes;edge quality;texture quality;fusion quality;quality characteristics;subjective evaluation;evaluation scheme;digital orthophoto map quality evaluation experiment;position accuracy detection;geometric consistency contrast;clarity indexes","","","","6","IEEE","18 Aug 2022","","","IEEE","IEEE Conferences"
"Fusion of Low-Cost UAV Point Cloud with TLS Point Cloud for Complete 3D Visualisation of a Building","I. Chauhan; A. Rawat; M. Chauhan; R. Garg","Department of Civil Engineering, G.B. Pant Institute of Engineering & Technology; Department of Civil Engineering, G.B. Pant Institute of Engineering & Technology; Department of Civil Engineering, G.B. Pant Institute of Engineering & Technology; Department of Civil Engineering, IIT Roorkee","2021 IEEE International India Geoscience and Remote Sensing Symposium (InGARSS)","13 Jun 2022","2021","","","234","237","3D modelling of buildings is an important task for getting geometrical knowledge of the building as well as for planning and monitoring purposes. The use of low-cost UAV (Unmanned Aerial Vehicle) derived point cloud is becoming a popular way for 3D model creation. To enhance the accuracy of such models generally researchers employ GCP’s(Ground Control Points), based on GNSS survey to georeference the point cloud. But using GCP’s has limitations like object’s accessibility & also it is time consuming. Another way to increase the accuracy of such point clouds is merging them with TLS(Terrestrial Laser Scanner) point cloud which is georeferenced using a geodetic GNSS. The research here is trying to achieve the same by fusing point clouds derived from low-cost UAV (Phantom 4 pro v.2) and TLS. This is achieved in 3 steps a) Metric based comparison b) C2C(Cloud to Cloud) & M3C2 (Multi scale Model to Model Cloud Comparison) algorithm-based comparison c) running linear regression for selected points on both the point clouds. An RMS(Root Mean Square) of 0.125 is achieved for Phantom pro metric-based comparison. The C2C & M3C2 algorithm-based comparison shows that most points in the facade of the building have almost zero error i.e., the point clouds are totally identical in these areas. Also, linear regression comparison gives very high R-squared value affirming the fact that point clouds obtained from UAV highly correlate to that obtained from TLS. Thus, based on above 3 comparison methods, it can be concluded that a UAV point cloud could be fused with a single view GNSS referenced TLS point cloud to obtain a complete 3D visualization of building.","","978-1-6654-4249-7","10.1109/InGARSS51564.2021.9792104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792104","Index Terms: 3D model;TLS;UAV;Point cloud;RMSE;Low cost;C2C;M3C2;Buildings","Point cloud compression;Global navigation satellite system;Visualization;Solid modeling;Three-dimensional displays;Buildings;Linear regression","autonomous aerial vehicles;data visualisation;geophysical image processing;image fusion;mean square error methods;optical scanners;regression analysis;remote sensing by laser beam;solid modelling","UAV point cloud;multiscale model to model cloud comparison;cloud to cloud algorithm-based comparison;M3C2 algorithm-based comparison;C2C algorithm-based comparison;3D visualisation;building;geodetic GNSS;single view GNSS referenced TLS point cloud;terrestrial laser scanner;linear regression comparison;Phantom pro metric-based comparison","","","","14","IEEE","13 Jun 2022","","","IEEE","IEEE Conferences"
"Texturing Buenos Aires Buildings with Worldview3 Images","M. d’Autume; E. Meinhardt-Llopis","CMLA, ENS Cachan, CNRS, Université Paris-Saclay, Cachan, France; CMLA, ENS Cachan, CNRS, Université Paris-Saclay, Cachan, France","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","2798","2801","We propose an algorithm to texture 3D models of urban regions from multiple satellite images. Our algorithm is well-suited for the case when the images are obtained from different dates, where the dynamic ranges are incompatible and the position of the shadows is different. The method relies on a robust fusion of the multiple candidate textures for each surface patch, and optionally on a geometric criterion to remove the shadows of the known objects. We showcase the results by building a 3D model of a city such that all facades are correctly textured, with uniform colors and without shadows, even if for each individual input image only one side of the buildings was visible.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900319","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900319","urban stereo;surface colorization;osmosis equation;robust fusion","Satellites;Image color analysis;Three-dimensional displays;Buildings;Image reconstruction;Cameras;Geology","geophysical image processing;image fusion;image texture;remote sensing;solid modelling","dynamic ranges;robust fusion;multiple candidate textures;surface patch;geometric criterion;individual input image;texturing buenos aires buildings;worldview3 images;texture 3D models;urban regions;multiple satellite images","","","","15","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"An efficiently volumetric fusing method for structure-frome-motion and terrestrial point cloud","W. Li; C. Wang; D. Zai; P. Huang; W. Liu; J. Li","Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, Xiamen, Fujian, China; Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, Xiamen, Fujian, China; Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, Xiamen, Fujian, China; Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, Xiamen, Fujian, China; Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, Xiamen, Fujian, China; Department of Geography and Environmental Management, University of Waterloo, Waterloo, Ontario, Canada","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","6036","6039","Airborne acquisition and ground-view 3D point cloud provide complementary 3D information at city scale. A complete but lacks ground-view details, while the latter is incomplete for higher floors and severe occlusion. In this paper, First, a volumetric fusion method based on graph cuts were applied for fusing of airborne and terrestrial 3D LiDAR data. Second, we propose a method of constraints based on the local centroid of point cloud to eliminate the gap of fusion boundary. Finally, the experiments show that the improved fusion algorithm implement blending effectively.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128386","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128386","graph cut;volumetric fusion;boundary constraints","Three-dimensional displays;Laser radar;Atmospheric modeling;Buildings;Image reconstruction;Solid modeling;Labeling","image fusion;optical radar;remote sensing by laser beam","graph cuts;fusion boundary;terrestrial point cloud;severe occlusion;volumetric fusion method;fusion algorithm;3D terrestrial LiDAR data;3D airborne LiDAR data;volumetric fusing method","","","","14","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Total Nuclear Norms of Gradients for Hyperspectral Image Pansharpening","R. Yuzuriha; R. Kurihara; M. Okuda; R. Matsuoka","The University of Kitakyushu, Japan; The University of Kitakyushu, Japan; The University of Kitakyushu, Japan; Kyushu Institute of Technology","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2695","2698","We introduce a pansharpening method based on a novel regularization function for hyperspectral images (HSI). The regularization is based on the nuclear norms of gradient images. Unlike conventional low-rank priors, we achieve a gradient-based low-rank approximation by minimizing the sum of nuclear-norms associated with rotated planes in the gradient of a HSI. Our method explicitly and simultaneously exploits the correlation in the spectral domain as well as the spatial domain. Our method achieves high-fidelity image pansharpening using a single regularization function without the explicit use of any sparsity-inducing priors such as l0, l1 and TV norms. The proposed regularization is validated on some HSI images with performance comparisons to state-of-the-art methods to demonstrate its superior performance.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324412","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324412","","Hyperspectral imaging;TV;Correlation;Image restoration;Three-dimensional displays;Pansharpening;Noise measurement","approximation theory;Bayes methods;geophysical image processing;geophysical techniques;gradient methods;hyperspectral imaging;image fusion;image resolution;minimisation;remote sensing","gradient images;low-rank priors;low-rank approximation;nuclear-norms;rotated planes;spectral domain;spatial domain;high-fidelity image pansharpening;single regularization function;sparsity-inducing priors;TV norms;HSI images;total nuclear norms;hyperspectral image pansharpening;pansharpening method;novel regularization function;hyperspectral images","","","","20","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Multisource Shadow-Based Fuzzy Set (MSFS) Approach for Impervious Surfaces Mapping from Optical and SAR Data","Y. Lin; H. Zhang; P. Ma; Y. Li","Institute of Space and Earth Information Science, The Chinese University of Hong Kong, Hong Kong, China; Department of Geography, The University of Hong Kong, Hong Kong, China; Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","6817","6820","Urban impervious surfaces (UIS) indicate the environmental and socioeconomic influences of rapid urbanization. Synthetic aperture radar (SAR) reflects the scattering behaviors of different land covers while multispectral data demonstrate their physicochemical properties. Numerous studies reported that the incorporation of SAR and optical data supplement each other for better extracting UIS, nevertheless, the shadow and layover effects remain unclear, especially in very high-resolution observations. This study analyzed the shadow and layover influences from both optical and SAR data for fine resolution UIS estimation. Given the SAR shadow and layover distribution, we proposed a multisource shadow-based fuzzy set (MSFS) approach for fusing optical and SAR in optical shadow areas using decision fusion. SAR layovers showed effectiveness in UIS extraction. MSFS delivered 3% and 7% improvement in overall accuracy compared with SVM and RF using feature fusion respectively.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554229","National Natural Science Foundation of China(grant numbers:42022061,42071390,41971278); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554229","Urban impervious surface;SAR;shadows;layovers;fuzzy set theory","Support vector machines;Fuzzy sets;Optical imaging;Feature extraction;Adaptive optics;Optical sensors;Optical scattering","fuzzy set theory;geophysical image processing;image fusion;remote sensing by radar;synthetic aperture radar;terrain mapping","multisource shadow-based fuzzy set approach;SAR data;rapid urbanization;synthetic aperture radar;land cover;multispectral data;optical data supplement;fine resolution UIS estimation;optical shadow areas;physicochemical properties;urban impervious surface extraction;decision fusion;optical data","","","","15","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Differential Information Residual Convolutional Neural Network for Pansharpening","M. Jiang; J. Li; Q. Yuan; H. Shen; X. Liu; M. Xu","School of Resource and Environmental Science, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Resource and Environmental Science, Wuhan University, Wuhan, China; College of Electrical and Information Engineering, Hunan University, China; College of Geosciences and Technology, China University of Petroleum, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","4865","4868","In this paper, a new pansharpening method with residual convolutional neural network (RCNN)) is proposed. The proposed method utilizes a novel end-to-end CNN, which maps the differential information between the high spatial resolution panchromatic image (HR-PAN) and the low spatial resolution multispectral image (LR-MS) to the differential information between the HR-PAN image and the high spatial resolution multispectral image (HR-MS). Unlike the CNN-based pansharpening methods in other literatures, the proposed method makes full use of the spatial information in the HR-PAN image, and simultaneously preserve the spectral information of the MS image. Experimental results at both reduced resolution and full resolution demonstrate the superior performance of the proposed method comparing to state-of-the-art pansharpening methods in both quantitative and visual assessments.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900270","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900270","Pansharpening;RCNN;differential information","","convolutional neural nets;geophysical image processing;image fusion;image resolution;remote sensing","differential information residual convolutional neural network;end-to-end CNN;high spatial resolution panchromatic image;low spatial resolution multispectral image;HR-PAN image;high spatial resolution multispectral image;HR-MS;CNN-based pansharpening methods;spatial information;spectral information;MS image;reduced resolution","","","","14","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"On the Characterization of Sen2Like Surface Reflectance Data Harmonization and Fusion Processes","S. Saunier; V. Debaecker; J. Louis; K. Garcia; C. Cuny; E. G. Cadau; V. Boccia; F. Gascon","Telespazio, France; Telespazio, France; Telespazio, France; Telespazio, France; Telespazio, France; Serco SpA c/o ESRIN, Italy; European Space Agency, ESRIN, Italy; European Space Agency, ESRIN, Italy","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4636","4639","There are growing and evolving expectations from the Earth Observation (EO) Community for merging an increasing number of input data streams recorded with the various space based Multi-Spectral (MS) High Resolution (HR) EO mission instruments. The various agencies and industries are now delivering Analysis Ready Data (ARD) aiming at stacking together multi source data. One critical objective of ARD is to enable the monitoring of rapid and subtle changes. This category of applications sets some strong requirements regarding the quality of the surface reflectance harmonization in the time series. In the context of Sentinel-2 / Landsat 8, this paper proposes a comprehensive analysis of the Sen2Like processing. It shows that it is possible to increase significantly the geometric coregistration accuracy and to ensure an appropriate cross calibration. However, some issues persist and are due to directional effects. These are partially corrected with the current approach but remain a major source of noise in time series.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553037","Analysis Ready Data;Sentinel-2;Landsat- 8;Sen2Like;processing;harmonization;validation","Reflectivity;Earth;Space missions;Time series analysis;Stacking;Software algorithms;Production","calibration;geophysical image processing;image fusion;image resolution;remote sensing;sensor fusion;time series","input data streams;space based MultiSpectral;Analysis Ready Data;multisource data;rapid changes;strong requirements;surface reflectance harmonization;time series;Sentinel-2;Sen2Like;surface reflectance data harmonization;fusion processes;Earth Observation Community;EO","","","","19","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Multitemporal Sar And Map Fusion For Extracting Persitent Scatterers On Roads","T. Tanaka; D. Ikefuji; O. Hoshuyama","Data Science Research Laboratories, NEC Corporation; Data Science Research Laboratories, NEC Corporation; Data Science Research Laboratories, NEC Corporation","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","182","185","This paper proposes a fusion method of multitemporal SAR images and a map to extract persistent scatterers (PSs) on roads, which are special SAR pixels used to monitor millimetric displacement of roads. The proposed method evaluates phase correlation of PSs in addition to shape similarity between their distribution in the SAR image and corresponding roads in the map. Since uncorrelated phase implies that PSs are on different objects, which have different heights and displacements, the proposed method can extract PSs matching to the road shape while excluding those on neighboring buildings even under severe distortion caused by layover. Evaluation results using real SAR images and map show that the proposed fusion method can find PSs corresponding to roads in an urban area highly affected by layover.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899061","PS;InSAR;Road Extraction,","Roads;Correlation;Shape;Radar polarimetry;Feature extraction;Data mining;Monitoring","image fusion;radar imaging;radar interferometry;remote sensing by radar;roads;synthetic aperture radar","persitent scatterers;fusion method;multitemporal SAR images;PSs;special SAR pixels;millimetric displacement;phase correlation;uncorrelated phase;road shape;urban area","","","","8","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Performance Assessment Metrics for Line-Infrastructure Monitoring with Multi-Sensor SAR Data","L. Chang; R. P. B. J. Dollevoet; R. F. Hanssen",University of Twente; Delft University of Technology; Delft University of Technology,"IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","4423","4426","Satellite radar interferometry (InSAR) has been used to monitor the structural health of line-infrastructure (e.g. railways, bridges, dams and dikes) in recent years. This enables the retrieval of millimeter-level changes in the line-infrastructure geometry on a bi-weekly basis. However, InSAR is an opportunistic method for which the location of the measurements (coherent scatterers) cannot be guaranteed, and the quality of the InSAR products vary from one case to another. Particularly, this is due to the orientation of the line-infrastructure relative to the satellite position, and its expected deformation magnitude and direction. Hence, the InSAR applicability and performance quality is not uniform. In operational situations, this tends to make asset managers skeptical about the potential of InSAR application on these assets. In this work, following [1] we develop new standard InSAR products for line-infrastructure monitoring, provide tools for predicting optimal multi-sensor SAR data combinations, and propose generic a priori performance assessment metrics for line-infrastructure. These products and metrics are tested on the Dutch railway line-infrastructure asset.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518364","Line-infrastructure;multi-sensor SAR","Strain;Satellites;Sensitivity;Monitoring;Rail transportation;Synthetic aperture radar","condition monitoring;image fusion;radar imaging;radar interferometry;railways;remote sensing by radar;synthetic aperture radar","generic a priori performance assessment metrics;Dutch railway line-infrastructure asset;line-infrastructure monitoring;satellite radar interferometry;line-infrastructure geometry;performance quality;InSAR application;standard InSAR products;millimeter-level changes retrieval;structural health monitoring;optimal multisensor SAR data combination prediction","","","","20","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Multi-Sensor Ocean Color Data Fusion and Applications","M. Wang; L. Jiang; X. Liu; S. Son; K. Mikelsons; J. Sun; W. Shi; L. Tan; X. Wang; M. Chu; V. Lance","NOAA/NESDIS Center for Satellite Applications and Research (STAR), College Park, Maryland, USA; NOAA/NESDIS Center for Satellite Applications and Research (STAR), College Park, Maryland, USA; NOAA/NESDIS Center for Satellite Applications and Research (STAR), College Park, Maryland, USA; NOAA/NESDIS Center for Satellite Applications and Research (STAR), College Park, Maryland, USA; NOAA/NESDIS Center for Satellite Applications and Research (STAR), College Park, Maryland, USA; NOAA/NESDIS Center for Satellite Applications and Research (STAR), College Park, Maryland, USA; NOAA/NESDIS Center for Satellite Applications and Research (STAR), College Park, Maryland, USA; NOAA/NESDIS Center for Satellite Applications and Research (STAR), College Park, Maryland, USA; NOAA/NESDIS Center for Satellite Applications and Research (STAR), College Park, Maryland, USA; NOAA/NESDIS Center for Satellite Applications and Research (STAR), College Park, Maryland, USA; NOAA/NESDIS Center for Satellite Applications and Research (STAR), College Park, Maryland, USA","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","5090","5093","We provide an overview of the progress on producing accurate global ocean color products from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi National Polar-orbiting Partnership (SNPP) and NOAA-20 satellites. VIIRS global ocean color products include normalized water-leaving radiance spectra nLw(l) at VIIRS five spectral bands, chlorophyll-a (Chl-a) concentration, water diffuse attenuation coefficients at 490 nm, Kd(490), and at the domain of photosynthetically available radiation (PAR), Kd(PAR). However, VIIRS-derived daily ocean color images on either SNPP or NOAA-20 have some limitations in ocean coverage due to its swath width, high sensor-zenith angle, sun glint, and cloud, etc. Merging VIIRS ocean color products derived from the SNPP and NOAA-20 significantly increases the spatial coverage of daily data images. The two VIIRS sensors on SNPP and NOAA-20 have similar sensor characteristics, and ocean color products are derived using the same Multi-Sensor Level-1 to Level2 (MSL12) ocean color data processing system. Therefore, the merged VIIRS ocean color data from the two sensors have high data quality with consistent statistical property and accuracy globally. We describe an approach to remove data gaps of missing pixels from the SNPP and NOAA-20 merged global ocean color data. Some applications using the new complete global data coverage of daily ocean color product are also presented and discussed.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900282","VIIRS;ocean color data;data fusion;applications","Oceans;Image color analysis;Data processing;Satellite broadcasting;Sensor systems;Sun","data analysis;geophysical image processing;image colour analysis;image fusion;infrared imaging;oceanographic techniques;radiometry;remote sensing;underwater optics","global ocean color products;daily ocean color product;complete global data coverage;data gaps;high data quality;merged VIIRS ocean color data;Level2 ocean color data processing system;MultiSensor Level-1;sensor characteristics;VIIRS sensors;daily data images;high sensor-zenith angle;ocean coverage;VIIRS-derived daily ocean color images;PAR;water diffuse attenuation coefficients;normalized water-leaving radiance spectra;VIIRS global ocean color products;NOAA-20 satellites;SNPP;Suomi National Polar-orbiting Partnership;Visible Infrared Imaging Radiometer Suite;MultiSensor ocean color data fusion","","","","15","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Pixel-Wise Cloud Dictionary Learning for Fusing Optical and SAR Data","J. Ling; H. Zhang","The University of Hong Kong Shenzhen Institute of Research and Innovation, Shenzhen, China; The University of Hong Kong Shenzhen Institute of Research and Innovation, Shenzhen, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","5840","5843","Urban land cover (ULC) is a fundamental indicator of urbanization, while cloud cover hinders accurate and timely ULC monitoring. The fusion of synthetic aperture radar (SAR) and cloud-free optical data has shown good performance in previous studies, while there is a lack of investigation in cloud-prone areas where optical data is contaminated by clouds. This study proposes a cloud-oriented framework for fusing the two data sources for ULC classification in cloud-prone areas. For alleviating cloud interference, the framework proposes a cloud probability weighting strategy and a pixel-wise cloud dictionary learning algorithm considering the interference difference in different cloud probability levels. Experiments show that all algorithms using fused data improve the overall accuracy (OA) of above 6% and 20% compared with using single SAR and optical data, respectively. Compared with traditional SVM, RF, and dictionary learning methods which ignore cloud interference and directly concatenate optical and SAR features, the proposed method shows a significant improvement of 3% in OA. It improves almost all land covers' producer accuracy (PA) and user accuracy (UA), up to 9%. Further experiments with three cloud probability level samples find that the higher the cloud probability, the lower the classification accuracy of the sample. At each probability level, the proposed pixel-wise cloud dictionary learning method improves more than 2% in OA, improves up to 4% to 10% in PA and UA.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884125","National Natural Science Foundation of China(grant numbers:42022061,42071390); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884125","Cloud;SAR;ULC;dictionary learning","Support vector machines;Radio frequency;Laser radar;Soft sensors;Machine learning;Interference;Adaptive optics","clouds;geophysical image processing;image classification;image fusion;land cover;radar imaging;remote sensing by radar;support vector machines;synthetic aperture radar","fusing optical;SAR data;urban land cover;cloud cover hinders;cloud-prone areas;optical data;cloud-oriented framework;data sources;ULC classification;alleviating cloud interference;cloud probability weighting strategy;pixel-wise cloud dictionary learning algorithm;interference difference;different cloud probability levels;fused data;single SAR;directly concatenate optical SAR features;land covers;cloud probability level samples;pixel-wise cloud dictionary learning method","","","","8","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Deep Spatial-Spectral Information Exploitation for Rapid Hyperspectral Image Super-Resolution","J. Hu; Y. Li; M. Zhao; Y. Zhang","School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, China; Joint Laboratory of High Speed Multi-source Image Coding and Processing, Xidian University, Xi’an, China; School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, China; Joint Laboratory of High Speed Multi-source Image Coding and Processing, Xidian University, Xi’an, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3109","3112","Limited by existing electromagnetic sensors, the hyperspectral image (HSI) is characterized by having a high spectral resolution but a low spatial resolution. The super-resolution (SR) technique, which aims at enhancing the spatial resolution of the input image, is a hot topic in computer vision. This paper presents a rapid HSI SR method based on a deep information distillation network (IDN) and an intra-fusion operation to fully utilize the spatial-spectral information. Specifically, some bands are firstly selected and super-resolved by utilizing their spatial information through IDN. Non-selected bands are super-resolved by spectral interpolation. Moreover, to take a full advantage of the information these non-selected bands conveys, intra-fusion is operated on the input HSI and the spectrally-interpolated high resolution HSI. Contrary to most existed fusion methods which require multiple observations of the same scene, this intra-fusion is more flexible, and makes further utilization of the information the input HSI conveys simultaneously. In addition, this method requires less computation and is more suitable for practical applications. Experimental data and comparative analysis have demonstrated the effectiveness this method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899111","hyperspectral image;super-resolution;deep spatial-spectral exploitation;intra-fusion","Spatial resolution;Correlation;Matrices;Hyperspectral sensors;Image reconstruction","geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image resolution;interpolation;remote sensing","IDN;spectrally-interpolated high resolution HSI;fusion methods;electromagnetic sensors;high spectral resolution;low spatial resolution;computer vision;deep information distillation network;intra-fusion operation;hyperspectral image super-resolution;deep spatial-spectral information exploitation;HSI SR method;nonselected bands conveys","","","","12","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Selection of the 3-D Shearlet Cubes for Improving Hyperspectral Image Joint Sparse Classification","Z. Mariem; B. Sonia","Laboratoire de recherche en Informatique, Modélisation et Traitement de l'Information et de la Connaissance, Institut Supérieur d'Informatique, Université de Tunis El Manar Equipe de recherche en Systémes Intelligents En Imagerie et Vision Artificielle, Ariana, Tunisia; Laboratoire de recherche en Informatique, Modélisation et Traitement de l'Information et de la Connaissance, Institut Supérieur d'Informatique, Université de Tunis El Manar Equipe de recherche en Systémes Intelligents En Imagerie et Vision Artificielle, Ariana, Tunisia","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","787","790","The emergence of developed hyperspectral sensors has improved the spectral and spatial resolutions of the images, leading to a high-level understanding of the remotely sensed scenes. Several methods have been proposed to exploit those resolutions and to extract representative features serving for diverse applications, like classification. For instance, contextual classifiers provide better accuracy using features that represent the spatial context, and not restricting themselves to the raw spectral observations that the image offers. In a previous work, we proposed a method that uses the 3D-Shearlet Transform features along with the spatial and spectral information to better classify a given HSI. We got interesting accuracies that outperformed state-of-the-art methods. In this paper, we propose not to use all the 3D-Shearlet Transform coefficients, but only the most relevant of them. To do so, we resort to use a feature selection method based on the Fisher Discrimination Criterion (FDC). Experimental results showed that the FDC helped select the most representative cubes and outperformed our previously obtained results.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884854","3D-Shearlet Transform;Hyperspectral images;Fisher Discrimination Criterion;spatio-spectral classification","Transforms;Feature extraction;Spatial resolution;Principal component analysis;Hyperspectral imaging","feature extraction;geophysical image processing;geophysical signal processing;image classification;image denoising;image fusion;remote sensing","high-level understanding;representative features;contextual classifiers;spatial context;raw spectral observations;image offers;3D-Shearlet Transform features;spatial information;spectral information;classify a given HSI;interesting accuracies;outperformed state-of-the-art methods;3D-Shearlet Transform coefficients;feature selection method;representative cubes;3-D Shearlet cubes;improving hyperspectral image joint sparse classification;developed hyperspectral sensors;spectral resolutions;spatial resolutions","","","","9","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Federated Learning: Dataset Management for Airport Object Representations Using Remote Sensing Images","R. A. Amit; C. K. Mohan","Indian Institute of Technology, Hyderabad, Kandi, Sangareddy, Telangana, India; Indian Institute of Technology, Hyderabad, Kandi, Sangareddy, Telangana, India","2022 IEEE Aerospace Conference (AERO)","10 Aug 2022","2022","","","1","14","Airport image analysis using machine learning is an efficient way to diagnose security. However, the sharing of in-formation across airports is usually constrained due to national security policies. The menace of cyber-attacks on airport secu-rity and surveillance systems continues to pose major threats to national security. Also, the protection and privacy preservation of information and technology assets are growing in complexity and importance. Airport image analysis (e.g., remote sensing, UAV screening, etc.) using deep learning emerges as the new methodology to perform surveillance tasks. Nevertheless, the major concern continues to be safety regulations and privacy issues for data sharing. Thus, causing insufficiency in dataset availability for training any computer vision application model. To improve data availability, security, performance, and communication efficiency, we propose a novel fusion-based feder-ated learning approach for airport surveillance through object detections using remote sensing images. The proposed archi-tecture serves as a guide for the Federated learning system design that preserves privacy by generating an independent global model based on local model updates from different clients without revealing the data itself. Secondly, a real-time fusion method is proposed to decide the participating clients according to their local model performance. Based on critical parameters like training time, dataset size, client system configuration, etc., at the participating clients, we schedule the model dynamically. A custom dataset annotated for 6 object categories, character-izing the real-world scenario with airports across the world is generated at different spatial resolutions. We evaluate the model for the object detection tasks using two benchmark algorithms (YOLOV3 and Faster R-CNN). Our experiments indicated that this methodology is robust to unbalanced and non-IID (Indepen-dent and Identically Distributed) datasets. Besides, this method can be used to reduce the round of communication needed to train a deep network on decentralized data.","1095-323X","978-1-6654-3760-8","10.1109/AERO53065.2022.9843800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9843800","","Training;Data privacy;Atmospheric modeling;Surveillance;Object detection;Airports;Security","aerospace computing;airports;computer vision;data privacy;geophysical image processing;image fusion;learning (artificial intelligence);object detection;remote sensing;video surveillance","dataset management;airport object representations;remote sensing images;airport image analysis;machine learning;national security policies;airport security;surveillance systems;technology assets;deep learning;surveillance tasks;privacy issues;data sharing;dataset availability;computer vision application model;communication efficiency;airport surveillance;independent global model;local model performance;dataset size;client system configuration;custom dataset;object detection tasks;object categories;federated learning system design","","","","14","IEEE","10 Aug 2022","","","IEEE","IEEE Conferences"
"Pansharpening of Multispectral Images based on Cycle-spinning Quincunx Lifting Transform","Y. Shi; W. Zhou; W. Li","Beijing Key Laboratory of Fractional Signals and Systems, School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Fractional Signals and Systems, School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Fractional Signals and Systems, School of Information and Electronics, Beijing Institute of Technology, Beijing, China","2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)","21 Aug 2020","2019","","","1","5","Pansharpening is a process of fusing the multispectral (MS) images with the panchromatic (PAN) image to improve the spatial resolution of the MS images. The key of pansharpening is how to extract the lost detail from the PAN image and add it to the MS images with an appropriate injection model. In this paper, a pansharpening approach based on cycle-spinning quincunx lifting transform (CQLT) is proposed. The CQLT features translation invariance and vanishing moments which can extract the detail properly. In order to reduce the spectral distortion of the fused image, the histogram matching along with two popular injection models are involved in the fusion. Experimental results show that the proposed method has better quantitative results as well as competitive visual results compared with some other state-of-the-art algorithms.","","978-1-7281-2345-5","10.1109/ICSIDP47821.2019.9172997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9172997","Pansharpening;multispectral image;image fusion;lifting transform;cycle-spinning;remote sensing","","image fusion;image matching;image resolution;transforms","translation invariance;vanishing moments;fused image;popular injection models;multispectral image pansharpening;cycle-spinning quincunx lifting transform;panchromatic image;spatial resolution;MS images;PAN image;injection model;pansharpening approach;CQLT;histogram matching","","1","","20","IEEE","21 Aug 2020","","","IEEE","IEEE Conferences"
"An Improved Injection Model for Pansharpening Based on Weighted Least Squares","Y. Shi; W. Wang; A. Tan","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China","2021 14th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","7 Dec 2021","2021","","","1","5","Pansharpening is a fusion technique to enhance the spatial resolution of multispectral images by combining with the panchromatic image. This problem can be formulated as detail extraction and injection model, thus the injection estimation is a key for fusion quality. In the literature, the regression-based models are extensively studied, where the solution is usually solved by the ordinary least squares method. To improve the accuracy and robustness of estimation, a new injection model based on weighted least squares is proposed in this paper. The weights are dependent on the local statistics of the input-output pairs, which enhance the regular area and suppress effect of outliers. Experimental results show that the proposed method outperforms the other state-of-the-art methods.","","978-1-6654-0004-6","10.1109/CISP-BMEI53629.2021.9624441","National Natural Science Foundation of China(grant numbers:61501029); Beijing Natural Science Foundation(grant numbers:L191004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9624441","pansharpening;multispectral image;image fusion;remote sensing;weighted least squares","Correlation coefficient;Visualization;Estimation;Pansharpening;Signal processing;Robustness;Sensors","image fusion;image resolution;least squares approximations;regression analysis","improved injection model;weighted least squares;pansharpening technique;fusion technique;spatial resolution;multispectral images;panchromatic image;injection estimation;fusion quality;regression-based models;ordinary least squares method","","1","","15","IEEE","7 Dec 2021","","","IEEE","IEEE Conferences"
"Global and local Gram-Schmidt methods for hyperspectral pansharpening","M. Dalla Mura; G. Vivone; R. Restaino; P. Addesso; J. Chanussot","Institut Polytechnique de Grenoble, Grenoble, RhÃ´ne-Alpes, FR; NATO STO Center for Maritime Research and Experimentation, La Spezia, Italy; Dept. of Information Eng., Electrical Eng. and Applied Math., University of Salerno, Salerno, Italy; Dept. of Information Eng., Electrical Eng. and Applied Math., University of Salerno, Salerno, Italy; GIPSA-Lab Grenoble Institute of Technology, Grenoble, France","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","37","40","Pansharpening algorithms enable to produce synthetic data with high spatial details and spectral diversity by combining a panchromatic image with multispectral or hyperspectral data. In classical approaches the details extracted from the panchromatic image are introduced into the original multichannel image through injection gains, which can be spatially variant on the image. In this paper we analyze several methods for partitioning an image into regions in which the pixels will share the same injection coefficients. Gram-Schmidt pansharpening methods are used as paradigmatic examples for assessing the performance of global and local gain estimation strategies, using hyperspectral data acquired by sensors mounted on one (Earth Observing-1) or multiple (PROBA and Quick-bird) satellite platforms.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325691","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325691","Pansharpening;Data Fusion;Remote Sensing;Gram-Schmidt procedure;HyperSpectral Data;Segmentation;Binary Partition Tree","Hyperspectral imaging;Sensors;Spatial resolution;Indexes;Image segmentation","geophysical image processing;geophysical techniques;hyperspectral imaging;image fusion","global Gram-Schmidt methods;local Gram-Schmidt methods;hyperspectral pansharpening algorithms;panchromatic image;multispectral data;multichannel image;local gain estimation strategy;gobal gain estimation strategy;Earth Observing-1 satellite platform;Quick-bird satellite platform;PROBA satellite platform","","7","","9","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Fusion of Multispectral LiDAR and Hyperspectral Imagery","B. Rasti; P. Ghamisi; R. Gloaguen","Exploration Division, The Machine Learning Group, Helmholtz Institute Freiberg for Resource Technology, Helmholtz-Zentrum Dresden-Rossendorf, Freiberg, Germany; Exploration Division, The Machine Learning Group, Helmholtz Institute Freiberg for Resource Technology, Helmholtz-Zentrum Dresden-Rossendorf, Freiberg, Germany; Exploration Division, The Machine Learning Group, Helmholtz Institute Freiberg for Resource Technology, Helmholtz-Zentrum Dresden-Rossendorf, Freiberg, Germany","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2659","2662","This paper presents a technique for the fusion of multispectral LiDAR and hyperspectral data. The proposed method is based on the fusion of the features of multispectral LiDAR and hyperspectral data projected in two different subspaces. First, the spatial features are extracted from both data using morphological filters. Then, the fused features are estimated by proposing a novel constraint penalized cost function. The estimated fused features are used for the purpose of mapping. The classification accuracies obtained by applying a random forest classifier on the fused data confirm considerable improvements compared with the other methods used in the experiments.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323179","University of Houston; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323179","Multispectral LiDAR;hyperspectral;fusion;remote sensing;urban mapping","Laser radar;Hyperspectral imaging;Feature extraction;Training;Data mining;Radio frequency;Cost function","feature extraction;geophysical image processing;image classification;image fusion;optical radar;pattern classification;sensor fusion","hyperspectral imagery;fused data;estimated fused features;spatial features;multispectral LiDAR;hyperspectral data","","1","","14","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A Review of Point Clouds Deep Learning Fusion for Unmanned Aerial Data Systems","F. Othman; P. Bartie; B. Kenwright","Heriot-Watt University, UK; Heriot-Watt University, UK; Heriot-Watt University, UK","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","123","126","Use of unmanned aerial systems (UAS) for surveillance are likely to expand into new areas such as aerial logistics and night time operations. Thus, there are foreseeable needs for autonomous perception capabilities to detect unwanted objects, to provide safety alerts and safe navigation. This review paper provides an overview on the subject of camera image and LiDAR point cloud fusion between sensor data, as well as fusion of deep learning (DL) methods for aerial applications. This review compares various DL methods for point clouds and fusion methods, which could guide future research work on this subject.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884073","deep learning;fusion;aerial;remote sensing;object detection;segmentation","Point cloud compression;Deep learning;Three-dimensional displays;Navigation;Surveillance;Object segmentation;Real-time systems","autonomous aerial vehicles;cameras;geophysical image processing;image fusion;mobile robots;optical radar;remotely operated vehicles;surveillance","safe navigation;camera image;LiDAR point cloud fusion;sensor data;deep learning methods;aerial applications;fusion methods;point clouds deep learning fusion;unmanned aerial data systems;unmanned aerial systems;UAS;surveillance;aerial logistics;night time operations;autonomous perception capabilities;unwanted objects;safety alerts","","","","29","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Vector Maps Fusion for Reliable Mapping of Building Footprints","M. A. Cherif; J. -P. Bauchet; Y. Tarabalka; I. Manighetti","LuxCarta Technology, Mouans-Sartoux, France; LuxCarta Technology, Mouans-Sartoux, France; LuxCarta Technology, Mouans-Sartoux, France; Université Côte d'Azur, Observatoire de la Côte d'Azur, IRD, CNRS, Géoazur, Valbonne, France","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","115","118","Recent technologies have enabled a significant growth of geographic datasets with different levels of detail and specifications. Subsequent analysis and mapping tasks may require to get the best out of the diversity of proposed sources. One solution is to integrate such maps through conflation. The purpose of this merging technique is to combine data that represent the same features from multiple datasets, into a new, richer dataset. Vector data conflation was intensively applied on linear networks like roads, streets and waterways, however combining polygonal building shapes has been relatively overlooked by the literature. In this paper, we propose an idea to aggregate a set of vector building maps to obtain a single fused representation. The proposed method takes as input two or more vector maps (more inputs lead to much more reliable maps), and decomposes the 2D space into a polygonal partition. A binary labelling procedure is applied using maps reliability weights, yielding contours of sets of connected buildings. Finally, a slicing algorithm decomposes contours into separate building instances. We show that our pipeline generates more accurate maps in terms of both IoU and F1 scores than any of the maps used as an input.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883451","remote sensing;fusion;building classification;computational geometry","Shape;Image edge detection;Roads;Buildings;Pipelines;Partitioning algorithms;Reliability","computational geometry;feature extraction;geographic information systems;geophysical image processing;geophysical signal processing;image fusion;image representation;image segmentation;object detection;sensor fusion;solid modelling","vector maps fusion;geographic datasets;mapping tasks;merging technique;multiple datasets;richer dataset;vector data conflation;linear networks;waterways;polygonal building shapes;vector building maps;single fused representation;reliable maps;polygonal partition;maps reliability weights;connected buildings;separate building instances;accurate maps","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"A Study on Full Scale Injection Coefficients for Pansharpening","G. Vivone; R. Restaino; J. Chanussot","DIEM, University of Salerno, Italy; DIEM, University of Salerno, Italy; Faculty of Electrical and Computer Engineering, University of Iceland, Iceland","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1756","1759","Pansharpening regards the fusion of a high spatial resolution but low spectral resolution (panchromatic) image with a high spectral resolution but low spatial resolution (multispectral) image. The estimation, at reduced resolution, of injection coefficients through regression is a widespread and powerful approach. In this work, the problem of the estimation of the injection coefficients at full resolution for regression-based pansharpening approaches is studied. Multiple approaches (based on guess images or an iterative method) are proposed. These are assessed at reduced resolution by exploiting a real dataset acquired by the IKONOS sensor. The quantitative results clearly demonstrate the superiority of the proposed iterative method.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519464","Iterative Methods;Full Scale Estimation;Pansharpening;Data Fusion;Remote Sensing","Spatial resolution;Iterative methods;Estimation;Indexes;Reactive power;Visualization","geophysical image processing;geophysical techniques;image fusion;image resolution","IKONOS sensor;iterative method;guess images;multiple approaches;regression-based pansharpening approaches;powerful approach;widespread approach;reduced resolution;low spatial resolution image;high spectral resolution;low spectral resolution image;high spatial resolution;full scale injection coefficients","","","","10","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Dynamic Wildfire Fuel Mapping Using Sentinel – 2 and Prisma Hyperspectral Imagery","R. U. Shaik; L. Giovanni; L. Fusilli","Dept. of Astronautical, Electrical and Energy Engineering (DIAEE), Sapienza University of Rome; School of Aerospace Engineering, Sapienza University of Rome, Italy; Dept. of Astronautical, Electrical and Energy Engineering (DIAEE), Sapienza University of Rome","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","5973","5976","Italy has witnessed a significant increase in wildfires in recent decades. Forest fire fuel maps play a vital role in the prevention, management and risk assessment of wildfires, and this paper presents the procedure implemented to develop a dynamic wildfire fuel map using PRISMA hyperspectral data and Sentinel-2 multispectral data. Freely available multispectral datasets are widely used for land cover and land use mapping, but they have limited utility for fuel mapping due to their coarse spectral resolution. So, in this study, hyperspectral imagery (HSI) from PRISMA has been used for fuel types classification. The feed-forward neural network showed an overall accuracy of 79% by validation. To convert the classification map into a dynamic fuel map, the knowledge of the proportion of live/dead herbaceous loads available in that area is essential. The Relative Greenness approach, which places the Normalized Difference Vegetation Index (NDVI) in the time series of measurements, was implemented using Sentinel - 2 multispectral data. By fusing the fuel types classification, relative greenness map and iso-bioclimatic map, a dynamic fuel map for the west of Latium in Italy was developed with reference to Scott/Burgan fuel models.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883095","Fuel Map;Hyperspectral;Image fusion;Multispectral;Wildfires","Neural networks;Time series analysis;Fires;Vegetation mapping;Time measurement;Fuels;Risk management","fires;geophysical image processing;geophysical techniques;geophysics computing;neural nets;remote sensing;time series;vegetation;vegetation mapping;wildfires","dynamic wildfire fuel mapping;Sentinel - 2;prisma hyperspectral imagery;wildfires;forest fire fuel maps;risk assessment;dynamic wildfire fuel map;PRISMA hyperspectral data;Sentinel-2 multispectral data;available multispectral datasets;land cover;land use mapping;fuel mapping due;fuel types classification;classification map;dynamic fuel map;relative greenness map;iso-bioclimatic map","","1","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Flood Extent Forecasting Using Synchronized Floodwater Index Coupling with in-Situ Data","Y. -J. Kwak; J. Park; W. Takeuchi","Research Center for Infrastructure Management, Infrastructure, Transport and Tourism (NILIM-MLIT), Tsukuba, Japan; Research Center for Infrastructure Management, Infrastructure, Transport and Tourism (NILIM-MLIT), Tsukuba, Japan; Research Center for Infrastructure Management, Infrastructure, Transport and Tourism (NILIM-MLIT), Tsukuba, Japan","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","4646","4648","A multi-dimensional approach is an upcoming technology in disaster risk to understand a real risk. In the case of flooding, dimensions of time and elevation (height) should be considered to produce rapid and accurate risk information. Dynamic spatio-temporal flood prediction is an urgent issue for the risk management of short- and long-duration flooding. The main objective of this paper is to propose a step-wise process for dynamic flood detection using the synchronized floodwater index (SfWI2) coupled with 5-day lead-time forecasting data and in-situ water-level data with the primary focus on daily change in flood extent. We mainly employed the 2015 time-series Moderate Resolution Imaging Spectrometer (MODIS) data acquired over the study area along the Brahmaputra River where flood events occur annually. Based on scenario-based forecasting water-levels, flood extent maps were created and show the possibility of the proposed multi-dimensional approach as a useful tool for short-duration flood prediction.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898316","Flood extent forecasting;MODIS;water level;image fusion","Floods;Forecasting;Rivers;Indexes;Land surface;Risk management;Clouds","disasters;floods;hydrological techniques;remote sensing;risk management;rivers;time series","rapid risk information;accurate risk information;dynamic spatio-temporal flood prediction;risk management;long-duration flooding;step-wise process;dynamic flood detection;5-day lead-time forecasting data;in-situ water-level data;flood events;scenario-based forecasting water-levels;flood extent maps;multidimensional approach;short-duration flood prediction;flood extent forecasting;synchronized floodwater index coupling;in-situ data;disaster risk;time-series Moderate Resolution Imaging Spectrometer","","","","5","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Research on Intelligent Image Scrambling Transform Encryption Algorithm Based on Big Data Analysis","T. Ting","Sichuan Vocational and Technical College, Suining, China","2021 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)","3 Sep 2021","2021","","","720","723","In order to improve the encryption ability of scrambling transformation of high-resolution knowledge map remote sensing image, a scrambling transformation encryption model of high-resolution knowledge map remote sensing image based on big data analysis and multi feature fusion is proposed. The edge pixel feature analysis model of high-resolution knowledge map remote sensing image is constructed. The edge pixel feature distributed reorganization method is used to realize the pixel structure reorganization. The image is processed by multispectral parameter big data fusion and chaos quantization linear feature analysis. The image encryption key feature analysis is realized by using the combination control method of shallow feature map. The personalized features extracted from image are identified by using intelligent image scrambling transform encryption algorithm, and the multilevel feature analysis model of image scrambling transform encryption is established. The simulation results show that the method has high adaptability and good scrambling performance, and improves the ability of fusion encryption and feature recognition of high-resolution knowledge map remote sensing image.","","978-1-6654-4854-3","10.1109/ICITBS53129.2021.00180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9525487","big data analysis;Intelligent image;Scrambling transformation;encrypt","Analytical models;Quantization (signal);Smart cities;Image edge detection;Simulation;Transforms;Big Data","Big Data;chaos;cryptography;data analysis;feature extraction;geophysical image processing;image fusion;image resolution;remote sensing","edge pixel feature distributed reorganization method;multispectral parameter big data fusion;chaos quantization linear feature analysis;image encryption key feature analysis;multilevel feature analysis model;fusion encryption;feature recognition;high-resolution knowledge map remote sensing image;intelligent image scrambling transform encryption algorithm;big data analysis;scrambling transformation encryption model;edge pixel feature analysis model","","","","15","IEEE","3 Sep 2021","","","IEEE","IEEE Conferences"
"Unsharp masking based pansharpening of high resolution satellite imagery","M. Teke; E. San; E. Koç","TÜBİTAK Uzay Teknolojileri Araştırma Enstitüsü, Ankara, Türkiye; TOBB ETÜ Ekonomi ve Teknoloji Üniversitesi, Elektrik ve Elektronik Mühendisliği Bölümü, Ankara, Türkiye; TOBB ETÜ Ekonomi ve Teknoloji Üniversitesi, Elektrik ve Elektronik Mühendisliği Bölümü, Ankara, Türkiye","2018 26th Signal Processing and Communications Applications Conference (SIU)","9 Jul 2018","2018","","","1","4","Pan sharpening is a pixel-level fusion technique for increasing the spatial resolution of low-resolution multi-spectral satellite imagery using high-resolution panchromatic imagery. In this work, the performance of various pansharpening algorithms in improving the 2.5m resolution of Göktürk-2 satellite imagery is compared. Eight different pansharpening algorithms are tested on seven different Göktürk-2 images and evaluated using eight different metrics which measures spectral and spatial quality of pansharpened images. We developed a preprocessing method, which is based on unsharp masking of pan band. Proposed method replace high pass filtering of Optimized HPF method instead it used sharpened image from unsharp masking of pan band. Then this information is added multispectral bands based on their standard deviation ratios. Pan band carries highest resolution spatial information while it lacks color. Unsharp masking is applied pan band to improve spatial contribution from this band. Then other pansharpening methods are applied to the image. Unsharp based preprocessing ofpan bands improves sharpness of the images without degraded image quality. Unsharpening increases sharpness of pansharpened images which performs poorly persevering spatial features. Our results show that the Optimized High Pass Filter (HPF) yields the sharpest pan sharpened image, while the hyper spherical Color Space (HCS) method preserves the truest colors while preserving sharpness to a certain degree.","","978-1-5386-1501-0","10.1109/SIU.2018.8404403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8404403","Göktürk-2;Unsharp;pansharpening;HPF;HCS","Satellites;Remote sensing;Optimized production technology;Spatial resolution;Image color analysis;Principal component analysis","high-pass filters;image classification;image colour analysis;image enhancement;image filtering;image fusion;image resolution;remote sensing","unsharp masking based pansharpening;high resolution satellite imagery;pan sharpening;pixel-level fusion technique;spatial resolution;low-resolution multispectral satellite imagery;high-resolution panchromatic imagery;Göktürk-2 satellite imagery;spectral quality;spatial quality;pansharpened images;high pass filtering;multispectral bands;pansharpening methods;ofpan bands;degraded image quality;spatial features;sharpest pan sharpened image;pansharpening algorithms;spatial information;optimized HPF method;optimized high pass filter;size 2.5 m","","2","","","IEEE","9 Jul 2018","","","IEEE","IEEE Conferences"
"An efficient method based on wavelet for fusion of multi-sensor satellite images","Mangalraj P.; Rajuraykar Anupam Agrawal","International Institute of Information Technology, Bangalore, Karnataka, IN; Indian Institute of Information Technology, Allahabad","2015 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)","27 Aug 2015","2015","","","1","6","In this paper an efficient fusion algorithm for remotely sensed images has proposed. The wavelet tool is used for the decomposition of the images at different resolutions. By applying the fusion rules at different components (Approximated and Detailed) the desired output was obtained. The results of the proposed fusion algorithm are compared with the conventional Principal Component Analysis method. It is proved by the quantitative analysis, the proposed one outperforms the existing conventional fusion technique.","","978-1-4799-6085-9","10.1109/ICECCT.2015.7226108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226108","Fusion;Wavelet;Quantitative Analysis","Image resolution;Principal component analysis;Discrete wavelet transforms;Visualization;Remote sensing;Computers","chemical analysis;image fusion;principal component analysis;radar imaging;remote sensing by radar;wavelet transforms","multi-sensor satellite images;remotely sensed images;wavelet tool;image decomposition;principal component analysis method;quantitative analysis","","2","","16","IEEE","27 Aug 2015","","","IEEE","IEEE Conferences"
"Feature Fusion based Unsupervised Change Detection in Optical Satellite Images","N. Gupta; P. Singh; S. Ari","Department of Electronics and Communication Engineering, National Institute of Technology, Rourkela, India; Department of Electronics and Communication Engineering, National Institute of Technology, Rourkela, India; Department of Electronics and Communication Engineering, National Institute of Technology, Rourkela, India","2019 IEEE 5th International Conference for Convergence in Technology (I2CT)","12 Mar 2020","2019","","","1","5","This paper proposes a feature fusion technique for unsupervised change detection. Features extracted from two different techniques are fused to get the final feature vectors. The first technique utilizes the Gabor wavelet at multiple orientations and scales, where maximum magnitude over all orientation in each scale is taken to create features of two multitemporal satellite images. The second technique applies canonical correlation analysis (CCA) on the combination of original multispectral bands and extracted local neighborhood information from all the bands. Next, the difference feature vectors obtained from individual techniques are fused to generate the final feature vectors. Furthermore, to get the binary change map, fuzzy c-means clustering is applied on final extracted features. In this feature fusion, the local neighborhood information from Gabor wavelet kernel is combined with joint change information from group of pixels extracted by CCA to produce more discriminant features. Experiments conducted on optical satellite images, which are collected by two sensors of Landsat satellite, and it shows the better performance of the proposed technique compared to earlier stated techniques.","","978-1-5386-8075-9","10.1109/I2CT45611.2019.9033712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9033712","Binary change map;Canonical correlation analysis (CCA);Change detection;fuzzy c-means clustering;Gabor wavelet kernel;multitemporal satellite image","Feature extraction;Kernel;Satellites;Correlation;Optical imaging;Remote sensing;Optical sensors","feature extraction;fuzzy set theory;geophysical image processing;image fusion;pattern clustering;remote sensing;wavelet transforms","fuzzy c-means clustering;CCA;canonical correlation analysis;Landsat satellite;Gabor wavelet kernel;feature extraction;binary change map;difference feature vectors;local neighborhood information;multispectral bands;multitemporal satellite images;feature vectors;feature fusion technique;optical satellite images;unsupervised change detection","","2","","19","IEEE","12 Mar 2020","","","IEEE","IEEE Conferences"
"On the performance analysis of classifier fusion for land cover classification","N. Minallah; A. Alkhalifah; R. Khan; H. U. Rahman; S. Khan","University of Engineering and Technology, Peshawar, Pakistan; Department of Information Technology, Qassim University, Al-Qassim, KSA; University of Engineering and Technology, Peshawar, Pakistan; University of Engineering and Technology, Peshawar, Pakistan; University of Engineering and Technology, Peshawar, Pakistan","2015 7th International Conference on Recent Advances in Space Technologies (RAST)","20 Aug 2015","2015","","","271","275","We investigate the performance evaluation of merging (fusing) the classification capabilities of classifiers for the land use analysis. For the fusion approach, we select the parametric and non-parametric classifiers. The set used includes: Bayesian Network, Multi-layer Perceptron (MLP), Support Vector Machines (SVM) and Random Forest. These classifiers are selected based on their good over-all performance for the land use analysis and in general for other classification tasks. We evaluate the concept on both the high and low resolution multispectral satellite imagery. The performance of the approach is evaluated using F-score, computation time and accuracy. Based on the experimental evaluation, we advocate the use of classifier fusion for the low resolution satellite imagery. While for high resolution satellite imagery, the fusion shows slight improvement in performance.","","978-1-4799-7697-3","10.1109/RAST.2015.7208354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208354","","Support vector machines;Satellites;Image resolution;Remote sensing;Accuracy;Artificial neural networks;Earth","belief networks;geophysical image processing;hyperspectral imaging;image classification;image fusion;land cover;land use;multilayer perceptrons;remote sensing;support vector machines","low resolution multispectral satellite imagery;high resolution multispectral satellite imagery;classification task;random forest;support vector machine;multilayer perceptron;Bayesian network;nonparametric classifier;fusion approach;land use analysis;classifier fusion evaluation;land cover classification;classifier fusion performance analysis","","1","","20","IEEE","20 Aug 2015","","","IEEE","IEEE Conferences"
"Pansharpening for Better Spectral and Spatial Clarity","G. Rohith; L. S. Kumar","Department of Electronics and Communication Engineering, National Institute of Technology-Puducherry, Karaikal, Puducherry; Department of Electronics and Communication Engineering, National Institute of Technology-Puducherry, Karaikal, Puducherry","2020 5th International Conference on Computing, Communication and Security (ICCCS)","9 Dec 2020","2020","","","1","6","Pansharpening is the fusion of a finer spectral resolution of a multispectral (MS) image with a coarser spatial resolution of a panchromatic (PAN) image. Conventional pan-sharpening algorithms can be addressed by utilizing band-dependent spatial-detail (BDSD). A filter-based approach finds a prominent application for retaining both the spatial information on the spectral quality. A Pan sharpening algorithm is proposed that uses the optimal filtered PAN image and merges with the MS image to achieve a superior Band-Dependent Spatial-Detail approach. The optimal filter injection in the BDSD approach extracts from the image the necessary and non-redundant data, which makes the BDSD approach an optimal solution for more spectral and spatial details. As compared to the conventional optimal filter and BDSD techniques, Qualitative and Quantitative analysis show that the proposed algorithm improves the fusion quality in terms of Peak Signal to Noise Ratio, Structural Similarity Index, Relative dimensionless global error in synthesis, spectral angle mapper, universal image quality index with the introduction of two parameters-quality without reference Entropy and Blind/ Referenceless Image Spatial Quality Evaluator -BRISQUE.","","978-1-7281-9180-5","10.1109/ICCCS49678.2020.9277131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9277131","BDSD;Optimal Filter;Pansharpening;Spatial resolution;visually pleasing image","Filtering algorithms;Remote sensing;Earth;Artificial satellites;Spatial resolution;Information filters;Low-pass filters","geophysical image processing;hyperspectral imaging;image filtering;image fusion;image resolution;optimisation;remote sensing;spatial filters","spatial clarity;finer spectral resolution;multispectral image;coarser spatial resolution;panchromatic image;filter-based approach;spatial information;spectral quality;optimal filtered PAN image;MS image;optimal filter injection;fusion quality;spectral angle mapper;universal image quality index;parameters-quality;BDSD approach;relative dimensionless global error;structural similarity index;peak signal to noise ratio;quantitative analysis;qualitative analysis;spectral clarity;pan sharpening algorithm;band-dependent spatial-detail approach","","1","","17","IEEE","9 Dec 2020","","","IEEE","IEEE Conferences"
"Evaluation of spatiotemporal fusion methods for high resolution daily NDVI prediction","A. Ibn El Hobyb; A. Radgui; A. Tamtaoui; A. Er-Raji; D. El Hadani; M. Merdas; F. M. Smiej","STRS Lab. Institut National des Postes et Télécommunications-INPT, Rabat, Morocco; STRS Lab. Institut National des Postes et Télécommunications-INPT, Rabat, Morocco; STRS Lab. Institut National des Postes et Télécommunications-INPT, Rabat, Morocco; Centre Royal de Télédetection Spatiale-CRTS, Rabat, Morocco; Centre Royal de Télédetection Spatiale-CRTS, Rabat, Morocco; Centre Royal de Télédetection Spatiale-CRTS, Rabat, Morocco; Centre Royal de Télédetection Spatiale-CRTS, Rabat, Morocco","2016 5th International Conference on Multimedia Computing and Systems (ICMCS)","24 Apr 2017","2016","","","121","126","The Normalized Difference Vegetation Index was introduced for monitoring vegetation dynamics. This index can be extracted from multispectral sensor data, such as Landsat and MODIS sensors, and therefore the NDVI can be obtained with high spatial resolution but low temporal resolution when using Landsat or with high temporal resolution but low spatial resolution when using MODIS. Spatiotemporal fusion methods were proposed as a solution for this limitation. By using these methods, images with high spatial and high temporal resolution can be obtained. STARFM, ESTARFM and FSDAF are ones of the methods that have been successfully applied for spatiotemporal fusion. The objective of this study is to compare and evaluate these three methods and apply it on actual NDVI Landsat 8 and MODIS data in the region of Tadla in Morocco, to generate daily NDVI at 30m resolution. This evaluation was supervised by experts in CRTS and this through two approaches. The evaluation approach one is applying the three methods to predict Landsat NDVI for 16 days based on predicted images. The evaluation approach two is based on predicting Landsat NDVI for 4 months and evaluating the results with available real Landsat images with statistic parameters. The Results show that only the ESTARFM method can handle the propagation of error for evaluation approach one and it is less sensible to the quality of inputs. For evaluation approach two, the ESTARFM method gives more accurate results than the STARFM and FSDAF method if input two pairs Landsat and MODIS NDVI are used from previous days with a RMSE attending 0,06.","2472-7652","978-1-5090-5146-5","10.1109/ICMCS.2016.7905614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7905614","Spatiotemporal fusion;Landsat;MODIS;NDVI;STARFM;ESTARFM;FSDAF","Remote sensing;Earth;Satellites;MODIS;Spatial resolution;Reflectivity","geophysical image processing;image fusion;remote sensing","spatiotemporal fusion methods;high resolution daily NDVI prediction;normalized difference vegetation index;MODIS sensors;Landsat;STARFM;ESTARFM;FSDAF;spatiotemporal fusion;CRTS;RMSE","","","","18","IEEE","24 Apr 2017","","","IEEE","IEEE Conferences"
"Optimizing the Snake Model Using Honey-Bee Mating Algorithm for Road Extraction from Very High-Resolution Satellite Images","A. Sarmadian; A. Moghimi; M. Amani; S. Mahdavi","Faculty of Geodesy and Geomatics Engineering, K. N. Toosi University of Technology, Tehran, Iran; Faculty of Geodesy and Geomatics Engineering, K. N. Toosi University of Technology, Tehran, Iran; Wood Environment and Infrastructure Solutions, Ottawa, Ontario, Canada; Wood Environment and Infrastructure Solutions, Ottawa, Ontario, Canada","2022 10th International Conference on Agro-geoinformatics (Agro-Geoinformatics)","23 Aug 2022","2022","","","1","6","Many geospatial applications rely on the extraction of spatial features, including road networks, from very high-resolution (VHR) satellite images. Researchers have developed many algorithms to achieve this goal, the majority of which are based on image fusion, fuzzy logic, and active contour models. The snake model is among the most widely used methods for road extraction by active contours. In most studies, an initial curve close to available roads is manually defined or based on prior knowledge. These methods also require manual adjustment of the snake model parameters, which is time-consuming. In order to address these limitations, this study proposes an algorithm for extracting roads from VHR satellite images in a semi-urban area that optimizes snake models by Honey-Bee Mating Optimization (HBMO). Based on a support vector machine and some image processing analysis, the presented method can extract an accurate initial curve, as well. According to the results of the experiments, the proposed approach not only eliminates the shortcomings of the snake model but also increases the accuracy of road extraction by 10% in all three study areas compared to the traditional snake method.","","978-1-6654-7078-0","10.1109/Agro-Geoinformatics55649.2022.9859090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9859090","Edge Detection;HBMO Algorithm;Remote Sensing;Road Extraction;Snake Model","Support vector machines;Satellites;Roads;Image edge detection;Estimation;Feature extraction;Numerical models","feature extraction;geophysical image processing;image fusion;image segmentation;optimisation;roads;support vector machines","Honey-Bee Mating algorithm;road extraction;high-resolution satellite images;road networks;image fusion;active contour models;active contours;available roads;snake model parameters;VHR satellite images;Honey-Bee Mating Optimization;image processing analysis;traditional snake method","","","","16","IEEE","23 Aug 2022","","","IEEE","IEEE Conferences"
"A novel fast back projection algorithm based on subaperture frequency spectrum fusion in Cartesian coordinates","L. Min; L. Zhou","Department of Applied Science and Frontier Technology, Qian Xuesen Laboratory of Space Technology, Beijing, china; Beijing Institute of remote sensing, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","999","1002","It is known to all that time-domain back projection (BP) algorithm has several advantages for synthetic aperture radar (SAR) imaging[1], such as easily motion compensation for any flight track, unlimited size of the observe scene, arbitrary wide bandwidth and large integration angle. Because of the heavy computational burden of BP algorithm, many fast back projection (FBP) algorithms have proposed in recent years[2-6]. We introduce a novel fast back projection algorithm which can speed up image formation and avoid coordinates transform at the same time. This method mainly utilizes the character of subapertures' oversampling and reconstructs the bandwidth by fusing all the subbands of subapertures. It also analyses the effects and influence factors for the number of subapertures. The simulation result shows that the algorithm can efficiently reduces the computational complexity while keep the image resolution.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729253","Synthetic aperture radar;fast back projection;polar coordinates;Cartesian coordinates","Apertures;Synthetic aperture radar;Bandwidth;Approximation algorithms;Azimuth;Image resolution;Algorithm design and analysis","computational complexity;image fusion;image reconstruction;image resolution;motion compensation;radar imaging;synthetic aperture radar","fast back projection algorithm;subaperture frequency spectrum fusion;Cartesian coordinates;time-domain back projection algorithm;synthetic aperture radar imaging;SARimaging;motion compensation;flight track;observe scene;arbitrary wide bandwidth;integration angle;FBP algorithm;image formation;subaperture oversampling;bandwidth reconstruction;subaperture fusion;computational complexity;image resolution","","2","","7","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Earthquake-Induced Building Damage Assessment on SAR Multi-Texture Feature Fusion","Y. Du; L. Gong; Q. Li; F. Wu","Institute of Crustal Dynamics, China Earthquake Administration, Beijing, P R China; Institute of Crustal Dynamics, China Earthquake Administration, Beijing, P R China; Institute of Crustal Dynamics, China Earthquake Administration, Beijing, P R China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","6608","6610","Multi-temporal RS data is not often available in time after an earthquake, so it is useful to assess the building situation with a single post-event SAR. Aiming at the problem that the single texture feature extraction method has inadequate information in the collapsed buildings classification, this paper proposes a SAR texture feature classification method that combines multiple features. Taking the 2016 Kumamoto earthquake as an example, the four methods based on gray histogram, GLCM, LBP, and Gabor filter are used to extract texture features and fused, then random forest classification is applied to obtain the collapse information of earthquake-damaged buildings. In addition, it is compared with the classification results of 26 texture features after principal component analysis. The results of two sets of experiments show that the extraction accuracy based on multi-feature fusion is higher than that of a single texture feature extraction method, and the multi-feature fusion classification result after principal component analysis improves the accuracy while improving the recognition efficiency.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323644","China Earthquake Administration(grant numbers:ZDJ-2018-14,ZDJ-2019-32); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323644","Earthquake;Building damage assessment;SAR;Multi-texture feature","Feature extraction;Buildings;Principal component analysis;Earthquakes;Gabor filters;Histograms;Data mining","earthquake engineering;earthquakes;feature extraction;geophysical image processing;image classification;image fusion;image texture;synthetic aperture radar","earthquake-induced building damage assessment;SAR multitexture feature;multitemporal RS data;single post-event SAR;single texture feature extraction method;collapsed buildings classification;SAR texture feature classification method;Kumamoto earthquake;random forest classification;earthquake-damaged buildings;texture features;multifeature fusion classification result;AD 2016","","1","","6","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Weighted fusion of LBP and CCH features for effective content based image retrieval","S. Jain; T. Zaveri; S. Patel","Dept. of Computer Sc and Engg. IT, Nirma University, Ahmedabad, India; Dept of Electrical Engineering IT, Nirma University, Ahmedabad, India; Dept. of Computer Sc and Engg. IT, Nirma University, Ahmedabad, India","2016 International Conference on Signal Processing and Communications (SPCOM)","17 Nov 2016","2016","","","1","5","Content based image retrieval for remote sensing images is challenging research area in recent years. In this paper, the weighted image fusion algorithm of morphological feature circular covariance histogram (CCH) and local binary and tetra patterns (LBP and LTrP) is presented. These features are effective for defining the image texture content and the content of the image but has limitations in defining the overall content of the image when individual features are used. Using proposed algorithm, retrieval results are improved when the features are combined compared to retrieval results of the individual features. For a given query image, there are instances where non-relevant images are assigned better ranks than relevant images. Such instances are minimized, thus weeding them out. This minimization process results into a weight matrix, which is used for obtaining the combined distance of the query image with database images. This combined distance is finally used for ranking the images. The proposed algorithm is experimented with UC Merced LULC image data set of 2100 images with 21 classes and 100 images in each class. All the 2100 images are taken as query image at a time and performance is evaluated for each image as a query image. The retrieval results obtained using proposed algorithm is compared with results obtained using individual and combined feature set with and without weight matrix. Simulation results are evaluated based on precision gain ratio and normalized rank which is significantly improved that proves that proposed algorithm is more effective.","","978-1-5090-1746-1","10.1109/SPCOM.2016.7746681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7746681","","Image retrieval;Mathematical model;Remote sensing;Computers;Electronic mail;Minimization","geophysical image processing;image retrieval;image texture;remote sensing","query image;UC Merced LULC image;minimization process;weight matrix;image texture content;local binary patterns;tetra patterns;circular covariance histogram;remote sensing images;effective content based image retrieval;LBP features;CCH features;weighted fusion","","2","","14","IEEE","17 Nov 2016","","","IEEE","IEEE Conferences"
"Heterogeneous image matching based on phase consistency","M. Liu; X. Mu; X. He","Rocket Force University of Engineering, Xi’an, China; Rocket Force University of Engineering, Xi’an, China; Beijing Institute of Remote Sensing Equipment, Beijing, China","2021 IEEE Conference on Telecommunications, Optics and Computer Science (TOCS)","28 Jan 2022","2021","","","864","868","The Image matching is an important step in image registration and image fusion. To solve the problem that SAR images and optical image matching are sensitive to radiation distortion, a novel heterogeneous image matching algorithm is proposed based on image frequent-domain wavelet decomposition based on phase congruency. Firstly, morphological denoising was carried out for image pairs, and then wavelet decomposition was performed by log-Gabor filter to obtain phase consistency of pixel points (PC graph). Then, feature descriptors were established based on the maximum index graph. Double-point matching was used as the search strategy for feature matching, and the random sampling consensus algorithm was used to eliminate false matching. Compare with traditional SIFT method. Experimental results show that the proposed algorithm has better matching effect than SIFT method and is an effective heterogeneous image matching algorithm","","978-1-6654-2498-1","10.1109/TOCS53301.2021.9689044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9689044","Heterogeneous image matching;Image morphology;Phase consistency;The Log Gabor filter","Optical filters;Image matching;Optical distortion;Feature extraction;Optical imaging;Radar polarimetry;Adaptive optics","feature extraction;Gabor filters;image fusion;image matching;image registration;synthetic aperture radar;wavelet transforms","false matching;matching effect;effective heterogeneous image matching algorithm;phase consistency;image registration;image fusion;SAR images;optical image matching;novel heterogeneous image matching algorithm;image frequent-domain wavelet decomposition;phase congruency;image pairs;double-point matching;feature matching;random sampling consensus algorithm","","","","11","IEEE","28 Jan 2022","","","IEEE","IEEE Conferences"
"Resolution enhancement for hyperspectral images: A super-resolution and fusion approach","C. Kwan; J. H. Choi; S. Chan; J. Zhou; B. Budavari","Signal Processing, Inc., Rockville, MD, USA; Purdue University, West Lafayette, IN, USA; Purdue University, West Lafayette, IN, USA; Google, Inc., Mountain View, CA, USA; Signal Processing, Inc., Rockville, MD, USA","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 Jun 2017","2017","","","6180","6184","Many remote sensing applications require a high-resolution hyperspectral image. However, resolutions of most hyperspectral imagers are limited to tens of meters. Existing resolution enhancement techniques either acquire additional multispectral band images or use a pan band image. The former poses hardware challenges, whereas the latter has limited performance. In this paper, we present a new resolution enhancement method that only requires a color image. Our approach integrates two newly developed techniques in the area: (1) A hybrid color mapping algorithm, and (2) A Plug-and-Play algorithm for single image super-resolution. Comprehensive experiments using real hyperspectral images are conducted to validate and evaluate the proposed method.","2379-190X","978-1-5090-4117-6","10.1109/ICASSP.2017.7953344","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953344","Hyperspectral Imaging;Remote Sensing;Hybrid Color Mapping;Plug-and-Play ADMM;Super-resolution","Hyperspectral imaging;Image resolution;Image color analysis;Color;Fuses;Google","hyperspectral imaging;image colour analysis;image enhancement;image fusion;image resolution","single image super-resolution;plug-and-play algorithm;hybrid color mapping algorithm;color image;fusion approach;super-resolution approach;hyperspectral images;resolution enhancement","","24","2","28","IEEE","19 Jun 2017","","","IEEE","IEEE Conferences"
"Hyperspectral pansharpening based on unmixing techniques","L. Loncan; J. Chanussot; S. Fabre; X. Briottet","ONERA, GIPSA-LAB; ONERA, GIPSA-LAB; ONERA, GIPSA-LAB; ONERA, GIPSA-LAB","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","23 Oct 2017","2015","","","1","4","Pansharpening first aims at fusing a panchromatic image with a multispectral image to generate an image with the high spatial resolution of the former and the spectral resolution of the latter. In the last decade many pansharpening algorithms have been presented in the literature using multispectral data. With the increasing availability of hyperspectral systems these methods are now extending to hyperspectral images. But the problem of the mixed pixel is generally ignored by the existing methods since their goal is to preserve the spectral information and add spatial information. In this work we compare different approaches to deal with mixed pixels as a pre-processing step before doing the fusion. This should improved the result by adding missing spectral information available in the reference image because of the mixed pixel.","2158-6276","978-1-4673-9015-6","10.1109/WHISPERS.2015.8075419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075419","Pansharpening;Hyperspectral;Panchromatique;Unmixing;Super-resolution","Spatial resolution;Hyperspectral imaging;Roads;Automobiles;Signal resolution","hyperspectral imaging;image colour analysis;image enhancement;image fusion;image resolution;spectral analysis","hyperspectral pansharpening;unmixing techniques;multispectral image;reference image;image resolution;panchromatic image fusion;panchromatique","","1","","6","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Geometric Low-Rank Tensor Approximation for Remotely Sensed Hyperspectral And Multispectral Imagery Fusion","N. Liu; W. Li; R. Tao","School of Information and Electronics, Beijing Institute of Technology; School of Information and Electronics, Beijing Institute of Technology; School of Information and Electronics, Beijing Institute of Technology","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","2819","2823","Improving the spatial resolution of a hyperspectral image (HSI) is of great significance in the remotely sensed field. By fusing a high-spatial-resolution multispectral image (MSI) with an HSI collected from the same scene, hyperspectral and multispectral (HS–MS) fusion has been an emerging technique to address the issue. Extracting complex spatial information from MSIs while maintaining abundant spectral information of HSIs is essential to generate the fused high-spatial-resolution HSI (HS2I). A common way is to learn low-rank/sparse representations from HSI and MSI, then reconstruct the fused HS2I based on tensor/matrix decomposition or unmixing paradigms, which ignore the intrinsic geometry proximity inherited by the low-rank property of the fused HS2I. This study proposes to estimate the high-resolution HS2I via low-rank tensor approximation with geometry proximity as side information learned from MSI and HSI by defined graph signals, which we name GLRTA. Row graph ${\mathcal{G}_r}$ and column graph ${\mathcal{G}_c}$ are defined on the horizontal slice and lateral slice of MSI tensor $\mathcal{M}$ respectively, while spectral band graph ${\mathcal{G}_b}$ is defined on a frontal slice of HSI tensor $\mathcal{H}$. Experimental results demonstrate that the proposed GLRTA can effectively improve the reconstruction results compared to other competitive works.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746041","China Postdoctoral Science Foundation; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746041","Graph signal processing;hyperspectral imagery;low-rank tensor approximation;remote sensing;super-resolution","Geometry;Tensors;Signal processing;Robustness;Sensors;Data mining;Speech processing","approximation theory;geophysical image processing;image fusion;image reconstruction;image representation;image resolution;matrix decomposition;remote sensing;spectral analysis;tensors","multispectral imagery fusion;spatial resolution;hyperspectral image;high spatial resolution multispectral image;HS-MS;complex spatial information;high spatial resolution HSI;fused HS;intrinsic geometry proximity;low rank property;defined graph signals;MSI tensor;spectral band graph;HSI tensor;remotely sensed hyperspectral;geometric low rank tensor approximation","","1","","19","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Development of gas detection software based on FTIR imaging spectrum","R. Wang; D. Dong; L. Jiao","Intelligent Equipment Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, China; Intelligent Equipment Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, China; Intelligent Equipment Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, China","2021 IEEE 3rd International Conference on Frontiers Technology of Information and Computer (ICFTIC)","24 Dec 2021","2021","","","674","677","Imaging spectroscopy technology has developed rapidly in the past 30 years and plays an important role in atmospheric remote sensing and environmental monitoring. But how to deal with the huge spectral data has become a major problem in current engineering applications. At present, the imaging spectrum processing software, such as PCI Geomatica, Erdas Imagine, Envi, etc. They require high computer performance, and are difficult to operate and meet a wide range of needs. Based on MATLAB, we developed a software for processing FTIR imaging spectral data, which uses techniques such as wavelet denoising, image threshold segmentation and Simeon Denis Poisson image fusion to identify the target gas region in the imaging spectral data, by adding pseudo-color to mark the target gas outline, and combining with the visible light image of the same area. We have realized the visualization of the two-dimensional spatial distribution of the target gas, which can intuitively display the spatial distribution and emission state of the gas.","","978-1-6654-0605-5","10.1109/ICFTIC54370.2021.9647171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647171","Imaging spectrum;FTIR;spectrum data processing;software architecture","Visualization;Spectroscopy;Graphical models;Noise reduction;Imaging;Data visualization;Data processing","computerised instrumentation;gas sensors;image fusion;image segmentation;infrared imaging;wavelet transforms","imaging spectrum processing software;PCI Geomatica;Erdas Imagine;image threshold segmentation;Simeon Denis Poisson image fusion;imaging spectral data;visible light image;gas detection software;FTIR imaging spectrum;imaging spectroscopy technology;atmospheric remote sensing","","","","10","IEEE","24 Dec 2021","","","IEEE","IEEE Conferences"
"Controlled Multi-modal Image Generation for Plant Growth Modeling","M. Miranda; L. Drees; R. Roscher","German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Data Science in Earth Observation, Technical University of Munich, Ottobrunn, Germany; Data Science in Earth Observation, Technical University of Munich, Ottobrunn, Germany","2022 26th International Conference on Pattern Recognition (ICPR)","29 Nov 2022","2022","","","5118","5124","Predicting plant development is an important task in precision farming and an essential metric for decision-making by researchers and farmers. In this work, we propose a novel generative modeling technique for plant growth prediction based on conditional generative adversarial networks. We formulate plant growth as an image-to-image translation task and predict the appearance of a plant growth stage as a function of its previous stage. We take into account that plant growth is inherently multi-modal, depending on numerous and highly variable environmental factors, and thus a single input belongs to a distribution of potential outputs. We encode the ambiguity in an interpretable and low-dimensional latent vector space representing the various factors of variation that are influencing plant growth. We use a novel encoder-based data fusion technique and combine information contained in remote sensing imagery of different cropping systems with data containing the factors of variation to adequately model plant growth. This offers several advantages over existing methods: (1) we show that we can model a distribution of potential appearances and simultaneously outperform existing methods in providing more realistic predictions, (2) the complexity of plant growth is more adequately captured, as various factors influencing plant growth can be included, (3) predictions are controllable by being conditioned by an interpretable latent vector representing the factors of variation along with an input image of a previous growth stage.","2831-7475","978-1-6654-9062-7","10.1109/ICPR56361.2022.9956115","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956115","","Measurement;Image synthesis;Ecosystems;Predictive models;Generative adversarial networks;Pattern recognition;Task analysis","agriculture;crops;encoding;environmental factors;geophysical image processing;image fusion;regression analysis;remote sensing;sensor fusion;vectors","controlled multimodal image generation;image-to-image translation task;plant growth modeling;plant growth prediction;plant growth stage;predicting plant development;previous growth stage","","","","36","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"A Spectral Mapping Based Intensity Modulation for Pan-Sharpening","X. Zhao; H. Liu; J. Zhang; Z. Wu; Z. Wei","School of Science, Nanjing University of Science and Technology, Nanjing, P.R.China; School of Science, Nanjing University of Science and Technology, Nanjing, P.R.China; School of Science, Nanjing University of Science and Technology, Nanjing, P.R.China; School of Compute Science and Engineering, Nanjing University of Science and Technology, Nanjing, P.R.China; School of Compute Science and Engineering, Nanjing University of Science and Technology, Nanjing, P.R.China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","843","846","A new pan-sharpening method based multispectral (MS) image intensity modulation is proposed. Firstly, according to the spectral correlation between MS image and panchromatic (PAN) image, a new spectral-enhanced PAN (SPAN) image is obtained. Then, SPAN image is utilized to calculate the spectral and spatial difference coefficients. Finally, the fused high resolution MS (HRMS) image is generated by modulating the intensity of the upsampled MS image. The experimental results demonstrate that the proposed method performs well both on spatial details preservation and spectral distortion reduction.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898228","Pan-sharpening;spectral mapping;component substitution;intensity modulation","Spatial resolution;Intensity modulation;Principal component analysis;Distortion;Hyperspectral imaging","image fusion;image resolution;intensity modulation","spectral mapping;pan-sharpening method;multispectral image intensity modulation;spectral-enhanced PAN image;SPAN image;spectral difference coefficients;spatial difference coefficients;spectral distortion reduction;high resolution MS image fusion","","","","10","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Sparse modeling of the land use classification problem","M. L. Mekhalfi; F. Melgani","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","3727","3730","In this paper, we present a fusion method contextualized within a land use classification framework. At first, feature vectors are extracted from all the color channels of the given test image. Then, the generated vectors are recovered over a bunch of training feature vectors extracted from training images. The resulting reconstruction residuals feed a fusion mechanism to further compose a final residual that serves for inferring the final decision of the class pertaining to the test image. Validated on a benchmark dataset, the presented method shows to promote drastic improvements over using only one single spectral channel. Furthermore, encouraging gains have been recorded with respect to reference works.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326633","Compressive sensing;data fusion;land use classification;textural representation","Feature extraction;Dictionaries;Training;Image reconstruction;Matching pursuit algorithms;Yttrium;Compressed sensing","feature extraction;geophysical image processing;image classification;image fusion;land use","spectral channel;reconstruction residual;color channel;feature vector extraction;image fusion method;land use classification problem sparse modeling","","","","12","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"An improved cartoon+texture decomposition based pansharpening method","M. Lotfi; H. Ghassemian","Faculty of Electrical and Computer, Engineering Tarbiat Modares University, Tehran, Iran; Faculty of Electrical and Computer, Engineering Tarbiat Modares University, Tehran, Iran","2017 Artificial Intelligence and Signal Processing Conference (AISP)","26 Mar 2018","2017","","","118","123","Pansharpening is the most widely used fusion method, in the field of remote sensing, to increase spatial information of the multispectral image while preserving spectral signatures. Based on the nature of spatial and spectral information, there is a lack of correlation between them. Therefore, separation of them can be considered as an image decomposition to uncorrelated components. Recently, the cartoon+texture decomposition was used in the pansharpening and decrease spectral distortion. However, details have not been strengthened enough. Therefore, in this paper we aim to use a filter based detail extraction to improve spatial information while mitigate spectral distortion.","","978-1-5386-2585-9","10.1109/AISP.2017.8324121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8324121","Pansharpening;image decomposition;cartoon;texture;low pass filter","Remote sensing;Distortion;Spatial resolution;Image decomposition;Artificial intelligence","feature extraction;image filtering;image fusion;image resolution;image texture;spectral analysis","remote sensing;spatial information;multispectral image;spectral signatures;spectral information;image decomposition;filter based detail extraction;improved cartoon+texture decomposition based pansharpening method;spectral distortion","","1","","22","IEEE","26 Mar 2018","","","IEEE","IEEE Conferences"
"Joint spatial variables nonnegative matrix factorization using constrained gradient method to pansharpen multispectral images","M. S. Karoui; I. Boukerch; K. Djerriri","Centre des Techniques Spatiales, Arzew, Algeria; Centre des Techniques Spatiales, Arzew, Algeria; Centre des Techniques Spatiales, Arzew, Algeria","2016 IEEE 12th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP)","4 Aug 2016","2016","","","1","5","Pansharpening is an important process in remote sensing that aims at combining observable panchromatic and multispectral images, to produce an unobservable pansharpened multispectral data. This fusion technique takes the advantage of the complementary spatial/spectral resolution characteristics of the considered images. In this paper, a new constrained gradient method for joint spatial-variables nonnegative matrix factorization is proposed to pansharpen multispectral images. In this method, a spatial degradation model between the two observable images is exploited in a joint spatial-variables criterion based on linear spectral unmixing concept. To optimize this criterion, new iterative gradient based update rules are designed. The proposed approach is applied to synthetic and real data. Its efficiency in both spatial and spectral domains is evaluated by commonly used performance criteria. Experimental results show that the proposed approach yields satisfactory spectral and spatial fidelities of the pansharpened images. Also, it outperforms tested pansharpening literature methods.","","978-1-5090-1929-8","10.1109/IVMSPW.2016.7528175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7528175","multispectral/panchromatic imaging;data fusion;pansharpening;linear spectral unmixing;nonnegative matrix factorization;constrained gradient method","Spatial resolution;Image reconstruction;Remote sensing;Degradation;Gradient methods;Convergence","gradient methods;image enhancement;image fusion;matrix decomposition;spectral analysis","joint spatial variables nonnegative matrix factorization;gradient method;pansharpen multispectral images;remote sensing;panchromatic images;fusion technique;linear spectral unmixing concept;synthetic data;real data","","","","19","IEEE","4 Aug 2016","","","IEEE","IEEE Conferences"
"Two Headed Dragons: Multimodal Fusion And Cross Modal Transactions","R. Bose; S. Pande; B. Banerjee","Centre of Studies in Resources Engineering, Indian Institute of Technology, Bombay, India; Centre of Studies in Resources Engineering, Indian Institute of Technology, Bombay, India; Centre of Studies in Resources Engineering, Indian Institute of Technology, Bombay, India","2021 IEEE International Conference on Image Processing (ICIP)","23 Aug 2021","2021","","","2893","2897","As the field of remote sensing is evolving, we witness the accumulation of information from several modalities, such as multispectral (MS), hyperspectral (HSI), LiDAR etc. Each of these modalities possess its own distinct characteristics and when combined synergistically, perform very well in the recognition and classification tasks. However, fusing multiple modalities in remote sensing is cumbersome due to highly disparate domains. Furthermore, the existing methods do not facilitate cross-modal interactions. To this end, we propose a novel transformer based fusion method for HSI and LiDAR modalities. The model is composed of stacked auto encoders that harness the cross key-value pairs for HSI and LiDAR, thus establishing a communication between the two modalities, while simultaneously using the CNNs to extract the spectral and spatial information from HSI and LiDAR. We test our model on Houston (Data Fusion Contest – 2013) and MUUFL Gulfport datasets and achieve competitive results.","2381-8549","978-1-6654-4115-5","10.1109/ICIP42928.2021.9506341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506341","Hyperspectral;LiDAR;domain mapping;multimodal fusion;cross-modal inferences","Laser radar;Image processing;Conferences;Data integration;Data models;Data mining;Character recognition","feature extraction;geophysical image processing;image classification;image fusion;optical radar;remote sensing;sensor fusion","headed dragons;multimodal Fusion;cross modal transactions;remote sensing;HSI;distinct characteristics;classification tasks;multiple modalities;highly disparate domains;cross-modal interactions;novel transformer based fusion method;LiDAR modalities;cross key-value pairs;spectral information;spatial information;Data Fusion Contest - 2013","","3","","11","IEEE","23 Aug 2021","","","IEEE","IEEE Conferences"
"A context-aware tracking method for aerial videos","F. Bi; M. Lei; Z. Yang; J. Hou; J. Zhang","School of Information Science and Technology, North China University of Technology, Beijing, China; School of Information Science and Technology, North China University of Technology, Beijing, China; School of Information Science and Technology, North China University of Technology, Beijing, China; School of Information Science and Technology, North China University of Technology, Beijing, China; School of Information Science and Technology, North China University of Technology, Beijing, China","2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)","21 Aug 2020","2019","","","1","5","Remote sensing target tracking in the aerial videos from aerial platforms is one of the research hotspots in visual tracking. In this paper, we propose a remote sensing target tracking method for aerial video based on a context-aware multi-domain convolutional neural network (CAMD). The process can be divided into two main stages: (1) in the design of the tracking network structure, we fuse multiple convolutional layers using residual connections to improve the effectiveness of regression learning. (2) in the “fuzzy interval”, a response-adaptive context-aware correlation filter (RA-CACF) module is introduced into our tracking network to boost the tracking performance. This method can greatly improve both the tracking efficiency and stability. We test the proposed method on the UAV123 datasets, and the experimental results demonstrate that our tracker can achieve high accuracy and efficiency results compared to state-of-the-art trackers.","","978-1-7281-2345-5","10.1109/ICSIDP47821.2019.9173211","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173211","aerial videos;context aware;multi domain learning;visual tracking","","convolutional neural nets;feature extraction;image fusion;image representation;learning (artificial intelligence);object detection;object tracking;regression analysis;remote sensing;target tracking;ubiquitous computing;video signal processing","aerial video;visual tracking;response adaptive context aware correlation filter module;context aware multidomain convolutional neural network;remote sensing target tracking;context aware tracking;CAMD;regression learning;RA-CACF module;multiple convolutional layer fusion;feature representation","","","","17","IEEE","21 Aug 2020","","","IEEE","IEEE Conferences"
"A Variational Pansharpening Algorithm Based on Total Variation and Primal-Dual Optimization","G. Khademi; H. Ghassemian","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2019 4th International Conference on Pattern Recognition and Image Analysis (IPRIA)","5 Aug 2019","2019","","","64","69","This paper proposes a variational framework to estimate the high-resolution (HR) multispectral (MS) image from the low-resolution (LR) MS image and the panchromatic (Pan) image. The LR MS image is modeled as a decimation of the HR MS image. Furthermore, the Pan image is considered as a linear combination of the HR MS bands. A super-resolution (SR) model is defined in accordance with the image observation model and the total variation (TV) regularization. The SR reconstruction problem is modeled as a minimization problem, which is solved by an efficient primal-dual algorithm in a Euclidean setting. The result of comparing the proposed method with some recent classical and variational pansharpening methods proves the superiority of the proposed variational pansharpening algorithm.","2049-3630","978-1-7281-1621-1","10.1109/PRIA.2019.8786000","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8786000","image fusion;pansharpening;primal-dual optimization;remote sensing;total variation","Optimization;Image resolution;TV;Minimization;Sensors;Signal processing algorithms;Satellite broadcasting","geophysical image processing;image reconstruction;image resolution;minimisation;remote sensing;variational techniques","Euclidean setting;minimization problem;pan image;LR MS image;panchromatic image;low-resolution MS image;high-resolution multispectral image;variational framework;primal-dual optimization;variational pansharpening algorithm;variational pansharpening methods;primal-dual algorithm;SR reconstruction problem;total variation regularization;image observation model;super-resolution model;HR MS bands;HR MS image","","4","","23","IEEE","5 Aug 2019","","","IEEE","IEEE Conferences"
"A Large-Scale Benchmark Data Set for Evaluating Pansharpening Performance: Overview and Implementation","X. Meng; Y. Xiong; F. Shao; H. Shen; W. Sun; G. Yang; Q. Yuan; R. Fu; H. Zhang","College of Electrical and Information Engineering, Hunan University, Changsha, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, China; School of Resources and Environmental Sciences, Wuhan University, China; Department of Geography and Spatial Information Techniques, Ningbo University, China; Department of Geography and Spatial Information Techniques, Ningbo University, China; School of Geodesy and Geomatics, Wuhan University, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, China; Collaborative Innovation Center for Geospatial Technology, Wuhan University, China","IEEE Geoscience and Remote Sensing Magazine","5 Mar 2021","2021","9","1","18","52","Pansharpening aims to sharpen a lowspatial-resolution (LR) multispectral (MS) image using a high-spatial-resolution (HR) panchromatic (Pan) image to obtain the HR MS image. It has been a fundamental and active research topic in remote sensing, and pansharpening methods have been developed for nearly 40 years. While the performance evaluation of pansharpening methods is still based on a small number of individual images, datadriven pansharpening approaches are attracting increasing attention. However, few publicly available benchmark data sets for pansharpening are available, especially large-scale ones. This has been a serious limitation for the future development of pansharpening methods.","2168-6831","","10.1109/MGRS.2020.2976696","Natural Science Foundation of Ningbo(grant numbers:2019A610098); K. C. Wong Magna Fund in Ningbo University; National Natural Science Foundation of China(grant numbers:41801252); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9082183","","Remote sensing;Benchmark testing;Spatial resolution;Satellites;Multiresolution analysis","artificial satellites;geophysical image processing;image fusion;image resolution;remote sensing;sensor fusion","large-scale benchmark data set;pansharpening aims;low-spatial-resolution multispectral image;high-spatial-resolution panchromatic image;HR MS image;fundamental research topic;pansharpening methods;datadriven pansharpening approaches","","44","","107","IEEE","29 Apr 2020","","","IEEE","IEEE Magazines"
"Efficiency determination of scanner data fusion methods of space multispectral images","V. V. Hnatushenko; O. O. Kavats; I. O. Kibukevych","Department automated information processing systems, O. Gonchar Dnepropetrovsk national university, Ukraine; Department information technologies and systems, National metallurgical academy of Ukraine, Ukraine; Department information technologies and systems, National metallurgical academy of Ukraine, Ukraine","2015 International Young Scientists Forum on Applied Physics (YSF)","23 Nov 2015","2015","","","1","4","This paper explores the major remote sensing data fusion techniques at pixel level and reviews the concept, principals, limitations and advantages for each technique. This paper focused on traditional techniques like intensity hues-aturation-(IHS), Brovey, principal component analysis (PCA) and Wavelet. The results show that in comparison with classical fusion methods synergistic processing of scanner multispectral data using the proposed information technology based on ICA- and wavelet transformation gives more qualitative result. The synthesized image has high information content without color distortion. For each of the indices used to assess spatial and spectral quality of the merged images, the best results are obtained with the proposed method.","","978-1-4673-6977-0","10.1109/YSF.2015.7333153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333153","panchromatic;multispectral image;fusion methods;ICA;wavelet","Principal component analysis;Wavelet transforms;Image color analysis;Entropy;Information technology;Distortion;Remote sensing","geophysical image processing;independent component analysis;principal component analysis;sensor fusion;wavelet transforms","scanner data fusion methods;space multispectral images;remote sensing data fusion techniques;intensity hue saturation;principal component analysis;PCA;fusion methods synergistic processing;scanner multispectral data;information technology;wavelet transformation;spectral quality","","2","","14","IEEE","23 Nov 2015","","","IEEE","IEEE Conferences"
"A CNN-based Super-resolution Technique for Active Fire Detection on Sentinel-2 Data","M. Gargiulo; D. A. G. Dell’ Aglio; A. Iodice; D. Riccio; G. Ruello","University of Naples “Federico II”, Italy; University of Naples “Federico II”, Italy; University of Naples “Federico II”, Italy; University of Naples “Federico II”, Italy; University of Naples “Federico II”, Italy","2019 PhotonIcs & Electromagnetics Research Symposium - Spring (PIERS-Spring)","2 Mar 2020","2019","","","418","426","Remote Sensing applications can benefit from a relatively fine spatial resolution multispectral (MS) images and a high revisit frequency ensured by the twin satellites Sentinel-2. Unfortunately, only four out of thirteen bands are provided at the highest resolution of 10 meters, and the others at 20 or 60 meters. For instance the Short-Wave Infrared (SWIR) bands, provided at 20 meters, are very useful to detect active fires. Aiming to a more detailed Active Fire Detection (AFD) maps, we propose a super-resolution data fusion method based on Convolutional Neural Network (CNN) to move towards the 10-m spatial resolution the SWIR bands. The proposed CNN-based solution is compared to alternative methods in terms of some accuracy metrics. Moreover we have tested the super-resolved bands from an application point of view by monitoring active fire through classic indices. Advantages and limits of our proposed approach are validated on specific geographical area (the mount Vesuvius, close to Naples) that was damaged by widespread fires during the summer of 2017.","1559-9450","978-1-7281-3403-1","10.1109/PIERS-Spring46901.2019.9017857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9017857","","Spatial resolution;Training;Springs;Measurement;Monitoring;Photonics","fires;geophysical image processing;geophysical signal processing;image fusion;image resolution;neural nets;remote sensing;sensor fusion;terrain mapping","CNN-based super-resolution technique;Sentinel-2 data;Remote Sensing applications;relatively fine spatial resolution multispectral images;high revisit frequency;twin satellites Sentinel-2;Short-Wave Infrared bands;super-resolution data fusion method;Convolutional Neural Network;SWIR bands;CNN-based solution;super-resolved bands;Active Fire Detection maps","","8","","26","IEEE","2 Mar 2020","","","IEEE","IEEE Conferences"
"Landslide Detection Based on Improved YOLOv5 and Satellite Images","T. Wang; M. Liu; H. Zhang; X. Jiang; Y. Huang; X. Jiang","State Key Laboratory of Geohazard Prevention and Geoenvironment Protection, Chengdu University of Technology, China, Chengdu, China; State Key Laboratory of Geohazard Prevention and Geoenvironment Protection, Chengdu University of Technology, China, Chengdu, China; State Key Laboratory of Geohazard Prevention and Geoenvironment Protection, Chengdu University of Technology, China, Chengdu, China; State Key Laboratory of Geohazard Prevention and Geoenvironment Protection, Chengdu University of Technology, China, Chengdu, China; State Key Laboratory of Geohazard Prevention and Geoenvironment Protection, Chengdu University of Technology, China, Chengdu, China; State Key Laboratory of Geohazard Prevention and Geoenvironment Protection, Chengdu University of Technology, China, Chengdu, China","2021 4th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)","4 Oct 2021","2021","","","367","371","Since the 21st century, due to the repeated deterioration of the natural climate and the increasing impact of human production activities on the ecological environment, landslides have become a common high-hazard natural phenomenon. Traditional landslide detection is mainly done through field detection, synthetic aperture radar, and other technologies. With the increase in the accuracy of satellite imagery and the rapid development of deep learning, the use of deep learning to realize landslide detection has gradually become a trend. Our work mainly includes two parts: landslide data set production and model performance improvement. We have produced a complete general landslide dataset based on the high-resolution remote sensing images obtained from open satellites and the annotations of professional researchers. We will publish our dataset later. Based on the research of previous researchers and based on the basic framework of YOLOv5, we improved the feature splicing method of YOLOv5 by adding Adaptively Spatial Feature Fusion (ASFF), and fused feature information of different scales to improve the model. To better mine shallow feature information, we introduced the Convolutional Block Attention Module (CBAM) module to improve the performance of the model. Experiments have proved that our model has a 1.64% performance improvement.","","978-1-6654-1322-0","10.1109/PRAI53619.2021.9551067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551067","landslide detection;YOLOv5;ASFF;Attention mechanism","Deep learning;Landslides;Adaptation models;Satellites;Biological system modeling;Splicing;Production","deep learning (artificial intelligence);feature extraction;geomorphology;geophysical image processing;image fusion;object detection;remote sensing","convolutional block attention module;high-hazard natural phenomenon;YOLOv5 feature splicing method;satellite images;adaptively spatial feature fusion;high-resolution remote sensing images;landslide data set production;deep learning;satellite imagery;synthetic aperture radar;landslide detection;human production activities","","6","","16","IEEE","4 Oct 2021","","","IEEE","IEEE Conferences"
"Research on Application of Computer Artificial Intelligence Technology in Remote Dynamic Monitoring of Environment","P. Feng; Q. Qiang","Urumqi Meteorological Satellite Ground Station, Urumqi, China; Foshan Graduate School of Beijing Foreign Studies University, Beijing, China","2021 IEEE International Conference on Emergency Science and Information Technology (ICESIT)","8 Feb 2022","2021","","","761","765","This article systematically discusses the importance of the ecological environment dynamic monitoring and management information system, the goals, principles and specific content of the system construction, technical routes and implementation plans, and the significance and role of the system after it is completed. The article describes the solution to several key technical problems in the research and development process of this system, including the integration and fusion of multi-source and multi-type data, the monitoring and refining of the status of the ecological environment and its change information based on remote sensing images, and the scale conversion of ecological environment information. Ecological environment evaluation model, ecological environment dynamic simulation and virtual expression, etc. Through the design of this system, it can provide a theoretical basis for the protection, governance, monitoring, and planning of the ecological environment, and also serve for the rational deployment and utilization of land resources and water resources.","","978-1-6654-3531-4","10.1109/ICESIT53460.2021.9696488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9696488","ecological environment;dynamic monitoring;information management;comprehensive control;information system","Biological system modeling;Decision making;Sociology;Dynamic scheduling;Solids;Planning;Statistics","artificial intelligence;data fusion;ecology;environmental monitoring (geophysics);environmental science computing;geographic information systems;geophysical image processing;image fusion;management information systems;remote sensing;water resources","computer artificial intelligence technology;remote dynamic monitoring;ecological environment dynamic monitoring;management information system;system construction;multisource data fusion;multitype data fusion;remote sensing images;ecological environment information;ecological environment evaluation model;ecological environment dynamic simulation;research and development process;scale conversion;virtual expression;ecological environment protection;land resources;water resources","","","","7","IEEE","8 Feb 2022","","","IEEE","IEEE Conferences"
"Multiscale directional bilateral filter based fusion of satellite images","S. Kaynak; D. Kumlu; I. Erer","Faculty of Electrical and Electronic Engineering, Istanbul Technical University, Maslak, Istanbul, Turkey; Department of Electronics Engineering, Turkish Naval Academy, Tuzla, Istanbul, Turkey; Faculty of Electrical and Electronic Engineering, Istanbul Technical University, Maslak, Istanbul, Turkey","2017 10th International Conference on Electrical and Electronics Engineering (ELECO)","22 Jan 2018","2017","","","1161","1165","The fusion of multispectral and panchromatic satellite images is widely used in remote sensing community to obtain high resolution images both in spatial and spectral domains. One approach is to obtain Panchromatic (PAN) image details through a multiresolution analysis (MRA) based decomposition such as wavelet transform and inject them to the low resolution Multispectral (MS) image. However, this process may lead to redundant detail injection, resulting to artifacts and overenhancement in the fusion results. In this paper, we propose a new fusion scheme based on the decomposition of the PAN image using a multidirectional bilateral filter. Obtained detail layers contain both resolution and direction information. By appropriate selection of these layers, it is possible to perform a proper detail injection. Visual and quantitative results for Quickbird images are presented to validate the proposed method.","","978-605-01-1134-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8266379","","Filter banks;Spatial resolution;Information filters;Filtering algorithms;Wavelet transforms","geophysical image processing;image enhancement;image filtering;image fusion;image resolution;remote sensing;spectral analysis","visual results;quantitative results;Quickbird images;direction information;multidirectional bilateral filter;PAN image;redundant detail injection;low resolution Multispectral image;multiresolution analysis based decomposition;Panchromatic image details;spectral domains;spatial domains;high resolution images;remote sensing community;panchromatic satellite images;multispectral satellite images;multiscale directional bilateral filter based fusion","","","","13","","22 Jan 2018","","","IEEE","IEEE Conferences"
"Spatial Quality Assessment of Pansharpened Images Based on Gray Level Co-Occurrence Matrix","S. Aghapour Maleki; H. Ghassemian","Image Processing and Information Analysis Laboratory Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Laboratory Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2022 International Conference on Machine Vision and Image Processing (MVIP)","22 Mar 2022","2022","","","1","6","Assessing the quality of pansharpened images is a critical issue in order to obtain a quantitative score to represent the quality and compare the performance of different fusion methods. Most of the introduced metrics for pansharpened image quality assessment, evaluate the spectral content of the image, while in different applications of remote sensing like detection and identification of image objects, spatial quality has an important role. In the current study, a new index for spatial quality assessment is introduced that extracts gray level co-occurrence matrix (GLCM) from distorted and reference images and compares the similarities of these features. The tempere image database 2013 (TID2013) that provides reference and different types of distorted images with subjective scores of each image is used as the desired database. To solve the high computational complexity of obtaining GLCM features, the fast GLCM method is employed. In this way, 16 different features are extracted. To select the features that have the most consistency with the human visual system (HVS), the forward floating search method is used as a feature selection method and five features are obtained as the final features to form the desired index. Experimental results show the efficiency of the proposed method in determining the spatial quality of fused images compared with that of the available quality assessment metrics.","2166-6784","978-1-6654-1216-2","10.1109/MVIP53647.2022.9738763","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9738763","pansharpened image;spatial quality assessment;gray level co-occurrence matrix (GLCM);feature selection;forward floating search","Measurement;Image quality;Search methods;Pansharpening;Visual systems;Feature extraction;Quality assessment","computational complexity;feature extraction;feature selection;geophysical image processing;image classification;image colour analysis;image fusion;image resolution;image texture;remote sensing;search problems;visual databases","spatial quality assessment;tempere image database 2013;gray level co-occurrence matrix extraction;distorted images;GLCM features;forward floating search method;feature selection method;quantitative score;fusion methods;pansharpened image quality assessment;image objects;reference images;TID2013;computational complexity;feature extraction;human visual system;HVS;remote sensing","","","","24","IEEE","22 Mar 2022","","","IEEE","IEEE Conferences"
"Mapping of Prosopis Juliflora by a Fusion assisted Pattern Based Classification","B. S. Bama; C. Shivashankar; R. Madhumitha; V. V. Priya; K. Kumar; A. Uppal","Electronics and Communication Engineering, Thiagarajar College of Engineering, Madurai, India; Electronics and Communication Engineering, Thiagarajar College of Engineering, Madurai, India; Electronics and Communication Engineering, Thiagarajar College of Engineering, Madurai, India; Electronics and Communication Engineering, Thiagarajar College of Engineering, Madurai, India; Electronics and Communication Engineering, Thiagarajar College of Engineering, Madurai, India; Electronics and Communication Engineering, Thiagarajar College of Engineering, Madurai, India","2020 5th International Conference on Devices, Circuits and Systems (ICDCS)","23 Apr 2020","2020","","","21","25","This paper proposes a fusion assisted classification method to locate and map invasive plant species, Prosopis Juliflora using remote sensing techniques towards conservation of biodiversity. This is two stage method. At first, wavelet based fusion method is proposed for the multispectral image to produce high resolution multispectral image.In the second stage a texture based classification is performed ith pattern study.Minimum distance classifier is used to classify the input image based on weighted texture features. Accuracy is computed by collecting the ground truth points from the study site. Similar procedure is repeated for the Google Maps data and accuracy comparison of World View 2 and Google Maps is carried out. Thus World View 2 data outperform Google Maps data by achieving accuwracy of 80 percentage.","2644-1802","978-1-7281-6368-0","10.1109/ICDCS48716.2020.243540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9075616","Wavelet based fusion method;Minimum distance classifier;Multispectral image","Vegetation;Feature extraction;Image resolution;Circuits and systems;Irrigation;Biodiversity;Google","feature extraction;geophysical image processing;image classification;image fusion;image texture;remote sensing;vegetation mapping;wavelet transforms","Google Maps data;Prosopis Juliflora;pattern based classification;fusion assisted classification method;map invasive plant species;remote sensing techniques;high resolution multispectral image;texture based classification;minimum distance classifier;weighted texture features;World View 2 data","","","","8","IEEE","23 Apr 2020","","","IEEE","IEEE Conferences"
"SSIM Prediction for H.265/HEVC based on Convolutional Neural Networks","S. Wang; Y. Zhang; D. Yang; Z. Chen","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","2019 IEEE Visual Communications and Image Processing (VCIP)","23 Jan 2020","2019","","","1","4","In signal compression, distortion information is significant for rate distortion optimization. In this paper, we propose a convolutional neural network (CNN) to predict distortion information for H.265/HEVC. With the strong representation power of CNN, structural similarity (SSIM) maps indicating distortion information can be predicted directly in an end-to-end, pixel-to-pixel way. Different from traditional CNNs which focus on learning one-to-one mappings from input to output, we show that our CNN model can predict SSIM maps conditioned on quantization parameters (QPs), realizing one-to-many mappings. To construct our CNN network, QP labels are designed as conditions to feed the CNN model. We also apply symmetrical network architecture and multi-level feature fusion method to ensure our network can utilize both high-level semantic features and low-level structure features. The experiments on MS COCO database demonstrate the effectiveness of our CNN-based method for SSIM prediction.","2642-9357","978-1-7281-3723-0","10.1109/VCIP47243.2019.8965734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8965734","SSIM;distortion prediction;convolutional neural network;H.265/HEVC;feature fusion;QP label","","convolutional neural nets;data compression;feature extraction;image fusion;optimisation;quantisation (signal);rate distortion theory;video coding","low-level structure features;CNN-based method;SSIM prediction;convolutional neural network;signal compression;distortion information;rate distortion optimization;structural similarity maps;pixel-to-pixel way;CNN model;SSIM maps;CNN network;symmetrical network architecture;H.265/HEVC;quantization parameters;one-to-many mappings;QP labels;multilevel feature fusion method;high-level semantic features","","6","","15","IEEE","23 Jan 2020","","","IEEE","IEEE Conferences"
"Single Image Super-Resolution Using Depth Map as Constraint","H. Shi; J. Jiang; J. Yao; Z. Xu","School of Remote Sensing and Information Engineering, Wuhan University, P.R. China; School of Remote Sensing and Information Engineering, Wuhan University, P.R. China; School of Remote Sensing and Information Engineering, Wuhan University, P.R. China; School of Remote Sensing and Information Engineering, Wuhan University, P.R. China","2020 IEEE International Conference on Image Processing (ICIP)","30 Sep 2020","2020","","","538","542","Single image super-resolution based on the deep neural network has achieved great performance recently, but generating photo-realistic images remains a challenging problem. To tackle this issue, we propose a method that uses depth maps as a constraint to get better visual quality. Specifically, we propose a self-adaptive feature transform (AFT) layer, which can perform affine transformation on the feature map based on the depth map to constrain the plausible solution space of the SR image. Furthermore, we propose a hierarchical residual multi-scale fusion block to improve the representational ability of the network. Experimental results on benchmark datasets demonstrate that our method is superior to other perceptually-oriented SISR methods in terms of visual quality and also achieves state-of-the-art performance on quantitative metrics.","2381-8549","978-1-7281-6395-6","10.1109/ICIP40778.2020.9190978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9190978","super-resolution;depth map;feature transform;weight map","Training;Transforms;Visualization;Feature extraction;Measurement;Spatial resolution","affine transforms;deep learning (artificial intelligence);feature extraction;image fusion;image representation;image resolution","single image super-resolution;depth map;deep neural network;visual quality;feature map;SR image;hierarchical residual multiscale fusion block;self-adaptive feature transform;network representational ability;affine transformation","","","","26","IEEE","30 Sep 2020","","","IEEE","IEEE Conferences"
"Heterogeneous two-Stream Network with Hierarchical Feature Prefusion for Multispectral Pan-Sharpening","D. Wang; Y. Bai; B. Bai; C. Wu; Y. Li","National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi'an, China; National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Communication and Information Engineering, Xi’an University of Posts and Telecommunications, Xi'an, China; National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, Shaanxi Provincial Key Laboratory of Speech & Image Information Processing, Northwestern Polytechnical University, Xi'an, China; National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, Shaanxi Provincial Key Laboratory of Speech & Image Information Processing, Northwestern Polytechnical University, Xi'an, China","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","1845","1849","Multispectral (MS) pan-sharpening aims at producing a high spatial resolution (HR) MS image by fusing a single-band HR panchromatic (PAN) image and a corresponding MS image with low spatial resolution. In this paper, we propose a heterogeneous two-stream network (HTSNet) with hierarchical feature prefusion for MS pan-sharpening. The HTSNet employs a heterogeneous group of spatial and spectral streams for spatial and spectral information extraction, respectively. The spatial stream utilizes a 2D CNN for spatial information extraction from the PAN images, and the spectral stream obtains spectral feature cubes from the MS images by a 3D CNN. At the same time, a prefusion module is introduced to prefuse the spatial details with spectral information and transfer information between different streams, which can enhance later processing. In the experiment, the Gaofen-2 satellite dataset is utilized to compare the proposed method with the state-of-the-art MS pan-sharpening methods. Experimental results demonstrate the superiority of our HTSNet in terms of visual effect and quantitative qualities.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413736","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413736","Pan-sharpening;heterogeneous network;two-stream network;hierarchical feature prefusion","Satellites;Three-dimensional displays;Fuses;Streaming media;Signal processing;Information retrieval;Visual effects","convolutional neural nets;geophysical image processing;hyperspectral imaging;image fusion;image resolution;image retrieval;remote sensing;stereo image processing","hierarchical feature prefusion;HTSNet;spatial information extraction;spectral information extraction;spatial stream;2D CNN;PAN images;spectral stream;3D CNN;transfer information;multispectral pan-sharpening;high spatial resolution MS image;low spatial resolution;heterogeneous two-stream network;single-band HR panchromatic image fusion;Gaofen-2 satellite dataset","","1","","23","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"A Gaussian Process Regression Approach for Fusion of Remote Sensing Images for Oil Spill Segmentation","F. S. Longman; L. Mihaylova; L. Yang","Department of Automatic Control and System Engineering, University of Sheffield, UK; Department of Automatic Control and System Engineering, University of Sheffield, UK; Department of Electrical and Computer Engineering, University of Canterbury, Chrischurch, New Zealand","2018 21st International Conference on Information Fusion (FUSION)","6 Sep 2018","2018","","","62","69","Synthetic Aperture Radar (SAR) satellite systems are very efficient in oil spill monitoring due to their capability to operate under all weather conditions. This paper presents a framework using Gaussian process (GP) to fuse SAR images of different modalities and to segment dark areas (assumed oil spill) for oil spill detection. A new covariance function; a product of an intrinsically sparse kernel and a Rational Quadratic Kernel (RQK) is used to model the prior of the estimated image allowing information to be transferred. The accuracy performance evaluation demonstrates that the proposed framework has 37% less RMSE per pixel and a compelling enhancement visually when compared with existing methods.","","978-0-9964527-6-2","10.23919/ICIF.2018.8455304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8455304","Oil Spill;Synthetic Aperture Radar (SAR);Registration;Image Fusion;Segmentation;Gaussian Processes","Oils;Kernel;Synthetic aperture radar;Image resolution;Gaussian processes;Feature extraction;Image segmentation","Gaussian processes;geophysical image processing;image segmentation;radar imaging;regression analysis;remote sensing;synthetic aperture radar","gaussian process regression approach;remote sensing images;oil spill segmentation;Synthetic Aperture Radar satellite systems;oil spill monitoring;weather conditions;SAR images;Rational Quadratic Kernel;intrinsically sparse kernel;covariance function;oil spill detection","","1","","35","","6 Sep 2018","","","IEEE","IEEE Conferences"
"Evaluation of Fuzzy Integral Data Fusion Methods for Rare Object Detection in High-Resolution Satellite Imagery","A. B. Cannaday; C. H. Davis; A. J. Maltenfort","Center for Geospatial Intelligence, University of Missouri, Columbia, MO; Center for Geospatial Intelligence, University of Missouri, Columbia, MO; National Geospatial - Intelligence Agency, Springfield, VA, USA","2020 IEEE International Conference on Big Data (Big Data)","19 Mar 2021","2020","","","1","10","Here we demonstrate how the results from multiple Deep Neural Networks (DNN) can be fused to improve detections and reduce error when searching for scarce or rare objects in high-resolution satellite imagery. A wide variety of experiments were conducted using the xView dataset to develop and evaluate multiple fusion strategies to improve the detection of Engineering Vehicles (e.g. excavators, cranes, bulldozers, etc.). The results demonstrate that fusion of multiple DNNs can increase the absolute True Positive Rate (TPR) by up to 5% while reducing the total error by ~20-60%. The best results were obtained by partitioning 8-band multi-spectral imagery into three sets of 3-band images to utilize existing RGB models for transfer learning. The multi-DNN results were then fused using fuzzy integrals that also included DNN detections from multiple scales. This multi-DNN fusion approach can be easily extended to a variety of other challenging rare or scarce object detection problems in large remote sensing image datasets.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9377990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9377990","data fusion;remote sensing;object detection;deep learning;multi-spectral imagery;fuzzy measure","Satellites;Transfer learning;Object detection;Big Data;Search problems;Sensors;Remote sensing","feature extraction;fuzzy set theory;geophysical image processing;image colour analysis;image fusion;image sensors;learning (artificial intelligence);neural nets;object detection;remote sensing;sensor fusion","fuzzy integral data fusion methods;high-resolution satellite imagery;multiple Deep Neural Networks;scarce objects;rare objects;multiple fusion strategies;multiple DNNs;total error;8-band multispectral imagery;multiDNN results;fuzzy integrals;DNN detections;multiDNN fusion approach;challenging rare object detection problems;scarce object detection problems","","","","26","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"A Residual Dense Generative Adversarial Network For Pansharpening With Geometrical Constraints","A. GASTINEAU; J. -F. AUJOL; Y. BERTHOUMIEU; C. GERMAIN","Bordeaux INP, CNRS, IMB, UMR 5251, Univ. Bordeaux, Talence, France; Bordeaux INP, CNRS, IMB, UMR 5251, Univ. Bordeaux, Talence, France; Bordeaux INP, CNRS, IMS, UMR 5218, Univ. Bordeaux, Talence, France; Bordeaux INP, CNRS, IMS, UMR 5218, Univ. Bordeaux, Talence, France","2020 IEEE International Conference on Image Processing (ICIP)","30 Sep 2020","2020","","","493","497","The pansharpening problem consists in fusing a high resolution panchromatic image with a low resolution multispectral image in order to obtain a high resolution multispectral image. In this paper, we adapt a Residual Dense architecture for the generator in a Generative Adversarial Network framework. Indeed, this type of architecture avoids the vanishing gradient problem faced when training a network by re-injecting previous information thanks to dense and residual connections. Moreover, an important point for the pansharpening problem is to preserve the geometry of the image. Hence, we propose to add a regularization term in the loss function of the generator: it preserves the geometry of the target image so that a better solution is obtained. In addition, we propose geometrical measures that illustrate the advantages of this new method.","2381-8549","978-1-7281-6395-6","10.1109/ICIP40778.2020.9191230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9191230","Pansharpening;Generative Adversarial Network;residual dense network;regularization;remote sensing","Generators;Gallium nitride;Spatial resolution;Generative adversarial networks;Geometry;Satellites","geometry;hyperspectral imaging;image colour analysis;image fusion;image resolution;neural nets","geometrical measures;Residual Dense Generative Adversarial Network;geometrical constraints;pansharpening problem;low resolution multispectral image;high resolution multispectral image;residual dense architecture;vanishing gradient problem;residual connections;high resolution panchromatic image fusion;regularization term;loss function;target image geometry","","3","","22","IEEE","30 Sep 2020","","","IEEE","IEEE Conferences"
"Contourlet image preprocessing for enhanced control point selection in airborne image registration","T. Sobolewski; N. Messer; A. Lutz; S. Ezekiel; E. Blasch; M. Alford; A. Bubalo","Soundararajan Ezekiel Indiana University of Pennsylvania, Indiana, PA; Soundararajan Ezekiel Indiana University of Pennsylvania, Indiana, PA; Soundararajan Ezekiel Indiana University of Pennsylvania, Indiana, PA; Soundararajan Ezekiel Indiana University of Pennsylvania, Indiana, PA; Air Force Research Laboratory Rome, NY; Air Force Research Laboratory Rome, NY; Air Force Research Laboratory Rome, NY","2015 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)","31 Mar 2016","2015","","","1","6","In applications such as airborne imagery, target tracking, remote sensing, and medical imaging; it is helpful to have an image set where all of the images lie on one fixed coordinate system. However, frequently a set of images cannot be captured from a fixed perspective using the same sensor or different sensors at the same time. Image registration presents a solution by mapping points from one image to corresponding points in another image; however existing registration methods are computationally expensive and not completely accurate. Hence, continual investigation of image registration methods is needed such as those using feature-based or intensity-based approaches, transformation models, spatial and frequency domain methods, and single or multi-modality data. In this paper, we investigate these processes by focusing on the identification of control points, which play a vital role in the process of registering images. By using the multi-resolution contourlet transform for image preprocessing, control points are better identified, which provides us a more reliable image registration for applications such as image fusion.","2332-5615","978-1-4673-9558-8","10.1109/AIPR.2015.7444529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444529","Airborne Imagery;Contourlet;Control Points;Coordinate System;Image Registration","Image registration;Feature extraction;Image edge detection;Image fusion;Wavelet transforms;Measurement","feature selection;image registration;remote sensing;transforms","contourlet image preprocessing;control point selection;airborne image registration;multiresolution contourlet transform","","2","","36","IEEE","31 Mar 2016","","","IEEE","IEEE Conferences"
"Multi-sharpening hyperspectral remote sensing data by Multiplicative Joint-Criterion Linear-Quadratic Nonnegative Matrix Factorization","F. Z. Benhalouche; M. S. Karoui; Y. Deville; I. Boukerch; A. Ouamri","UPS-OMP, Université de Toulouse, Toulouse, France; UPS-OMP, Université de Toulouse, Toulouse, France; UPS-OMP, Université de Toulouse, Toulouse, France; Centre des Techniques Spatiales, Arzew, Algeria; Université des Sciences et de la Technologie, Oran, Algeria","2017 IEEE International Workshop of Electronics, Control, Measurement, Signals and their Application to Mechatronics (ECMSM)","15 Jun 2017","2017","","","1","6","Multi-sharpening consists in fusing a multispectral image with a hyperspectral one, to produce an unobservable image with the high spatial resolution of the former and the high spectral resolution of the latter. In this paper, a new fusion method, based on the spectral unmixing concept, is proposed. The proposed method, related to linear-quadratic spectral unmixing techniques, and based on linear-quadratic nonnegative matrix factorization, optimizes a new joint criterion by using new designed multiplicative update rules. This joint criterion exploits a spatial degradation model between the considered images. The proposed method is applied to synthetic data, and its effectiveness is evaluated with established performance criteria. Obtained results prove that the proposed method yields multi-sharpened hyperspectral data with good spectral and spatial fidelities. These results also illustrate that the proposed method outperforms the considered multi-sharpening literature approaches.","","978-1-5090-5582-1","10.1109/ECMSM.2017.7945884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7945884","Hyper/multispectral imaging;data fusion;multi-sharpening;hypersharpening;linear-quadratic spectral unmixing;linear-quadratic nonnegative matrix factorization","Hyperspectral imaging;Spatial resolution;Degradation;Sparse matrices","geophysical image processing;geophysical techniques;hyperspectral imaging;image fusion;image resolution;matrix decomposition","multisharpening hyperspectral remote sensing data;multiplicative joint-criterion linear-quadratic nonnegative matrix factorization;multispectral image;hyperspectral image;spatial image resolution;image fusion method;spectral unmixing concept;linear-quadratic spectral unmixing technique;joint criterion;multiplicative update rule;sharpened hyperspectral data;multisharpening approach","","2","","11","IEEE","15 Jun 2017","","","IEEE","IEEE Conferences"
"GLCM, Gabor, and morphology profiles fusion for hyperspectral image classification","M. Imani; H. Ghassemian","Faculty of Electrical and Computer Engineering, Tarbiat Modares University Tehran, Iran; Faculty of Electrical and Computer Engineering, Tarbiat Modares University Tehran, Iran","2016 24th Iranian Conference on Electrical Engineering (ICEE)","10 Oct 2016","2016","","","460","465","A fusion method for combination of spectral and spatial features for classification improvement of hyperspectral images is proposed in this paper. Gray level co-occurance matrix (GLCM), Gabor filters, and morphology profiles are powerful tools for extraction of texture, shape, and size from the neighboring pixels. We study different combinations of theses spatial features with spectrum data and find the best choice for fusion of spectral and spatial features to increase the classification accuracy. Moreover, we assess the performance of PCA for feature reduction of fused feature vector in the best case. The experimental results on two real hyperspectral images show the good performance of proposed fusion method compared to other studied cases.","","978-1-4673-8789-7","10.1109/IranianCEE.2016.7585566","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7585566","Gray level co-occurance matrix;Gabor filter;morphology profiles;hyperspectral;classification","Decision support systems","feature extraction;Gabor filters;geophysical image processing;image classification;image fusion;image resolution;image texture;matrix algebra;principal component analysis;remote sensing;vectors","GLCM;gray level cooccurance matrix;Gabor filters;morphology profiles;hyperspectral image classification;spectral feature fusion method;spatial feature fusion method;texture extraction;shape extraction;size extraction;neighboring pixels;spectrum data;PCA;principal component analysis;feature reduction;fused feature vector;remote sensing techniques","","15","","30","IEEE","10 Oct 2016","","","IEEE","IEEE Conferences"
"Hyperspectral Pansharpening: A Review","L. Loncan; L. B. de Almeida; J. M. Bioucas-Dias; X. Briottet; J. Chanussot; N. Dobigeon; S. Fabre; W. Liao; G. A. Licciardi; M. Simões; J. -Y. Tourneret; M. A. Veganzones; G. Vivone; Q. Wei; N. Yokoya","ONERA, Toulouse, France; Instituto de Telecomunicações, Universidade de Lisboa; Instituto de Telecomunicações, Universidade de Lisboa; ONERA, Toulouse, France; Gipsa-lab, Grenoble, France; IRIT, University of Toulouse; ONERA, Toulouse, France; Ghent University, Ghent, Belgium; Gipsa-lab, Grenoble, France; Instituto de Telecomunicações, Gipsa-lab, Grenoble, France; IRIT, University of Toulouse; Gipsa-lab, Grenoble, France; North Atlantic Treaty Organization (NATO), Centre for Maritime Research and Experimentation (CMRE); IRIT, University of Toulouse; University of Tokyo","IEEE Geoscience and Remote Sensing Magazine","30 Sep 2015","2015","3","3","27","46","Pansharpening aims at fusing a panchromatic image with a multispectral one, to generate an image with the high spatial resolution of the former and the high spectral resolution of the latter. In the last decade, many algorithms have been presented in the literatures for pansharpening using multispectral data. With the increasing availability of hyperspectral systems, these methods are now being adapted to hyperspectral images. In this work, we compare new pansharpening techniques designed for hyperspectral data with some of the state-of-the-art methods for multispectral pansharpening, which have been adapted for hyperspectral data. Eleven methods from different classes (component substitution, multiresolution analysis, hybrid, Bayesian and matrix factorization) are analyzed. These methods are applied to three datasets and their effectiveness and robustness are evaluated with widely used performance indicators. In addition, all the pansharpening techniques considered in this paper have been implemented in a MATLAB toolbox that is made available to the community.","2168-6831","","10.1109/MGRS.2015.2440094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284770","","Data integration;Remote sensing;Hyperspectral imaging;Algorithm design and analysis;Multiresolution analysis;Bayes methods","Bayes methods;geophysical image processing;hyperspectral imaging;image fusion;image resolution;mathematics computing;matrix decomposition","hyperspectral pansharpening system;multispectral panchromatic image fusion;spatial image resolution;component substitution;multiresolution analysis;Bayesian method;matrix factorization;MATLAB toolbox","","490","","87","IEEE","30 Sep 2015","","","IEEE","IEEE Magazines"
"Analysis of the Effects of Wavelength Band Selection and Data Fusion Techniques on Multiple-Modality Homeland Security Airborne Scenes via Deep Learning Models","C. D. Good; D. B. Megherbi","Department of Electrical and Computer Engineering, CMINDS Research Center, University of Massachusetts, Lowell, USA; Department of Electrical and Computer Engineering, CMINDS Research Center, University of Massachusetts, Lowell, USA","2022 IEEE International Symposium on Technologies for Homeland Security (HST)","30 Jan 2023","2022","","","1","7","In this work, we study the problem of band selection in multimodal remote sensing scenes. We present a deep learning system based on a three-dimensional variation of the DenseNet model architecture that we further modify to incorporate early and late feature fusion for multimodal learning of land cover classification. Band selection is applied during data preprocessing in order to counteract the Hughes' phenomenon (also known as the “Curse of Dimensionality”), with the intent of improving classification performance. We evaluate this deep learning data fusion system with the IEEE Geoscience and Remote Sensing Society (GRSS) data fusion contest (DFC) 2018 University of Houston dataset, a multimodal urban land usage and land cover (LULC) dataset. The experimental test harness for this work uses the TensorFlow and Keras deep learning frameworks to implement the proposed system, and our models are trained in the cloud via Google Colab notebooks. Our findings show that intelligent selection of hyperspectral bands and careful arrangement of feature fusion can result in an 8%-15% improvement in classification accuracy from the GRSS DFC 2018 contest winners when ignoring ad-hoc postprocessing. Finally, we present tables and plots comparing the efficacy of various modality fusion combinations and band selection methods to provide an in-depth analysis of how different bands and sensor modalities affect classification.","","978-1-6654-9404-5","10.1109/HST56032.2022.10025446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10025446","Deep Learning;CNN;DenseNet;Band Selection;Data Fusion;Hyperspectral;LiDAR;Remote Sensing","Deep learning;Training;Analytical models;Solid modeling;Atmospheric modeling;Data integration;US Department of Homeland Security","geophysical image processing;geophysical signal processing;image classification;image fusion;land cover;learning (artificial intelligence);neural nets;pattern classification;remote sensing;sensor fusion;terrain mapping","band selection methods;classification accuracy;classification performance;data fusion techniques;deep learning data fusion system;deep learning models;deep learning system;DenseNet model architecture;early feature fusion;GRSS DFC 2018 contest winners;Houston dataset;hyperspectral bands;intelligent selection;land cover classification;late feature fusion;modality fusion combinations;multimodal learning;multimodal remote sensing scenes;multimodal urban land usage;multiple-modality homeland security airborne scenes;three-dimensional variation;wavelength band selection","","","","21","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Soil organic matter estimation by using Landsat-8 pansharpened image and machine learning","A. Bouasria; K. I. Namr; A. Rahimi; E. M. Ettachfini","Department of Geology, Faculty of Sciences, Chouaib Doukkali University, El Jadida, Morocco; Department of Geology, Faculty of Sciences, Chouaib Doukkali University, El Jadida, Morocco; Department of Geology, Faculty of Sciences, Chouaib Doukkali University, El Jadida, Morocco; Department of Geology, Faculty of Sciences, Chouaib Doukkali University, El Jadida, Morocco","2020 Fourth International Conference On Intelligent Computing in Data Sciences (ICDS)","30 Nov 2020","2020","","","1","8","Considering the significant position of soil organic matter (SOM) in soil quality and maintenance, and its role in the functioning of soil physicochemical and biological processes, it is essential to monitor frequently the SOM status and its dynamics. It is a time-consuming and expensive task if we depend exclusively on chemical analysis, particularly in a semi-arid irrigated zone and with intensive agricultural activities and a very fragmented landscape. It is the Sidi Bennour region, which is situated in Doukkala Irrigated Perimeter in Morocco. Data from satellites could be a good alternative to conventional methods and fill this void with low costs. There has been a great deal of interest in satellite image prediction models, especially with free and abundant availability of satellite data. This work intends to predict SOM using Decision Trees (DT), k-Nearest Neighbors (k-NN), and Artificial Neural Networks (ANN). The soil samples (369 points) were collected at 0-30 cm of depth and the laboratory analysis was carried out. A multispectral Landsat-8 image was acquired and then calibrated. An image pansharpening processing was applied to produce a PAN image with 15m of resolution from 30m image resolution (MS). The obtained results indicate that the ANN model outperformed the other predictive models for both images (MS and PAN) with R2= 0.6553 and R2=0.6985, respectively. The statistical RMSE of predictive models was 0.2153 and 0.2014, and MAE was 0.1682 and 0.1573 for both images, MS and PAN respectively. For this predictive model, the image pansharpening could increase the prediction accuracy of R2 by 4.32%and reduce the RMSE by 1.39%.","","978-1-7281-8084-7","10.1109/ICDS50568.2020.9268725","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268725","Soil organic matter;digital soil mapping;remote sensing;machine learning;neural networks","Soil;Predictive models;Artificial neural networks;Remote sensing;Neurons;Data models;Earth","agriculture;decision trees;geophysical image processing;geophysical techniques;image fusion;image resolution;irrigation;learning (artificial intelligence);neural nets;remote sensing;soil","Artificial Neural Networks;soil samples;laboratory analysis;multispectral Landsat-8 image;image pansharpening processing;PAN image;image resolution;ANN model;predictive model;prediction accuracy;soil organic matter estimation;soil quality;biological processes;SOM status;expensive task;chemical analysis;semiarid irrigated zone;intensive agricultural activities;fragmented landscape;Sidi Bennour region;Doukkala Irrigated Perimeter;satellite image prediction models;free availability;abundant availability;satellite data;Landsat-8 pansharpened image;Decision Trees;k-Nearest Neighbors;statistical RMSE","","10","","58","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Deep Belief Networks for Feature Fusion in Hyperspectral Image Classification","M. Ghassemi; H. Ghassemian; M. Imani","Image Processing and Information Analysis Lab., Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Lab., Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Lab., Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2018 IEEE International Conference on Aerospace Electronics and Remote Sensing Technology (ICARES)","3 Jan 2019","2018","","","1","6","Hyperspectral data classification is a great challenging method for remote sensing. In recent years, the researchers have had a great attention to the feature fusion of hyperspectral data. In this paper, based on distinctive advantage over machine learning, we suggest a novel technique to classification of hyperspectral images, which employs deep belief networks (DBNs) to fuse spectral and spatial features together. In the light of the above-mentioned descriptions, DBN be able to extract the hierarchical features from raw data, which are cost-effective for classification based on support vector machine (SVM). First, we verify the eligibility of DBN, and SVM-based classification and then, suggest a new framework, stacking the spectral and spatial features, fuses features by DBN, and classifies them by SVM to get most accuracy. First, we extract spatial features by applying principal component analysis (PCA) and extended morphology (EMP) and append at the end of spectral features, then fuse and classify achieved features by the suggested method. The experimental test results demonstrate the suggested method yields to most accuracies results.","","978-1-5386-6032-4","10.1109/ICARES.2018.8547136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8547136","deep belief networks (DBNs);extended morphology (EMP);feature fusion;hyperspectral classification","Feature extraction;Hyperspectral imaging;Support vector machines;Data mining;Training;Image classification","belief networks;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;principal component analysis;remote sensing;support vector machines","DBN;SVM-based classification;spectral features;spatial features;extended morphology;deep belief networks;feature fusion;hyperspectral image classification;hyperspectral data classification;remote sensing;support vector machine;hierarchical features extraction;principal component analysis","","4","","24","IEEE","3 Jan 2019","","","IEEE","IEEE Conferences"
"Comparison of Support Vector Machine, Artificial Neural Networks and Spectral Angle Mapper Classifiers on Fused Hyperspectral Data for Improved LULC Classification","V. S. Sahithi; S. Subbiah; S. Agrawal","Centre for Space Science and Technology, Education in Asia and Pacific, Dehradun, India; Electrical & Electronics Division, Bahrain Training Institute, Isa, Bahrain; Indian Institute of Remote Sensing, Dehradun, India","2019 8th International Conference on Modeling Simulation and Applied Optimization (ICMSAO)","24 Oct 2019","2019","","","1","6","The present paper focuses on analyzing the performance of three different advanced classifiers on hyperspectral data for improved land use/land cover classification. Considering the limitations of spatial and spectral resolution of high resolution multispectral and hyperspectral datasets, image fusion techniques were used for obtaining a spectrally and spatially rich space borne hyperspectral data. The Gram Schmidt sharpening, Principal Component sharpening and Color Normalized spectral sharpening were used for fusing the 30m, 242 band EO-1 Hyperion image with the 1.8m eight band World View 2 data. Correlation, SNR and entropy measures were adopted for analyzing the performance of the fused outputs. Color Normalized spectral sharpening was observed to exhibit highest correlation of 0.78 between spectra of Hyperion and fused images. Classification of the fused and unfused images was performed using three different classifiers - Spectral Angle Mapper, Artificial Neural Networks and Support Vector Machine by taking end members from the image and the ground spectra. Confusion matrix was generated using a ground truth points obtained during field visit. Support Vector Machine classifier with RBF kernel and lOO penalty value was observed to give an improved classified output than the remaining classifiers with an accuracy of 89.56% and a kappa coefficient of 0.88%.","2573-5276","978-1-5386-7684-4","10.1109/ICMSAO.2019.8880336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8880336","Hyperion;WV 2– 2;Fusion techniques;SAM;ANN and SVM;Confusion matrix","Spatial resolution;Sensors;Hyperspectral imaging;Atmospheric modeling;Transforms","geophysical image processing;geophysical techniques;image classification;image fusion;land cover;neural nets;principal component analysis;remote sensing;support vector machines;terrain mapping","Spectral Angle Mapper classifiers;fused hyperspectral data;improved LULC classification;spatial resolution;spectral resolution;hyperspectral datasets;image fusion techniques;spectrally space;spatially rich space;Gram Schmidt sharpening;Principal Component sharpening;Color Normalized spectral sharpening;242 band EO-1 Hyperion image;fused outputs;unfused images;Artificial Neural Networks;Support Vector Machine;Vector Machine classifier;improved classified output;band World View 2 data;lOO penalty value;kappa coefficient","","","","25","IEEE","24 Oct 2019","","","IEEE","IEEE Conferences"
"Classification of Land Cover Usage from Satellite Images using Deep Learning Algorithms","D. R. Rao; S. Noorjahan; S. A. Fathima","Dept. of CSE, VR Siddhartha Engineering College, Vijayawada, India; Dept. of CSE, VR Siddhartha Engineering College, Vijayawada, India; Dept. of CSE, VR Siddhartha Engineering College, Vijayawada, India","2022 International Conference on Electronics and Renewable Systems (ICEARS)","13 Apr 2022","2022","","","1302","1308","Earth's environment and its evolution can be seen through satellite images in near real time. Through satellite imagery, remote sensing data provides crucial information that can be used for a variety of applications, including image fusion, change detection, land cover classification, agriculture, mining, disaster mitigation, and monitoring climate change. The objective of this project is to propose a method for classifying satellite images according to multiple predefined land cover classes. The proposed approach involves collecting data in image format. The data is then preprocessed using data preprocessing techniques. The processed data is fed into the proposed algorithm and the obtained result is analyzed. Some of the algorithms used in satellite imagery classification are U-Net, Random Forest, Deep Labv3, CNN (Convolutional Neural Network), ANN(Artificial neural network), Resnet etc. In this project, DeepLabv3 (Atrous convolution) algorithm is used for land cover classification. The dataset used is the deep globe land cover classification dataset. DeepLabv3 is a semantic segmentation system that uses atrous convolution to capture multi-scale context by adopting multiple atrous rates in cascade or in parallel to determine the scale of segments.","","978-1-6654-8425-1","10.1109/ICEARS53579.2022.9752282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9752282","Area Calculation;Atrous convolution;Deep globe land cover classification;DeepLabv3;Land cover classification;Resnet 50.","Image segmentation;Renewable energy sources;Satellites;Convolution;Semantics;Real-time systems;Classification algorithms;Climate change","geophysical image processing;geophysical techniques;image classification;image fusion;image segmentation;land cover;remote sensing;terrain mapping","satellite imagery classification;DeepLabv3 algorithm;deep globe land cover classification dataset;land cover usage;satellite images;deep learning algorithms;remote sensing data;image fusion;climate change monitoring;multiple predefined land cover classes;image format;convolutional neural network;semantic segmentation system","","","","20","IEEE","13 Apr 2022","","","IEEE","IEEE Conferences"
"Variation-Net: Interpretable Variation-Inspired Deep Network for Pansharpening","K. Li; W. Zhang; X. Tian; J. Ma; H. Zhou; Z. Wang","Electronic Information School, Wuhan University, China; Electronic Information School, Wuhan University, China; Electronic Information School, Wuhan University, China; Electronic Information School, Wuhan University, China; Hubei Key Laboratory of Intelligent Robot, Wuhan Institute of Technology, China; School of Computer Science, Wuhan University, China","2021 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2021","2021","","","1","6","In this study, we propose Variation-net, an interpretable variation-inspired deep network for pansharpening, which aims to fuse panchromatic (PAN) and multispectral (MS) images for a high-resolution MS image. We first construct a novel variational pan-sharpening model with clear physical meanings. As the relationship between the PAN and MS images in the real situation is complex and nonlinear, we explore the similarity between PAN and MS images from the sparsity of nonlinear transforms in this variational pansharpening model. As a result, spatial details can be accurately transferred from PAN image to MS image. Furthermore, we build the Variation-net by unrolling the iterative shrinkage-thresholding algorithm to solve the proposed variational pansharpening model. Therefore, all modules in Variation-net have clear physical meanings and are easily observed, leading to good generalization capability. Meanwhile, nonlinear transforms and other parameters in the variational pansharpening model are learned end–to–end. The experiments demonstrate that Variation-net outperforms the state-of-the-art methods from the aspects of visual effect and objective quality analysis.","1945-788X","978-1-6654-3864-3","10.1109/ICME51207.2021.9428314","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428314","pansharpening;interpretable deep network;variational model;image fusion","Deep learning;Fuses;Conferences;Pansharpening;Transforms;Visual effects;Iterative algorithms","geophysical image processing;geophysical techniques;image fusion;image resolution;iterative methods;learning (artificial intelligence);remote sensing;variational techniques","Variation-net;interpretable variation-inspired deep network;high-resolution MS image;variational pansharpening model;PAN image;iterative shrinkage-thresholding algorithm;visual effect;objective quality analysis;multispectral images;image fusion;panchromatic images","","2","","27","IEEE","9 Jun 2021","","","IEEE","IEEE Conferences"
"A new method to improve classification accuracy of fused RADAR and optical data","D. Karimi; K. Rangzan; G. Akbarizadeh; M. Kabolizadeh","Department of Remote sensing and GIS, Shahid Chamran University of Ahvaz, Ahvaz, Iran; Department of Remote sensing and GIS, Shahid Chamran University of Ahvaz, Ahvaz, Iran; Department of Electrical Engineering, Shahid Chamran University of Ahvaz, Ahvaz, Iran; Department of Remote sensing and GIS, Shahid Chamran University of Ahvaz, Ahvaz, Iran","2016 6th International Conference on Computer and Knowledge Engineering (ICCKE)","2 Jan 2017","2016","","","337","341","In past few decades, feature selection and learning have been considered by many researchers in terms of reducing the dimensionality of feature space and optimal feature selection. In traditional methods, feature selection and learning, are separately done. In this paper, a new method of supervised feature selection and learning, based on sparse regularization, was used to improve the classification accuracy of two pairs of fused radar and optical data for the first time. NMF features extracted from the images and the extracted features were used in two learned and unlearned forms as input to the SVM classifier, which choose as a base classifier. The results showed significant improvement in classification accuracy, resulting from the implementation of the sparse regularization algorithm based on L2, p norm.","","978-1-5090-3586-1","10.1109/ICCKE.2016.7802163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7802163","Sparse Regularization;RADAR and Optical data fusion;Sentinel;SVM","Agriculture;Support vector machines;Rivers;Image coding;Optical imaging;Optical sensors;Neural networks","feature extraction;feature selection;image classification;image fusion;optical information processing;radar imaging;support vector machines","fused radar data;optical data;feature space;optimal feature selection;supervised feature selection;NMF features;support vector machines;SVM classifier;base classifier;sparse regularization algorithm","","1","","21","IEEE","2 Jan 2017","","","IEEE","IEEE Conferences"
"A Feasibility Study for Health and Life-Threatening Conditions Recognition via Sensor Fusion Approach","L. Anishchenko; V. Lobanova; V. Slizov","Remote Sensing Laboratory, BMSTU, Moscow, Russia; Remote Sensing Laboratory, BMSTU, Moscow, Russia; Remote Sensing Laboratory, BMSTU, Moscow, Russia","2022 Ural-Siberian Conference on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT)","25 Oct 2022","2022","","","036","039","The paper presents a feasibility study for health and life-threatening conditions such as falls or postural balance abnormalities recognition via sensor fusion approach. As a sensor fusion system we used the Intel RealSense Depth Camera D435 that combines standard video and depth cameras. However, the possibility of recognition of different types of movement patterns especially falls with RGB-D cameras has been studied for more than a decade now, the usage of such methods to detect life and health-threatening conditions which may be manifested by impaired postural balance has not been paid enough attention yet. The proposed approach was validated on an experimental dataset collected for two experimental surroundings with participation of 13 volunteers (9 females and 4 males, age: 19–37 years). Each subject performed 11 different types of motion patterns including 2 patterns specific for life and health threatening situations. We used the Caffe neural network to find key point coordinated and form the feature vector than was used for training models. The proposed classifier for video sensor data classification showed 81 % accuracy and Cohen’s kappa of 78 %, while the usage of sensor fusion approach reduces the number of errors, and increase the classification accuracy and Cohen’s kappa by 7 and 8 %, respectively. Nevertheless, the economic feasibility of such a solution requires an additional assessment.","","978-1-6654-6092-7","10.1109/USBEREIT56278.2022.9923326","Russian Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9923326","activity monitoring;depth-sensor;deep learning;falls prevention;personalized medicine;sensor fusion","Economics;Training;Neural networks;Lighting;Sensor fusion;Cameras;Pattern recognition","biomechanics;cameras;feature extraction;geriatrics;image classification;image fusion;medical computing;neural nets;patient monitoring;pattern classification;video signal processing","Intel RealSense depth camera D435;video sensor data classification;health threatening situations;impaired postural balance;health-threatening conditions;RGB-D cameras;depth cameras;standard video;sensor fusion system;postural balance abnormalities recognition;sensor fusion approach;life-threatening conditions recognition","","","","15","IEEE","25 Oct 2022","","","IEEE","IEEE Conferences"
"Design and implementation of multi-feature fusion kernel correlation filtering algorithm based on HLS","P. Cong; M. Xie; K. Yang; X. Zhang; H. Su; X. Fu","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Science and Technology on Millimeter-wave Laboratory, Beijing Institute of Remote-sensing Equipment, Beijing, China; Science and Technology on Millimeter-wave Laboratory, Beijing Institute of Remote-sensing Equipment, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China","IET International Radar Conference (IET IRC 2020)","22 Sep 2021","2020","2020","","645","649","In order to balance the accuracy and real-time performance of the moving target tracking system, an optimized design and implementation method based on high-level synthesis (HLS) of multi-feature fusion with kernel correlation filtering algorithms on FPGA is designed. This design improves the KCF algorithm with LBP and HOG features, and proposes a new dimensionality reduction method for LBP, which enhances the real-time performance while maintaining effective extraction of target features. The algorithm is implemented with FPGA, and a well acceleration effect is obtained on the basis of high precision. In test, the frame rate reaches 35 frames per second. Finally, it is verified through simulation that this feature extraction method can be used to process various image data such as infrared detection and SAR radar imaging, and has a wide range of applications.","","","10.1049/icp.2021.0761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9545666","","","feature extraction;field programmable gate arrays;filtering theory;image fusion;object detection;radar imaging;synthetic aperture radar;target tracking","multifeature fusion kernel correlation;HLS;real-time performance;moving target tracking system;optimized design;high-level synthesis;FPGA;KCF algorithm;LBP;HOG features;dimensionality reduction method;effective extraction;target features;acceleration effect;feature extraction method","","","","","","22 Sep 2021","","","IET","IET Conferences"
"Sensors for Automotive Remote Road Surface Classification","A. Bystrov; E. Hoare; T. -Y. Tran; N. Clarke; M. Gashinova; M. Cherniakov","University of Birmingham, Birmingham, UK; University of Birmingham, Birmingham, UK; Research Department, Jaguar Land Rover, Coventry, UK; Research Department, Jaguar Land Rover, Coventry, UK; University of Birmingham, Birmingham, UK; University of Birmingham, Birmingham, UK","2018 IEEE International Conference on Vehicular Electronics and Safety (ICVES)","4 Nov 2018","2018","","","1","6","In this paper, we compare the common remote sensing technologies in terms of their application for road surface classification. The presence of surface classification system in a vehicle will increase the safety of driving, especially in adverse weather conditions, as well as when driving off-road. The paper presents an overview of the application of optical, laser, ultrasonic, and microwave sensors for surface classification. From the analysis it follows that sensor data fusion allows obtaining more accurate and reliable results.","","978-1-5386-3543-8","10.1109/ICVES.2018.8519499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519499","advanced driver assistance systems;optical imaging;radar remote sensing;sonar applications;sensor fusion","Roads;Optical surface waves;Snow;Laser radar;Rough surfaces;Surface roughness;Ice","image classification;image fusion;remote sensing","sensor data fusion;adverse weather conditions;surface classification system;common remote sensing technologies;automotive remote road surface classification","","8","","35","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Memory-augmented Deep Conditional Unfolding Network for Pansharpening","G. Yang; M. Zhou; K. Yan; A. Liu; X. Fu; F. Wang","University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China; Chinese Academy of Sciences, Hefei Institute of Physical Science, China","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","1778","1787","Pansharpening aims to obtain high-resolution multispectral (MS) images for remote sensing systems and deep learning-based methods have achieved remarkable success. However, most existing methods are designed in a black-box principle, lacking sufficient interpretability. Additionally, they ignore the different characteristics of each band of MS images and directly concatenate them with panchromatic (PAN) images, leading to severe copy artifacts [9]. To address the above issues, we propose an interpretable deep neural network, namely Memory-augmented Deep Conditional Unfolding Network with two specified core designs. Firstly, considering the degradation process, it formulates the Pansharpening problem as the minimization of a variational model with denoising-based prior and non-local auto-regression prior which is capable of searching the similarities between long-range patches, benefiting the texture enhancement. A novel iteration algorithm with built-in CNNs is exploited for transparent model design. Secondly, to fully explore the potentials of different bands of MS images, the PAN image is combined with each band of MS images, selectively providing the high-frequency details and alleviating the copy artifacts. Extensive experimental results validate the superiority of the proposed algorithm against other state-of-the-art methods.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880283","Photogrammetry and remote sensing","Learning systems;Degradation;Deep learning;Computer vision;Neural networks;Pansharpening;Search problems","geophysical image processing;image denoising;image fusion;image resolution;image texture;iterative methods;learning (artificial intelligence);neural nets;remote sensing","specified core designs;Pansharpening problem;transparent model design;MS images;PAN image;Memory-augmented Deep Conditional Unfolding Network;high-resolution multispectral images;remote sensing systems;deep learning-based methods;black-box principle;panchromatic images;interpretable deep neural network","","3","","61","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"A Feature Level Multi-Sensor Information Fusion Strategy for Land Region Classification","J. Schierl; V. Asari; N. Singer; T. Aspiras; A. Stokes; B. Keaffaber; A. Van Rynbach; K. Decker; D. Rabb","Department of Electrical and Computer Engineering, Vision Lab, University of Dayton, Dayton, OH; Department of Electrical and Computer Engineering, Vision Lab, University of Dayton, Dayton, OH; Department of Electrical and Computer Engineering, Vision Lab, University of Dayton, Dayton, OH; Department of Electrical and Computer Engineering, Vision Lab, University of Dayton, Dayton, OH; Air Force Research Laboratory, Dayton, OH; Air Force Research Laboratory, Dayton, OH; Air Force Research Laboratory, Dayton, OH; Defense Engineering Corporation, Dayton, OH; Air Force Research Laboratory, Dayton, OH","2021 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)","26 Apr 2022","2021","","","1","7","Deep learning has seen success in image recognition and classification in remote sensing research areas. Specifically, it is capable of classifying land coverage by its usage, such as determining if a plot of land is a residential area, industrial area, forest, etc. This is useful for quickly parsing datasets for surveillance applications and also for autonomously categorizing municipal land for urban planning. However, current methods learn and classify based on a single data stream (EO) and consequentially a single set of features. Our approach for land classification, LandNet, uses a feature-level fusion approach of both EO and LiDAR data in a deep learning classification system. In our approach, we use an aerial dataset of geo-registered scenes captured by both EO and LiDAR sensors. We propose a deep-learning based feature-level fusion framework for integrating information extracted from 2D and 3D data. In our preliminary testing, we observed that this proposed network is a more effective approach as it employs feature sets of multiple sensors, and it outperforms each of the individual modalities.","2332-5615","978-1-6654-2471-4","10.1109/AIPR52630.2021.9762086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762086","lidar;aerial imagery;fusion;point clouds;3D processing;deep learning;remote sensing;GIS;mapping;geospa-tial intelligence;computer vision;machine learning;wide area surveillance;automatic target recognition;scene understanding;classification;transfer learning","Deep learning;Three-dimensional displays;Laser radar;Image recognition;Target recognition;Surveillance;Urban planning","feature extraction;geographic information systems;image classification;image fusion;image recognition;learning (artificial intelligence);optical radar;pattern classification;remote sensing;sensor fusion","municipal land;urban planning;current methods learn;single data stream;EO;land classification;feature-level fusion approach;LiDAR data;deep learning classification system;aerial dataset;LiDAR sensors;deep-learning based feature-level fusion framework;feature sets;multiple sensors;feature level multisensor information fusion strategy;land region classification;image recognition;remote sensing research areas;land coverage;residential area;industrial area;surveillance applications","","","","17","IEEE","26 Apr 2022","","","IEEE","IEEE Conferences"
"Hyperspectral Data to Relative Lidar Depth: An Inverse Problem for Remote Sensing","S. Ozkan; G. B. Akar","Department of Electrical and Electronics Engineering, Middle East Technical University, Ankara, Turkey; Department of Electrical and Electronics Engineering, Middle East Technical University, Ankara, Turkey","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","9 Apr 2020","2019","","","956","963","Hyperspectral data provides rich information about a scene in terms of spectral details since it encapsulates measurements/observations from a wide large range of spectrum. To this end, it has been used in different problems mostly related to identification and detection processes. However, the main limitation arises for the accessibility of data. More precisely, there is no sufficient amount of hyperspectral data available compared to visible range data for trainable models. In this paper, we tackle an inverse problem to estimate the relative lidar depth from hyperspectral data. To solve its limitation, we integrate semantic information existed in data with supervised labels to decrease the possibility of parameter overfitting. Moreover, details of the output responses are enhanced with Laplacian pyramids and attention layers in which the model makes predictions from each subsequent scale instead of a single shot prediction from the top of the model. In our experiments, we use the 2018 IEEE GRSS Data Fusion Challenge dataset. From the experimental results, we prove that use of hyperspectral data instead of visible range data improves the performance. Moreover, we show that results are significantly improved if a sparse set of depth measurements is used along with hyperspectral data. Lastly, the integration of semantic information to the solution yields more stable and better results compared to the baselines.","2160-7516","978-1-7281-2506-0","10.1109/CVPRW.2019.00126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025362","","Hyperspectral imaging;Laser radar;Inverse problems;Semantics;Manifolds;Data models","geophysical image processing;geophysical signal processing;image classification;image fusion;optical radar;remote sensing;sensor fusion;terrain mapping","2018 IEEE GRSS Data Fusion Challenge dataset;visible range data;inverse problem;relative lidar depth;hyperspectral data","","1","","32","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Improving 3D face geometry by adapting reconstruction from stereo image pair to generic Morphable Model","H. Jain; O. Hellwich; R. Anand","Berlin & Electrical Engineering Department IIT, Computer Vision and Remote Sensing Technical University, Roorkee, India; Computer Vision and Remote Sensing Technical University, Berlin; Electrical Engineering Department IIT, India","2016 19th International Conference on Information Fusion (FUSION)","4 Aug 2016","2016","","","1720","1727","Stereo reconstruction from image pairs is a standard method for 3D acquisition of human faces. Depending on available imagery and accuracy requirements the resulting 3D reconstructions may have deficits. In this work we remedy such deficits combining the 3D stereo reconstruction with a generic Morphable Model. Prior shape information can be obtained by already developed methods, which uses landmarks to fit a morphable model to a single image resulting in a second 3D reconstruction. This alternative to the stereo reconstruction is combined with it, allowing to prefer information from the single image reconstruction whenever the stereo reconstruction shows untypical deviations from the expected 3D features of a human face. The fusion is conducted in a global and a local deformation step. A comparison of the output with high quality scan is presented at the end.","","978-0-9964-5274-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7528091","","Face;Three-dimensional displays;Shape;Solid modeling;Image reconstruction;Computational modeling;Adaptation models","face recognition;geometry;image fusion;image reconstruction;shape recognition;stereo image processing","3D face geometry;stereo reconstruction;stereo image pair;generic morphable model;shape information;image fusion","","","","19","","4 Aug 2016","","","IEEE","IEEE Conferences"
"Dark Image Enhancement Based on Virtual Exposure Fusion","L. Rong; S. Yong; Z. Peng; Y. Yufan; L. Huafeng","Shanghai Key Laboratory of Power Station Automation Technology, Shanghai; Shanghai Key Laboratory of Power Station Automation Technology, Shanghai; Shanghai Key Laboratory of Power Station Automation Technology, Shanghai; Shanghai Key Laboratory of Power Station Automation Technology, Shanghai; Shanghai Key Laboratory of Power Station Automation Technology, Shanghai","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","3380","3384","Image processing technology plays an important role in the field of monitoring, medical treatment and remote sensing. However, in some low light environment, the image captured by hardware will have the characteristics of loss of detail information and low contrast, which limits the practical application. In order to improve the brightness of low illumination image, alleviate the image contour distortion and restore more details of the image, a low light image enhancement algorithm based on virtual exposure fusion is proposed in this paper. In this method, the dark image is processed by histogram equalization, gamma correction and virtual exposure to generate three images which details and brightness are enhanced. The three processed images and the original image are weighted and fused by weight matrix afterwards to get the final enhanced image.Through the comparison of objective experiments, the algorithm has a certain improvement effect on image enhancement and has a better effect on dark image enhancement.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9549914","National Defense Basic Scientific Research Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9549914","Image enhancement;Image fusion;Virtual exposure","Histograms;Face recognition;Brightness;Tools;Distortion;Cameras;Real-time systems","distortion;image capture;image colour analysis;image enhancement;image fusion;image restoration;lighting","image contour distortion;low light image enhancement algorithm;virtual exposure fusion;dark image enhancement;image processing technology;low illumination image;image capture;image restoration;histogram equalization;gamma correction;weight matrix","","","","18","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Towards a better hybrid pansharpening algorithm for high resolution satellite imagery","P. Rekha; M. Y. J. Shirur","Dept. of electronics and Communication, VTU, Bangalore; Dept. of electronics and Communication, B.N.M. College of engineering, under VTU, Bangalore, India","2015 International Conference on Communications and Signal Processing (ICCSP)","12 Nov 2015","2015","","","1252","1256","Most pansharpened images from existing algorithms are apt to present a tradeoff relationship between the spectral preservation and the spatial enhancement. In this paper, a hybrid pansharpening algorithm using neural networks based on primary and secondary high-frequency information injection method is used to efficiently improve the spatial quality of the pansharpened image is being developed. The injected high-frequency information in the proposed algorithm is the differential data of panchromatic and intensity images and the Laplacian filtered image of high frequency information are obtained with the help of regression method. The extracted high frequencies are injected by the multispectral image using the local adaptive fusion parameter and post processing of the fusion parameter. The proposed algorithm gives better spatial quality when compared to available fusion algorithms with high spectral information. MATLAB 13 [b] version is used to build a GUI to apply and to present the results of the image fusion algorithms. Subjective (visual) and objective evaluation of the fused images have been performed to evaluate the success of the approaches. The objective evaluation methods include Correlation Coefficient (CC), Root Mean Squared Error (RMSE), Relative Global Dimensional Synthesis Error (ERGAS), Relative Average Spectral Error (RASE), Degree Of Distortion (DD) and Average Gradient (AG).","","978-1-4799-8081-9","10.1109/ICCSP.2015.7322708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7322708","Hybrid pansharpening;Primary and Secondary high-frequency information;Spatial quality","Spatial resolution;MATLAB;Remote sensing;Artificial neural networks","graphical user interfaces;image filtering;image fusion;mean square error methods;neural nets;regression analysis","hybrid pansharpening algorithm;high resolution satellite imagery;spectral preservation;spatial enhancement;neural networks;secondary high-frequency information injection method;primary high-frequency information injection method;pansharpened image spatial quality;injected high-frequency information;differential data;panchromatic image;intensity image;Laplacian filtered image;regression method;extracted high frequencies;multispectral image;local adaptive fusion parameter;postprocessing;high spectral information;MATLAB 13;GUI;objective evaluation method;correlation coefficient method;root mean squared error method;relative global dimensional synthesis error method;relative average spectral error method;degree of distortion method;average gradient method","","2","","15","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Research on Automatic Extraction of Tobacco Plants Based on U2-Net of Convolutional Block Attention Module and UAV Visible Images","C. Feng; H. Deng; W. Zhang; Z. Ye; H. Zhang; W. Zhuo","College of Earth Science, Chengdu University of Technology, Chengdu, China; College of Earth Science, Chengdu University of Technology, Chengdu, China; College of Earth Science, Chengdu University of Technology, Chengdu, China; Forestry College, Fujian Agriculture and Forestry University, Fuzhou, China; Forestry College, Fujian Agriculture and Forestry University, Fuzhou, China; College of Earth Science, Chengdu University of Technology, Chengdu, China","2022 5th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)","4 Oct 2022","2022","","","606","614","Acquiring the growth information of tobacco plants quickly and accurately is an important prerequisite for their daily management. In view of the problem that the convolutional neural network is easily affected by background information, this study introduces the Convolutional Block Attention Module into the U2-Net. This method is mainly composed of an encoder and a decoder. Before the feature map is decoded, it will go through the Convolutional Block Attention Module to filter more important features and inhibit the invalid features. In the decoder, in order to fuse the features of objects in different scales, the feature maps in the decoder are up-sampled and then concatenated. This information aggregation solves the problem of difficult object detection at different scales to some extent. The results of this method are compared with four current mainstream deep learning models(U2-Net, DeeplabV3+, PSP-Net, U-Net). The result shows: 1) The U2-Net network based on the Convolutional Block Attention Module has the highest extraction accuracy for the UAV visible light remote senseing of tobacco(accuracy was 98.53%, recall was 96.00%,F1-Score was 96.38%, kappa coefficient was 0.95, IOU was 93.02%); 2)In the extraction of the three test areas, the extraction value of this method is highly consistent with the label area value(the area ratios of a total of 13 plants in the three regions were kept between 90% and 100%),the effect is much higher than that of DeeplabV3+ and PSP-Net, and slightly higher than the extraction effect of U2-Net and U-Net. 3)In the segmentation of three typical canopy closure regions of A, B, and C, most of the accuracy factors in different regions can reach 94% and above. It can be seen that the method in this study has certain advantages in feature extraction and has certain value in practical application.","","978-1-6654-9916-3","10.1109/PRAI55851.2022.9904111","Science and Technology Department of Tibet; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9904111","UAV;remote sensing;tobacco;deep learning;convolutional block attention module","Deep learning;Fuses;Object detection;Feature extraction;Information filters;Decoding;Sensors","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image fusion;image segmentation;object detection;remote sensing","PSP-Net;feature extraction;convolutional block attention module;convolutional neural network;decoder;feature map;Net network;tobacco plant automatic extraction;U2-net;encoder;feature fusion;information aggregation;object detection;mainstream deep learning model;UAV visible light remote sensing;typical canopy closure region segmentation","","","","26","IEEE","4 Oct 2022","","","IEEE","IEEE Conferences"
"A new pansharpening method based on cartoon+texture decomposition","M. Lotfi; H. Ghassemian","Department of Electrical Engineering, Tarbiat Modares University, Tehran, Iran; Department of Electrical Engineering, Tarbiat Modares University, Tehran, Iran","2016 8th International Symposium on Telecommunications (IST)","20 Mar 2017","2016","","","468","471","Pansharpening is a fusion of a high resolution panchromatic image and a low resolution multispectral (MS) image to obtain a high resolution MS image. A robust pansharpening method should increase the spatial resolution while preserving the nature of spectral information. Spectral distortion caused by fusion, could be due to the details injection in unnecessary pixels and injecting too much details. In this paper, a cartoon+texture decomposition is used to discriminate between necessary and unnecessary pixels to inject details. The piecewise-smooth areas and sharp edges of the cartoon component need less or no details injection. As a result, the fusion only be done on the texture components. The proposed method is compared with the IAIHS and SFIM methods. Results show it preserves more spectral features with less spatial distortion.","","978-1-5090-3435-2","10.1109/ISTEL.2016.7881865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881865","Pansharpening;multispectral;spectral distortion;cartoon;texture","Remote sensing;Spatial resolution;Distortion;Image fusion;Principal component analysis;Image edge detection","compressed sensing;image resolution;image texture","pansharpening method;cartoon+texture decomposition;high resolution panchromatic image;low resolution multispectral image;spatial resolution","","4","","24","IEEE","20 Mar 2017","","","IEEE","IEEE Conferences"
"Improvement of spectral and spatial information using WAMM and bi-cubic interpolation method in Non-subsampled contourlet transform domain","R. James; M. Vadivel","ETCE Department, Sathyabama University, Chennai, Tamil Nadu, India; ETCE Department, Sathyabama University, Chennai, Tamil Nadu, India","2015 International Conference on Robotics, Automation, Control and Embedded Systems (RACE)","30 Apr 2015","2015","","","1","5","Pan-sharpening is widely used as a tool toconstruct remotely sensed image, which is rich in both spectral and spatial contents. Panchromatic sharpening is used to improve the spatial resolution and provide an enhanced visualization of a multiband image using the high-resolutions, single-band image. The main scope of pan-sharpening is to maximize the spatial content simultaneously and minimize the spectral distortions which occur during processing. There are many approaches to implement pan-sharpening like principal component analysis (PCA), Intensity, hue and saturation (HIS), and WAVELET. The main drawback of these approaches is the distortion of spectral resolutions. However, Non-subsampled contourlet-transform (NSCT) approach is proved to be efficient. It is alsovery efficient in representing directional information and to capture intrinsic geometrical structure of the object. It combines features like high resolutions, shift-invariance and high-directionality. In this paper modified bi-cubic interpolation method and weighted average merging method (WAMM) are used. By using these methods the image issharper and there is no softening of details in the image, thus improving the quality. In the proposed paper it is possible to improve the efficiency of an image both spectrally and spatially with better efficiency compared to existing methods. The evaluation of efficiency of these methods is done by simulation, using performance metrics like RGE and Q4.","","978-8-1925-9743-0","10.1109/RACE.2015.7097248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7097248","WAMM;NSCT;CT;HIS;GIS","Interpolation;Spatial resolution;Transforms;Remote sensing;Merging;Image fusion","image colour analysis;image enhancement;image resolution;interpolation;principal component analysis;wavelet transforms","Q4;RGE;weighted average merging method;shift-invariance;intrinsic geometrical structure;wavelet transform;HIS;intensity-hue-and-saturation;PCA;principal component analysis;spectral distortion;single-band image;high-resolution image;multiband image;panchromatic sharpening;nonsubsampled contourlet transform;bicubic interpolation method;WAMM;spatial resolution;spectral resolution","","","","12","","30 Apr 2015","","","IEEE","IEEE Conferences"
"Performance analysis of pansharpening approaches on GÖKTÜRK-2 satellite images","S. Kahraman; A. Ertürk","Kocaeli Universitesi, Kocaeli, TR; Kocaeli Universitesi, Kocaeli, TR","2018 26th Signal Processing and Communications Applications Conference (SIU)","9 Jul 2018","2018","","","1","4","In this study, a detailed performance comparison of standard and recent pansharpening approaches for GÖKTÜRK-2 satellite images is presented. The Wald's Protocol was adopted in the experimental analysis, and Modulation Transfer Function (MTF) and Spectral Response Function (SRF) values of the GÖKTÜRK-2 satellite were used for the first time in the literature for this purpose. By this means, the synthetic multispectral and panchromatic images were obtained realistically from GÖKTÜRK-2 satellite images and the pansharpened results obtained with pansharpening methods were compared with the original images consistently. Quantatitave and visual results over multiple GÖKTÜRK-2 images have shown that recent multi-resolution analysis methods provide better performance than standard component substitution methods and the most recent methods which use a priori knowledge about the satellite provide best pansharpening performance as expected.","","978-1-5386-1501-0","10.1109/SIU.2018.8404810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8404810","GÖKTÜRK-2;pansharpening;performance analysis","Remote sensing;Satellites;Spatial resolution;Principal component analysis;Image fusion;Histograms","geophysical image processing;image resolution;optical transfer function","pansharpening performance;GÖKTÜRK-2 satellite images;synthetic multispectral images;panchromatic images;multiresolution analysis;Walds Protocol;Modulation Transfer Function;Spectral Response Function","","","","","IEEE","9 Jul 2018","","","IEEE","IEEE Conferences"
"Hyperspectral image super-resolution using sparse spectral unmixing and low-rank constraints","Z. Li; C. Li; C. Deng; J. Li","School of Electronic Engineering, Xidian University, Xi'an, China; School of Electronic Engineering, Xidian University, Xi'an, China; School of Electronic Engineering, Xidian University, Xi'an, China; School of Electronic Engineering, Xidian University, Xi'an, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","7224","7227","Hyperspectral images play an important role in real-world applications, such as recognition and remote sensing, etc. How to enhance the spatial resolution of hyperspectral image is still a challenging problem in this field. In this paper, we propose a novel hyperspectral image super-resolution approach by jointly incorporating the sparse, low-rank constraints and spectral mixture priori into a linear unmixing framework, which will make the unmixing framework more consistent with the real-world scenarios of the spectral mixture. Experiments on two public databases show that our proposed approach achieves much lower average reconstruction errors than other state-of-the-art methods.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730884","Hyperspectral image;super-resolution;image fusion;unmixing;joint constraints","Spatial resolution;Hyperspectral imaging;Databases;Sparse matrices;Image reconstruction","hyperspectral imaging;image resolution","sparse spectral unmixing;hyperspectral image super-resolution;low-rank constraints;spectral mixture;linear unmixing framework;reconstruction errors","","","","10","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Research on Aerial Image Stitching Technology","R. Ma; C. Zhang; Q. Guo; F. Wan","School of Aeronautics, Northwestern Polytechnical University, Xi’an, China; School of Aeronautics, Northwestern Polytechnical University, Xi’an, China; School of Aeronautics, Northwestern Polytechnical University, Xi’an, China; School of Aeronautics, Northwestern Polytechnical University, Xi’an, China","2019 International Conference on Sensing, Diagnostics, Prognostics, and Control (SDPC)","17 Aug 2020","2019","","","226","232","The rise of UAV has made UAV play a major role in various industries, especially in place of manpower to solve difficult problems such as complex terrain detection and on-site monitoring in disaster areas. The pictures taken by the UAV are spliced to realize the global display of the scene, which is convenient for analysis and research. The integrity and clarity of image stitching depends on the performance of the stitching algorithm. The key to image registration and stitching is the extraction and matching of feature points. The article compares and analyzes the traditional three feature point (key-point) extraction algorithms. The article summarizes their advantages and disadvantages and scope of application. In this paper, SIFT, SURF and ORB algorithms were used respectively to extract feature points from the two images. Then, the nearest neighbor matching method was used to select the optimal matching points to remove the pseudo-matching points and improve the matching accuracy. After image registration, the composite image is prone to splicing gaps and brightness differences due to error accumulation, color differences, and the like. Therefore, in order to make the final panoramic image better, it is more necessary to perform image fusion after image registration processing, correct the difference, and eliminate the stitching gap. In this paper, the improved ORB algorithm combined with the weighted average fusion algorithm is used to achieve smooth transition of the two images. The improved algorithm time is reduced and the efficiency is significantly improved. The experimental results also show that the weighted average algorithm has high effectiveness and practicability in image fusion.","","978-1-7281-0199-6","10.1109/SDPC.2019.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9169029","feature points;image stitching;SIFT;SURF;ORB","Feature extraction;Robustness;Image registration;Dogs;Monitoring;Real-time systems;Remote sensing","feature extraction;image fusion;image matching;image registration","aerial image stitching technology;UAV;complex terrain detection;on-site monitoring;disaster areas;global display;clarity;stitching algorithm;feature points;traditional three feature point extraction algorithms;key-point;ORB algorithms;nearest neighbor matching method;optimal matching points;pseudomatching points;matching accuracy;composite image;splicing gaps;final panoramic image;image fusion;image registration processing;stitching gap;improved ORB algorithm;weighted average fusion algorithm;improved algorithm time;weighted average algorithm","","1","","15","IEEE","17 Aug 2020","","","IEEE","IEEE Conferences"
"Video Super Resolution Using Temporal Encoding ConvLSTM and Multi-Stage Fusion","Y. Zhang; Z. Chen; S. Liu","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Tencent Media Lab, Palo Alto, CA, USA","2020 IEEE International Conference on Visual Communications and Image Processing (VCIP)","29 Dec 2020","2020","","","298","301","Video super resolution is a challenging task and has attracted the attention of many researchers in recent years. In this paper, we propose a multi-stage spatio-temporal feature fusion network. Different from existing methods that only aggregate features from temporal branch once at a specified s tage of network, the proposed network is organized in a multi-stage manner so that the temporal correlation in features at different stages of the network can be fully exploited. Furthermore, we propose the temporal encoding convLSTM to effectively capture the temporal information at the end of each stage. Experiments on vid4 and viemo-90K demonstrate the effectiveness of the proposed method.","2642-9357","978-1-7281-8068-7","10.1109/VCIP49819.2020.9301823","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301823","video super resolution;temporal correlation;convLSTM","Feature extraction;Correlation;Encoding;Image reconstruction;Training;Task analysis;Decoding","expectation-maximisation algorithm;feature extraction;image fusion;image resolution;spatiotemporal phenomena;video signal processing","temporal branch;temporal correlation;temporal encoding convLSTM;temporal information;video super resolution;multistage spatio-temporal feature fusion network;aggregate features","","","","9","IEEE","29 Dec 2020","","","IEEE","IEEE Conferences"
"Cotton Yield Estimation Model Based on Fusion Image from MODIS and Landsat Data","L. Meng; H. Liu; X. Zhang; M. Xu; D. Guo; Y. Pan","Chinese Academy of Sciences, Northeast Institute of Geography and Agroecology, Changchun, China; Chinese Academy of Sciences, Northeast Institute of Geography and Agroecology, Changchun, China; College of Resources and Environment al Sciences, Northeast Agricultural University, Harbin, China; College of Resources and Environment al Sciences, Northeast Agricultural University, Harbin, China; Chinese Academy of Sciences, Northeast Institute of Geography and Agroecology, Changchun, China; College of Resources and Environment al Sciences, Northeast Agricultural University, Harbin, China","2018 7th International Conference on Agro-geoinformatics (Agro-geoinformatics)","30 Sep 2018","2018","","","1","5","Deficiency in the spatiotemporal resolution of remote sensing images limits crop yield estimations at the farm and field scale. High spatiotemporal resolution fusion images blended from moderate resolution imaging spectro-radiometer (MODIS) and Landsat data may alleviate this problem. This paper selected California's San Joaquin Valley Sheely Farm as study area, using the (Enhanced Spatial and Temporal Adaptive Reflectance Fusion Model) ESTARFM/ (Flexible Spatiotemporal Data Fusion Model) FSDAF to fusion image, it can enhance the efficiency of the high spatial resolution remote sensing image for yield estimation. And this paper chose MOD09GA and Landsat images to fusion analysis respectively, then on the basis of the study of the original cotton estimation model, the estimation model of two cotton plots in the study area was established, and the evaluation of the accuracy was carried out. The results showed that: 1) compared with FSDAF model, the ESTARFM model and the fusion image result of ESTARFM and accuracy for the yield estimation was higher, indicating that the two reference Landsat images were superior than the single image for fusion; In addition, from the results of the study, the fusion effect of ESTARFM is better than that of FSDAF. The ESTARFM model referred to the crop growth in the early and middle stage of cotton growth. The fusion result was better than FSDAF model, and FSDAF only referred to the high precision of a reference point in the middle of crop growth. 2) the correlation coefficient between predicted NDVI by the fusion image and cotton yield of A block is higher than that of block B, and the shape of the plot and the type of crop around the block had influence on the fusion result. 3) the predicted NDVI mean value of the three-phase in the flower-blooming stage of cotton growth by fusion images was used for the cotton yield estimation model, and the results showed that it had a higher accuracy than the single stage. And the scatter diagram is more concentrated, which proves the time-series vegetation index played an important role on the crop yield estimation, so fusion images are more important; 4) the accuracy of fusion image estimation is slightly lower than that of Landsat TM image, but the accuracy is above 0.60, and the predicted NDVI of fusion images can also be used for the yield prediction model as the estimated production factor. This study predicts crop yields using fusion images at the field scale, which can be used as a reference for studying vegetation monitoring using remote sensing at the field scale.","","978-1-5386-5038-7","10.1109/Agro-Geoinformatics.2018.8476140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8476140","FSDAF;ESTARFM;MOD09GA;cotton growth;yield estimation;field scale","","cotton;geophysical image processing;image fusion;image resolution;spatiotemporal phenomena;time series;vegetation mapping","FSDAF model;cotton yield estimation model;crop yield estimation;fusion image estimation;Landsat TM image;remote sensing images;high spatiotemporal resolution fusion images;moderate resolution imaging spectro-radiometer;Temporal Adaptive Reflectance Fusion Model;Flexible Spatiotemporal Data Fusion Model;high spatial resolution remote sensing image;original cotton estimation model;ESTARFM model;vegetation monitoring","","1","","19","IEEE","30 Sep 2018","","","IEEE","IEEE Conferences"
"Pan-Sharpening With Color-Aware Perceptual Loss And Guided Re-Colorization","J. L. G. Bello; S. Seo; M. Kim",Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST),"2020 IEEE International Conference on Image Processing (ICIP)","30 Sep 2020","2020","","","908","912","In remote sensing, “pan-sharpening” is the task of enhancing the spatial resolution of a multi-spectral (MS) image by exploiting the high-frequency information in a panchromatic (PAN) reference image. We present a novel color-aware perceptual (CAP) loss for learning the task of pan-sharpening. Our CAP loss is designed to focus on the deep features of a pre-trained VGG network that are more sensitive to spatial details and ignore color information to allow the network to extract the structural information from the PAN image while keeping the color from the lower resolution MS image. Additionally, we propose “guided re-colorization”, which generates a pan-sharpened image with real colors from the MS input by “picking” the closest MS pixel color for each pan-sharpened pixel, as a human operator would do in manual colorization. Such a re-colorized (RC) image is completely aligned with the pan-sharpened (PS) network output and can be used as a self-supervision signal during training, or to enhance the colors in the PS image during test. We present several experiments where our network trained with our CAP loss generates naturally looking pan-sharpened images with fewer artifacts and outperforms the state-of-the-arts on the WorldView3 dataset in terms of ERGAS, SCC, and QNR metrics.","2381-8549","978-1-7281-6395-6","10.1109/ICIP40778.2020.9190785","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9190785","Pan-sharpening;pan-colorization;deep convolutional neural network (DCNN);perceptual loss;satellite imagery.","Image color analysis;Task analysis;Spatial resolution;Satellites;Training;Network architecture","geophysical image processing;image colour analysis;image fusion;image resolution;remote sensing","color-aware perceptual loss;guided re-colorization;spatial resolution;multispectral image;high-frequency information;panchromatic reference image;CAP loss;pre-trained VGG network;color information;PAN image;lower resolution MS image;pan-sharpened image;closest MS pixel color;pan-sharpened pixel;manual colorization;image re-colorization;pan-sharpened network output;PS image;self-supervision signal","","2","","25","IEEE","30 Sep 2020","","","IEEE","IEEE Conferences"
"Online Multi-resolution Fusion of Space-borne Multispectral Images","H. Li; B. Duvvuri; R. Borsoi; T. Imbiriba; E. Beighley; D. Erdoğmuş; P. Closas","Department of ECE, Northeastern University, Boston, MA; Department of CEE, Northeastern University, Boston, MA; Department of EE, Federal University of Santa Catarina, Florianópolis, SC, Brazil; Department of ECE, Northeastern University, Boston, MA; Department of CEE, Northeastern University, Boston, MA; Department of ECE, Northeastern University, Boston, MA; Department of ECE, Northeastern University, Boston, MA","2022 IEEE Aerospace Conference (AERO)","10 Aug 2022","2022","","","1","12","Satellite imaging has a central role in monitoring, detecting and estimating the intensity of key natural phenomena. One important feature of satellite images is the trade-off between spatial/spectral resolution and their revisiting time, a consequence of design and physical constraints imposed by satellite orbit among other technical limitations. In this paper, we focus on fusing multi-temporal, multi-spectral images where data acquired from different instruments with different spatial resolutions is used. We leverage the spatial relationship between images at multiple modalities to generate high-resolution image sequences at higher revisiting rates. To achieve this goal, we formulate the fusion method as a recursive state estimation problem and study its performance in filtering and smoothing contexts. The proposed strategy clearly outperforms competing methodologies, which is shown in the paper for real data acquired by the Landsat and MODIS instruments.","1095-323X","978-1-6654-3760-8","10.1109/AERO53065.2022.9843578","National Geographic(grant numbers:NGS-86713T-21); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9843578","","Satellites;Smoothing methods;Instruments;Orbits;Spatial resolution;State estimation;Image fusion","geophysical image processing;geophysical techniques;image fusion;image resolution;image sequences;remote sensing;sensor fusion;state estimation;terrain mapping;vegetation mapping","online multiresolution fusion;space-borne multispectral images;satellite imaging;key natural phenomena;satellite images;revisiting time;physical constraints;satellite orbit;fusing multitemporal;different instruments;spatial relationship;high-resolution image sequences;higher revisiting rates;fusion method;recursive state estimation problem","","","","42","IEEE","10 Aug 2022","","","IEEE","IEEE Conferences"
"Deep Panchromatic Image Guided Residual Interpolation For Multispectral Image Demosaicking","Z. Pan; B. Li; Y. Bao; H. Cheng","Baidu Research, Sunnyvale, CA, USA; Baidu Research, Sunnyvale, CA, USA; Baidu Research, Sunnyvale, CA, USA; Baidu Shenzhen R&D, Shenzhen, China","2019 10th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","5 Dec 2019","2019","","","1","5","Snapshot multispectral imaging based on multispectral filter arrays (MSFA) has gained popularity recently for its size and speed. To process these multispectral images, demosaicking is the most crucial and challenging step to reduce artifacts in both spatial and spectral domain. In this work, a novel ResNet based deep learning model is first proposed to reconstruct the full-resolution panchromatic image from MSFA mosaic image. Then, the reconstructed deep panchromatic image (DPI) is deployed as the guide to recover the full-resolution multispectral image using a two-pass guided residual interpolation method. Experiment results demonstrate that the proposed method outperforms the state-of-the-art conventional and deep learning demosaicking methods both qualitatively and quantitatively.","2158-6276","978-1-7281-5294-3","10.1109/WHISPERS.2019.8920868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8920868","image demosaicking;multispectral filter array;deep learning;guided residual interpolation","Interpolation;Machine learning;Image resolution;Measurement;Training;Image reconstruction;Two dimensional displays","filtering theory;image colour analysis;image fusion;image reconstruction;image resolution;image segmentation;image sensors;interpolation;learning (artificial intelligence)","spatial domain;spectral domain;deep learning model;full-resolution panchromatic image;MSFA mosaic image;reconstructed deep panchromatic image;full-resolution multispectral image;two-pass guided residual interpolation method;deep learning demosaicking methods;multispectral image demosaicking;snapshot multispectral imaging;multispectral filter arrays;multispectral images;crucial step;challenging step","","5","","17","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Bayesian fusion of multispectral and hyperspectral images using a block coordinate descent method","Q. Wei; N. Dobigeon; J. -Y. Tourneret","IRIT/INP-ENSEEIHT, University of Toulouse, France; IRIT/INP-ENSEEIHT, University of Toulouse, France; IRIT/INP-ENSEEIHT, University of Toulouse, France","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","23 Oct 2017","2015","","","1","5","This paper studies a new Bayesian optimization algorithm for fusing hyperspectral and multispectral images. The hyperspectral image is supposed to be obtained by blurring and subsampling a high spatial and high spectral target image. The multispectral image is modeled as a spectral mixing version of the target image. By introducing appropriate priors for parameters and hyperparameters, the fusion problem is formulated within a Bayesian estimation framework, which is very convenient to model the noise and the target image. The high spatial resolution hyperspectral image is then inferred from its posterior distribution. To compute the Bayesian maximum a posteriori estimator associated with this posterior, an alternating direction method of multipliers within block coordinate descent algorithm is proposed. Simulation results demonstrate the efficiency of the proposed fusion method when compared with several state-of-the-art fusion techniques.","2158-6276","978-1-4673-9015-6","10.1109/WHISPERS.2015.8075373","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075373","Fusion;multispectral and hyperspectral images;Bayesian estimation;block coordinate descent;alternating direction method of multipliers","Bayes methods;Optimization;Hyperspectral imaging;Sensors;Spatial resolution","Bayes methods;gradient methods;hyperspectral imaging;image fusion;image resolution;maximum likelihood estimation;optimisation","hyperspectral images;multispectral images;high spectral target image;spectral mixing version;Bayesian estimation framework;high spatial resolution hyperspectral image;Bayesian maximum a posteriori estimator;Bayesian fusion;Bayesian optimization algorithm;block coordinate descent method","","2","","27","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Spectral-spatial classification for hyperspectral image by bilateral filtering and morphological features","W. Liao; D. E. Ochoa Donoso; F. Van Coillie; J. Li; C. Qi; S. Gautama; W. Philips","IGhent University-TELIN-IPI-iMinds, Ghent, Belgium; Facultad de Ingeniera en Electricidad y Computacin, Escuela Superior Politcnica del Litoral (ESPOL), Guayaquil, Ecuador; Department of Forest and Water Management, Gent University, Ghent, Belgium; School of Electronics & Information Engineering, Xi'an Jiaotong University, Xian, China; School of Electronics & Information Engineering, Xi'an Jiaotong University, Xian, China; IGhent University-TELIN-IPI-iMinds, Ghent, Belgium; IGhent University-TELIN-IPI-iMinds, Ghent, Belgium","2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Oct 2017","2016","","","1","4","Hyperspectral (HS) imagery contains a wealth of spectral and spatial information that can improve target detection and recognition performance. Conventional spectral-spatial classification methods cannot fully exploit both spectral and spatial information of HS image. In this paper, we propose a new method to fuse the spectral and spatial information for HS image classification. Our approach transfers the spatial structures of the whole morphological profile into the original HS image by using bilateral filtering, and obtains an enhanced HS image enriching both spectral and spatial information. Meanwhile, the enhanced HS image has the same spectral and spatial dimensions as the original HS image, which may provide a new input to improve the performances of existing HS image classification methods. Experimental results on real HS images are very encouraging. Compared to the methods using only single feature and stacking all the features together, the proposed fusion method improves the overall classification accuracy more than 10% and 5%, respectively.","2158-6276","978-1-5090-0608-3","10.1109/WHISPERS.2016.8071680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071680","Hyperspectral images;data fusion;mathematical morphology;bilateral filtering","Hyperspectral imaging;Stacking;Principal component analysis;Shape;Asphalt","feature extraction;hyperspectral imaging;image classification;image filtering;image fusion;object detection;object recognition","spectral-spatial classification;HS image classification methods;morphological features;target detection;target recognition;spatial information;spectral information;bilateral filtering;hyperspectral image","","2","","11","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"Hyperspectral image classification using multiple features and nearest regularized subspace","B. Peng; X. Xie; W. Li; Q. Du","College of Information Science and Technology, Beijing University of Chemical Technology; College of Information Science and Technology, Beijing University of Chemical Technology; College of Information Science and Technology, Beijing University of Chemical Technology; Department of Electrical and Computer Engineering, Mississippi State University","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","23 Oct 2017","2015","","","1","4","Gabor features have been proved to be effective for the recently-proposed nearest regularized subspace (NRS) classifier. In this paper, we further investigate a residual fusion based strategy with multiple features and NRS. Multiple features include local binary patterns (LBP), Gabor features and the original spectral signatures. In the proposed classification framework, each type of feature is first coupled with the NRS classifier, obtaining the output of residuals. And then, all the residuals are added together and the label of the test pixel is determined according to the minimum residual. The motivation of this work is due to that different features represent the test pixel from different perspectives and the fusion in the residual domain is able to enhance the discriminative ability, especially for small-sample-size situations. Experimental results of several hyperspectral image datasets demonstrate that the proposed residual-based fusion strategy is superior to the traditional NRS and Gabor-NRS.","2158-6276","978-1-4673-9015-6","10.1109/WHISPERS.2015.8075426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075426","Local binary patterns (LBP);nearest regularized subspace(NRS);Gabor features;hyperspectral image classification","Hyperspectral imaging;Training;Gabor filters;Image classification;Feature extraction","feature extraction;hyperspectral imaging;image classification;image fusion;image representation","Gabor-NRS;hyperspectral image classification;multiple features;Gabor features;nearest regularized subspace classifier;residual fusion based strategy;NRS classifier;residual domain;hyperspectral image datasets;residual-based fusion strategy","","1","","12","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"A Multi-Sensor Subspace-Based Clustering Algorithm Using RGB and Hyperspectral Data","K. R. Shahi; P. Ghamisi; R. Jackisch; B. Rasti; P. Scheunders; R. Gloaguen","Department of Physics, Imec-Visionlab, University of Antwerp, Belgium; Department of Physics, Imec-Visionlab, University of Antwerp, Belgium; Helmholtz-Zentrum Dresden-Rossendorf (HZDR), Helmholtz Institute Freiberg for Resource Technology (HIF), Germany; Helmholtz-Zentrum Dresden-Rossendorf (HZDR), Helmholtz Institute Freiberg for Resource Technology (HIF), Germany; Department of Physics, Imec-Visionlab, University of Antwerp, Belgium; Helmholtz-Zentrum Dresden-Rossendorf (HZDR), Helmholtz Institute Freiberg for Resource Technology (HIF), Germany","2021 11th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Jul 2021","2021","","","1","5","In this work, we introduce a multi-sensor subspace-based clustering algorithm that benefits from fine spectral-resolution hyperspectral images (HSIs) and fine spatial-resolution RGB images. In order to extract spatial information, a hidden Markov random field (HMRF) is employed on the fine spatial-resolution RGB image, whereas, spectral information is derived from an HSI using an advanced sparse subspace clustering algorithm. The proposed algorithm is validated on two real geological data sets. The experimental results in this study show that the proposed algorithm outperforms the state-of-the-art clustering algorithms in terms of clustering accuracy.","2158-6276","978-1-6654-3601-4","10.1109/WHISPERS52202.2021.9483953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9483953","Hyperspectral images;RGB images;UAV data;hidden Markov random field;spectral-spatial clustering;sparse representation;data fusion","Geology;Conferences;Clustering algorithms;Signal processing algorithms;Hidden Markov models;Markov processes;Data mining","geology;geophysical image processing;hidden Markov models;hyperspectral imaging;image colour analysis;image fusion;image resolution;image sensors;pattern clustering","hyperspectral data;spectral-resolution hyperspectral images;spatial-resolution RGB images;spatial information;hidden Markov random field;spectral information;advanced sparse subspace clustering algorithm;geological data sets;clustering accuracy;multisensor subspace-based clustering algorithm;HSIs;HMRF","","1","","11","IEEE","19 Jul 2021","","","IEEE","IEEE Conferences"
"A Transformer-Based Three-Branch Siamese Network For Hyperspectral Object Tracking","N. Su; H. Liu; C. Zhao; Y. Yan; J. Wang; J. He","Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin; Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin; Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin; Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin; Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin; Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin","2022 12th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","22 Nov 2022","2022","","","1","5","Hyperspectral videos can provide more information for the object tracking task. Due to the limited training samples, most current hyperspectral trackers do not fully use hyperspectral information to improve the tracking performance. To solve this problem, we propose a Transformer-based three-branch Siamese network (TrTSN) for hyperspectral object tracking. First, we construct a three-branch structure based on the Siamese network to obtain the semantic information of hyperspectral data fully. Second, we design a Transformer-based fusion module (TFM) and use two TFMs to adaptively combine the information obtained by different branches to obtain more robust features. Finally, the two sets of classification response and regression response generated by two fusion features are corresponding merged to improve the tracking network’s ability to predict the object’s position. Experimental results show that the TrTSN tracker is superior to the state-of-the-art trackers, demonstrating the effectiveness of this method.","2158-6276","978-1-6654-7069-8","10.1109/WHISPERS56178.2022.9955082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955082","Transformer;siamese network;hyperspectral object tracking;deep learning","Training;Conferences;Semantics;Signal processing;Transformers;Object tracking;Task analysis","feature extraction;image classification;image fusion;image representation;image retrieval;learning (artificial intelligence);object detection;object tracking;regression analysis;target tracking;video signal processing","current hyperspectral trackers;different branches;hyperspectral data;hyperspectral information;hyperspectral object tracking;hyperspectral videos;object tracking task;semantic information;three-branch structure;tracking network;tracking performance;Transformer-based fusion module;Transformer-based three-branch Siamese network","","1","","14","IEEE","22 Nov 2022","","","IEEE","IEEE Conferences"
"Spectral-Spatial-Aware Transformer Fusion Network For Hyperspectral Object Tracking","Y. Wang; Y. Liu; G. Zhang; Y. Su; S. Zhang; S. Mei","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China","2022 12th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","22 Nov 2022","2022","","","1","5","Single object tracking (SOT) based on deep learning methods has been developed by leaps and bounds. Nevertheless, these methods seldom take spectral information into consideration, which does not utilize the spectral properties of objects. Since that spectral imaging can probably distinguish a particular object more discriminatively, a more effective model is expected to be proposed for the exploration of spectral information. To study the advantages of the hyperspectral image in extracting spectral features of the target in a tracking task, this work proposed a novel Spectral-spatial-aware Transformer Fusion Network (SSATFN) for hyperspectral single object tracking, which efficiently combines spectral and spatial features with the template and search region branches. Specifically, the method focuses on multiband feature fusion by Hyperspectral Transformer Self-attention (HTSA) and Hyperspectral Transformer Cross-attention (HTCA). Meanwhile, a multi-scale spectral feature fusion auxiliary branch is utilized for template enhancement. Finally, we present the online tracking learning networks to fine-tune the last two layers of the feature extraction network. Quantitative experiments are conducted on a close-up hyperspectral video dataset, and verified that the proposed SSATFN achieves promising tracking performances, compared with the other state-of-the-art trackers.","2158-6276","978-1-6654-7069-8","10.1109/WHISPERS56178.2022.9955100","Northwestern Polytechnical University; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955100","Spatial-spectral-aware;Transformer Fusion;hyperspectral object tracking","Target tracking;Video sequences;Imaging;Signal processing;Feature extraction;Transformers;Search problems","deep learning (artificial intelligence);feature extraction;hyperspectral imaging;image classification;image fusion;object detection;object tracking;video signal processing","deep learning methods;feature extraction network;hyperspectral image;hyperspectral single object tracking;hyperspectral transformer cross-attention;hyperspectral transformer self-attention;hyperspectral video dataset;multiband feature fusion;multiscale spectral feature fusion auxiliary branch;online tracking learning networks;spectral-spatial-aware transformer fusion network;SSATFN;template enhancement;template search region branches","","","","15","IEEE","22 Nov 2022","","","IEEE","IEEE Conferences"
"A Fast Hyperspectral Object Tracking Method Based On Channel Selection Strategy","Y. Zhang; X. Li; F. Wang; B. Wei; L. Li","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China","2022 12th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","22 Nov 2022","2022","","","1","5","Hyperspectral object tracking aims to take advantage of the rich spatial and spectral information in hyperspectral videos to effectively improve the robustness and accuracy of object tracking. Compared with color videos, hyperspectral videos have huge amount of data bringing a challenge to the efficiency of object tracking. We propose a fast hyperspectral object tracking method based on channel selection strategy. The strategy considers the spatial and spectral changes of local regions in the frame image and selects only three channels fed to tracker to speed up. The experimental results show that our method reaches 11.5 FPS on the dataset of Hyperspectral Object Tracking Challenge, which is faster than the state-of-the-art methods.","2158-6276","978-1-6654-7069-8","10.1109/WHISPERS56178.2022.9955094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955094","Hyperspectral video;object tracking;channel selection","Target tracking;Image color analysis;Signal processing;Robustness;Hardware;Object tracking;Information entropy","feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;image sensors;image sequences;object detection;object tracking;target tracking;video signal processing","channel selection strategy;fast hyperspectral object tracking method;Hyperspectral Object Tracking Challenge;hyperspectral videos;rich spatial information;spectral information","","","","16","IEEE","22 Nov 2022","","","IEEE","IEEE Conferences"
"Pansharpening of Mastcam images","C. Kwan; B. Budavari; M. Dao; B. Ayhan; J. F. Bell",Applied Research LLC; Applied Research LLC; Applied Research LLC; Applied Research LLC; Applied Research LLC,"2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","5117","5120","This paper summarizes a new investigation of applying advanced pansharpening algorithms to enhance the images of the left imager in the Mastcam onboard the Curiosity rover, which landed on Mars in 2012. The various instruments on the rover have already made great contributions in the understanding of Mars. The goal of our research is to generate both high spatial and high spectral image cube by using the left and right Mastcam imagers. Eleven algorithms have been investigated using five objective performance metrics. Subjective evaluations have also been conducted. The image enhancement results are encouraging.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128154","Mastcam;image registration;pansharpening;image enhancement","","aerospace computing;geophysical image processing;image enhancement;image fusion;image registration;image resolution;Mars;planetary rovers;planetary surfaces","image enhancement;right Mastcam imagers;left Mastcam imagers;image registration;Curiosity rover;advanced pansharpening algorithms;Mastcam images;objective performance metrics;high spectral image cube;Mars","","21","1","24","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Pop-Net: Encoder-Dual Decoder for Semantic Segmentation and Single-View Height Estimation","Z. Zheng; Y. Zhong; J. Wang","State Key Laboratory of Information Engineering in Surveying, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Wuhan University, Wuhan, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","4963","4966","The single-view semantic 3D challenge in 2019 Data Fusion Contest is to predict both semantic labels and normalized digital surface model (nDSM) for urban scenes from single-view satellite images. We propose a novel pyramid on pyramid network (Pop-Net) based on Encoder-Dual Decoder framework to end-to-end multi-task learning. The encoder is a deformable ResNet-101 backbone network. Two feature pyramid networks, as decoders, are responsible for semantic segmentation and height estimation, respectively. Semantic information is crucial to estimate height. Therefore, regression pyramid on the semantic pyramid is introduced to leverage semantic features to help height estimation. To deal with outliers in heights, we leverage anchor-based regression and smooth L1 loss for optimization to obtain more robust height estimation. Without bells and whistles, our single model entry achieves 77.78% mIoU and 53.40% mIoU-3 on test set, ranking 2nd in the Single-view Semantic 3D Challenge of the 2019 IEEE GRSS Data Fusion Contest. The code is available at https://github.com/Z-Zheng/PopNet.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8897927","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8897927","single-view semantic 3D challenge;pyramid on pyramid;semantic segmentation;height estimation","Semantics;Decoding;Estimation;Three-dimensional displays;Image segmentation;Data integration;Satellites","decoding;estimation theory;geophysical image processing;image coding;image fusion;image segmentation;learning (artificial intelligence);optimisation;regression analysis","semantic segmentation;leverage semantic features;leverage anchor-based regression;robust height estimation;2019 IEEE GRSS Data Fusion Contest;Pop-Net;single-view semantic 3D challenge;normalized digital surface model;deformable ResNet-101 backbone network;encoder-dual decoder framework;single-view satellite imaging;single-view height estimation;semantic pyramid network;regression pyramid network","","10","","9","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Covariance Matrix Based Feature Fusion for Scene Classification","N. He; L. Fang; S. Li; A. J. Plara","Department of Technology of Computers and Communications, University of Extremadura, Spain; College of Electrical and Information Engineering, Hunan University, China; College of Electrical and Information Engineering, Hunan University, China; Department of Technology of Computers and Communications, University of Extremadura, Spain","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","3587","3590","In this paper, a covariance matrix based feature fusion (CMF-F) framework is proposed to combine two low-level visual features i.e., the Gabor feature and color feature for scene classification. Generally, the proposed method consists of following three steps. Firstly, the Gabor feature and color feature are extracted from original image and stacked together. Then, a covariance matrix is extracted to fuse these two low-level visual features. Each nondiagonal entry in the covariance matrix stands for the correlation of two different feature dimensions. Finally, the obtained covariance matrix is handled by a kernel linear discriminative analysis algorithm followed with nearest neighboring classifier for label assignment. The proposed method is tested on a public 21-classes UC Merced land use data set and compared with mid-level visual feature oriented method and the high-level feature oriented methods. The experimental results demonstrate that the proposed CMFF framework can not only improve the classification performance of the low-level visual feature (the Gabor feature and the color feature), but also can outperform the conventional mid-level visual feature oriented methods.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517914","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517914","Scene classification;feature representation;feature fusion","Feature extraction;Image color analysis;Covariance matrices;Visualization;Kernel;Support vector machines;Fuses","covariance matrices;feature extraction;image classification;image colour analysis;image fusion","high-level feature oriented methods;mid-level visual feature oriented method;color feature;Gabor feature;low-level visual feature;covariance matrix based feature fusion framework;scene classification","","7","","10","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Pansharpening of hyperspectral images: Exploiting data acquired by multiple platforms","D. Picone; R. Restaino; G. Vivone; P. Addesso; J. Chanussot","Dept. of Information Eng., Electrical Eng. and Applied Math., University of Salerno, Salerno, Italy; Dept. of Information Eng., Electrical Eng. and Applied Math., University of Salerno, Salerno, Italy; NATO STO Centre for Maritime Research and Experimentation, La Spezia, Italy; Dept. of Information Eng., Electrical Eng. and Applied Math., University of Salerno, Salerno, Italy; Faculty of Electrical and Computer Engineering, University of Iceland","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","7220","7223","Accurate representations of the Earth surface in both spatial and spectral domains are highly desirable in many applications using remotely sensed data. An effective solution is achieved by combining hyperspectral data, which are characterized by a high spectral diversity, with high spatial resolution images, collected by multispectral or panchromatic sensors. In this work, we compare the outcomes provided by fusing single-platform or multi-platform data. We demonstrate that the optimal choice depends on the target spatial resolution to be achieved. To this aim, real images collected by the Hyperion sensor are combined with data acquired by the ALI sensor or the QuickBird sensor assessing the fused outcomes at reduced resolution.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730883","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730883","Data Fusion;Pansharpening;HyperSpectral Images;Multiplatform Data","Spatial resolution;Sensor fusion;Satellites;Image sensors;Hyperspectral sensors","geophysical image processing;image fusion","hyperspectral images pansharpening;Earth surface;remotely sensed data;hyperspectral data;high spectral diversity;panchromatic sensors;Hyperion sensor;ALI sensor;QuickBird sensor","","7","","9","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Cross-domain image localization by adaptive feature fusion","N. Bhowmik; Li Weng; V. Gouet-Brunet; B. Soheilian","Universite Paris-Est, Marne-la-Vallee, ÃŽle-de-France, FR; Univ. Paris-Est, LASTIG MATIS, Saint-Mande, France; Univ. Paris-Est, LASTIG MATIS, Saint-Mande, France; Univ. Paris-Est, LASTIG MATIS, Saint-Mande, France","2017 Joint Urban Remote Sensing Event (JURSE)","11 May 2017","2017","","","1","4","We address the problem of cross-domain image localization, i.e., the ability of estimating the pose of a landmark from visual content acquired under various conditions, such as old photographs, paintings, photos taken at a particular season, etc. We explore a 2D approach where the pose is estimated from geo-localized reference images that visually match the query image. This work focuses on the retrieval of similar images, which is a challenging task for images across different domains. We propose a Content-Based Image Retrieval (CBIR) framework that adaptively combines multiple image descriptions. A regression model is used to select the best feature combinations according to their spatial complementarity, globally for a whole dataset as well as adaptively for each given image. The framework is evaluated on different datasets and the experiments prove its advantage over classical retrieval approaches.","","978-1-5090-5808-2","10.1109/JURSE.2017.7924572","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7924572","","Detectors;Image retrieval;Feature extraction;Training;Visualization;Computational modeling;Adaptation models","geophysical image processing;image fusion;image retrieval","regression model;Content-Based Image Retrieval framework;query image;images;2D approach;old photographs;visual content;adaptive feature fusion;cross-domain image localization","","6","","33","IEEE","11 May 2017","","","IEEE","IEEE Conferences"
"Fusion of Lidar, Hyperspectral and RGB Data for Urban Land Use and Land Cover Classification","S. Sukhanov; D. Budylskii; I. Tankoyeu; R. Heremans; C. Debes","AGT International, Darmstadt, Germany; AGT International, Darmstadt, Germany; AGT International, Darmstadt, Germany; AGT International, Darmstadt, Germany; AGT International, Darmstadt, Germany","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","3864","3867","In this paper, we present an ensemble-based classification approach for urban land use and land cover classification based on multispectral LiDAR, hyperspectral and very high resolution RGB data. The approach has been evaluated on the data set provided for the IEEE GRSS 2018 Data Fusion Contest organized by the GRSS IADF technical committee and has been proven to have a high operational performance, being able to distinguish between different grass-, building- and street-types among other classes like water, railways and parking lots as well as other non-typical classes like cars, trains, stadium seats, etc.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517333","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517333","multispectral LiDAR;hyperspectral imaging;very high-resolution RGB;land use classification","Radio frequency;Laser radar;Hyperspectral imaging;Forestry;Training;Feature extraction;Training data","geophysical image processing;image classification;image colour analysis;image fusion;image resolution;land cover;land use;optical radar;terrain mapping","hyperspectral;urban land use;land cover classification;classification approach;multispectral LiDAR;high resolution RGB data;IEEE GRSS 2018 Data Fusion Contest;GRSS IADF technical committee;high operational performance;grass-, building- and street-types","","6","","11","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Urban Land Use/Land Cover Classification Based on Feature Fusion Fusing Hyperspectral Image and Lidar Data","Q. Cao; Y. Zhong; A. Ma; L. Zhang","The State Key Laboratory of Information Engineering in Surveying, Wuhan University, Wuhan, China; The State Key Laboratory of Information Engineering in Surveying, Wuhan University, Wuhan, China; The State Key Laboratory of Information Engineering in Surveying, Wuhan University, Wuhan, China; The State Key Laboratory of Information Engineering in Surveying, Wuhan University, Wuhan, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","8869","8872","Hyperspectral images have been widely used in classification because of the abundant spectral information. But it can't distinguish the objective with similar spectral character but different elevation. However, LiDAR data can obtain elevation information. Therefore, it will obtain better classification maps if fusing the two data. In recent years, CNN has attracted much attention due to its powerful ability to excavate the potential representation and features of the raw data. However, it's difficult to distinguish the objects with different spectral information but similar surface character. Unlike CNN features, the traditional manual features, such as the normalized vegetation index (NDVI), have a certain characteristic expression significance. In order to consider both the semantic information of traditional manual features and the advanced features of CNN features, this paper proposes a fusion algorithm of hyperspectral and LiDAR fusion based on feature fusion. The proposed algorithm has achieved a good fusion classification effect on the MUUFL Gulfport Hyperspectral and LiDAR Data set.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517361","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517361","Hyperspectral;LIDAR;land-use/land-cover classification;feature fusion;deep learning;convolutional neural network","Feature extraction;Hyperspectral imaging;Laser radar;Manuals;Classification algorithms","feature extraction;geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;image fusion;optical radar;terrain mapping;vegetation","spectral information;surface character;fusion classification effect;LiDAR Data;MUUFL Gulfport Hyperspectral;hyperspectral LiDAR fusion;fusion algorithm;semantic information;traditional manual features;CNN features;raw data;potential representation;classification maps;elevation information;LiDAR data;abundant spectral information;Hyperspectral images;lidar Data;feature fusion fusing Hyperspectral image;urban land use/land cover classification","","6","","14","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Hyperspectral Pansharpening Based on Guided Filter and Deep Residual Learning","Y. Zheng; J. Li; Y. Li","State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","616","619","Recently, deep learning technology has gained impressive effectiveness in the field of hyperspectral pansharpening. However, the existing methods with relatively shallow architectures ignores the deep features of hyperspectral image (HSI) and panchromatic (PAN) image, which leads to a limitation of the fusion performance. To address this issue, a novel hyperspectral pansharpening framework based on guided filter and deep residual learning is proposed in this paper. The proposed framework mainly consists of two parts: generating the initial HSI through enhancing spatial information while preserving the original spectral information, and mapping the residuals between the initialized HSI and the reference HSI for further improvement of the fusion accuracy. Experimental results demonstrate that the proposed framework can achieve superior fusion accuracy compared with other state-of-the-art hyperspectral pansharpening methods while providing better edge information.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899015","Contrast limited adaptive histogram equalization;guided filter;deep residual convolutional neural network;hyperspectral pansharpening","Hyperspectral imaging;Spatial resolution;Image edge detection;Testing;Bayes methods;Indexes","hyperspectral imaging;image fusion;learning (artificial intelligence)","deep residual learning;deep learning technology;shallow architectures;deep features;fusion performance;guided filter;initialized HSI;hyperspectral pansharpening methods;panchromatic image;hyperspectral image;PAN image;spectral information","","5","","12","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Pairwise Stereo Image Disparity and Semantics Estimation with the Combination of U-Net and Pyramid Stereo Matching Network","R. Qin; X. Huang; W. Liu; C. Xiao","Department of Civil, The Ohio State University; Department of Civil, The Ohio State University; Department of Civil, The Ohio State University; Department of Civil, The Ohio State University","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","4971","4974","Stereo images are one of the most common resources for 3D reconstruction. In the pairwise semantic stereo challenge of the 2019 IEEE GRSS Data Fusion Contest, we generate the classification and the disparity map based on U-Net and Pyramid Stereo Matching Network (PSMNet), respectively. By using a dynamic and class-weighted loss function, the UNet is effectively trained with the imbalanced training samples. By voting classification results of the augmented prediction data with models trained under different epochs, we further refine the classification maps with the constraints of pseudo DSM, water index and mean-shift segmentation.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900262","PSMNet;U-Net;Pseudo DSM;Semantic Classification","Semantics;Image segmentation;Training;Satellites;Estimation;Data integration;Indexes","image classification;image fusion;image matching;image reconstruction;image sampling;image segmentation;stereo image processing","pairwise Stereo image disparity;Stereo images;pairwise semantic stereo challenge;IEEE GRSS Data Fusion Contest;class-weighted loss function;pyramid stereo matching network","","5","","7","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Collaborative Classification of Hyperspectral and Lidar Data With Information Fusion and Deep Nets","C. Chen; X. Zhao; W. Li; R. Tao; Q. Du","College of Information Science & Technology, Beijing University of Chemical Technology; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Department of Electrical and Computer Engineering, Mississippi State University","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","2475","2478","Convolutional neural network (CNN) receives extensive attention in hyperspectral image classification. While hyper-spectral images contain abundant spectral information but lack spatial information, which usually contributes to poor classification results. In this paper, a novel classification framework called information fusion based CNN (IF-CNN) is proposed to compensate for the shortcomings of hyper-spectral images. The proposed method merges hyperspectral images with abundant spectral information and LiDAR images with rich spatial information as the input of classification framework. Furthermore, the framework consists of two convolutional neural networks: one-dimensional CNN for extracting spectral features, and two-dimensional CNN for extracting spatial correlation features. Experimental results demonstrate that the proposed method achieves excellent performance compared with some existing methods.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898443","Hyperspectral Image;Information Fusion;Convolutional Neural Network;Deep Learning;Pattern Recognition","Laser radar;Hyperspectral imaging;Feature extraction;Support vector machines;Training;Buildings","convolutional neural nets;feature extraction;hyperspectral imaging;image classification;image fusion;optical radar","LiDAR data;information fusion;convolutional neural network;hyperspectral image classification;abundant spectral information;novel classification framework;IF-CNN;rich spatial information;one-dimensional CNN;spectral features;two-dimensional CNN;spatial correlation features;spatial information;collaborative classification","","5","","9","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Weakly Supervised Semantic Segmentation in the 2020 IEEE GRSS Data Fusion Contest","C. Robinson; K. Malkin; L. Hu; B. Dilkina; N. Jojic",Georgia Institute of Technology; Yale University; University of Southern California; University of Southern California; Microsoft Research,"IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","27 Sep 2021","2020","","","7046","7049","We propose an iterative clustering-based label super-resolution approach and epitome-based approach to weakly supervised semantic segmentation, as well as a deep learning-based postprocessing step for land cover segmentation. An ensemble of the iterative clustering and epitome approaches with the proposed postprocessing step results in a top validation leaderboard average accuracy of 70.43%. A similar ensemble, that also considers class accuracy feedback from the leaderboard, achieves a top Track 1 leaderboard average accuracy of 57.49%.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9547211","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547211","land cover mapping;weakly supervised semantic segmentation;deep learning","Image segmentation;Smoothing methods;Semantics;Superresolution;Clustering algorithms;Data integration;Training data","geophysical image processing;image classification;image fusion;image resolution;image segmentation;iterative methods;learning (artificial intelligence);sensor fusion;terrain mapping","weakly supervised semantic segmentation;2020 IEEE GRSS data fusion;iterative clustering-based label super-resolution approach;epitome-based approach;deep learning-based;land cover segmentation;epitome approaches;postprocessing step results;validation leaderboard average accuracy","","4","","7","IEEE","27 Sep 2021","","","IEEE","IEEE Conferences"
"A CNN-Based Model for Pansharpening of WorldView-3 Images","S. Vitale; G. Ferraioli; G. Scarpa","Dipartimento di Ingegneria, Università degli Studi di Napoli “Parthenope”, Naples, Italy; Dipartimento di Scienze e Tecnologie, Università degli Studi di Napoli “Parthenone”, Naples, Italy; DIETI, Università degli Studi di Napoli “Federico II”, Naples, Italy","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5108","5111","Fusing a multispectral image with a co-registered higher resolution single panchromatic band, provided by any multiresolution satellite systems, to rise the resolution of the former to that of the latter is known as pansharpening, and can be regarded as a guided super-resolution problem. Recently the use of convolutional neural networks (CNNs) has been extended to the pansharpening problem achieving state-of-the-art performance. Following this research line, the objective of this work was two-fold: provide a trained CNN model fitted to a specific sensor (WorldView-3) and explore a range of architectural configurations varied in both width and depth, seeking for the optimal one. Numerical and visual results show that the proposed solution compares favourably against reference methods.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519202","Super-resolution;data-fusion;machine learning;multi-resolution;convolutional neural network","Spatial resolution;Training;Indexes;Convolutional neural networks;Visualization;Convolution","convolution;feedforward neural nets;geophysical image processing;hyperspectral imaging;image fusion;image resolution","WorldView-3 images;multispectral image;higher resolution single panchromatic band;multiresolution satellite systems;super-resolution problem;convolutional neural networks;pansharpening problem;CNN","","4","","17","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Fusion of Hyperspectral and Lidar Data Based On Dual-Branch Convolutional Neural Network","J. Wang; J. Zhang; Q. Guo; T. Li","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3388","3391","With to the development of sensors, the fusion of features from multisource data becomes an interesting but challenging problem. In this paper, the fusion of hyperspectral imagery (HSI) and light detection and ranging (LiDAR) data is investigated with a novel and simplified deep learning architecture, named the dual-branch convolutional neural network (DB-CNN). More specifically, a 3D CNN framework as one of the two branches is used to extract spectral-spatial features simultaneously from HSI, which can keep three-dimensional structural characteristics of HSI. Another one is 2D CNN with cascade blocks, which is developed to extract elevation feature from LiDAR data, and it can exploit the multiscale features. Finally, the features of two branches will be flattened and stacked, and then sent to the fully connected layers. The experiments show that the proposed DB-CNN method can effectively fuse the HSI and LiDAR data, and yield higher classification performance than some existing methods.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899332","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899332","data fusion;deep learning;feature extraction;dual-branch CNN","Laser radar;Feature extraction;Three-dimensional displays;Two dimensional displays;Convolution;Hyperspectral imaging;Training","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;learning (artificial intelligence);optical radar","lidar data fusion;dual-branch convolutional neural network;multisource data;HSI;deep learning architecture;light detection and ranging data;3D CNN framework;spectral-spatial feature extraction;2D CNN;DB-CNN method;hyperspectral data fusion;hyperspectral imagery;three-dimensional structural characteristics;elevation feature extraction","","4","","6","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Fusing Information from Subpixel to Superpixel for Hyperspectral Anomaly Detection","Z. Huang; S. Li; L. Fang","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1260","1263","In this paper, a novel fusion framework, which is called as subpixel, pixel and superpixel fusion based anomaly detection (SPSF-AD), is proposed for hyperspectral anomaly detection. Most existing methods are based on single pixel-level data representation, which fail to fully exploit spectral-spatial information in hyperspectral image for anomaly detection. For the proposed SPSF-AD method, the first step is to separately extract subpixel, pixel, and superpixel features from HSI via the spectral unmixing, morphological operation, and super-pixel segmentation technique. Then, three sparse anomalies detection results based on different features can be generated via low-rank decomposition (LRD) technique. Finally, since the different features contain highly complementary information, an effective decision fusion technique, i.e., average operator, is employed to estimate the final anomaly detection result. Experiments on three real HSI data sets can demonstrate the effectiveness of the proposed method.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517767","Anomaly detection;hyperspectral image;spectral unmixing;superpixel;low-rank decomposition","Anomaly detection;Hyperspectral imaging;Feature extraction;Sparse matrices;Detectors","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image segmentation;sensor fusion","highly complementary information;low-rank decomposition technique;sparse anomalies detection results;super-pixel segmentation technique;superpixel features;SPSF-AD method;hyperspectral image;spectral-spatial information;single pixel-level data representation;superpixel fusion;fusion framework;hyperspectral anomaly detection;final anomaly detection result;effective decision fusion technique","","4","","10","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Hyperspectral Image Classification Based on Generative Adversarial Networks with Feature Fusing and Dynamic Neighborhood Voting Mechanism","Y. Zhan; J. Qin; T. Huang; K. Wu; D. Hu; Z. Zhao; Y. Wang; Y. Cao; R. Jiao; Y. Medjadba; G. Wang; X. Yu","College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; Beijing Institute of Geology, Beijing, China; Beijing Institute of Geology, Beijing, China; Beijing Institute of Geology, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; Libary, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","811","814","Classifying Hyperspectral images with few training samples is a challenging problem. The generative adversarial networks (GAN) are promising techniques to address the problems. GAN constructs an adversarial game between a discriminator and a generator. The generator generates samples that are not distinguishable by the discriminator, and the discriminator determines whether or not a sample is composed of real data. In this paper, by introducing multilayer features fusion in GAN and a dynamic neighborhood voting mechanism, a novel algorithm for HSIs classification based on 1-D GAN was proposed. Extracting and fusing multiple layers features in discriminator, and using a little labeled samples, we fine-tuned a new sample 1-D CNN spectral classifier for HSIs. In order to improve the accuracy of the classification, we proposed a dynamic neighborhood voting mechanism to classify the HSIs with spatial features. The obtained results show that the proposed models provide competitive results compared to the state-of-the-art methods.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899291","Hyperspectral images classification;semi-supervised learning (SSL);generative adversarial networks (GAN);deep learning;spectral-spatial classification","Generative adversarial networks;Feature extraction;Hyperspectral imaging;Semisupervised learning;Image classification;Gallium nitride","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence)","hyperspectral image classification;generative adversarial networks;dynamic neighborhood voting mechanism;multilayer features fusion;HSI classification;CNN spectral classifier;feature extraction;Indian Pines dataset","","4","","15","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Classification of oyster habitats by combining wavelet-based texture features and polarimetric SAR descriptors","O. Regniers; L. Bombrun; I. Ilea; V. Lafon; C. Germain","Laboratoire IMS, Université de Bordeaux, Talence, France; Laboratoire IMS, Université de Bordeaux, Talence, France; Laboratoire IMS, Université de Bordeaux, Talence, France; Géo-Transfert, Pessac, France; Laboratoire IMS, Université de Bordeaux, Talence, France","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","3890","3893","In this study, we propose to evaluate the potential of combining very high resolution optical and SAR images for the classification of oyster habitats in tidal flats. To describe the classes of interest in both data, features are extracted by using wavelet-based texture features and polarimetric inter-band dependencies. A multisensor fusion scheme is then applied by adopting a maximum probability rule based on the outputs of SVM classifiers. Classification results show higher accuracies of detection of cultivated and abandoned oyster fields in comparison to classifications obtained using only texture features. This demonstrate the benefit of using both optical and SAR data for oyster habitats mapping in tidal flats.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326674","texture;multi-sensor fusion;wavelet;SVM;classification;very high resolution;oyster habitats","Feature extraction;Support vector machines;Data mining;Synthetic aperture radar;Production;Tides;Training","feature extraction;geophysical image processing;image classification;image fusion;image texture;oceanographic techniques;radar imaging;radar polarimetry;support vector machines;synthetic aperture radar;tides","wavelet-based texture features;polarimetric SAR descriptor;oyster habitat classification;high resolution optical image;polarimetric interband dependency;SVM classifier;oyster habitat mapping;tidal flat;maximum probability rule;multisensor fusion scheme","","4","","9","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"SAR Object Detection With a Saliency Method Based on PCA and Global Contrast","H. -x. Li; X. -l. Yu; Y. -h. Tang; X. -g. Wang","University of Electronic Science and Technology of China, Chengdu, Sichuan, China; University of Electronic Science and Technology of China, Chengdu, Sichuan, China; University of Electronic Science and Technology of China, Chengdu, Sichuan, China; University of Electronic Science and Technology of China, Chengdu, Sichuan, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","1172","1175","Object detection is a significant step in synthetic aperture radar (SAR) image interpretation. This paper contributes to propose a saliency method based on principle component analysis (PCA) and global contrast for SAR object detection. Firstly, the original SAR image is smoothed by Gaussian filter and then segmented into superpixels by the linear iterative cluster (SLIC) algorithm. Secondly, two feature maps are created based on PCA and global contrast respectively, and are fused together to construct the saliency map. Lastly, we adopt an adaptive threshold to segment the saliency map and extract object areas. Experimental results on real SAR images indicate that the constructed saliency map is able to highlight objects and inhibit other areas effectively. Moreover, the proposed method performs well in both detection accuracy and time cost.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900464","SAR;saliency detection;PCA;global contrast;superpixel","Radar polarimetry;Feature extraction;Synthetic aperture radar;Principal component analysis;Image segmentation;Object detection;Computational modeling","feature extraction;Gaussian processes;image filtering;image fusion;image segmentation;iterative methods;object detection;principal component analysis;radar detection;radar imaging;smoothing methods;synthetic aperture radar","principle component analysis;PCA;SAR object detection;linear iterative cluster algorithm;SAR images;saliency map;detection accuracy;saliency method;synthetic aperture radar image interpretation;superpixels by the linear iterative cluster algorithm;SLIC algorithm","","3","","11","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Semantic segmentation of hyperspectral images with the fusion of LiDAR data","H. Aytaylan; S. E. Yuksel","Department of Electrical and Electronics Engineering, Hacettepe University, Ankara, Turkey; Department of Electrical and Electronics Engineering, Hacettepe University, Ankara, Turkey","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","2522","2525","Semantic segmentation is an emerging field in the computer vision community where one can segment and label an object all at once. In this paper, we propose a semantic segmentation algorithm that takes into account both the hyperspectral images and the LiDAR data. In our segmentation framework, we propose a new energy function that is composed of two terms: a unary energy term and a pairwise energy term. The unary energy term provides the segmentation maps for the hyperspectral data as well as for the LiDAR data which is explained with Fisher Vectors. The pairwise spatial term uses both the UTM coordinates as well as the LiDAR data. Finally, the system is solved with graph-cuts. We report the effect of the parameters in energy minimization and show that the best results are achieved with an SVM-MRF classifier among the several classifiers.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729651","Semantic Segmentation;Hyperspectral image classification;LiDAR;Graph Cuts;Fisher Vectors","Hyperspectral imaging;Laser radar;Image segmentation;Support vector machines;Semantics;Probabilistic logic","computer vision;graph theory;hyperspectral imaging;image classification;image fusion;image segmentation;optical radar;radar imaging;support vector machines","hyperspectral image semantic segmentation;LiDAR data fusion;computer vision community;object segmentation;object labelling;energy function;unary energy term;pairwise energy term;segmentation maps;hyperspectral data;Fisher vectors;pairwise spatial term;UTM coordinates;graph-cuts;energy minimization;SVM-MRF classifier","","3","","16","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"SAR Image Registration Based on Optimized Ransac Algorithm with Mixed Feature Extraction","F. Liao; Y. Chen; Y. Chen; Y. Lu","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; China Centre for Resources Satellite Data and Application, Beijing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1153","1156","In this paper, to address the problem in the registration of synthetic aperture radar (SAR), the speeded up robust features (SURF) approach based on multi-level FAST and improved random sampling consistency is proposed. This approach is used to address the high mismatch rate in SAR image registration, and it is based on the characteristics of SAR images. We construct a new corner detection by combining with the hierarchical theory of the system, which is referred to as multi-level FAST (MFAST). The contribution of MFAST algorithm mainly lies in the efficiency of SURF algorithm, which determines the main direction and feature descriptor. And improving the random sampling consistency (RANSAC) to remove mismatched feature points and ameliorate the registration effectiveness. The proposed algorithm is suitable for multi-sensor images with large gray differences and significant edge features. The experimental results show that the registration efficiency of the proposed algorithm is more than three times of the SURF algorithm, and the registration accuracy also is better than traditional SURF algorithm, which can reach 0.39 pixels.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323180","image registration;random sampling consistency;speeded up robust features","Radar polarimetry;Feature extraction;Satellites;Synthetic aperture radar;Approximation algorithms;Image matching;Euclidean distance","edge detection;feature extraction;image fusion;image matching;image registration;radar imaging;synthetic aperture radar","SAR image registration;optimized ransac algorithm;mixed feature extraction;synthetic aperture radar;robust features approach;multilevel FAST;random sampling consistency;high mismatch rate;SAR images;MFAST algorithm;main direction;feature descriptor;mismatched feature points;registration effectiveness;multisensor images;significant edge features;registration efficiency;registration accuracy;traditional SURF algorithm","","3","","5","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Image-based 3D model and hyperspectral data fusion for improved scene understanding","A. Ortiz; D. Rosario; O. Fuentes; S. Blair","Department of Computer Science, University of Texas at El Paso, El Paso, TX; U.S. Army Research Laboratory, MD; Department of Computer Science, University of Texas at El Paso, El Paso, TX; Headwall Photonics, Fitchburg, MA","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","4020","4023","We address the problem of automatically fusing hyperspectral data of a digitized scene with an image-based 3D model, overlapping the same scene, in order to associate material spectra with corresponding height information for improved scene understanding. The datasets have been independently collected at different spatial resolutions by different aerial platforms and the georegistration information about the datasets is assumed to be insufficient or unavailable. We propose a method to solve the fusion problem by associating Scale Invariant Feature Transform (SIFT) descriptors from the hyperspectral data with the corresponding 3D point cloud in a large scale 3D model. We find the correspondences efficiently without affecting matching performance by limiting the initial search space to the centroids obtained after performing k-means clustering. Finally, we apply the Optimal Randomized RANdom Sample Consensus (RANSAC) algorithm to enforce geometric alignment of the hyperspectral images onto the 3D model. We present preliminary results that show the effectiveness of the method using two large datasets collected from drone-based sensors in an urban setting.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127882","Hyperspectral Images;Image-based 3D Reconstruction;Data Fusion;Image Registration","Three-dimensional displays;Hyperspectral imaging;Solid modeling;Data models;Drones;Visualization","feature extraction;geophysical image processing;image fusion;image matching;image registration;image sensors;pattern clustering;pose estimation;sensor fusion;transforms","spatial resolutions;aerial platforms;scale invariant feature transform descriptors;hyperspectral images;Optimal Randomized RANdom Sample Consensus algorithm;correspondences;scale 3D model;corresponding 3D point cloud;fusion problem;georegistration information;material spectra;digitized scene;improved scene understanding;hyperspectral data","","3","","12","USGov","4 Dec 2017","","","IEEE","IEEE Conferences"
"Game theory based data fusion for precision agriculture applications","L. M. Bruce; D. Reynolds","Mississippi State University, U.S.A; Mississippi State University, U.S.A","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","3563","3566","This paper investigates the utilization of game theory models for automated analysis of hyperspectral imagery fused with other remotely sensed and/or in situ data. The author analyzes two approaches to using strategic, competitive game theory models for groundcover classification, including the application of game theory models to (i) feature-level fusion and (ii) decision fusion for hypertemporal-hyperspectral datasets. Proposed model (i) uses conflict data filtering based on mutual entropy along with the Nash equilibrium as the means to find a steady state solution. Proposed model (ii) utilizes a strategic coalition game, specifically the weighted majority game (WMG). Both models are implemented under the assumption that all players are rational. The author incorporates each of the proposed approaches, (i) and (ii), into a multi-classifier decision fusion (MCDF) system for automated ground cover classification of hyperspectral imagery collected via an unmanned airborne system (UAS) over multiple dates. The paper provides experimental results demonstrating the efficacy of the proposed game theoretic approaches, presenting significant improvements over existing methods.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729923","Game theory;hyperspectral;feature fusion;classification;decision fusion;precision agriculture","Games;Hyperspectral imaging;Biological system modeling;Nash equilibrium;Agriculture","agriculture;decision theory;entropy;feature extraction;game theory;hyperspectral imaging;image classification;image filtering;image fusion;land cover","game theory;data fusion;precision agriculture;hyperspectral imagery;feature-level fusion;conflict data filtering;mutual entropy;Nash equilibrium;strategic coalition game;weighted majority game;WMG;multiclassifier decision fusion;MCDF system;ground cover classification","","3","","16","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Registration of SAR and Optical Images by Weighted Sift Based on Phase Congruency","S. Jiang; U. Jzang; B. Wang; X. Zhu; M. Xiang; X. FU; X. Sun","Institute of Electronics Chinese Academy of Sciences, Beijing, CN; Institute of Electronics Chinese Academy of Sciences, Beijing, CN; State Key Lab. of Microwave Imaging Technol., Inst. of Electron., Beijing, China; Institute of Electronics Chinese Academy of Sciences, Beijing, CN; Institute of Electronics Chinese Academy of Sciences, Beijing, CN; Institute of Electronics Chinese Academy of Sciences, Beijing, CN; University of Chinese Academy of Sciences, Beijing, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","8885","8888","In this paper, to address problems in the registration of synthetic aperture radar (SAR) and optical images due to large gray differences, the scale-invariant feature transform (SIFT) approach based on phase congruency (PC-SIFT) is proposed. This approach is used to address the gradient inversion in multi-source images, and it is based on optimizing the dominant direction interval of the descriptors. We construct a new descriptor by combining phase consistency and the gradient amplitude, which is referred to as PCG-SIFT descriptor. The proposed algorithm is suitable for multi-sensor images with large gray differences and significant edge features The results of experiments show that compared to the traditional gradient-based SIFT descriptor, the PC-SIFT descriptor and PCG-SIFT descriptor improve the robustness and matching probability of the registration algorithm for multi-source images.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519181","image registration;optical image;scale invariant feature transform;synthetic aperture radar","Optical imaging;Synthetic aperture radar;Adaptive optics;Feature extraction;Optical sensors;Optical filters;Image edge detection","gradient methods;image colour analysis;image fusion;image registration;image sensors;optical information processing;probability;radar imaging;synthetic aperture radar;transforms","optical image registration;multisensor imaging;weighted SIFT approach;synthetic aperture radar registration;scale-invariant feature transform approach;SAR registration algorithm;multisource image gradient inversion;gradient-based SIFT descriptor;probability;PC-SIFT descriptor;PCG-SIFT descriptor;phase consistency;phase congruency","","3","","7","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Convolutional Neural Network for Natural Color Visualization of Hyperspectral Images","P. Duan; X. Kang; S. Li","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3372","3375","In this paper, a novel deep learning based visualization method is proposed for natural color visualization of hyperspectral images, which consists of the following steps. First, the spectral bands of the hyperspectral image are divided into two groups, i.e., the red, green, and blue (RGB) bands and the remaining bands. Then, a pretrained convolutional neural network (CNN) model, i.e., VGG-19, is explored to fuse the remaining bands so as to obtain a fused band with rich details. Next, the intensity-hue-saturation (IHS) transform is performed on the averaged red, green, and blue bands to obtain three different components, i.e., intensity (I), hue (H), saturation (S). Finally, the fused band is utilized to replace the intensity component followed by an inverse IHS transform. Experiments performed on two hyperspectral data sets demonstrate that the proposed method cannot only obtain a natural color resulting image, but also well preserving image details with respect to several state-of-the-art methods.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900359","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900359","Convolutional neural network;band selection;hyperspectral image visualization","Hyperspectral imaging;Image color analysis;Principal component analysis;Transforms;Visualization","convolutional neural nets;geophysical image processing;image classification;image colour analysis;image fusion;learning (artificial intelligence)","natural color visualization;hyperspectral image;deep learning based visualization method;spectral bands;pretrained convolutional neural network model;fused band;intensity-hue-saturation;averaged red bands;green bands;intensity component;hyperspectral data sets;natural color resulting image;image details;IHS","","3","","10","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Hyperspectral images classification by fusing extinction profiles feature","N. He; L. Fang; S. Li; P. Ghamisi; J. A. Benediktsson","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Signal Processing in Earth Observation, German Aerospace Center (DLR), Munich, Germany; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","2267","2270","Extinction profile (EP) is an effective feature extraction method which can well preserve the geometrical characteristics of a hyperspectral image (HSI) and by extracting the EP from first three independent components (ICs) of an HSI, three correlated and complementary groups of EP features can be constructed. In this paper, an EPs fusion (EPs-F) strategy is proposed for HSI classification by exploring spatial-spectral information within and among three EP features. In general, the EPs-F method includes two stages. In the first stage, within each EP feature, a superpixel-based composite kernel strategy is proposed to adaptively fuse the spatial information of EP and the spectral feature of HSI. Then, the obtained adaptive composite kernel is used to create a classification map for each EP. In the second stage, decision fusion is further applied on different classification maps to create the final classification result. Experiments on two real HSIs verify the effectiveness of the proposed EPs-F algorithm.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127441","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127441","Classification;hyperspectral image (HSI);extinction profiles (EPs);composite kernel;decision fusion","Feature extraction;Kernel;Support vector machines;Data mining;Integrated circuits;Hyperspectral imaging;Fuses","feature extraction;geometry;hyperspectral imaging;image classification;image fusion","EPs fusion strategy;HSI classification;EP feature;EPs-F method;spectral feature;EPs-F algorithm;hyperspectral images classification;feature extraction method;extinction profile feature;geometrical characteristics;superpixel-based composite kernel strategy;adaptive composite kernel;classification map","","3","","10","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"A Comparative Study of Fusion-Based Change Detection Methods for Multi-Band Images with Different Spectral and Spatial Resolutions","V. Ferraris; N. Yokoya; N. Dobigeon; M. Chabert","IRIT/INP-ENSEEIHT, University of Toulouse, Toulouse, France; Geoinformatics Unit, RIKEN Center for Advanced Intelligence Project, Japan; IRIT/INP-ENSEEIHT, University of Toulouse, Toulouse, France; IRIT/INP-ENSEEIHT, University of Toulouse, Toulouse, France","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5021","5024","This paper deals with a fusion-based change detection (CD) framework for multi-band images with different spatial and spectral resolutions. The first step of the considered CD framework consists in fusing the two observed images. The resulting fused image is subsequently spatially or spectrally degraded to produce two pseudo-observed images, with the same resolutions as the two observed images. Finally, CD can be performed through a pixel-wise comparison of the pseudo-observed and observed images since they share the same resolutions. Obviously, fusion is a key step in this framework. Thus, this paper proposes to quantitatively and qualitatively compare state-of-the-art fusion methods, gathered into four main families, namely component substitution, multi-resolution analysis, unmixing and Bayesian, with respect to the performance of the whole CD framework evaluated on simulated and real images.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517712","Change detection;hyperspectral and multispectral imaging;fusion;heterogeneous sensors","Spatial resolution;Fuses;Bayes methods;Hyperspectral imaging;Optical sensors","image fusion;image resolution","spectral resolutions;spatial resolutions;component substitution;unmixing;multiresolution analysis;pseudoobserved images;fusion-based change detection framework;multiband images;fusion-based change detection methods","","2","","14","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Sub-block PCA-wavelet image sharpening approach for hyperspectral images","J. Sun; Q. Lv; Z. Tan; J. Yin","Academy of Opto-Electronics, Chinese Academy of Sciences, Beijing, China; Academy of Opto-Electronics, Chinese Academy of Sciences, Beijing, China; Academy of Opto-Electronics, Chinese Academy of Sciences, Beijing, China; School of Astronautics, Beihang University, Beijing, China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","3310","3313","One of the most crucial issues to judge image quality is the spatial resolution. Hyperspectral image (HSI) sharpening is the process of combining spatial information to enhance spatial resolution of HSIs. Huge volumes of HSI data cause difficulties during the sharpening process. This paper proposed a practical and effective strategy to deal with HSI sharpening. We utilized sub-block method and combined PCA and wavelet fusion approaches to achieve the proposed scheme. Sub-block method helped reduce the calculation complexity and promote the efficiency. PCA and wavelet image sharpening contributed to enhance spatial resolution of HSI with less spectral distortion. The experiment demonstrated an efficient processing result and a good visualized effect. Qualitative and quantitative assessments were both used to evaluate the proposed approach.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326526","Image sharpening;PCA;Wavelet;Sub-block method","Principal component analysis;Hyperspectral imaging;Spatial resolution;Correlation;Correlation coefficient","geophysical image processing;hyperspectral imaging;image fusion;image resolution;principal component analysis;terrain mapping","hyperspectral image;subblock PCA-wavelet image sharpening approach;image quality;spatial image resolution;spatial information;HSI data;sharpening process;wavelet fusion approach;spectral distortion;spatial HSI resolution;visualized effect","","2","","11","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Spectral Detection on a Manifold for Finding Undersea Objects","A. Schaum",U.S. Naval Research Laboratory,"IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","4253","4256","We describe a mathematical methodology for finding closed-form solutions to a class of manifold detection problems. The approach exploits the general principles of clairvoyant fusion to produce robust algorithms. The fusion technique requires solving a maximization problem equivalent to the logical ORing of an infinite number of detectors. These clairvoyants are individually optimal, but depend on parameters with potentially unknown values. The particular choice of problem class we examine is motivated by the detection of a spectrum that has been modified in traversing ocean water for an unknown distance. We solve a prototype model in detail to illustrate some of the issues that must be faced in a more ambitious problem definition.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518965","Hyperspectral detection;Manifolds;Clairvoyant Fusion;Undersea detection","Manifolds;Detectors;Closed-form solutions;Mathematical model;Oceans;Probability density function;Ellipsoids","image fusion;object detection;optimisation;spectral analysis","undersea objects;spectral detection;logical ORing;maximization problem;fusion technique;clairvoyant fusion;manifold detection problems;closed-form solutions;mathematical methodology","","2","","4","USGov","4 Nov 2018","","","IEEE","IEEE Conferences"
"Multiscale superpixel-based fusion framework for hyperspectral image classification","S. Jia; X. Deng; K. Wu","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China","2018 Fifth International Workshop on Earth Observation and Remote Sensing Applications (EORSA)","3 Jan 2019","2018","","","1","5","Since it is usually difficult and time-consuming to obtain sufficient labeled samples in practice, the samll number of sample is one of the challenging issue for hyperspectral image classifiction. Fortunately, due to the spatial correlation of the surface of the materials, it is feasible to improve classification performance from the perspective of superpixel. In this paper, a multiscale superpixel-based fusion (MSSF) framework has been proposed for hyperspectral image classification. First, several superpixel maps of different scales are generated from the hyperspectral images. Then, calculate the spectral mean of superpixels and each class of training sample, the label of the superpixel is determined by the label of the training sample which distance is closest to the superpixel. Finally, we use majority voting strategy to determine the final label of each pixel. The experimental results on two real HSI data sets show that the performance of the proposed framework is better than support vector machine(SVM), K nearest neighbor(KNN), sparse representation-based classifier(SRC).","","978-1-5386-6642-5","10.1109/EORSA.2018.8598612","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598612","Multiscale superpixel;majority voting;hyperspectral images classification.","Hyperspectral imaging;Training;Entropy;Image segmentation;Conferences;Earth","geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;learning (artificial intelligence);pattern classification;support vector machines","sufficient labeled samples;classification performance;multiscale superpixel-based fusion framework;hyperspectral image classification;superpixel maps;hyperspectral images;training sample;final label;SRC;sparse representation-based classifier;MSSF;support vector machine;SVM;k nearest neighbor;KNN","","1","","17","IEEE","3 Jan 2019","","","IEEE","IEEE Conferences"
"Decision fusion of pixel-level and superpixel-level hyperspectral image classifiers","T. Lu; S. Li; L. Fang","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","1524","1527","In this paper, a decision fusion of pixel-level and superpixel-level classifiers (DFPSC) for the HSI is proposed. First, the support vector machine based classification probability combined with the local spatial information is introduced to classify the HSI in a pixel-by-pixel manner. Then, the HSI is over-segmented into non-overlapping superpixels. Each superpixel contains spatially-connected and spectrally-similar pixels, which are assigned to the same label via joint sparse regularization. Finally, a guided map is generated based on the edge map and superpixel map, which is used to guide the fusion of both pixel-level and superpixel-level classification results. With the proposed decision fusion scheme, the classification results in homogeneous and structural areas can be better balanced, leading to the improvement of the overall classification accuracy. The experimental results demonstrate the superiority of the proposed method over some well-known classification methods.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729389","hyperspectral image;classification;support vector machine;superpixel;decision fusion","Support vector machines;Training;Hyperspectral imaging;Noise measurement;Image edge detection","geophysical image processing;hyperspectral imaging;image classification;image fusion;probability;support vector machines","decision fusion of pixel-level and superpixel-level hyperspectral image classifiers;DFPSC;HSI;support vector machine based classification probability;local spatial information;nonoverlapping superpixels;spatially-connected pixel;spectrally-similar pixel;joint sparse regularization;guided map;edge map;superpixel map;superpixel-level classification;homogeneous areas;structural areas","","1","","8","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Statistical Fusion-Based Transfer Learning for Hyperspectral Image Classification","X. Liu; S. Jia; M. Xu; J. Zhu","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","1108","1111","Hyperspectral images (HSIs) has practical applications in many fields. In practical scenarios, machine learning often fails to handle changes between training (source) and testing (target) input distributions due to domain shifts. A big challenge in hyperspectral image classification is the small size of labeled data for classifier learning. We always face the situation that an HSI scene is not labeled all or with very limited number of labeled pixels, but we have sufficient labeled pixels in another HSI scene with similar land cover classes. In this paper, we propose a simple and effective method for domain adaptation called statistical fusion to minimize domain shifts by aligning the second-order and fourth-order statistics of source and target distributions. After two hyperspectral scenes are transformed into the similar property-space, any traditional HSI classification approaches can be used, and experimental results have validated the generalization of the proposed method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898515","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898515","Hyperspectral image classification;domain adaptation;transfer learning","Training;Hyperspectral imaging;Predictive models;Testing;Task analysis;Correlation","geophysical image processing;hyperspectral imaging;image classification;image fusion;land cover;learning (artificial intelligence);statistical analysis","HSI classification;statistical fusion;hyperspectral image classification;machine learning;classifier learning;HSI scene;land cover classes;domain adaptation;fourth-order statistics;hyperspectral scenes;transfer learning;second-order statistics","","1","","10","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Gabor feature based dictionary fusion for hyperspectral imagery classification","S. Jia; J. Hu; G. Tang; L. Shen; L. Deng","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Information Engineering, Shenzhen University, Shenzhen, China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","433","436","Multiple kinds of features extracted from hyperspectral imagery (HSI) have shown great potential for pixel-oriented classification. However, two difficulties can be encountered during the classification process. Firstly, it is time consuming to directly utilize the large amount of features. Secondly, because each kind of feature is usually processed individually, the high-level relationship among different features is not completely configured, decreasing the performance eventually. In this paper, a new strategy to fuse the features and exploit dictionary learning for HSI classification is proposed. Based on the high-level relationship, the extracted Gabor features have been integrated into a more compact and more discriminative representation through a Fisher-based criterion. Experimental results have shown that the fused features can not only produce competitive performance for HSI classification, but also greatly reduce the computational complexity.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325793","HSI classification;sparse coding;dictionary fusion","Dictionaries;Feature extraction;Hyperspectral imaging;Training;Three-dimensional displays;Yttrium","feature extraction;Gabor filters;geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;image fusion","HSI classification;computational complexity;competitive performance;fused features;Fisher-based criterion;high-level relationship;HSI classification;high-level relationship;classification process;pixel-oriented classification;gabor feature extraction;dictionary fusion;hyperspectral imagery classification","","1","","10","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Decision Fusion of Spot6 And Multitemporal Sentinel2 Images For Urban Area Detection","C. Wendl; A. L. Bris; N. Chehata; A. Puissant; T. Postadjian","LASTIG MATIS, IGN, ENSG, Univ. Paris-Est; LASTIG MATIS, IGN, ENSG, Univ. Paris-Est; LASTIG MATIS, IGN, ENSG, Univ. Paris-Est; CNRS UMR 7362 LlVE-Universite de Strasbourg; LASTIG MATIS, IGN, ENSG, Univ. Paris-Est","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1734","1737","Fusion of very high spatial resolution multispectral (VHR) images and lower spatial resolution image time series with more spectral bands can improve land cover classification' combining geometric and semantic advantages of both sources. This study presents a workflow to extract the extent of urban areas using decision -level fusion of individual classifications on Sentine12 (S2) and SPOT6 satellite images. First, both sources are classified individually in five classes, using state-of-the-art supervised classification approaches and Convolutional Neural Networks. Obtained results are merged in order to extract buildings as accurately as possible. Then, detected buildings are merged again with the S2 classification to extract urban area; a prior to be in an urban area is derived from these building objects and merged with a binary classification derived from the original S2 classification. Both fusions involve a per pixel decision level fusion followed by a contrast sensitive regularization.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517476","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517476","Multispectral;Decision fusion;Regularization;Urban classification;Urban area;Segmentation","Buildings;Urban areas;Radio frequency;Roads;Support vector machines;Time series analysis;Spatial resolution","feature extraction;geophysical image processing;image classification;image fusion;image resolution;neural nets;terrain mapping;time series","decision fusion;multitemporal sentinel2 images;urban area detection;high spatial resolution multispectral images;lower spatial resolution image time series;spectral bands;land cover classification;geometric advantages;semantic advantages;decision -level fusion;SPOT6 satellite images;classification approaches;Convolutional Neural Networks;binary classification;original S2 classification;pixel decision level fusion;buildings","","1","","5","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Hyperspectral Pansharpening via Multitask Joint Sparse Representation","J. Liu; Z. Wu; Z. Xiao; J. Yang","School of Internet of Things Engineering, Jiangnan University, Wuxi, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Internet of Things Engineering, Jiangnan University, Wuxi, China; School of Internet of Things Engineering, Jiangnan University, Wuxi, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","7192","7195","In this paper, a high spatial resolution (HR) hyperspectral image is inferred from a low spatial resolution (LR) hyperspectral image and a HR panchromatic image by taking advantage of the sparse representation pansharpening (SRP) method. Different from the conventional SRP or joint SRP (JSRP) method, this paper proposes a multitask JSRP method for hyperspectral pansharpening, in order to improve the generalization performance of the model. First, multiple HR/LR dictionary pairs are generated by partitioning the multiple features of the panchromatic image and their corresponding downsampled LR versions into patches. Second, the patch-level sparse representation coefficients of the multiple LR hyperspectral image features are jointly estimated under the multiple LR dictionaries. Finally, the estimated sparse representation coefficients are utilized to reconstruct the HR patches under the original HR dictionary, and the desired HR hyperspectral image is obtained by aggregating the HR patches. Experimental results conducted on two hyperspectral scenes demonstrate the effectiveness of the proposed method.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518362","Hyperspectral pansharpening;sparse representation;multi-task learning","Hyperspectral imaging;Dictionaries;Spatial resolution;Feature extraction;Image reconstruction","feature extraction;geophysical image processing;geophysical techniques;hyperspectral imaging;image enhancement;image fusion;image reconstruction;image representation;image resolution","hyperspectral pansharpening;multitask joint sparse representation;high spatial resolution hyperspectral image;low spatial resolution hyperspectral image;HR panchromatic image;sparse representation pansharpening method;multitask JSRP method;patch-level sparse representation coefficients;multiple LR hyperspectral image features;multiple LR dictionaries;estimated sparse representation coefficients;HR patches;desired HR hyperspectral image;hyperspectral scenes;SRP method;downsampled LR versions;multiple HR-LR dictionary pairs;LR hyperspectral image;HR dictionary","","1","","15","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"An enhanced spatial and temporal adaptive reflectance fusion model based on optimal window","B. Ping; Y. Meng; F. Su","Institute of Surface-Earth System Science, Tianjin University, Tianjin, China; National Marine Information Center, Tianjin, China; LREIS, University of the Chinese Academy of Sciences, Beijing, China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","3377","3380","In this paper, we introduce an enhanced spatial and temporal adaptive reflectance fusion model based on optimal sub-window size. The sub-window size can affect the accuracy of fusion and instead of using the fixed sub-window size in the original STARFM algorithm, the proposed algorithm uses the density of similar pixels to determine the optimal sub-window size. Compared with the original STARFM algorithm, the proposed algorithm can enhance the accuracy of fusion.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127722","ndex Term-STARFM;data fusion;data interpolation","Bibliographies","geophysical image processing;image fusion;image resolution","temporal adaptive reflectance fusion model;fixed sub-window size;original STARFM algorithm;spatial adaptive reflectance fusion model;optimal subwindow size","","1","","8","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"A method for extracting InSAR image features of negative and positive obstacles","Z. Jiang; Q. Song; J. Wang; Z. Zhou","College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","1170","1173","Radar sensors have received more and more interest for unmanned ground vehicle to sense positive and negative obstacles in unstructured environments or out fields, especially on negative obstacle. In this paper, we present an approach for extracting the features of obstacles from radar images. Based on interferometric synthetic aperture radar (InSAR) images focused by the back-projection (BP) algorithm, range compensation, speckle filtering and threshold segmentation are performed. And morphological operations are used to perform some simple connectivity filtering to smooth the image and remove spurious pixels. Finally, feature fusion is applied to the amplitude and the correlation coefficient images. Both the theoretical analysis and the experimental results indicate that the proposed method is an efficient method.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729296","Positive and negative obstacles;interferometric synthetic aperture radar (InSAR);range compensation;threshold segmentation;morphological operations","Radar imaging;Coherence;Filtering;Image segmentation;Speckle;Feature extraction","feature extraction;image filtering;image fusion;image segmentation;radar imaging;radar interferometry;radar receivers;remotely operated vehicles;synthetic aperture radar","InSAR image feature extraction;negative obstacle;positive obstacle;radar sensors;unmanned ground vehicle;interferometric synthetic aperture radar image;back-projection algorithm;range compensation;speckle filtering;threshold segmentation;morphological operation;spurious pixel removal;feature fusion;correlation coefficient image filtering","","1","","11","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"A Novel Approach for Hyperspectral Image Superresolution Using Spectral Unmixing and Transfer Learning","J. R. Patel; M. V. Joshi; J. S. Bhatt","Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar, India; Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar, India; Indian Institute of Information Technology, Vadodara, India","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1512","1515","Hyperspectral image (HSI) Super-resolution (SR) methods enhance the spatial resolution. In this paper, we propose a novel SR approach for HSIs by making use of spectral unmixing and transfer learning. We first train a deep convolutional neural network (CNN) to learn the mapping between the low-resolution (LR) and high-resolution (HR) natural images and use the same for transfer learning to get the initial estimates of the super-resolved abundances where the input corresponds to LR abundances. To get the better estimates of abundances and hence improve the SR of HSIs, we use a regularization framework in which both the LR and HR abundances are modelled as Inhomogeneous Gaussian Markov field (IGMRF) that serves as the prior. Finally, the SR HSIs are obtained by using a linear mixing model that uses the SR abundances and the endmembers estimated using an appropriate technique. Experiments on synthetic as well as on real HSIs show that the proposed method performs better when compared to other existing approaches. The advantages of the proposed approach are: 1. The method do not require auxiliary image as used in many of the existing methods, 2. Spectral details are better preserved since the SR is carried out in abundance domain, 3. Computational complexity is reduced since the SR is carried out on abundances which are few in number when compared to HSIs.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324059","Convolutional neural network (CNN);Deep learning;Hyperspectral image (HSI) super-resolution;Inhomogeneous Gaussian Markov field (IGMRF)","Spatial resolution;Superresolution;Optimization;Signal resolution;Indexes;Hyperspectral imaging;Training","Gaussian processes;geophysical image processing;geophysical signal processing;hyperspectral imaging;image classification;image fusion;image resolution;learning (artificial intelligence);Markov processes;neural nets","spectral unmixing;transfer learning;deep convolutional neural network;low-resolution;initial estimates;super-resolved abundances;LR abundances;Inhomogeneous Gaussian Markov field;SR HSIs;linear mixing model;SR abundances;auxiliary image;spectral details;abundance domain;hyperspectral image superresolution;hyperspectral image Super-resolution methods;spatial resolution;novel SR approach","","","","21","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Multi-View Fusion Based on Expectation Maximization for SAR Target Recognition","Y. Zhang; X. Guo; H. Ren; Q. Wan; X. Shen","University of Electronic and Science Technology of China, Chengdu, China; University of Electronic and Science Technology of China, Chengdu, China; University of Electronic and Science Technology of China, Chengdu, China; University of Electronic and Science Technology of China, Chengdu, China; University of Electronic and Science Technology of China, Chengdu, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","778","781","Images from different aspect views for one target, known as multiple views, are widely applied to improve synthetic aperture radar (SAR) target recognition. However, most of existing multi-view methods have strict constraint on the angle interval among multiple views. In this paper, a new multiview fusion method free from interval limitation using expectation maximization (EM) is explored for SAR image classification. Firstly, we apply convolutional neural network (CNN) to extract features effectively owning to its powerful ability of feature learning and then obtain the classification probability. Secondly, Multi-view Label Set (MLS) is automatically constructed from multiple views according to the probability and finally we use EM algorithm to classify SAR images intelligently. It is worth noting that the proposed method can be used flexibly according to the number of perspectives obtained and without angle interval constraint among multiple views. Experiments demonstrate that the proposed method has better recognition performance than some state-of-the-art methods on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323545","National Natural Science Foundation of China(grant numbers:61371184,61671137,61771114); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323545","Expectation maximization;synthetic aperture radar;Multi-view Label Set","Target recognition;Reliability;Feature extraction;Synthetic aperture radar;Testing;Radar polarimetry;Prediction algorithms","feature extraction;image classification;image fusion;image recognition;learning (artificial intelligence);neural nets;object recognition;radar imaging;radar target recognition;synthetic aperture radar","expectation maximization;SAR image classification;Multiview Label Set;SAR images;angle interval constraint;SAR Target Recognition;different aspect views;synthetic aperture radar target recognition;existing multiview methods;multiview fusion method free","","","","8","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Decision Fusion Based on Joint Low Rank and Sparse Component for Hyperspectral Image Classification","F. Li; W. Li; H. Huo; Q. Ran","College of Information Technology and Cyber Security, People’s Public Security University of China, Beijing, China; College of Information Science and Technology, Beijing Institute of Technology, Beijing, China; College of Information Technology and Cyber Security, People’s Public Security University of China, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","401","404","Sparse and low rank matrix decomposition is a method that has recently been developed for estimating different components of hyperspectral data. The rank component is capable of preserving global data structures of data, while a sparse component can select the discriminative information by preserving details. In order to take advantage of both, we present a novel decision fusion based on joint low rank and sparse component (DFJLRS) method for hyperspectral imagery in this paper. First, we analyzed the effects of different components on classification results. Then a novel method adopts a decision fusion strategy which combines a SVM classifier with the information provided by joint sparse and low rank components. With combination of the advantages, the proposed method is both representative and discriminative. The proposed algorithm is evaluated using several hyperspectral images when compared with traditional counterparts.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8897839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8897839","Hyperspectral Imagery;Pattern Classification;Decision fusion;Sparse and low rank matrix decomposition","Hyperspectral imaging;Support vector machines;Sparse matrices;Matrix decomposition;Principal component analysis;Feature extraction","data structures;hyperspectral imaging;image fusion;matrix decomposition;pattern classification;support vector machines","hyperspectral image classification;low rank matrix decomposition;hyperspectral data;rank component;discriminative information;decision fusion based on joint low rank and sparse component method;DFJLRS;SVM classifier;data structures","","","","13","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Super-resolution of infrared images based on hierarchical distillation network","W. Cai; B. Jiang; X. Jiang","Xi’an Research Institute of High Technology, Xi’an, Shaanxi, China; Xi’an Research Institute of High Technology, Xi’an, Shaanxi, China; Xi’an Research Institute of High Technology, Xi’an, Shaanxi, China","2022 3rd International Conference on Geology, Mapping and Remote Sensing (ICGMRS)","18 Aug 2022","2022","","","756","761","Aiming at the problem of low image resolution that easily occurs in the process of infrared images acquisition, this paper proposes a novel hierarchical distillation network to achieve infrared images super-resolution. By designing a cascaded residual distillation module, the negative impact of the over-deep network model is reduced; meanwhile, a dual-path feature fusion module is constructed to further enhance the feature expression capability of the network model. Experiments were conducted on public datasets and evaluated using two evaluation metrics, Peak Signal-to-Noise Ratio (PSNR) and Structure Similarity Index Measure (SSIM). The experimental results show that the method in this paper improves 1.97 and 0.033 in PSNR and SSIM, respectively, compared with RCAN, and generates images with high definition, strong structure and rich detail information.","","978-1-6654-8595-1","10.1109/ICGMRS55602.2022.9849268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9849268","super-resolution;infrared images;convolutional neural network;attention mechanism","Measurement;PSNR;Fuses;Geology;Superresolution;Indexes;Image reconstruction","convolutional neural nets;deep learning (artificial intelligence);image denoising;image fusion;image resolution;infrared imaging","infrared image super-resolution;cascaded residual distillation module;deep network model;feature expression capability;evaluation metrics;peak signal-to-noise ratio;structure similarity index measure;infrared image acquisition;hierarchical distillation network;dual-path feature fusion module;PSNR;SSIM","","","","16","IEEE","18 Aug 2022","","","IEEE","IEEE Conferences"
"Multi-frequency and multi-resolution EO images for Smart Asset Management","A. Brunetti; M. Gaeta; P. Mazzanti","NHAZCA S.r.l., startup of “Sapienza” University of Rome, Rome, Italy; NHAZCA S.r.l., startup of “Sapienza” University of Rome, Rome, Italy; NHAZCA S.r.l., startup of “Sapienza” University of Rome, Rome, Italy","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","5192","5195","This paper describes some of the data fusion methods developed by NHAZCA S.r.l. in the frame of the project “MUSAR”, funded by ASI, for the integration of data from multi-sensor/multiband satellite images. The aim of MUSAR is to extend the exploitation of EO data in the research area of natural hazard, with a specific focus on their interaction and interference with structures and infrastructures. The proposed methods are based on the post-processing of results achieved from InSAR and A-DInSAR analyses and by Photomonitoring techniques. Some preliminary results on real case studies are also presented.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883325","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883325","","Solid modeling;Three-dimensional displays;Satellites;Space missions;Licenses;Asset management;Pressure measurement","asset management;geophysical image processing;geophysical techniques;image fusion;radar imaging;radar interferometry;synthetic aperture radar","photomonitoring techniques;A-DInSAR analysis;multisensor satellite images;multiband satellite images;MUSAR;data fusion methods;smart asset management;multiresolution EO images;multifrequency;interference;natural hazard;EO data;ASI","","","","6","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Spectral-Driven Pansharpening Using Adaptive Image Segmentation to Reduce Color Distortion","J. Jiao; X. Gong; L. Wu; X. Meng","Science and Technology on Complex Electronic System Simulation Laboratory, Space Engineering University, Beijing, China; Science and Technology on Complex Electronic System Simulation Laboratory, Space Engineering University, Beijing, China; Science and Technology on Complex Electronic System Simulation Laboratory, Space Engineering University, Beijing, China; Science and Technology on Complex Electronic System Simulation Laboratory, Space Engineering University, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3133","3136","Extraction and injection of spatial details from the panchromatic (PAN) image and preservation of the spectral characteristics of the multispectral (MS) image are the key issues in pansharpening methods, which are of great significance for the application of image interpretation and target recognition. In this paper, a spectral-driven pansharpening method in which the injection detailed coefficients are estimated over each component segmented by the adaptive K-means algorithm is proposed. The pansharpening method relies on a multi-resolution framework, generalized Laplacian pyramid (GLP) technique, which is applied for the extraction of detail image. The fused image is used as the feedback element, its distance from the original MS image is used to adjust the number of segments adaptively so as to reduce the spectral distortion. Experiments carried out on GeoEye-1 and QuickBird data sets demonstrate the efficiency and effectiveness of our proposed method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900146","Context-adaptive algorithm;fusion of multispectral and panchromatic images;image segmentation;adaptive K-means","Image segmentation;Spatial resolution;Satellites;Estimation;Nonlinear distortion","image colour analysis;image fusion;image recognition;image resolution;image segmentation;object recognition","adaptive image segmentation;color distortion;spectral characteristics;multispectral image;image interpretation;target recognition;spectral-driven pansharpening method;injection detailed coefficients;adaptive K-means algorithm;multiresolution framework;generalized Laplacian pyramid technique;fused image;original MS image;spectral distortion;GeoEye-1;QuickBird data set","","","","15","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Research on Building Extraction method based on Object-oriented and ArcGIS Engine","J. Huang; P. Li; W. Wang; Y. Pei","Chinese Academy of Geological Sciences, Institute of Geomechanics, Beijing, China; School of Recreation, Sport and Tourism Beijing Sport University, Beijing, China; Chinese Academy of Geological Sciences, Institute of Geomechanics, Beijing, China; Chinese Academy of Geological Sciences, Institute of Geomechanics, Beijing, China","2022 3rd International Conference on Geology, Mapping and Remote Sensing (ICGMRS)","18 Aug 2022","2022","","","483","488","Building extraction is of great significance to urban planning and geographic data updating. In this paper, a building extraction method based on object-oriented and ArcGIS Engine is proposed. A community in Zhuhai city, Guangdong Province is taken as the experimental area to carry out the relevant research on building extraction. Firstly, SPOT-5 fusion image was selected to obtain the building vector data draft after small scale segmentation, large scale combination, knowledge rule construction, morphological repair and edge detection. Then, based on ArcGIS Engine platform, the building digitization system is developed. According to the different forms of the building, the methods of hand tracking digitization, node tracking digitization and model digitization are respectively used to extract the building vector data accurately. The experiment shows that buildings extracted by this method are clear and complete, with high precision and fast speed, which greatly improves the automation and intelligence level of building extraction.","","978-1-6654-8595-1","10.1109/ICGMRS55602.2022.9849324","Chinese Academy of Geological Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9849324","object-oriented;ArcGIS Engine;ENVI;digitization;Building extraction","Image segmentation;Object oriented modeling;Image edge detection;Geology;Buildings;Urban planning;Maintenance engineering","edge detection;feature extraction;geographic information systems;geophysical image processing;image fusion;image segmentation;town and country planning","object-oriented;building vector data draft;ArcGIS Engine platform;building digitization system;node tracking digitization;model digitization;buildings;building extraction method","","","","14","IEEE","18 Aug 2022","","","IEEE","IEEE Conferences"
"Dense 3D Model Reconstruction for Digital City Using Computationally Efficient Multi-View Stereo Networks","Y. Hu; Z. Liu; T. Fu; M. -O. Pun","School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China; Shenzhen Research Institute of Big Data, Shenzhen, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","959","962","Deep learning has shown promising results on dense three-dimensional (3D) model reconstruction from RGB images in recent years. However, the reconstruction of large-scale 3D models required for digital city remains very challenging even for such deep learning based methods. In this paper, we propose a convolutional neural network (CNN)-based Multi-View-Stereo (MVS) method that uses a double U-Net approach searching for image features. The proposed network first utilizes a double U-Net to extract the image features of a coarser resolution for the sake of reduced memory requirements. After that, the cost volume is built via the differentiable homography warping. The cascade structure is designed to extract the information in a small-scale cost volume before a large-scale cost volume carries out fusion and finer depth map estimation. As a result, the proposed network can efficiently produce highly accurate 3D point clouds using a fraction of the GPU memory and runtime required by conventional methods. Extensive experiments on the DTU benchmarks as well as the Tanks and Temples benchmarks confirm that the proposed network can achieve outstanding reconstruction accuracy and model completeness.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883484","","Deep learning;Solid modeling;Costs;Three-dimensional displays;Computational modeling;Urban areas;Memory management","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image colour analysis;image fusion;image reconstruction;solid modelling;stereo image processing","large-scale 3D models;digital city;deep learning based methods;U-Net approach;image features;double U-Net;reduced memory requirements;small-scale cost volume;large-scale cost volume;highly accurate 3D point clouds;reconstruction accuracy;model completeness;dense 3D model reconstruction;three-dimensional model reconstruction;RGB images;depth map estimation;convolutional neural network-based multiview-stereo method;DTU benchmarks","","","","5","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Assessment of the relevance of information derived from the unmixing of polarimetric radar images","S. Giordano; G. Mercier; J. -P. Rudant","IGN-MATIS Lab, Universite Paris Est, France; Institut Mines-Telecom/Telecom Bretagne, France; IGN-MATIS Lab, Universite Paris Est, France","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","3778","3781","A new method to unmix radar polarimetric images with optical images was proposed. This method has pointed out that the unmixing model is able to split off polarimetric information on a land cover type basis. In this paper unmixed radar polarimetric images obtained are compared with the observed ones in non-mixed conditions. Then, Cloude and Pottier decomposition is performed on the unmixed and observed radar images to asses whether the understanding of physical scattering mechanisms is improved with the unmixing. Finally, a classification experiment is designed to determine whether this fusion framework make the transfer of information from the optical images to the unmixed radar images possible.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326646","unmixing;radar polarimetry;polarimetric decomposition","Laser radar;Optical imaging;Radar imaging;Radar polarimetry;Optical scattering;Optical sensors","image fusion;radar imaging;radar polarimetry","information relevance;polarimetric radar image unmixing;optical images;polarimetric information;Cloude-Pottier decomposition;fusion framework","","","","5","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"An interpolation-free FFBP algorithm for spotlight SAR processing","Q. Dong; P. Shao; Z. Yang; Y. Li; M. Xing","National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","4498","4501","In this paper, an interpolation-free fast factorized back-projection (IF-FFBP) algorithm is proposed for high-resolution spotlight synthetic aperture radar (SAR) processing. Different from the original FFBP utilizing two-dimensional image-domain interpolation for sub-aperture fusion, IF-FFBP finishes the image merging steps using chirp-z transform and circular shifting. Under the restriction of the applicable scope, IF-FFBP yields enhanced efficiency over the 4 times upsampling interpolation based FFBP, and keeps the high precision simultaneously. Finally, Real-data experiment verifies the efficiency superiorities of the FIM-FFBP.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326827","SAR;FFBP;interpolation-free","Interpolation;Azimuth;Algorithm design and analysis;Apertures;Synthetic aperture radar;Image resolution;Signal processing algorithms","image fusion;image resolution;interpolation;radar imaging;radar resolution;synthetic aperture radar;Z transforms","interpolation-free FFBP algorithm;high-resolution spotlight SAR processing;IF fast factorized back-projection algorithm;high-resolution spotlight synthetic aperture radar processing;two-dimensional image-domain interpolation;subaperture fusion;chirp-z transform;circular shifting;upsampling interpolation","","","","2","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Semantic Labeling for High-Resolution Aerial Images Based on the DMFFNet","Z. Cao; W. Diao; Y. Zhang; M. Yan; H. Yu; X. Sun; K. Fu","Institute of Electronic, Chinese Academy of Sciences, Beijing, China; Institute of Electronic, Chinese Academy of Sciences, Beijing, China; Institute of Electronic, Chinese Academy of Sciences, Beijing, China; Institute of Electronic, Chinese Academy of Sciences, Beijing, China; Institute of Electronic, Chinese Academy of Sciences, Beijing, China; Institute of Electronic, Chinese Academy of Sciences, Beijing, China; Institute of Electronic, Chinese Academy of Sciences, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","1021","1024","Semantic labeling in high-resolution aerial images is important for its wide range of applications. In this paper, we propose an end-to-end dual multi-scale feature fusion network (DMFFNet) for high-resolution aerial multi-source images. DMFFNet aims to further improve the semantic labeling results of the region where the multispectral features are indistinguishable. Specifically, we design a channel fusion strengthen (CFS) module, which can fuse features adaptively by modelling interdependencies between channels. Furthermore, a multiscale context aggregation (MCA) module is utilized to obtain larger receptive field and more contextual information. The experiment results confirm the DMFFNet with CFS and MCA improve the semantic labeling performance by utilizing multi-source data.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900306","semantic labeling;deep learning;high-resolution aerial images;convolutional neural networks","Semantics;Labeling;Feature extraction;Convolution;Computer vision;Image segmentation;Fuses","feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image resolution;image segmentation;neural nets","multispectral features;DMFFNet;semantic labeling performance;multisource data;high-resolution aerial multisource images;channel fusion strengthen;feature fusion;multiscale context aggregation;dual multiscale feature fusion network","","","","9","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Joint Spatial and Graph Convolutional Neural Networks - A Hybrid Model for Spatial-Spectral Geospatial Image Analysis","F. F. Shahraki; S. Prasad","Department of Electrical and Computer Engineering, University of Houston, USA; Department of Electrical and Computer Engineering, University of Houston, USA","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","4003","4006","How to efficiently exploit useful information from hyperspectral data by joint analysis of spectral and spatial information is an important problem. In this work, we demonstrate a fusion network that can leverage recent developments in graph convolutional neural networks (GCNs) to effectively analyze reflectance spectra of hyperspectral image pixels and two-dimensional convolutional neural networks (CNNs) which can extract object-specific spatial characteristics from hyperspectral image. To design this fusion model, we study both decision fusion and deep feature fusion approaches and show that the fusion approach outperforms other traditional deep learning methods such as variants of CNNs.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323658","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323658","HSI classification;Graph CNNs;Feature Fusion;Decision Fusion;Spatial-Spectral Feature","Two dimensional displays;Feature extraction;Hyperspectral imaging;Training;Convolutional neural networks;Convolution;Task analysis","convolution;feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);neural nets","joint spatial;graph convolutional neural networks;hybrid model;spatial-spectral geospatial image;hyperspectral data;joint analysis;spectral information;spatial information;fusion network;leverage recent developments;hyperspectral image pixels;two-dimensional convolutional neural networks;object-specific spatial characteristics;fusion model;fusion approach","","","","13","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A novel algorithm for tridimensional reconstruction using data from low-cost sensors","I. E. Villalon-Turrubiates; A. Barrera-Pelayo","Instituto Tecnológico y de Estudios Superiores de Occidente (ITESO), Universidad Jesuita de Guadalajara, Tlaquepaque, Jalisco, México; Instituto Tecnológico y de Estudios Superiores de Occidente (ITESO), Universidad Jesuita de Guadalajara, Tlaquepaque, Jalisco, México","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","3014","3017","A novel algorithm to generate a tridimensional reconstruction of an object using a series of images obtained with sensors within low-cost cameras is proposed. As a matter of particular study, this paper present the methodology employed to set an array of sensors to extract the necessary information from a particular group of acquired images surrounding the sample, the processing schema for its interpretation and its mapping, in order to approximate a tridimensional model with the use of real data. The simulation results can verify the efficiency of the proposed approach, showing an application where it could be a useful tool for decision support or resource management.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326450","Image Processing;Optical Sensor;Array Signal Processing;Remote Monitoring","Cameras;Image reconstruction;Accuracy;Sensor arrays;CMOS integrated circuits","array signal processing;cameras;image fusion;image reconstruction;image sensors;image sequences","low-cost sensor data;tridimensional object reconstruction;image series;low-cost cameras;sensor array;information extraction;decision support;resource management","","","","7","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Information Fusion of GF-1 and GF-4 Satellite Imagery for Ship Surveillance","Y. Liu; P. Guo; L. Cao; M. Ji; L. Yao","National Innovation Institute of Defense Technology, Academy of Military Science, Beijing, China; National Innovation Institute of Defense Technology, Academy of Military Science, Beijing, China; National Innovation Institute of Defense Technology, Academy of Military Science, Beijing, China; National Innovation Institute of Defense Technology, Academy of Military Science, Beijing, China; Institute of Information Fusion, Naval Aviation University, Yantai, ShangDong, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","5044","5047","Gaofen-4 (GF-4) satellite is the first high-resolution geostationary orbit (GEO) optical satellite in the world, which has appealing potential in wide-area maritime surveillance and can provide dynamic information of ships as its high revisit time. However, its spatial resolution is not high enough to obtain attributes of ships, while Gaofen-1 (GF-1) satellite in low earth orbit (LEO) can extract rich features, as its high spatial resolution. Therefore, the integration of GF-1 and GF-4 satellites can provide a better maritime situational awareness. The aim of the paper is to provide a feasible architecture of data fusion of GF-1 and GF-4 imagery for maritime surveillance, and a novel ship association method using multi-level information in order to improve the association accuracy, and experimental results verify the effectiveness of our method.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553591","National Natural Science Foundation of China(grant numbers:61901504); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553591","Ship surveillance;GF-4 and GF-1 satellite;ship association;information fusion","Satellites;Surveillance;Low earth orbit satellites;Data integration;Feature extraction;Robustness;Orbits","artificial satellites;feature extraction;geophysical image processing;image fusion;image resolution;oceanographic techniques;ships;surveillance","GF-1 satellite imagery;ship surveillance;Gaofen-4 satellite;high-resolution geostationary orbit optical satellite;wide-area maritime surveillance;revisit time;Gaofen-1 satellite;spatial resolution;GF-4 satellite imagery;information fusion;GEO;low earth orbit;LEO;feature extraction;maritime situational awareness;data fusion;ship association;multilevel information","","","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"2D-SSA Based Multiscale Feature Fusion for Feature Extraction and Data Classification in Hyperspectral Imagery","H. Fu; G. Sun; J. Ren; J. Zabalza; A. Zhang; Y. Yao","Laboratory for Marine Mineral Resources, Qingdao National Laboratory for Marine Science and Technology, Qingdao, China; Laboratory for Marine Mineral Resources, Qingdao National Laboratory for Marine Science and Technology, Qingdao, China; Department of Electronic and Electrical Engineering, University of Strathclyde, Glasgow, United Kingdom; Department of Electronic and Electrical Engineering, University of Strathclyde, Glasgow, United Kingdom; Laboratory for Marine Mineral Resources, Qingdao National Laboratory for Marine Science and Technology, Qingdao, China; Satellite Environment Center, Ministry of Environmental protection of China, Beijing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","76","79","Singular spectrum analysis (SSA) and its 2-D variation (2D-SSA) have been successfully applied for effective feature extraction in hyperspectral imaging (HSI). However, they both cannot effectively use the spectral-spatial information, leading to a limited accuracy in classification. To tackle this problem, a novel 2D-SSA based multiscale feature fusion method, combining with segmented principal component analysis (SPCA), is proposed in this paper. The SPCA method is used for dimension reduction and spectral feature extraction, while multiscale 2D-SSA can extract abundant spatial features at different scales. In addition, a postprocessing via SPCA is applied on fused features to enhance the spectral discriminability. Experiments on two widely used datasets show that the proposed method outperforms two conventional SSA methods and other spectral-spatial classification methods in terms of the classification accuracy and computational cost.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323776","National Natural Science Foundation of China(grant numbers:41971292,41871270,41801275); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323776","segmented principal component analysis (SPCA);hyperspectral imagery (HSI);Multiscale 2D-SSA;feature extraction;data classification","Feature extraction;Support vector machines;Principal component analysis;Hyperspectral imaging;Trajectory;Kernel;Dimensionality reduction","feature extraction;geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;image fusion","hyperspectral imagery;singular spectrum analysis;2-D variation;effective feature extraction;hyperspectral imaging;spectral-spatial information;2D-SSA based multiscale feature fusion method;segmented principal component analysis;SPCA method;spectral feature extraction;abundant spatial features;fused features;conventional SSA methods;spectral-spatial classification methods;classification accuracy;data classification","","","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Super-Resolved Multi-Temporal Segmentation with Deep Permutation-Invariant Networks","D. Valsesia; E. Magli",Politecnico di Torino; Politecnico di Torino,"IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","995","998","Multi-image super-resolution from multi-temporal satellite acquisitions of a scene has recently enjoyed great success thanks to new deep learning models. In this paper, we go beyond classic image reconstruction at a higher resolution by studying a super-resolved inference problem, namely semantic segmentation at a spatial resolution higher than the one of sensing platform. We expand upon recently proposed models exploiting temporal permutation invariance with a multi-resolution fusion module able to infer the rich semantic information needed by the segmentation task. The model presented in this paper has recently won the AI4EO challenge on Enhanced Sentinel 2 Agriculture.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884811","Super-resolution;image segmentation;deep neural networks","Deep learning;Uncertainty;Satellites;Superresolution;Neural networks;Semantics;Sensors","geophysical image processing;image fusion;image reconstruction;image resolution;image segmentation;learning (artificial intelligence)","super-resolved multitemporal segmentation;deep permutation-invariant networks;multiimage super-resolution;multitemporal satellite acquisitions;great success thanks;deep learning models;classic image reconstruction;super-resolved inference problem;semantic segmentation;spatial resolution;recently proposed models;temporal permutation invariance;multiresolution fusion module;rich semantic information;segmentation task","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Generative Adversarial Network for SAR-to-Optical Image Translation with Feature Cross-Fusion Inference","J. Wei; H. Zou; L. Sun; X. Cao; M. Li; S. He; S. Liu","College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","6025","6028","The translation of synthetic aperture radar (SAR) to optical images provides a new solution for the interpretation of SAR images. Most of the existing translation networks are based on generative adversarial networks and use 9-residual blocks or U-Net structures in the feature inference phase. Both structures cause a large amount of information lost during the conversion of SAR image features to optical features, making the outline of the translated image blurred or semantic information lost. Aiming at this problem, this paper proposes a cross-fusion inference network structure, which preserves both high-resolution features and low-resolution features in the whole process of feature inference. Our proposed method broadens the network horizontally while deepening it vertically and improving the image translation performance. The experiments conducted on the public dataset sen1-2 show that the proposed method is superior to other networks.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884166","National Natural Science Foundation of China(grant numbers:62071474); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884166","SAR-to-optical image translation;Generative adversarial network (GAN);Cross-fusion inference structure","Optical losses;Laser radar;Semantics;Optical fiber networks;Optical imaging;Generative adversarial networks;Radar polarimetry","feature extraction;image fusion;image resolution;image restoration;inference mechanisms;neural nets;optical images;radar imaging;radar resolution;synthetic aperture radar","generative adversarial network;SAR-to-optical image translation;synthetic aperture radar;optical images;U-Net structures;SAR image features;cross-fusion inference network structure;high-resolution features;low-resolution features;translated image blurred;semantic information lost","","","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"A Multi-Direction Subbands and Deep Neural Networks Bassed Pan-Sharpening Method","W. Huang; X. Fei; J. Yin; Y. Liu","School of Computer and Communication Engineering, Zhengzhou University of Light Industry, Zhengzhou, P. R. China; College of Information Science and Engineering, Henan University of Technology, Zhengzhou, P. R. China; School of Computer and Communication Engineering, Zhengzhou University of Light Industry, Zhengzhou, P. R. China; School of Computer and Communication Engineering, Zhengzhou University of Light Industry, Zhengzhou, P. R. China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5139","5142","This paper proposes a pan-sharpening method based on multi-direction subbands and deep neural networks. First, by utilizing the multi-scale and multi-direction properties of the nonsubsampled contourlet transform (NSCT), panchromatic (PAN) image is decomposed into the low frequency subbands in different resolutions and the high frequency subbands in different directions. Pan-sharpening method aims to fuse the high frequency subband coefficients of PAN image and the low frequency subband coefficients of multispectral (MS) image. Second, in order to better extract the feature of the high frequency subbands in different directions of PAN image, the deep neural network (DNN) is trained using the image patches of high frequency subbands of PAN image. Third, in the fusion stage, we exploit NSCT on the principal component of resampled low resolution (LR) MS image. The high frequency subbands of output high resolution (HR) MS image is obtained by forward propagation of the trained DNN, which input is the high frequency subbands of LR MS image. Finally, a new subband set is obtained by fusing the reconstructed high frequency subband and the original low frequency subband of LR MS image. The HR MS image is produced by executing the inverse transform of NSCT and adaptive PCA (A-PCA) on the new subband set. The experimental results show the proposed method outperforms other well-known methods in terms of both objective measurements and visual evaluation.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517817","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517817","deep neutral network (DNN);nonsub-sampled contourlet transform (NSCT);pan-sharpening;adaptive Principal Component Analysis (A-PCA)","Training;Image resolution;Neural networks;Transforms;Feature extraction;Image reconstruction","feature extraction;hyperspectral imaging;image fusion;image reconstruction;image resolution;inverse transforms;neural nets;principal component analysis;wavelet transforms","high frequency subband coefficients;PAN image;low frequency subband coefficients;high frequency subbands;resampled low resolution MS image;output high resolution MS image;LR MS image;subband set;reconstructed high frequency subband;original low frequency subband;multidirection subbands;multidirection properties;low frequency subbands;deep neural network based pan-sharpening method;nonsubsampled contourlet transform;NSCT;panchromatic image decomposition;multispectral image;feature extraction;image patches;forward propagation;LR MS image reconstruction;adaptive PCA;inverse transform;A-PCA;objective measurements;visual evaluation","","","","8","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Multistatic SAR Information Fusion Based on Image Registration and Fake Color Synthesis","W. Wang; J. Wu; X. Yang; Y. Miao; J. Yang; H. Yang","School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","7275","7278","Due to finite of information dimension of single bistatic synthetic aperture radar (SAR), to expand more information about ground objects we research multistatic SAR which can be decomposed into groups of bistatic SAR. It is known that scattering properties of the different viewing angles is different. In this paper, we focus on system of single transmitter and triple receivers and use polar format algorithm(PFA) to obtain images of ground objects for the triple receivers. A geometric distortion correction method is proposed due to elevation of ground objects. After the distortion correction, the three SAR images are registrated, then we put images into red, green, blue(RGB) channels respectively to realize fake color synthesis, and thus realize information fusion.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518875","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518875","Multistatic SAR;geometric distortion;fake color;information fusion","Synthetic aperture radar;Receivers;Image color analysis;Distortion;Transmitters;Imaging;Scattering","image colour analysis;image fusion;image registration;radar imaging;synthetic aperture radar","SAR images;fake color synthesis;multistatic SAR information fusion;image registration;single bistatic synthetic aperture radar;polar format algorithm;ground objects image","","","","8","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Subspace selection for hyperspectral pansharpening using sparse unmixing","C. Ge; Y. Li; J. Li; K. Wang","School of Telecommunications Engineering, Xidian University, Xi'an, China; School of Telecommunications Engineering, Xidian University, Xi'an, China; School of Telecommunications Engineering, Xidian University, Xian, Shaanxi, CN; School of Telecommunications Engineering, Xidian University, Xi'an, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","7232","7235","Hyperspectral (HS) pansharpening aims at fusing a panchromatic (PAN) image with a hyperspectral image, generating an image with the high spatial resolution of the former and the high spectral resolution of the latter. Recently, in order to enhance this process, researches have combined hyperspectral unmixing with the HS fusion model, improving the fusion results . In this combined model , endmember subspace extraction is a crucial step. Traditionally, VCA and SVD are used for subspace extraction, but VCA extracts pixels in the HS image that contain impure material signatures. In this work, we use the sparse unmixing technique to extract the endmember subspace which contains pure material signatures, which may better represent the HS image. We combine sparse unmixing-based subspace extraction with the bayesian fusion model. Results indicate that the fusion algorithm using our subspace extraction method had better global performance.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730886","Hyperspectral pansharpening;subspace;sparse unmixing;bayesian inference","Hyperspectral imaging;Bayes methods;Spatial resolution;Sparse matrices;Inference algorithms","Bayes methods;feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image resolution;singular value decomposition","subspace selection;hyperspectral pansharpening;sparse unmixing;HS pansharpening;panchromatic image;PAN image;hyperspectral image;hyperspectral unmixing;HS fusion model;endmember subspace extraction;SVD;VCA;subspace extraction;material signatures;material signatures;sparse unmixing-based subspace extraction;Bayesian fusion model","","","","9","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Analysis of object oriented classification on high resolution images","N. Bashit; Harintaka; A. Basith","Department of Geodesy and Geomatics, Gadjah Mada University, Yogyakarta, Indonesia; Department of Geodesy and Geomatics, Gadjah Mada University, Yogyakarta, Indonesia; Department of Geodesy and Geomatics, Gadjah Mada University, Yogyakarta, Indonesia","2016 6th International Annual Engineering Seminar (InAES)","19 Jan 2017","2016","","","187","191","Classification is one step of image processing which aims to obtain information from remote sensing data. Object oriented classification is a classification algorithm which can be used to process a high resolution image because it uses the object elements as spectral, spatial, and texture. The purpose of this study is to examine the application of object oriented classification on two types of images which are categorized as high resolution images. The study uses two types high resolution image. They are SPOT 5 and QuickBird image covering Ngaglik district, Sleman, Yogyakarta. SPOT 5 was acquired on September 2, 2014 which has 2.5 meters spatial resolution and QuickBird was acquired in 2010 which has 0.6 meters spatial resolution. A field survey was conducted for identifying the classification scheme and testing the classification accuracy. Classification has been done through several stages, involving segmentation, merge parameter determination, rule-based classification, and post classification. Object oriented classification extracts meaningful image objects into several classes based on their spectral, spatial, and texture elements. The utilization of object's spatial element is the advantage of object oriented classification in making classification on the high resolution image. The result of this study shows that object oriented classification algorithm can classify the land use on SPOT 5 image in up to 80.00% accuracy with 16 classes, and QuickBird image in up to 92.00% accuracy with 17 classes. Therefore, object oriented classification is very effective to make classification on the high resolution image. Object oriented classification which uses the high resolution image can produce a good accuracy.","","978-1-5090-0741-7","10.1109/INAES.2016.7821931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7821931","Object oriented classification;high resolution image;accuracy","Image segmentation;Spatial resolution;Image fusion;Merging;Shape;Image classification","feature extraction;image classification;object-oriented methods","object oriented classification;high resolution image;image processing;classification algorithm;remote sensing data;SPOT 5 image;QuickBird image","","","","12","IEEE","19 Jan 2017","","","IEEE","IEEE Conferences"
"Hyperspectral image classification based on spectral-spatial feature extraction","Z. Ye; L. Tan; L. Bai","School of Electronics and Control Engineering Chang'an University, Xi'an, China; School of Electronics and Control Engineering Chang'an University, Xi'an, China; School of Electronics and Control Engineering Chang'an University, Xi'an, China","2017 International Workshop on Remote Sensing with Intelligent Processing (RSIP)","26 Jun 2017","2017","","","1","4","A novel hyperspectral classification algorithm based on spectral-spatial feature extraction is proposed. First, spectral-spatial features are extracted by Gabor transform in PCA-projected space. Following that, Gabor-feature bands are partitioned into multiple subsets. Afterwards, the adjacent features in each subset are fused. Finally, the fused features are processed by recursive filtering before feeding into support vector machine (SVM) classifier. Experimental results demonstrate that the proposed algorithm substantially outperforms the traditional and state-of-the-art methods.","","978-1-5386-1990-2","10.1109/RSIP.2017.7958808","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7958808","hyperspectral image;classification;Gabor feature extraction;image fusion;recursive filtering","Feature extraction;Hyperspectral imaging;Support vector machines;Classification algorithms;Filtering","feature extraction;Gabor filters;hyperspectral imaging;image classification;image filtering;recursive filters;support vector machines","hyperspectral image classification;spectral-spatial feature extraction;Gabor transform;PCA-projected space;recursive filtering;support vector machine classifier","","4","","11","IEEE","26 Jun 2017","","","IEEE","IEEE Conferences"
"Fast Unsupervised Spatiotemporal Super-Resolution for Multispectral Satellite Imaging Using Plug-and-Play Machinery Strategy","C. -H. Lin; C. -Y. Sie; P. -Y. Lin; J. -T. Lin","Miin Wu School of Computing, National Cheng Kung University, Tainan, Taiwan; Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2568","2571","Acquiring high-spatial-resolution (HSR) images at high temporal sampling rate is not economical and even not achievable using contemporary multispectral satellite imaging hardware. An alternative is to fuse a set of HSR images acquired at low sampling rate, with another set of low-spatial-resolution images acquired at high sampling rate, and such fusion problem is referred to as spatiotemporal super-resolution (STSR). We mitigate the ill-posedness of the STSR problem by incorporating the image self-similarity prior (S2P), which is the key behind the design of several state-of-the-art imaging inverse problems. Unlike most super-resolution works in the computer vision area, our method does not rely on collecting big data. Instead, we propose a fully unsupervised STSR method by adopting the popular strategy in machine learning, known as plug-and-play optimization, and by carefully refining the required matrix computation/inversion. We term our method as STSRS2P, whose superiority and low computational complexity will be experimentally verified.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554710","Einstein Program (Young Scholar Fellowship Program) of Ministry of Science and Technology (MOST), Taiwan(grant numbers:MOST 109-2636-E-006-022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554710","Multispectral satellite;image fusion;spatiotemporal super-resolution;image self-similarity;plug-and-play","Computer vision;Satellites;Superresolution;Refining;Imaging;Big Data;Benchmark testing","","","","1","","14","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"The extraction and fusion of faint mural based on feature transform of hyperspectral images","Ning Pan; M. Hou","College of Geoscience and Surveying Engineering, China University of Mining and Technology Beijing, Beijing, China; School of Surveying and Mapping, Beijing University of Civil Engineering and Architecture, Beijing, China","2016 4th International Workshop on Earth Observation and Remote Sensing Applications (EORSA)","29 Aug 2016","2016","","","161","164","The characteristics that non-destructive, wide spectral range, high spectral resolution, and image-spectrum merged make it possible to study colored relics based on hyperspectral imaging. As for mural, the spectral information contained in the hyperspectral image can be used to identify pigments, but the presented fuzzy pattern is hard to provide the intuitive visual effect; on the other hand, the spectral information in the visual enhanced image has changed, which could not be used for pigment identification anymore. For more objective study of faded mural, the pattern information is extracted and fused with the spectrum in this study. Firstly, the hyperspectral image of mural is pretreated by both radiometric correction and geometric correction, getting the image with reflective spectrum. Then, the faded pattern information is extracted from the feature transformed image. Finally, the extracted pattern image is fused with the image containing spectrum through Gram-Schmidt spectral sharpening, creating the target image. The result image is shown to visual enhanced the faint pattern effectively. And the spectral curve contained in the result image has the same spectral features as the original spectrum. It proved that the fusion image can provide both intuitive visual effect and the spectrum used for material identification. It is conducive to the study of faint mural and the combination with other technological means.","","978-1-5090-1479-8","10.1109/EORSA.2016.7552788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552788","hyperspectral imaging;mural;minimum noise fraction;pattern extraction;image fusion","Hyperspectral imaging;Feature extraction;Pigments;Transforms;Data mining;Art","feature extraction;hyperspectral imaging;pigments;radiometry","hyperspectral images;fusion extraction;faint mural;spectral information;pigment identification;fuzzy pattern;intuitive visual effect;visual enhanced image;radiometric correction;geometric correction;faded pattern information;extracted pattern image;Gram-Schmidt spectral sharpening;spectral curve","","","","11","IEEE","29 Aug 2016","","","IEEE","IEEE Conferences"
"A Variational Approach with Nonlocal Self-Similarity and Joint-Sparsity for Hyperspectral Image Super-Resolution","T. Xu; T. -Z. Huang; Y. Chen; J. Huang; L. -J. Deng","School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Computer and Information Engineering, Jiangxi Normal University, Nanchang, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2444","2447","The aim of hyperspectral image super-resolution (HSI-SR) is to produce high spatial resolution hyperspectral image (HR - HSI) by exploiting the available high spatial resolution multispectral image (HR-MSI) and low spatial resolution hyperspectral image (LR-HSI). In this work, we develop a novel matrix factorization (MF)-based HSI -SR way, which formulates the HSI -SR problem as estimating the spectral dictionary from the observed LR - HSI and the coefficient matrix from both the observed HR-MSI and LR-HSI. Specifically, we first estimate the spectral dictionary from the observed LR - HSI by the dictionary learning algorithm with redundancy assumption. Moreover, based on the superpixel segmentation technology used in the observed HR-MSI, the coefficient vectors are grouped. By concatenating the joint-sparse, nonlocallow-rank, and nonnegative priors of the grouped coefficient vectors, we develop a novel coefficient matrix estimation variational model, which fully explores the nonlocal self-similarity of the desired HR-HSI. The proposed coefficient matrix estimation model is solved under the alternating direction method of multipliers (ADMM) framework. Experimental results prove the superiority of the proposed way from the quantitative and qualitative analysis.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554815","NSFC(grant numbers:61772003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554815","Hyperspectral image fusion;joint-sparsity;nonlocal self-similarity;spectral unmixing","Dictionaries;Image color analysis;Superresolution;Redundancy;Estimation;Machine learning;Convex functions","geophysical image processing;hyperspectral imaging;image representation;image resolution;image segmentation;matrix algebra;matrix decomposition","high spatial resolution hyperspectral image;HR - HSI;available high spatial resolution multispectral image;low spatial resolution hyperspectral image;novel matrix factorization-based HSI -SR way;HSI -SR problem;spectral dictionary;observed LR - HSI;observed HR-MSI;LR-HSI;novel coefficient matrix estimation variational model;nonlocal self-similarity;desired HR-HSI;coefficient matrix estimation model;hyperspectral image super-resolution;HSI-SR","","","","14","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Land cover classification in fused multisensor multispectral satellite imagery","D. I. Moody; D. E. Bauer; S. P. Brumby; E. D. Chisolm; M. S. Warren; S. W. Skillman; R. Keisler","Descartes Labs, Los Alamos, NM; Planet Labs, San Francisco, CA; Descartes Labs, Los Alamos, NM 87544, United States; Descartes Labs, Los Alamos, NM; Descartes Labs, Los Alamos, NM; Descartes Labs, Los Alamos, NM; Descartes Labs, Los Alamos, NM","2016 IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI)","28 Apr 2016","2016","","","85","88","The increase in number of deployed satellite constellations and the improvement in sensing capabilities have led to large volumes of data with a wide range of temporal and spatial coverage. The data analysis capability, however, has been lagging, and has historically focused on single-sensor individual images. We present results from an ongoing effort to develop satellite imagery analysis tools that aggregate information across multiple sensors and bands, and at multiple scales. We focus on field and landmark separation around Clinton, Iowa, and show land cover classification results that combine fused imagery from Planet Labs and Landsat 8. Classification performance is assessed using Cropland Data Layer images generated by USDA. Our method combines spectral, spatial, and temporal information to improve the accuracy of practical land cover classification.","","978-1-4673-9919-7","10.1109/SSIAI.2016.7459181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7459181","multiscale multispectral satellite imagery;unsupervised classification;data fusion;multi-INT","Satellites;Earth;Remote sensing;Training;Radio frequency;Spatial resolution","image classification;image fusion;land cover","fused multisensor multispectral satellite imagery;satellite constellation deployment;sensing capability improvement;temporal coverage;spatial coverage;data analysis capability;single-sensor individual image;information aggregation;landmark separation;field separation;Clinton;Iowa;land cover classification;Planet Labs;Landsat 8;cropland data layer image;USDA;temporal information;spatial information;spectral information","","4","","7","IEEE","28 Apr 2016","","","IEEE","IEEE Conferences"
"Empirical analysis of SIFT, Gabor and fused feature classification using SVM for multispectral satellite image retrieval","C. Joshi; S. Mukherjee","AIM&ACT, Banasthali University, Rajasthan, India; AIM&ACT, Banasthali University, Rajasthan, India","2017 Fourth International Conference on Image Information Processing (ICIIP)","12 Mar 2018","2017","","","1","6","High Level image understanding and Content extraction is becoming a challenging task in Content based image retrieval system for satellite images. Retrieval based on the low level extraction techniques does not bridge the semantic gap. In the experiment high level feature extraction techniques i.e. scale invariant feature transform and Gabor descriptors are used. The novel approach is proposed in which both the feature descriptors are fused to retrieve the results with more accuracy rate. The experiment is conducted on the multispectral satellite images, of Landsat 8 sensor. The similarity of the query image to that of stored database images is matched by the Manhattan distance. The Precision and Recall is computed for the data set. The results have shown the improved retrieval rate. The retrieval efficiency is further increased by using the SVM classifier by classifying the satellite images based on Urban area, Water body and Vegetation. The experimental results shows that the fusion technique gives better result and more accuracy can be obtained by classifying the dataset using SVM.","","978-1-5090-6734-3","10.1109/ICIIP.2017.8313776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8313776","SIFT;Gabor;CBIR;Landsat 8;Precision;Recall;SVM","Feature extraction;Support vector machines;Satellites;Information processing;Remote sensing;Vegetation mapping;Gabor filters","content-based retrieval;feature extraction;geophysical image processing;image classification;image fusion;image matching;image retrieval;support vector machines","empirical analysis;fused feature classification;multispectral satellite image retrieval;High Level image understanding;Content extraction;Content based image retrieval system;low level extraction techniques;semantic gap;scale invariant feature;Gabor descriptors;feature descriptors;multispectral satellite images;query image;stored database images;improved retrieval rate;retrieval efficiency;SVM classifier;fusion technique;experiment high level feature extraction techniques;Manhattan distance;Landsat 8 sensor;Precision;Recall","","3","","17","IEEE","12 Mar 2018","","","IEEE","IEEE Conferences"
"Research on Low Altitude Aerial Image Stitching","J. Lu; Y. Bai","Pattern Recognition and Intelligent System, Beijing Institute of Technology, Beijing, P. R. China; Pattern Recognition and Intelligent System, Beijing Institute of Technology, Beijing, P. R. China","2018 37th Chinese Control Conference (CCC)","7 Oct 2018","2018","","","9292","9296","In this paper, we study the problem of low altitude aerial image stitching. Though the traditional approach is well studied, batch-process image stitching is more difficult. In this work, we present a simple and effective approach to handle problems in low altitude aerial image stitching by considering GPS information of images and seam cutting algorithm. Our method develops an efficient approach to generate the flight path of UAV. We then select alternative images and calculate the overlapping region according to the distance between adjacent images. Features are efficiently detected not only in longitude overlap, but also in lateral, which largely improves stitching accuracy and decreases accumulative errors. In order to handle parallax caused by low altitude flight, we use seam finding method to avoid ghost. Experimental results show that our approach can effectively stitch a large number of images and successfully handle small parallax with low cost.","1934-1768","978-988-15639-5-8","10.23919/ChiCC.2018.8483043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8483043","Aerial Image;GPS Information;Seam Cutting;Image Registration;Image Mosaic","Strips;Feature extraction;Global Positioning System;Geography;Remote sensing;Lapping;Unmanned aerial vehicles","autonomous aerial vehicles;feature extraction;Global Positioning System;image fusion;image matching;image segmentation","low altitude aerial image stitching;batch-process image stitching;alternative images;adjacent images;low altitude flight;image GPS information;seam cutting algorithm;UAV flight path;overlapping region;feature detection;longitude overlap;lateral overlap;stitching accuracy;parallax handling","","3","","20","","7 Oct 2018","","","IEEE","IEEE Conferences"
"Hybrid Change Detection Based on ISFA for High-Resolution Imagery","J. Xu; C. Zhao; B. Zhang; Y. Lin; D. Yu","Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China; Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China; Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China; Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China; Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China","2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC)","18 Oct 2018","2018","","","76","80","Hybrid change detection (HCD) for high-resolution imagery usually adopt decision-level method and rely on artificial design. To address this issue, we propose a novel feature-level fusion strategy for HCD based on iterative slow feature analysis (ISFA). First, objects are obtained by multiresolution segmentation of bi-temporal images respectively, and corresponding feature sets are constructed through stacking pixel- and object-level spectral features. Then, slow feature analysis (SFA) is used for transforming the feature sets into a new feature space at the first time. And iteration method with variable weights is introduced to get the last slow feature fusion map, where the changed pixels and unchanged pixels can be separated more easily. At last, K-means cluster is adopted to separate changed area and unchanged area automatically and generate final change result. Experiments were conducted on bi-temporal multi-spectral images, demonstrating the good performance of the proposed approach.","","978-1-5386-4991-6","10.1109/ICIVC.2018.8492758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8492758","hybrid change detection;feature-level fusion;iterative slow feature analysis;multi-scale fusion","Image segmentation;Feature extraction;Eigenvalues and eigenfunctions;Remote sensing;Spatial resolution;Iterative methods","feature extraction;image fusion;image resolution;iterative methods;set theory","slow feature fusion map;bi-temporal multispectral images;hybrid change detection;ISFA;high-resolution imagery;HCD;decision-level method;artificial design;iterative slow feature analysis;multiresolution segmentation;bi-temporal images;object-level spectral features;feature sets;feature-level fusion strategy;stacking pixel-level spectral features;K-means cluster","","2","","16","IEEE","18 Oct 2018","","","IEEE","IEEE Conferences"
"Thin cloud removal for satellite images using normalized intensity mapping","P. Shanmugavadivu; A. Shanthasheela; V. Sivakumar","Department of Computer Science and Applications, Gandhigram Rural Institute, Dindigul, Tamil Nadu, India; Department of Computer Science and Applications, Gandhigram Rural Institute, Dindigul, Tamil Nadu, India; Department of Computer Science and Applications, Gandhigram Rural Institute, Dindigul, Tamil Nadu, India","2017 Second International Conference on Electrical, Computer and Communication Technologies (ICECCT)","23 Nov 2017","2017","","","1","4","Clouds inhibit the features of images such as color and brightness in the image regions. The light diffusion, scattering and attenuation of clouds create blur effect or affect the contrast of the images. The interference of cloud over images degrades the quality of satellite images and tends to suppress the essential information in those images. Hence, the removal of clouds greatly help in the efficient processing of satellite images. The proposed cloud removal method, Normalized Intensity Mapping (NIM) is confirmed to be effective for cloud removal. This technique was experimented on the cloud covered real-time images and was found to outperform its competitive techniques. The quality of NIMCR is assessed quantitatively by Peak Signal to Noise Ratio (PSNR) and qualitatively by Human Visual Perception (HVP). This technique due to its applicability in real-time and satellite images, qualifies itself as a valuable addition in the array of image pre-processing techniques.","","978-1-5090-3239-6","10.1109/ICECCT.2017.8117825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8117825","cloud removal;thin cloud;intensity transformation;intensity mapping;normalized intensity","Satellites;Image color analysis;Brightness;Remote sensing;Image restoration;Histograms;Image enhancement","clouds;image denoising;image enhancement;image fusion;image restoration;image segmentation;visual perception","satellite images;normalized intensity mapping;image regions;thin cloud removal;image pre-processing;image contrast;peak signal to noise ratio;PSNR;human visual perception;HVP","","1","","16","IEEE","23 Nov 2017","","","IEEE","IEEE Conferences"
"Pansharpening Multispectral Images Based on Unconstrained Least Square Spectral Unmixing","M. A. Bendoumi; T. Benlefki; R. Saadi","Ecole Militaire Polytechnique (EMP), Bordj el Bahri, Algiers, Algeria; Ecole Militaire Polytechnique (EMP), Bordj el Bahri, Algiers, Algeria; Ecole Militaire Polytechnique (EMP), Bordj el Bahri, Algiers, Algeria","2018 International Conference on Signal, Image, Vision and their Applications (SIVA)","7 Mar 2019","2018","","","1","6","A new fusion framework for pansharpening multi-spectral (MS) image is suggested in this paper. The introduced method relies on linear spectral unmixing, and employs unconstrained least square (ULS) estimation for combining the high spatial features of the panchromatic (PAN) image and the rich spectral features of the MS image (MSI), in one single high spatial-spectral image. Applied to real coincident MS and PAN data sets with different texture complexity, the proposed approach can fuse the spatial and the spectral characteristics in a single image with the minimum of spectral distortion in comparison to some famous methods. Moreover, incorporating the ULS in the unmixing process is quite simple to formulate and straightforward to implement.","","978-1-5386-7120-7","10.1109/SIVA.2018.8660988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8660988","","Remote sensing;Spatial resolution;Principal component analysis;Artificial satellites;Earth;Measurement;Distortion","geophysical image processing;geophysical techniques;image fusion;image texture","unconstrained least square spectral unmixing;fusion framework;multispectral image;introduced method;unconstrained least square estimation;ULS;high spatial features;rich spectral features;MS image;spatial-spectral image;spectral characteristics;single image;spectral distortion;famous methods;unmixing process;texture complexity;pansharpening multispectral images;linear spectral unmixing;real coincident MS data set;real coincident PAN data set","","","","16","IEEE","7 Mar 2019","","","IEEE","IEEE Conferences"
"Evaluation of wavelet fusion method on land cover classification in Bodetabek area, Indonesia","S. M. Isa; Suharjito","Bina Nusantara University, West Jakarta, DKI Jakarta, ID; Bina Nusantara University, West Jakarta, DKI Jakarta, ID","2017 19th International Conference on Advanced Communication Technology (ICACT)","30 Mar 2017","2017","","","348","354","This paper aims to present an evaluation of wavelet fusion method on land cover classification task. Wavelet fusion is one of the pan-sharpening methods which combines the higher spatial resolution panchromatic image with the lower resolution multispectral image to create high resolution fused image. Data fusion using multispectral and high spatial resolution panchromatic images are useful for improving classification accuracy. The study area of our research is Bodetabek (Bogor, Depok, Tangerang, and Bekasi) area, Indonesia. Different wavelet bases (Haar, Db2 to Db6, Coif1 to Coif5, and Sym1 to Sym5) were examined to determine the best basis for the data fusion process. This study also examined the effect of wavelet decomposition level to the spatial and spectral quality of the fused image. The experimental results on LANDSAT data show that the best basis for wavelet fusion is Coif5. The classification accuracy assessment on different wavelet decomposition level fused image also demonstrates that the higher wavelet decomposition, the higher spatial quality of the fused image. Although the spectral quality was degraded as the wavelet decomposition level increased, the classification accuracy assessment results show that higher wavelet decomposition level yields better overall classification accuracy (96.28% for eight decomposition level vs 82.77% for two decomposition level).","","978-89-968650-9-4","10.23919/ICACT.2017.7890111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890111","LANDSAT;Pan-sharpening;Wavelet Fusion;Land cover Classification","Earth;Satellites;Remote sensing;Spatial resolution;Urban areas;Monitoring;Wavelet transforms","geophysical image processing;image classification;image fusion;image resolution;land cover","wavelet fusion method;land cover classification;pan-sharpening methods;spatial resolution panchromatic image;multispectral image;classification accuracy;Bodetabek;Depok;Tangerang;Bekasi;Indonesia;wavelet bases;data fusion process;spectral quality;spatial quality;LANDSAT data;classification accuracy assessment;wavelet decomposition level fused image;decomposition level","","","","5","","30 Mar 2017","","","IEEE","IEEE Conferences"
"A New Benchmark Based on Recent Advances in Multispectral Pansharpening: Revisiting Pansharpening With Classical and Emerging Pansharpening Methods","G. Vivone; M. Dalla Mura; A. Garzelli; R. Restaino; G. Scarpa; M. O. Ulfarsson; L. Alparone; J. Chanussot","Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; GIPSA-Lab, Grenoble Institute of Technology, Grenoble, France; Department of Information Engineering and Mathematics, University of Siena, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Department of Electrical Engineering and Information Technology, University Federico II, Naples, Italy; Electrical and Computer Engineering, University of Iceland, Reykjavik; Department of Information Engineering, University of Florence, Italy; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing","IEEE Geoscience and Remote Sensing Magazine","5 Mar 2021","2021","9","1","53","81","Pansharpening refers to the fusion of a multispectral (MS) image and panchromatic (PAN) data aimed at generating an outcome with the same spatial resolution of the PAN data and the spectral resolution of the MS image. In the last 30 years, several approaches to deal with this issue have been proposed. However, the reproducibility of these methods is often limited, making the comparison with the state of the art hard to achieve. Thus, to fill this gap, we propose a new benchmark consisting of recent advances in MS pansharpening. In particular, optimized classical approaches [multiresolution analysis (MRA) and component substitution (CS)] are compared with methods belonging to the third generation of pansharpening, represented by variational optimization-based (VO) and machine learning (ML) techniques. The benchmark is tested on different scenarios (from urban to rural areas) acquired by different commercial sensors [i.e., IKONOS (IK), GeoEye-1 (GE-1), and WorldView-3 (WV-3)]. Both quantitative and qualitative assessments and the computational burden are analyzed in this article, and all of the implementations have been collected in a MATLAB toolbox that is made available to the community.","2168-6831","","10.1109/MGRS.2020.3019315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9245579","","Image resolution;Multiresolution analysis;Sensors;Principal component analysis;Optimization;Benchmark testing;Pansharpening","geophysical image processing;image fusion;image resolution;remote sensing","panchromatic data;spatial resolution;PAN data;spectral resolution;MS image;MS pansharpening;optimized classical approaches;component substitution;commercial sensors;multispectral pansharpening;multispectral image","","89","","143","IEEE","30 Oct 2020","","","IEEE","IEEE Magazines"
"Hyperspectral Image High-resolution via Subspace-Based Non-Convex Arctangent-Rank Regularization","M. Niu; X. Sun; X. Zhang","College of Mathematics and Statistics, Shenzhen University, Shenzhen, China; College of Mathematics and Statistics, Shenzhen University, Shenzhen, China; School of Electronic and Communication Engineering, Shenzhen Polytechnic, Shenzhen, China","2021 17th International Conference on Computational Intelligence and Security (CIS)","11 Feb 2022","2021","","","277","284","In this paper, we focus on improving the spatial resolution of hyperspectral images. The combination of low spatial resolution hyperspectral image (LR-HSI) and high spatial resolution multispectral image (HR-MSI) into high spatial resolution hyperspectral image (HR-HSI) has been attracting significant research interest in recent years. The traditional nuclear norm approximation adds all singular values together directly, which may depend heavily on the magnitudes of singular values. However, tensor arctangent rank is a tighter approximation to the rank function. The rank of tensor is obtained by adding the constraint of arctangent to the rank of each frontal slice of the tensor. We propose a novel subspace-based low tensor arctangent rank regularization method for this fusion. In this process, it makes full use of spectral correlation and non-local similarity in the HR-HSI. Firstly, we learn subspace from LR-HSI. In this step, we use the prior condition that the spectral vector usually exists in the low dimensional subspace. Secondly, we use K-means algorithm to cluster HR-MSI, and the coefficient is also grouped according to the clustering. Finally, the coefficient is estimated by low tensor arctangent rank prior. Extensive experiments on two public HSI datasets show that the proposed method outperfoms the state-of-the-art methods.","","978-1-6654-9489-2","10.1109/CIS54983.2021.00065","National Natural Science Funds of China(grant numbers:61772343,61872429,62072312); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9701791","high-resolution;non-convex arctangent rank;image fusion;subspace;clustering","Manifolds;Tensors;Correlation;Clustering algorithms;Security;Spatial resolution;Computational intelligence","approximation theory;geophysical image processing;hyperspectral imaging;image classification;image fusion;image resolution;iterative methods;learning (artificial intelligence);pattern clustering;remote sensing;singular value decomposition;tensors","nonlocal similarity;LR-HSI;low dimensional subspace;public HSI datasets;hyperspectral image high-resolution;subspace-based nonconvex arctangent-rank regularization;low spatial resolution hyperspectral image;high spatial resolution multispectral image;high spatial resolution hyperspectral image;HR-HSI;traditional nuclear norm approximation;singular values;rank function;subspace-based low tensor arctangent rank regularization method;HR-MSI;K-means algorithm","","","","23","IEEE","11 Feb 2022","","","IEEE","IEEE Conferences"
"Full-Resolution Quality Assessment of Pansharpening: Theoretical and hands-on approaches","A. Arienzo; G. Vivone; A. Garzelli; L. Alparone; J. Chanussot","Department of Information Engineering, University of Florence, Florence, Italy; Institute of Methodologies for Environmental Analysis, National Research Council, Tito Scalo, Italy; Department of Information Engineering and Mathematics, University of Siena, Siena, Italy; Department of Information Engineering, University of Florence, Florence, Italy; Grenoble Institute de Physique, Jean Kuntzmann Laboratory, Grenoble, France","IEEE Geoscience and Remote Sensing Magazine","2 Nov 2022","2022","10","3","168","201","Panchromatic (Pan) sharpening, or pansharpening, refers to the combination of a multispectral (MS) image and Pan data with a finer spatial resolution. Since the early days of this research topic, the issue of quality assessment has played a central role in the related literature, pushing investigators toward extensive research. The solution to this problem is nontrivial because of its ill-posed nature. Indeed, no reference image is available to compare with the outcome of the fusion process.","2168-6831","","10.1109/MGRS.2022.3170092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779258","","Protocols;Pansharpening;Indexes;Spatial resolution;Sensor phenomena and characterization;Quality assessment;Optimization","geophysical image processing;geophysical signal processing;image fusion;image resolution;remote sensing;sensor fusion","early days;finer spatial resolution;hands-on approaches;MS;Pan;panchromatic sharpening;pansharpening;reference image;resolution quality assessment","","5","","82","IEEE","20 May 2022","","","IEEE","IEEE Magazines"
"Determination of Open Pit Mining Zones Through Digital Processing of Multi-Spectral Images and PPI Method - A Case Study of Southern Ecuador","M. V. Cepeda-Velastegui; M. C. López Estévez; O. Padilla-Almeida; T. Toulkeridis","Universidad de las Fuerzas Armadas ESPE, Sangolquí, Ecuador; Instituto Espacial Ecuatoriano, Quito, Ecuador; Universidad de las Fuerzas Armadas ESPE, Sangolquí, Ecuador; Universidad de las Fuerzas Armadas ESPE, Sangolquí, Ecuador","2019 Sixth International Conference on eDemocracy & eGovernment (ICEDEG)","13 Jun 2019","2019","","","188","193","In the current study, the identification of open pit mining areas has been conducted using multispectral satellite images of medium (Landsat 8)and high (Sentinel 2B)spatial resolution of the province of Zamora Chinchipe in southern Ecuador. Such research involved several digital processes such as atmospheric correction, image fusion through the Brovey algorithm, noise elimination among others. We worked simultaneously with spatial and spectral patterns through the application of algorithms in order to increase spatial resolution., and spectral indices that enhance variables of interest. In addition, the Pixel Purity Index (PPI)methodology has been applied in order to obtain the classes involved in the study area based on the pure pixels and the results were compared between the two methodologies. The zones considered open-pit mining have been obtained in both cases.","2573-1998","978-1-7281-1704-1","10.1109/ICEDEG.2019.8734455","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8734455","open pit mining;spectral indexes;Brovey algorithm;PPI;endmembers","Indexes;Earth;Satellites;Artificial satellites;Spatial resolution;Hyperspectral imaging","geophysical image processing;geophysical techniques;image fusion;image resolution;mining;remote sensing","Brovey algorithm;noise elimination;spatial patterns;spectral patterns;spatial resolution;spectral indices;Pixel Purity Indexmethodology;open pit mining zones;digital processing;multispectral images;PPI method;southern Ecuador;open pit mining areas;multispectral satellite images;highspatial resolution;Sentinel 2B;Zamora Chinchipe;digital processes;atmospheric correction;image fusion","","2","","36","IEEE","13 Jun 2019","","","IEEE","IEEE Conferences"
"Multi-static MIMO-SAR three dimensional deformation measurement system","T. Zeng; C. Mao; C. Hu; X. Yang; W. Tian","School of Information and Electronics, Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China; School of Information and Electronics, Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China; School of Information and Electronics, Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China; School of Information and Electronics, Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China; School of Information and Electronics, Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China","2015 IEEE 5th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)","29 Oct 2015","2015","","","297","301","Herein is proposed a novel high-steep rock slope landslide monitoring system based on the configuration of multi-static multiple-input multiple-output synthetic aperture radar (MIMO-SAR). In this system, three spastically distributed radars are employed. Each is a Ku-band MIMO-SAR, transmitting mutually orthogonal waveforms and sensing the slope movement from different view angles. The three radars can then achieve the deform measurements in the three line of sight (LOS) directions. After an effective fusion of these measurements, the deform vector, with direction and amount, can be resolved. This vector is particularly important for risk warning as well as for understanding the motion mechanism of landslide. Moreover, the data acquisition period can be largely reduced, since the use of MIMO radar allows a simultaneous sampling of the scene echo. The time decorrelation is therefore very small, and hence high measurement accuracy can be expected. It can be proved that, using this system, the three-dimensional accuracy can reach 0.1 ~ 1 mm. Simulations and experimental datasets are used to validate this proposal.","","978-1-4673-7297-8","10.1109/APSAR.2015.7306212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306212","Deformation measurement;ground based SAR;MIMO;three-dimensional measurement","Accuracy;Arrays;Synthetic aperture radar;Monitoring;Geologic measurements;Terrain factors;Imaging","decorrelation;deformation;environmental monitoring (geophysics);geomorphology;geophysical image processing;image fusion;mechanical variables measurement;MIMO radar;radar imaging;remote sensing by radar;synthetic aperture radar","multistatic MIMO-SAR 3D deformation measurement system;high-steep rock slope landslide monitoring system;multistatic multiple-input multiple-output synthetic aperture radar;Ku-band MIMO-SAR;slope movement;mutually orthogonal waveform transmission;line-of-sight directions;risk warning;landslide motion mechanism;data acquisition period reduction;simultaneous scene echo sampling;time decorrelation","","12","1","14","IEEE","29 Oct 2015","","","IEEE","IEEE Conferences"
"New sparsity based pansharpening algorithms for hyperspectral images","C. Kwan; B. Budavari; M. Dao; J. Zhou","Signal Processing, Inc., Rockville, Maryland, USA; Signal Processing, Inc., Rockville, Maryland, USA; Applied Research LLC, Rockville, Maryland, USA; Signal Processing, Inc., Rockville, Maryland, USA","2017 IEEE 8th Annual Ubiquitous Computing, Electronics and Mobile Communication Conference (UEMCON)","8 Jan 2018","2017","","","88","93","In this paper, we present new sparsity based algorithms in generating a high resolution hyperspectral image by fusing a high resolution color image with a low resolution hyperspectral image. Mathematical formulation of the sparsity based approaches is presented. Comparison with other pansharpening algorithms using actual data has been carried out using two hyperspectral image data sets. Initial results are encouraging. Most importantly, the new sparsity formulation points to a new direction in generating high resolution hyperspectral images where the raw images may be noisy.","","978-1-5386-1104-3","10.1109/UEMCON.2017.8248993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8248993","hyperspectral images;color image;pansharpening;sparsity;fusion;hybrid color mapping","Hyperspectral imaging;Image resolution;Image color analysis;Color;Signal processing algorithms;Signal resolution;Measurement","hyperspectral imaging;image colour analysis;image fusion;image resolution;remote sensing","new sparsity based pansharpening algorithms;sparsity formulation points;hyperspectral image data sets;low resolution hyperspectral image;high resolution color image;high resolution hyperspectral image","","9","","46","IEEE","8 Jan 2018","","","IEEE","IEEE Conferences"
"The Fusion of Morphological and Contextual Information for Building Detection from Very High-Resolution SAR Images","S. Adelipour; H. Ghassemian","Image Processing and Information Analysis Lab, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Lab, Tarbiat Modares University, Tehran, Iran","Electrical Engineering (ICEE), Iranian Conference on","27 Sep 2018","2018","","","389","393","Nowadays, very high-resolution synthetic aperture radar (VHR SAR) images are available for interpretation of the built-up area. Buildings are one of the most important parts of the urban area, and in this paper a new method for building detection from a single SAR image is proposed. First, the contextual information of the buildings, such as double bounce, layover and shadow areas are extracted. Using these features, a set of primary detection is made. Second, morphological profiles (MP), with different structural elements (SE), are utilized to build a differential morphological profile (DMP) that provides the building structural information. This structural information is used to make a secondary detection set of building candidates. The final detection result is made by fusion of these two sets. Performance evaluation of the proposed method is reported by the implementation of the method on two different real TerraSAR-X images. The results show that the proposed method has a high detection rate (DR), while the false alarm rate (FAR) is low.","","978-1-5386-4916-9","10.1109/ICEE.2018.8472581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8472581","Building detection;Feature Extraction;Information fusion;Morphological profile;VHR SAR images","Buildings;Synthetic aperture radar;Detectors;Feature extraction;Data mining;Urban areas;Electrical engineering","geophysical image processing;image fusion;object detection;radar imaging;remote sensing by radar;synthetic aperture radar","contextual information;building detection;high-resolution SAR images;high-resolution synthetic aperture radar images;built-up area;urban area;single SAR image;primary detection;morphological profiles;building structural information;secondary detection set;building candidates;final detection result;TerraSAR-X images;high detection rate;layover area;morphological profile;structural elements;shadow area;VHR SAR images","","4","","29","IEEE","27 Sep 2018","","","IEEE","IEEE Conferences"
"Pan-Sharpening Framework Based on Laplacian Sharpening with Brovey","S. S. Khan; Q. Ran; M. Khan; Z. Ji","College of Information Science & Technology Beijing University of Chemical Technology, Beijing, China; College of Information Science & Technology Beijing University of Chemical Technology, Beijing, China; Department of Computer & Software Technology, University of Swat, Swat, Pakistan; College of Information Science & Technology Beijing University of Chemical Technology, Beijing, China","2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)","21 Aug 2020","2019","","","1","5","Pan-sharpening is a rapidly growing independent discipline considered with and finds many applications in several research fields. Pan-sharpening is the merger of high spec-tral resolution multispectral image and high spatial resolution panchromatic image which gives the best of both in the resultant image. This paper presents a novel pan-sharpening hybrid technique by combining Brovey with Laplacian filter (LF). The Laplacian filter is an advanced edge sharpening technique using the second derivative. The Laplacian edge sharpening is playing a significant role to enhance edge contrast to highly improve image visibility (Acutance). The sharpening filter performs to identify sharp edge boundaries in the image, such as the edge between a subject and a background of a contrasting color, and increase the image contrast in the area immediately around the edge. The Brovey is also a popular fusion method used for its ability in preserving the spatial information of an image. The proposed hybrid technique encompasses Brovey with LF to improve the results (Qualitatively and quantitatively) of pan-sharpening. The resultant image is assessed by three performance matrices quantitative metric, qualitative metric, and qualitative error image.","","978-1-7281-2345-5","10.1109/ICSIDP47821.2019.9173129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173129","Multispectral and panchromatic images;Laplacian Filter;Brovey;Qualitative Error Image","Measurement;Laplace equations;Image color analysis;Corporate acquisitions;Image edge detection;Conferences;Information filters","edge detection;filtering theory;geophysical image processing;image enhancement;image fusion;image resolution;remote sensing","Brovey;high spec-tral resolution;high spatial resolution panchromatic image;pan-sharpening hybrid technique;Laplacian filter;advanced edge sharpening technique;Laplacian edge sharpening;edge contrast;image visibility;sharpening filter performs;sharp edge boundaries;image contrast;qualitative error image;pan-sharpening framework;Laplacian sharpening","","4","","12","IEEE","21 Aug 2020","","","IEEE","IEEE Conferences"
"Coarse-to-fine ship detection using visual saliency fusion and feature encoding for optical satellite images","Y. Yin; N. Liu; C. Li; W. Wan; T. Fang","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","2016 International Conference on Audio, Language and Image Processing (ICALIP)","9 Feb 2017","2016","","","705","710","In order to overcome cloud clutters and varied sizes of objects in high-resolution optical satellite images, a novel coarse-to-fine ship detection framework is proposed. Initially, a modified saliency fusion algorithm is derived to reduce cloud clutters and extract ship candidates. Then, in coarse discrimination stage, candidates are described by introducing shape feature to eliminate regions which are not conform to ship characteristics. In fine discrimination stage, candidates are represented by local descriptor-based feature encoding, and then linear SVM is used for discrimination. Experiments on 60 images (including 467 objects) collected from Microsoft Virtual Earth demonstrate the effectiveness of the proposed framework. Specifically, the fusion of visual saliency achieves 17.07% higher Precision and 7.23% higher Recall compared with those of individual one. Moreover, using local descriptor in fine discrimination makes Precision and F-measure further be improved by 7.23% and 1.74%, respectively.","","978-1-5090-0654-0","10.1109/ICALIP.2016.7846579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846579","feature encoding;optical satellite images;ship detection;visual saliency","Marine vehicles;Feature extraction;Visualization;Encoding;Shape;Adaptive optics;Optical imaging","clouds;feature extraction;image coding;image fusion;object detection;optical information processing;remote sensing;ships;support vector machines","Microsoft Virtual Earth;linear SVM;local descriptor-based feature encoding;shape feature;coarse discrimination stage;ship candidate extraction;modified saliency fusion algorithm;coarse-to-fine ship detection framework;cloud clutter reduction;optical satellite image encoding","","3","","13","IEEE","9 Feb 2017","","","IEEE","IEEE Conferences"
"Geostatistical Data Fusion: Application to Red Edge Bands of Sentinel 2","M. J. Pereira; A. Ramos; R. Nunes; L. Azevedo; A. Soares","CERENA/DECivil, Instituto Superior Técnico U. Lisboa, Lisboa, Portugal; CERENA/DECivil, Instituto Superior Técnico U. Lisboa, Lisboa, Portugal; CERENA/DECivil, Instituto Superior Técnico U. Lisboa, Lisboa, Portugal; CERENA/DECivil, Instituto Superior Técnico U. Lisboa, Lisboa, Portugal; CERENA/DECivil, Instituto Superior Técnico U. Lisboa, Lisboa, Portugal","2016 International Conference on Computational Science and Computational Intelligence (CSCI)","20 Mar 2017","2016","","","758","761","The new ESA Sentinel-2 satellite delivers images of the red-edge band at a spatial resolution of 20m. These bands are particular useful for vegetation monitoring in general and present high potential of application in precision agriculture. For this, we propose a data fusion methodology for downscaling rededge bands to 10 m spatial resolution using the information of visible and near infrared band. The methodology is based on inverse modelling by combining geostatistical stochastic simulation methods and genetic programming. A case study case presents the preliminary results.","","978-1-5090-5510-4","10.1109/CSCI.2016.0147","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881440","inverse modeling;data fusion;genetic programming;direct sequential cosimulation;sentinel 2;red-edge bands","Correlation coefficient;Spatial resolution;Genetic programming;Histograms;Stochastic processes;Data integration","genetic algorithms;geophysical image processing;image fusion;image resolution;remote sensing;stochastic processes","geostatistical data fusion;red edge bands;ESA Sentinel-2 satellite;spatial resolution;near infrared band;visible band;inverse modelling;geostatistical stochastic simulation methods;genetic programming","","3","","6","IEEE","20 Mar 2017","","","IEEE","IEEE Conferences"
"Dilated Residual Convolutional Neural Networks for Low-Dose CT Image Denoising","N. Thanh Trung; D. -H. Trinh; N. Linh Trung; T. Thi Thuy Quynh; M. -H. Luu","University of Information and Communication Technology, Thai Nguyen University, Thai Nguyen, Vietnam; VIBOT ERL CNRS 6000 / ImViA, University of Bourgogne, France; VNU University of Engineering and Technology, Vietnam National University, Hanoi, Vietnam; VNU University of Engineering and Technology, Vietnam National University, Hanoi, Vietnam; VNU University of Engineering and Technology, Vietnam National University, Hanoi, Vietnam","2020 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)","29 Dec 2020","2020","","","189","192","X-ray computed tomography (CT) imaging, which uses X-ray to acquire image data, is widely used in medicine. High X-ray doses may be harmful to the patient's health. Therefore, X-ray doses are often reduced at the expense of reduced quality of CT images. This paper presents a convolutional neural network model for low-dose CT image denoising, inspired by a recently introduced dialated residual network for despeckling of synthetic aparture radar images (SAR-DRN). In particular, batch normalization is added to some layers of SAR-DRN in order to adapt SAR-DRN for low-dose CT denoising. In addition, a preprocessing layer and a post-processing one are added in order to improve the receptive field and to reduce computational time. Moreover, the perceptual loss combined with MSE one are used in the training phase so that the proposed denoising model can preserve more subtle details of denoised images. Experimental results show that the proposed model can denoise low-dose CT images efficiently as compared to some state-of-the-art methods.","","978-1-7281-9396-0","10.1109/APCCAS50809.2020.9301693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301693","Computer tomography;low dose imaging;medical image denoising;dilated residual network;convolutional neural network;perceptual loss","Computed tomography;Noise reduction;Training;Image denoising;X-ray imaging;Residual neural networks;Image reconstruction","computerised tomography;image classification;image colour analysis;image denoising;image fusion;medical image processing;neural nets;radar imaging;radial basis function networks;remote sensing by radar;synthetic aperture radar;transforms","dilated residual convolutional neural networks;low-dose CT image denoising;X-ray computed tomography imaging;image data;high X-ray doses;convolutional neural network model;residual network;synthetic aparture radar images;SAR-DRN;low-dose CT denoising;denoised images;low-dose CT images;MSE","","1","","14","IEEE","29 Dec 2020","","","IEEE","IEEE Conferences"
"Using Deep Networks for Semantic Segmentation of Satellite Images","T. Selea; M. Neagul","Faculty of Mathematics and Informatics, West University of Timisoara, Timisoara, România; Faculty of Mathematics and Informatics, West University of Timisoara, Timisoara, România","2017 19th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)","11 Nov 2018","2017","","","409","415","In this paper we aim to investigate different deep learning techniques for automatic extraction of valuable information from large sized satellite image data. We focus on the problem of semantic segmentation which attaches a class label to each pixel from the image. We investigate two semantic segmentation architectures based an convolutional neural networks: segnet and u-net. We analyse different tiling strides with reverse aggregation methods. We compare two classical methods (averaging and maximum) and propose a new method based on entropy. We test the models with distinct types of images, emphasizing the need to predict the results using information from all of them. We discuss various fusion strategies and introduce a fusion strategy based on the observations obtained from separately analysing the distinct image types.","","978-1-5386-2626-9","10.1109/SYNASC.2017.00074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8531320","semantic segmentation;segnet;u net;cnn;satellite image","Machine-to-machine communications;Scientific computing","convolution;entropy;feature extraction;feedforward neural nets;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);remote sensing","deep networks;class label;semantic segmentation architectures;convolutional neural networks;reverse aggregation methods;fusion strategy;deep learning techniques;information extraction;satellite image data;tiling strides;entropy","","1","","15","IEEE","11 Nov 2018","","","IEEE","IEEE Conferences"
"Development of Method for Change Detection Based on Information Fusion for PALSAR-2 Data","A. Jain; D. Singh","Department of Electronics and Communication Engineering, Indian Institute of Technology Roorkee, Roorkee, India; Department of Electronics and Communication Engineering, Indian Institute of Technology Roorkee, Roorkee, India","2019 URSI Asia-Pacific Radio Science Conference (AP-RASC)","20 Jun 2019","2019","","","1","4","This paper presents an approach for change detection which helps in detecting change information to achieve final change map. The different techniques are available to generate difference image but there is a need to explore a technique for threshold detection. Therefore, the work presented in this paper explored Expectation Maximization (EM) algorithm which is used for threshold selection for change detection map. Further, information fusion is performed in the approach to achieve resultant fused change map. The proposed algorithm is implemented on fully polarimetric PALSAR-2 data.","","978-908-25987-5-9","10.23919/URSIAP-RASC.2019.8738679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8738679","","Change detection algorithms;Data integration","expectation-maximisation algorithm;geophysical image processing;image fusion;radar polarimetry;remote sensing by radar;synthetic aperture radar","fused change map;Expectation Maximization;fully polarimetric PALSAR-2 data;change detection map;threshold detection;final change map;change information;information fusion","","1","","14","","20 Jun 2019","","","IEEE","IEEE Conferences"
"Detection of flooded areas from multitemporal SAR images","N. Kalpana; A. Sivasankar","FACULTY/ECE, Anna University Regional Campus, Madurai, India; FACULTY/ECE, Anna University Regional Campus, Madurai, India","2016 Second International Conference on Science Technology Engineering and Management (ICONSTEM)","8 Sep 2016","2016","","","1","5","Multi temporal synthetic aperture radar images are available, precise calibration and perfect spatial register are required to get a useful image for displaying changes that contain occurred. SAR calibration is a extremely complex and sensitive problem; a few errors may persist after calibration that interferes with subsequent steps in the data fusion and visualization process. Because of the non-Gaussian model of radar backscattering, traditional image pre processing procedures cannot be used here. To solve this problem “cross-calibration/normalization,” method can be used. In image enhancement and the numerical comparison of many image takes together with data fusion and visualization processes. The proposed processing which contain filtering, histogram truncation, and equalization steps and region growing and merging algorithm applied in an adaptive way to the images. RGB composition is used to combining an pre & post flood image or identify an flooded areas.","","978-1-5090-1706-5","10.1109/ICONSTEM.2016.7560951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7560951","Data fusion;flood detection;image enhancement;multi temporal synthetic aperture radar (SAR) imagery;RGB composition","Synthetic aperture radar;Histograms;Entropy;Speckle;Calibration;Radar imaging","calibration;data visualisation;floods;geophysical image processing;image colour analysis;image enhancement;image filtering;image fusion;remote sensing by radar;synthetic aperture radar","flooded area detection;multitemporal synthetic aperture radar images;perfect spatial register;SAR calibration;data fusion;visualization process;nonGaussian model;radar backscattering;image preprocessing procedures;cross-calibration method;image enhancement;numerical comparison;filtering step;histogram truncation step;equalization step;RGB composition;postflood image;preflood image;cross-normalization method","","1","","15","IEEE","8 Sep 2016","","","IEEE","IEEE Conferences"
"Building Detection Using Very High Resolution SAR Images With Multi-Direction Based on Weighted-Morphological Indexes","F. Amjadipour; H. Ghassemian; M. Imani","Image Processing and Information Analysis Lab, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Lab, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Lab, Tarbiat Modares University, Tehran, Iran","2022 International Conference on Machine Vision and Image Processing (MVIP)","22 Mar 2022","2022","","","1","6","Today, technological advancement in production of radar images can be seen with high spatial resolution and also the availability of these images’ significant growth in interpretation and processing of high-resolution radar images. The building extraction from urban areas is one of the most challenging applications in VHR SAR image, which is used to estimate the population and urban development. Detection of individual buildings in the urban context is highly considered by researchers due to complexity of interpreting radar images in these fields. On the other hand, one of the main issues in the complexity of the scatters received from buildings is change in direction of the building relative to the horizon, which is correlated with the look angle. Other influential parameters are geometric distortions, which include layover and shadow effects. In some cases, the effect of shadow is an auxiliary parameter in detection of these targets that increases accuracy of the detection. In this paper, we intend to extract the building from high spatial resolution SAR images using fuzzy fusion of two morphological indicators, SI and DI, which represent the shadow and bright area, respectively. Due to the effect of SAR imaging geometry on ground targets, different sizes and directions of structural elements were applied to the image. The use of indicators weights with different sizes is proposed in this work. The Detection Ratio of experiment of TerraSAR-X image has a result of 95.3%.","2166-6784","978-1-6654-1216-2","10.1109/MVIP53647.2022.9738776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9738776","Building extraction;top-hat transform;morphology index;building orientation;fuzzy system","Geometry;Buildings;Urban areas;Radar;Radar imaging;Radar polarimetry;Complexity theory","fuzzy set theory;geophysical image processing;image fusion;image resolution;radar imaging;remote sensing by radar;synthetic aperture radar","high spatial resolution SAR images;SAR imaging geometry;building detection;weighted-morphological indexes;high-resolution radar images;building extraction;urban areas;VHR SAR image;urban development;layover;shadow effects","","1","","19","IEEE","22 Mar 2022","","","IEEE","IEEE Conferences"
"Spectral and spatial assessment of the TDW Wavelet transform decimated and not decimated for the fusion of OrbView-2 satellite images","J. Medina; I. Carrillo; E. Upegui","Universidad Distrital Francisco Jose de Caldas, Bogota, CO; Maestría en Ciencias de la Información y las Comunicaciones, Facultad de Ingeniería, Universidad Distrital Francisco José de Caldas Bogotá D. C., Colombia; Facultad de Ingeniería, Grupo GEFEM, IEEE-UD/GRSS Universidad Distrital Francisco José de Caldas Bogotá D. C., Colombia","2018 13th Iberian Conference on Information Systems and Technologies (CISTI)","28 Jun 2018","2018","","","1","6","The objective of this article is to develop and evaluate two methodologies that allow to improve the spatial resolution without significant loss of the spectral resolution of a multispectral image (MULT) and panchromatic (PAN) OrbView-2. In the first method, the algorithm is used: Discrete Transformation of Decimal Wavelet (TWD decimated) or Mallat Algorithm. The Value is obtained from the MULT. Then applying the fusion to the Value and PAN component through the TWD decimated daubechies (db4) generates a new Value (nval-m). With the nuance and saturation of the MULT image, the inverse HSV-RGB transformation is performed to generate a new multispectral image (N-MULT1). In the second methodology, the algorithm is used: Discrete Wavelet Transform not decimated (TWD not decimated) or Algorithm of À trous, applying the fusion to the Value component and PAN through the TWD not decimated generates a new Value (nval-a) and with the nuance and saturation of the MULT image the inverse HSV-RGB transform is made to generate a new multispectral image (N-MULT2). Finally, the results of the two methods are presented using the ERGAS, RASE and Qu indices for assessment. It is obtained that the à trous method is better spatially and spectrally.","","978-989-98434-8-6","10.23919/CISTI.2018.8399418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8399418","Wavelet;Fusion;satellite images;Mallat;À trous","Image resolution;Discrete wavelet transforms;Image color analysis;Electronic mail;Splines (mathematics)","discrete wavelet transforms;image fusion;image resolution;remote sensing","OrbView-2 satellite images;TWD;Mallat Algorithm;PAN component;MULT image;inverse HSV-RGB transformation;multispectral image;Discrete Wavelet Transform;TDW wavelet transform;Discrete Transformation of Decimal Wavelet;panchromatic OrbView-2;Algorithm of À trous;ERGAS index;RASE index;Qu index","","1","","","","28 Jun 2018","","","IEEE","IEEE Conferences"
"An Efficient Application of Fusion Approach for Agriculture Drought Estimation of Uttarakhand, India with Modis Data","N. J. K. Anjana; D. Murugan; A. Agarwal; D. Singh","Department of Electronics and Communication Engineering, Indian Institute of Technology Roorkee, Roorkee, India 247667; Department of Electronics and Communication Engineering, Indian Institute of Technology Roorkee, Roorkee, India 247667; Department of Computer Science and Engineering, Indian Institute of Technology Roorkee, Roorkee, India 247667; Department of Electronics and Communication Engineering, Indian Institute of Technology Roorkee, Roorkee, India 247667","2018 9th International Symposium on Signal, Image, Video and Communications (ISIVC)","9 May 2019","2018","","","39","42","Monitoring vegetation drought is quite a challenging task although researchers are using satellite data to monitor it. Still more attention is required in monitoring vegetation drought using conventional drought indices because sometimes they provide either over estimation or under estimation of drought condition. Currently researchers are commonly using ground-based observations for drought monitoring which is quite cumbersome and time consuming. Some reported works are available based on satellite data where some vegetation indices like Vegetation Condition Index (VCI) and Thermal Condition Index (TCI) have been used. But these indices have to be critically analyzed for optimally using them for agriculture drought monitoring. For this purpose, various fusion techniques like principal component analysis (PCA), discrete wavelet transform (DWT) and expectation maximization (EM) algorithm have been used and critically analyzed. The performance of the obtained fused results was compared with the ground truth information. It is found that PCA and DWT are giving better performance compared to EM based fusion techniques.","","978-1-5386-8173-2","10.1109/ISIVC.2018.8709220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8709220","MODIS;VCI;TCI;fusion","Vegetation mapping;Land surface temperature;Discrete wavelet transforms;Principal component analysis;Monitoring;Indexes;Agriculture","agriculture;discrete wavelet transforms;environmental monitoring (geophysics);expectation-maximisation algorithm;geophysical image processing;hydrological techniques;hydrology;image fusion;principal component analysis;remote sensing;vegetation","agriculture drought estimation;satellite data;drought condition;Vegetation Condition Index;Thermal Condition Index;agriculture drought monitoring;EM based fusion techniques;drought indices;vegetation drought monitoring;MODIS data;principal component analysis;discrete wavelet transform;expectation maximization algorithm;Uttarakhand;India;expectation-maximisation algorithm","","1","","11","IEEE","9 May 2019","","","IEEE","IEEE Conferences"
"Localization in Aerial Imagery with Grid Maps using LocGAN","H. Hu; J. Zhu; S. Wirges; M. Lauer","Institute of Measurement and Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Measurement and Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany; FZI Research Center for Information Technology, Karlsruhe, Germany; Institute of Measurement and Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany","2019 IEEE Intelligent Transportation Systems Conference (ITSC)","28 Nov 2019","2019","","","2860","2865","In this work, we present LocGAN, our localization approach based on a geo-referenced aerial imagery and LiDAR grid maps. Currently, most self-localization approaches relate the current sensor observations to a map generated from previously acquired data. Unfortunately, this data is not always available and the generated maps are usually sensor setup specific. Global Navigation Satellite Systems (GNSS) can overcome this problem. However, they are not always reliable especially in urban areas due to multi-path and shadowing effects. Since aerial imagery is usually available, we can use it as prior information. To match aerial images with grid maps, we use conditional Generative Adversarial Networks (cGANs) which transform aerial images to the grid map domain. The transformation between the predicted and measured grid map is estimated using a localization network (LocNet). Given the geo-referenced aerial image transformation the vehicle pose can be estimated. Evaluations performed on the data recorded in region Karlsruhe, Germany show that our LocGAN approach provides reliable global localization results.","","978-1-5386-7024-8","10.1109/ITSC.2019.8917236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8917236","","Generators;Global navigation satellite system;Principal component analysis;Training;Laser radar;Reliability;Transforms","distance measurement;geophysical image processing;image fusion;mobile robots;optical radar;remote sensing;remotely operated vehicles","self-localization approaches;current sensor observations;generated maps;Global Navigation Satellite Systems;shadowing effects;grid map domain;conditional generative adversarial networks;predicted measured grid map;localization network;geo-referenced aerial image transformation;LocGAN approach;reliable global localization results;localization approach;geo-referenced aerial imagery;LiDAR grid maps","","1","","22","IEEE","28 Nov 2019","","","IEEE","IEEE Conferences"
"Unsupervised change detection from multitemporal SAR images based on a detail preserving approach and a robust threshold estimation","B. Chabira; T. Skanderi; A. Belhadj Aissa","Dept. of telecommunications FEI, University of sciences and technology Houari Boumediene USTHB, Algiers, Algeria; Dept. of telecommunications FEI, University of sciences and technology Houari Boumediene USTHB, Algiers, Algeria; Dept. of telecommunications FEI, University of sciences and technology Houari Boumediene USTHB, Algiers, Algeria","2017 5th International Conference on Electrical Engineering - Boumerdes (ICEE-B)","14 Dec 2017","2017","","","1","5","In environment monitoring and disaster management, Synthetic aperture radars (SARs) have shown their great efficiency due to the fact that they provide short revisit times and they can operate in day and/or night and their independence of weather conditions. Different applications have been addressed a lot in recent years, but the one that receives a lot of attention is the change detection of the observed earth surface by exploiting the multitemporal SAR images. In this paper, we propose an unsupervised method for the change detection from multitemporal SAR images that does not require any speckle filtering. This method is based on: i) generating a multiresolution set of the single-channel log ratio image using stationary wavelet transform (SWT); ii) applying the T-point algorithm for all the images of the multiresolution sets; and iii) fusing the obtained images at the optimum reliable scale to generate the change map. The proposed method was experimentally validated using semisimulated and real SAR images acquired by RADARSAT-2 satellite in the region of Algiers.","","978-1-5386-0686-5","10.1109/ICEE-B.2017.8192061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8192061","Change detection;SAR;multi-temporal SAR images;speckle;single-channel;SWT;T-point algorithm;robust threshold estimation","Image resolution;Speckle;Robustness;Histograms;Synthetic aperture radar;Error analysis","geophysical image processing;image fusion;image resolution;object detection;remote sensing by radar;speckle;synthetic aperture radar;wavelet transforms","unsupervised change detection;multitemporal SAR images;robust threshold estimation;Synthetic aperture radars;short revisit times;unsupervised method;single-channel log ratio image;change map;semisimulated SAR images;real SAR images;detail preserving approach;environment monitoring;disaster management;weather conditions;change detection;observed earth surface;speckle filtering;stationary wavelet transform;T-point algorithm;multiresolution sets;optimum reliable scale;RADARSAT-2 satellite;Algiers","","","","8","IEEE","14 Dec 2017","","","IEEE","IEEE Conferences"
"Sparse fusion based on SAM elective sample dictionary establishment","X. Sun","Department of Geography, Minjiang College, Fuzhou, Fujian, China","2016 IEEE International Conference on Mechatronics and Automation","5 Sep 2016","2016","","","68","72","In the paper, it is proposed that multi-spectral image pure pixel is utilized for completing SAM classification. The classified samples are utilized for electively constructing sparse dictionary, thereby improving the representativeness of the dictionary. Eight surface feature types are set in Landsat8 image. PPI index is used for calculating pure pixel index of each pixel. Pure pixel of each surface feature is further extracted through N-D visualizer, which is used for SAM calculation. Eight kinds of surface feature samples are selected from SAM image for online dictionary learning. Multi-spectral image sparse dictionary is generated. Multi-spectral image sparse coefficient is calculated through dictionary and OMP. Meanwhile, online dictionary and OMP are utilized for obtaining panchromatic image sparse coefficient. Fusion sparse coefficient is generated by maximum values both sparse coefficients. Multi-spectral image sparse dictionary is combined for reconstructing and generating fusion image. Eight quantitative fusion evaluation indicators are adopted for comparing algorithm fusion and weighted fusion in the paper. Fusion method proposed in the paper contains more information, fusion image texture detail information is improved, and better image multi-spectral information is kept.","2152-744X","978-1-5090-2396-7","10.1109/ICMA.2016.7558536","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7558536","Sparse dictionary;Online dictionary learning algorithm;OMP algorithm;SAM","Dictionaries;Indexes;Feature extraction;Hyperspectral imaging;Image resolution;Classification algorithms","feature extraction;geophysical image processing;image classification;image fusion;image reconstruction;image texture;learning (artificial intelligence);remote sensing;sparse matrices;spectral analysis","sparse fusion;SAM elective-sample dictionary establishment;multispectral image pure pixel;SAM classification;dictionary representativeness;Landsat8 image;surface feature types;PPI index;pure-pixel index;surface feature extraction;N-D visualizer;online dictionary learning;multispectral image sparse dictionary;multispectral image sparse coefficient;online dictionary;OMP;panchromatic image sparse coefficient;fusion sparse coefficient;maximum sparse coefficient values;fusion image reconstruction;fusion image generation;quantitative fusion evaluation indicators;algorithm fusion;weighted fusion;image texture detail information improvement;image multispectral information","","","","14","IEEE","5 Sep 2016","","","IEEE","IEEE Conferences"
"Implementation and evaluation of the High Pass Filter and Wavelet À Trous transforms in Matlab to fusion Landsat 8 OLI/TIRS Satellite Images","R. J. M. Daza; E. Upegui","Maestría en Ciencias de la Información y las Comunicaciones Grupo GEFEM, Universidad Distrital Francisco José de Caldas, Bogotá D. C., Colombia; Proyecto Curricular Ingeniería Catastral y Geodesia Grupo GEFEM, Universidad Distrital Francisco José de Caldas, Bogotá D. C., Colombia","2022 17th Iberian Conference on Information Systems and Technologies (CISTI)","14 Jul 2022","2022","","","1","6","In this article, the High Pass Filter-HPF, and Wavelet À trous transforms are developed mathematically, to later implement them in Matlab. The fusion of satellite images is performed with each one of the implemented transforms, with a proposed methodology. A Landsat 8 OLI/TIRS image (Panchromatic - PAN and multispectral - MULTI) of a northwestern sector of Peru -where sugar cane crops are evident- is used to generate two fused images, namely: MULTIHPF and MULTITWA. The fused images were evaluated both in spatial and spectral quality through four indices, specifically: correlation index, ERGAS, RASE and Q index, in order to determine the efficiency of the proposed methods. Best results of the spectral evaluation were obtained with the MULTITWA image, achieving correlations higher than 0.95, a Q index of 0.96 and a RASE value of 9.2%, while spatially higher values than 0.96. Regarding spatial richness, the best results were obtained with MULTIHPF with an ERGAS of 14.6, a RASE of 29.3% and a Q of 0.7.","2166-0727","978-9-8933-3436-2","10.23919/CISTI54924.2022.9820476","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820476","fusion;satellite images;Landsat 8;high pass filter;À trous","Wavelet transforms;Earth;Satellites;Correlation;Artificial satellites;Information filters;Sugar industry","crops;filtering theory;geophysical image processing;geophysical techniques;high-pass filters;image fusion;remote sensing;transforms","correlation index;spectral evaluation;MULTITWA image;Wavelet trous transforms;High Pass Filter-HPF;implemented transforms;sugar cane crops;spatial quality;spectral quality","","","","0","","14 Jul 2022","","","IEEE","IEEE Conferences"
"Feature Extraction of Hyperspectral Image Structure Based on Spatial-Spectral Fusion","S. Zhang; X. Zhang; K. Zhang; B. Yuan","School of Science, China University of Geosciences, Beijing, Beijing, China; School of Statistics, Beijing Normal University, Beijing, China; School of Science, China University of Geosciences, Beijing, Beijing, China; School of Science, China University of Geosciences, Beijing, Beijing, China","2021 4th International Conference on Artificial Intelligence and Big Data (ICAIBD)","28 Jun 2021","2021","","","469","473","Hyperspectral images (HSIs) contain abundant spectral information and spatial information. Fully mining the hidden information of the data itself can effectively improve the classification accuracy. For this reason, a spatial-spectral fusion classification method combining the spatial structure characteristics of data is proposed in order to improve the conventional spatial-spectral fusion method. The spatial structure information of the data in the spatial domain is extracted through Fourier transform, so that it has position invariance. With this operation, the comparability and analyzability are increased. Then the cross convolution is used to improve the response of different ground objects to features Then the spatial-spectral fusion method is used to fuse the original spectrum information to classify. Tree model is picked as the method of classification, in consideration of its strong interpretability and good selectivity for structure feature. Experimental results show that this method can refine the extraction of spatial structure information in the spatial domain. When this method is applied to HSI dataset, the classification accuracy can be significantly improved even with a small number of samples. Three types of HSI datasets (Indian Pines, Salinas, Tea Farm) have 12.69%, 9.09%, and 4.23% accuracy improvements combining with spectral information respectively.","","978-1-6654-1515-6","10.1109/ICAIBD51990.2021.9458987","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458987","hyperspectral images;Fourier transform;structure feature extraction;spatial-spectral fusion","Fourier transforms;Convolution;Fuses;Feature extraction;Spatial databases;Data mining;Object recognition","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image resolution;remote sensing;spectral analysis","feature extraction;hyperspectral image structure;abundant spectral information;spatial information;hidden information;classification accuracy;spatial-spectral fusion classification method;spatial structure characteristics;spatial structure information;spatial domain","","","","10","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Fusion Methods for Hyperspectral Image and LIDAR Data at Pixel-Level","C. D. Abraham; J. Aravinth","Department of Electronics and Communication Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Department of Electronics and Communication Engineering, Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India","2018 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)","18 Nov 2018","2018","","","1","3","Hyperspectral image data and LIDAR data have found to be complimentary modailities in case of remotely sensed images, which can be fused if both are geo-referenced. Hyperspectral images provide the spectral response of each object in the area and can be used to identify the material composition of the image which can be used for the object classification. LIDAR data provides the elevation and geometrical information of the objects in the scene. Pixel-level fusion ensures no loss of information because there is no dimensionality reduction. This paper assesses the different methods of pixel fusion like wavelet transform, IHS transform and linear pixel fusion.","","978-1-5386-3624-4","10.1109/WiSPNET.2018.8538460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8538460","Hyperspectral image;LIDAR;Pixel level fusion.","Laser radar;Hyperspectral imaging;Wavelet transforms;Spatial resolution","geophysical image processing;geophysical signal processing;hyperspectral imaging;image classification;image fusion;image sensors;optical radar;remote sensing;sensor fusion;wavelet transforms","fusion methods;LIDAR data;hyperspectral image data;complimentary modailities;remotely sensed images;object classification;pixel-level fusion;pixel fusion","","","","12","IEEE","18 Nov 2018","","","IEEE","IEEE Conferences"
"Multistatic Radar Fusion imaging based on Beidou navigation signal","X. Peng; X. Li; D. Ding; R. Chen","School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China","2020 Cross Strait Radio Science & Wireless Technology Conference (CSRSWTC)","11 Mar 2021","2020","","","1","2","This paper studies the multistatic radar fusion imaging based on Beidou navigation signal. For Beidou navigation system, the transmitter and receiver are represented by different navigation satellites, forming a dual-station radar system. Due to the sparseness of the target signal received by the radar, based on the two-dimensional radar scatter echo sparse representation model and data fusion as a means, the bistatic radar can be expanded into a multistatic radar to broaden the frequency band and viewing angle range And then improve the range and azimuth resolution.","","978-1-7281-8181-3","10.1109/CSRSWTC50769.2020.9372610","Natural Science(grant numbers:61890541,61731001,61871443); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9372610","multistatic radar;navigation signal;fusion imaging","Image resolution;Imaging;Radar imaging;Radar scattering;Satellite navigation systems;Multistatic radar;Signal resolution","image fusion;radar cross-sections;radar imaging;remote sensing by radar;satellite navigation;synthetic aperture radar","multistatic radar fusion imaging;Beidou navigation signal;Beidou navigation system;different navigation satellites;dual-station radar system;target signal;two-dimensional radar scatter;sparse representation model;data fusion;bistatic radar","","","","3","IEEE","11 Mar 2021","","","IEEE","IEEE Conferences"
"An Enhanced Pansharpening Approach Based on Second-Order Polynomial Regression","H. Hallabia; H. Hamam","LIS, GMOD Aix Marseille Université, Marseille, France; School of Elect. Eng. and Electronic Eng., Uni. of Johannesburg, South-Africa","2021 International Wireless Communications and Mobile Computing (IWCMC)","9 Aug 2021","2021","","","1489","1493","In this work, we propose applying a non-linear strategy to estimate the novel synthetic intensity component through a second-order polynomial regression analysis, such that it could be expressed as a linear combination of multispectral bands in addition to their weighted joint products. Details maps are, therefore, obtained as the residual between the panchromatic (PAN) and the novel synthetic intensity component. Indeed, the high-resolution Multispectral (MS) image is produced by transferring the details into the expanded multispectral bands. The quality improvements achievable by our proposal are evaluated by using two high resolution data sets collected by the WorldView-3 and WorldView-4 sensors. The performance of our proposed technique is clearly shown against the existing state-of-the-art pansharpening methods (especially with linear schemes) and particularly for multispectral channels with non-overlapping spectral wavelengths with respect to the PAN image.","2376-6506","978-1-7281-8616-0","10.1109/IWCMC51323.2021.9498829","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9498829","Pansharpening;Synthetic intensity component;Polynomial regression;WorldView-3/4 images","Wireless communication;Image resolution;Linear regression;Pansharpening;Sensors;Proposals;Task analysis","geophysical image processing;image fusion;image resolution;polynomials;radiometry;regression analysis;remote sensing","enhanced pansharpening approach;nonlinear strategy;synthetic intensity component;second-order polynomial regression analysis;linear combination;weighted joint products;details maps;high-resolution Multispectral image;expanded multispectral bands;high resolution data sets;WorldView-3;WorldView-4 sensors;existing state-of-the-art pansharpening methods;linear schemes;multispectral channels;nonoverlapping spectral wavelengths;PAN image","","","","18","IEEE","9 Aug 2021","","","IEEE","IEEE Conferences"
"Hyperspectral fluorescence data fusion using quaternion and octonion phase","S. Bauer; F. P. León","Institute of Industrial Information Technology (lIlT), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Karlsruher Institut fur Technologie, Karlsruhe, Baden-WÃ¼rttemberg, DE","2016 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)","13 Feb 2017","2016","","","615","621","While common hyperspectral images comprise two spatial and one spectral dimension, hyperspectral fluorescence images can have the excitation wavelength as an additional fourth dimension. We consider hyperspectral fluorescence spectra of minerals for which the emitted fluorescence spectra vary with the irradiated excitation wavelength. In comparison with reflectance spectra, the fluorescence spectra at a fixed excitation wavelength exhibit far less characteristic details and are rather flat. Using these spectra for object classification leads to low classification rates. For this reason, the data contained in the fourth dimension (excitation wavelength) is fused to obtain more characteristic spectra. In this contribution, we will use quaternions and octonions for the fusion: By considering their phase, the information contained in the fourth dimension is compressed and a three-dimensional image containing “pseudo spectra” in the original spectral dimension is derived. Doing so results in more discriminative spectra that allow for a more robust classification. We will demonstrate the benefit of the proposed method applied to a hyperspectral fluorescence dataset of mineral samples. While the classification rates using only hyperspectral fluorescence images of one wavelength are low, using the pseudo spectra derived from the fusion results in an object classification rate of 85 to 90 %.","","978-1-4673-9708-7","10.1109/MFI.2016.7849555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849555","","Quaternions;Hyperspectral imaging;Minerals;Data integration;Shape;Image color analysis","fluorescence;geophysical image processing;hyperspectral imaging;image classification;image fusion;remote sensing","hyperspectral fluorescence data fusion;quaternion phase;octonion phase;hyperspectral fluorescence imaging;hyperspectral fluorescence spectra;wavelength;object classification","","","","21","IEEE","13 Feb 2017","","","IEEE","IEEE Conferences"
"Improving tropical deforestation detection by fusing multiple SAR change measures","X. Dong; S. Quegan; W. Liu; K. Cui; X. Lv","School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China; CTCD, University of Sheffield, Sheffield S3 7RH, United Kingdom; School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China; School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China; School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China","IET International Radar Conference 2015","21 Apr 2016","2015","","","1","5","The paper studies tropical deforestation detection in Riau province, Indonesia with L-band ALOS PALSAR and Cband ENVISAT ASAR data. Multiple change measures as SAR image intensity, texture, and temporal variations of ScanSAR time series are extracted and employed for deforestation detection. These measures are then combined for improving the detection with subsequent performance evaluation by comparing with the World Wildlife Fund's land cover maps as reference data. When applied on the FBD scene overlaid by both the PALSAR and ASAR data, the detection rate achieves up to 83.2%(at a false alarm rate of 20%) with a significant improvement of over 18% compared with the detection based on the single measure.","","978-1-78561-039-4","10.1049/cp.2015.1309","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7455531","Change detection;forest monitoring;synthetic aperture radar;tropical deforestation","","forestry;geophysical image processing;geophysical techniques;image fusion;image texture;radar imaging;remote sensing by radar;synthetic aperture radar;time series","tropical deforestation detection;synthetic aperture radar;fusing multiple SAR change measures;Riau province;Indonesia;L-band ALOS PALSAR;C-band ENVISAT ASAR data;SAR image intensity;ScanSAR time series;World Wildlife Fund;land cover maps;FBD scene;false alarm rate","","","","","","21 Apr 2016","","","IET","IET Conferences"
"A new pansharpening method using multistage joint bilateral filtering","Y. Pan; X. Li; L. Li; A. Gao; S. Yue","School of Electronics and Information, Northwestern Polytechnical University, Xi'an, P.R. China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, P.R. China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, P.R. China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, P.R. China; School of Computer Science, University of Lincoln, Lincoln, UK","2018 13th IEEE Conference on Industrial Electronics and Applications (ICIEA)","28 Jun 2018","2018","","","1212","1216","Due to the inaccuracy of detail extraction, most of pansharpening methods often suffer from spectral and/or spatial information distortions more or less and produce unsatisfied pansharpened results. To deal with this problem, we present a new pansharpening method using multistage joint bilateral filtering. Our method first separates the multispectral data into intensity and angular components by hyperspherical color transform (HCT). Then the proposed multistage joint bilateral filter is used to capture the different-scale details of both the panchromatic and the intensity images. Specifically, multistage joint bilateral filter performs joint bilateral filtering iteratively while the reference image remains unchanged and the filtering parameters vary progressively. The structure information with different scale is aware and the corresponding spatial details can be effectively extracted. The total spatial details are then combined and injected into the intensity component of the MS imagery. Finally, all the fused MS bands are obtained by the inverse HCT. The experiment is carried out on IKONOS and GeoEye-1 satellite images, respectively. Visual and objective analysis show that our method can produce high-quality pansharpened results and outperform some existing methods.","2158-2297","978-1-5386-3758-6","10.1109/ICIEA.2018.8397894","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8397894","pansharpening;hyperspherical color transform;joint bilateral filter","Filtering;Satellites;Spatial resolution;Image color analysis;Data mining;Distortion;Transforms","image filtering;image fusion;remote sensing","pansharpening method;multistage joint bilateral filtering;spectral information distortions;spatial information distortions;filtering parameters;hyperspherical color transform;panchromatic images;GeoEye-1 satellite images;IKONOS images","","","","16","IEEE","28 Jun 2018","","","IEEE","IEEE Conferences"
"Integrating Low-Cost LiDAR and Stereo Camera for 3D Object Detection with Lidar-Stereo Fusion and Cost Volume Enhancement","S. -M. Kao; C. -W. Ma; J. -W. Hsieh","Institute of Smart Industry and Green Energy, National Yang Ming Chiao Tung University, Tainan City, Taiwan; College of Artificial Intelligence, National Yang Ming Chiao Tung University, Tainan City, Taiwan; College of Artificial Intelligence, National Yang Ming Chiao Tung University, Tainan City, Taiwan","TENCON 2022 - 2022 IEEE Region 10 Conference (TENCON)","20 Dec 2022","2022","","","1","6","We propose a fusion network that integrates LiDAR and stereo images multiple times for 3D object detection. It fuses projected left/right LiDAR maps with stereo camera images at the beginning of the network, then adopts the LiDAR maps again for cost volume enhancement with a novel parallel fusion network (PFNet). The PFNet combines two distinctive strategies: a learnable strategy and a physical modeling strategy. The learnable design modifies HeirCCVNorm (Hierarchical Conditional Cost Volume Normalization). The physical modeling involves Gaussian embedding of LiDAR signals. Our simulations showed that the proposed network has better 3D object detection performance than a recent acclaimed state-of-the-art method when using the same dataset for training.","2159-3450","978-1-6654-5095-9","10.1109/TENCON55691.2022.9977732","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9977732","3D object detection;Depth estimation;Pseudo LiDAR;Multi-sensor feature fusion;Feature enhancement","Training;Solid modeling;Laser radar;Three-dimensional displays;Costs;Fuses;Neural networks","cameras;image fusion;object detection;optical radar;remote sensing by laser beam;stereo image processing","3D object detection performance;cost volume enhancement;distinctive strategies;Hierarchical Conditional Cost Volume Normalization;learnable strategy;LiDAR maps;LiDAR signals;lidar-stereo fusion;low-Cost LiDAR;parallel fusion network;PFNet;physical modeling strategy;stereo camera images;stereo images multiple times","","","","23","IEEE","20 Dec 2022","","","IEEE","IEEE Conferences"
"Unsupervised Image Change Detection Based on Ground-Based Imaging Radar","H. Ren; P. Huang; W. Tan; W. Xu; F. Liu; M. Zhang","Inner Mongolia Key Laboratory of Radar Technology and Application, Hohhot, China; Inner Mongolia Key Laboratory of Radar Technology and Application, Hohhot, China; Inner Mongolia Key Laboratory of Radar Technology and Application, Hohhot, China; Inner Mongolia Key Laboratory of Radar Technology and Application, Hohhot, China; Inner Mongolia Key Laboratory of Radar Technology and Application, Hohhot, China; China Geological Environment Monitoring Institute, Beijing, China","2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)","21 Aug 2020","2019","","","1","5","Multitemporal ground-based imaging radar images have been successfully used for the detection of different types of terrain changes. The image change detection of the ground-based imaging radar is used to estimate the interference change information in the monitoring area within a short time baselines to avoid misjudging the radar deformation image. Many scholars have proposed many effective detection methods for spaceborne radar images, but there are few researches on the change detection method of ground-based imaging radar images. In this paper, a change detection method for ground-based imaging radar image is proposed. To obtain the change information of ground-based imaging radar, the difference map and coherence coefficient map were used to obtain the change region. Experiment results show that the proposed method is effective for ground-based imaging radar image change detection in terms of smooth the background noise and enhance the contrast between the changed and unchanged portions.","","978-1-7281-2345-5","10.1109/ICSIDP47821.2019.9173172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173172","Ground-based imaging radar;Change detection;Gaussian operators;Coherence coefficient;K-means","","geophysical image processing;geophysical techniques;image fusion;radar imaging;remote sensing by radar;spaceborne radar;synthetic aperture radar","unsupervised image change detection;ground-based imaging radar;multitemporal ground-based;radar deformation image;spaceborne radar images;change detection method","","","","15","IEEE","21 Aug 2020","","","IEEE","IEEE Conferences"
"Scattering and Regional Features Fusion Using Collaborative Representation for PolSAR Image Classification","M. Imani","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2022 9th Iranian Joint Congress on Fuzzy and Intelligent Systems (CFIS)","19 Apr 2022","2022","","","1","6","While the collaborative representation has been used for classification of multi-channel images in several works, it is suggested for scattering and spatial features fusion of polarimetric synthetic aperture radar (PolSAR) images in this work. With considering a neighboring region around each pixel of the PolSAR image, its approximation is computed by its adjacent samples in the local region by solving a convex optimization problem. The samples with more similar scattering characteristics will have more important role in the pixel representation. The obtained collaborative representation can be considered as a fused polarimetric-contextual feature space, which can be given to any arbitrary classifier. The experimental results on three real PolSAR images show the good performance of the fused feature space in providing a clear and accurate classification map.","2771-1374","978-1-6654-7872-4","10.1109/CFIS54774.2022.9756487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9756487","PolSAR;feature fusion;classification;collaborative representation","Collaboration;Scattering;Speckle;Polarimetric synthetic aperture radar;Convex functions;Intelligent systems;Image classification","feature extraction;geophysical image processing;image classification;image fusion;radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","regional features fusion;collaborative representation;PolSAR image classification;multichannel images;spatial features fusion;polarimetric synthetic aperture radar images;neighboring region;adjacent samples;local region;convex optimization problem;similar scattering characteristics;pixel representation;polarimetric-contextual feature space;fused feature space;clear classification map;accurate classification map","","","","22","IEEE","19 Apr 2022","","","IEEE","IEEE Conferences"
"Spectral and spatial assessment of the Ikonos images fusion, using the Principal Components, Brovey Transform and Multiplicative","J. Medina; L. Becerra González; E. Upegui","Universidad Distrital Francisco Jose de Caldas, Bogota, CO; Facultad Seccional Centro de Gestión Agroempresarial del Oriente, SENA, Colombia; Universidad Distrital Francisco Jose de Caldas, Bogota, CO","2019 14th Iberian Conference on Information Systems and Technologies (CISTI)","15 Jul 2019","2019","","","1","6","This article presents the assessment of the fusion of Ikonos satellite images using three methods, the first corresponds, Principal Component second Brovey Transform and the Multiplication method implemented in the Matlab Software, with the objective of identifying which of the three methods improves the special resolution without significant loss of spectral richness. Assessment of two implemented methods were performed using the following indices: ERGAS (spectral and spatial), RASE, Universal Quality Index Qu, and Correlation Index, where the best results on spatially and spectrally richness were obtained with first method called transform Brovey.","2166-0727","978-9-8998-4349-3","10.23919/CISTI.2019.8760873","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8760873","Principal Component;Transform Brovey;Multiplication method;satellite images;fusion;Ikonos","Image resolution;Image color analysis;Matlab;Transforms;Information systems;Software;Satellites","geophysical image processing;image fusion;image resolution;principal component analysis;remote sensing;transforms","spectral assessment;spatial assessment;Ikonos images fusion;Brovey Transform;Ikonos satellite images;Matlab Software;special resolution;spectral richness;transform Brovey;principal component;multiplication method;ERGAS;RASE;universal quality index;correlation index","","","","","","15 Jul 2019","","","IEEE","IEEE Conferences"
"Contextual and Spectral Feature Fusion Using Local Binary Graph for Hyperspectral Images Classification","Z. F. Farahani; H. Ghassemian; M. Imani","Image processing and Information Analysis Lab, Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Image processing and Information Analysis Lab, Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Image processing and Information Analysis Lab, Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2021 29th Iranian Conference on Electrical Engineering (ICEE)","4 Oct 2021","2021","","","771","775","There are several approaches for hyperspectral (HS) image classification. But the best approach is using contextual and spectral information concurrently. A major challenge for researchers is fusing spectral and spatial contextual features. LBG (Local Binary Graph) is an efficient technique among them. An enhanced type of LBG is suggested in this paper, which involves the class label for feature extraction to minimize within class similarity. The proposed method considers three constraints for selection of the nearest spectral-spatial contextual neighbors and sharing between them. The constraints include the minimum distance of the spectral features vector, minimum distance of the spatial contextual features vector and the belongings to the same class. So, the proposed method can fuse the spectral and spatial contextual features with increasing the class discrimination ability. The experiments indicate that the proposed method improves the overall classification performance on Pavia University and Indian pines data sets.","2642-9527","978-1-6654-3365-5","10.1109/ICEE52715.2021.9544359","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9544359","spectral-spatial;classification;hyperspectral;feature fusion;local graph","Support vector machines;Electrical engineering;Fuses;Feature extraction;Kernel;Optimization;Hyperspectral imaging","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;land cover;remote sensing","hyperspectral image classification;feature extraction;spatial contextual features vector;local binary graph;nearest spectral-spatial contextual neighbors;Pavia University;Indian pines datasets;spectral feature fusion","","","","15","IEEE","4 Oct 2021","","","IEEE","IEEE Conferences"
"Preprocessing and fusion analysis of GF-2 satellite Remote-sensed spatial data","D. -D. Zhang; F. Xie; L. Zhang","Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, Shanghai, China; Shanghai Institute of Technical Physics, East China Normal University, Shanghai, China; MOE International Joint Lab of Trustworthy Software, East China Normal University, China","2018 International Conference on Information Systems and Computer Aided Education (ICISCAE)","14 Mar 2019","2018","","","24","29","There is no pansharpening method that can be applied to all kinds of images at present due to the principle of fusion processing and the characteristics of sensors that acquire images. In order to explore the suitable fusion method for the "" Gaofen-2 "" satellite image, PCA, HPF, Gram-Schmidt and NNDiffuse four kinds of fusion methods were selected to merge the panchromatic and multi spectral data of Gaofen-2 satellite images, the fusion results of the image were synthetically compared and evaluated with subjective evaluation and quantitative analysis. The test results show that the NNDiffuse transform method has the best combination effect and is very prominent in the fusion effect of the visible light band; And in the fusion of near-infrared band, Gram-Schmidt method can be considered. The research results of this paper can provide reference for the fusion processing and application of Gaofen-2 satellite image data.","","978-1-5386-5738-6","10.1109/ICISCAE.2018.8666873","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666873","Pansharpening;Gaofen-2;NNDiffuse;subjective evaluation;quantitative analysis","Spatial resolution;Satellites;Principal component analysis;Transforms;Distortion;Remote sensing","geophysical image processing;geophysical techniques;image fusion;remote sensing","NNDiffuse transform method;fusion effect;Gram-Schmidt method;fusion processing;Gaofen-2 satellite image data;pansharpening method;suitable fusion method;fusion methods;fusion results;fusion analysis;GF-2 satellite Remote-sensed spatial data preprocessing;PCA;HPF;panchromatic data;multi spectral data;quantitative analysis;visible light band;near-infrared band","","5","","21","IEEE","14 Mar 2019","","","IEEE","IEEE Conferences"
"PanFormer: A Transformer Based Model for Pan-Sharpening","H. Zhou; Q. Liu; Y. Wang","The State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; Hangzhou Innovation Institute, Beihang University, Hangzhou, China; The State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China","2022 IEEE International Conference on Multimedia and Expo (ICME)","26 Aug 2022","2022","","","1","6","Pan-sharpening aims at producing a high-resolution (HR) multi-spectral (MS) image from a low-resolution (LR) multi-spectral (MS) image and its corresponding panchromatic (PAN) image acquired by a same satellite. Inspired by a new fashion in recent deep learning community, we propose a novel Transformer based model for pan-sharpening. We explore the potential of Transformer in image feature extraction and fusion. Following the successful development of vision transformers, we design a two-stream network with the self-attention to extract the modality-specific features from the PAN and MS modalities and apply a cross-attention module to merge the spectral and spatial features. The pan-sharpened image is produced from the enhanced fused features. Extensive experiments on GaoFen-2 and WorldView-3 images demonstrate that our Transformer based model achieves impressive results and outperforms many existing CNN based methods, which shows the great potential of introducing Transformer to the pan-sharpening task. Codes are available at https://github.com/zhysora/PanFormer.","1945-788X","978-1-6654-8563-0","10.1109/ICME52920.2022.9859770","NSFC(grant numbers:62176017,41871283); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9859770","Pan-sharpening;transformer;attention mechanism;remote sensing","Deep learning;Satellites;Codes;Fuses;Transformers;Feature extraction;Sensors","cellular neural nets;correlation methods;feature extraction;filtering theory;geophysical image processing;image classification;image enhancement;image fusion;image resolution;learning (artificial intelligence)","novel Transformer based model;image feature extraction;vision transformers;modality-specific features;cross-attention module;spectral features;spatial features;pan-sharpened image;enhanced fused features;WorldView-3 images;existing CNN based methods;pan-sharpening task;pan-sharpening aims;MS;low-resolution multispectral image;corresponding panchromatic image;recent deep learning community","","1","","27","IEEE","26 Aug 2022","","","IEEE","IEEE Conferences"
"Guided filtering based real time flood area identification on bitemporal satellite images","A. Asokan; J. Anitha","Department of ECE, Sri Krishna College of Technology Kovaipudur, Coimbatore, Tamil Nadu, India; Department of ECE, Karunya Institute of Technology and Sciences","2021 Fourth International Conference on Electrical, Computer and Communication Technologies (ICECCT)","29 Nov 2021","2021","","","1","4","Satellite images are considered as the most reliable source of information concerning disaster affected areas. The ease of satellite image acquisition in extreme weather and frequent availability with wider coverage makes it effective for proper disaster response. This paper focuses on change detection in flood areas in bitemporal satellite imagery. The proposed method is based on three stages: guided filter for image enhancement and spatial feature extraction, difference image creation and classification. The difference image is classified to identify the changes in pre-event and post-event images to form a binary change map. The performance of the proposed method is assessed by applying on image sets acquired before and after the floods. Experiments show that the proposed method gives promising results over existing methods.","","978-1-6654-1480-7","10.1109/ICECCT52121.2021.9616955","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9616955","bitemporal;image fusion;multispectral;change map;feature extraction","Satellites;Feature extraction;Information filters;Real-time systems;Communications technology;Floods;Reliability","disasters;feature extraction;floods;geophysical image processing;image classification;image enhancement;remote sensing","time flood area identification;bitemporal satellite images;disaster affected areas;satellite image acquisition;extreme weather;frequent availability;wider coverage;proper disaster response;change detection;flood areas;bitemporal satellite imagery;image enhancement;spatial feature extraction;difference image creation;classification;post-event images;binary change map;image sets;floods","","","","20","IEEE","29 Nov 2021","","","IEEE","IEEE Conferences"
"Multi-modal local terrain maps from vision and LiDAR","H. Jaspers; M. Himmelsbach; H. -J. Wuensche","Department of Aerospace Engineering, University of the Bundeswehr München, Neubiberg, Germany; BMW Group, Munich, Germany; Department of Aerospace Engineering, University of the Bundeswehr München, Neubiberg, Germany","2017 IEEE Intelligent Vehicles Symposium (IV)","31 Jul 2017","2017","","","1119","1125","In this paper, we present a method to build precise local terrain maps for an autonomous vehicle from vision and LiDAR that surpass most existing maps used in the field, both in the details they represent and in efficiency of construction. The high level of detail is obtained by spatio-temporal fusion of data from multiple, complementary sensors in a grid map. The map not only consists of obstacle probabilities, but contains different features of the environment: elevation, color, infrared reflectivity, terrain slopes and surface roughness. Still, an efficient way to manage the map's memory allows us to build the maps online on-board our autonomous vehicle. As we demonstrate by describing some of its applications, the maps can serve as a unified representation to solve further perception and navigation problems without having to resort to individual sensor data again. The proposed terrain maps have proven their robustness and precision in many real-world scenarios, leading to award-winning performances at international robotics competitions.","","978-1-5090-4804-5","10.1109/IVS.2017.7995863","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7995863","","Sensors;Laser radar;Image color analysis;Three-dimensional displays;Computer architecture;Autonomous vehicles;Navigation","geophysical image processing;image fusion;image representation;mobile robots;optical radar;probability;remote sensing by laser beam;remote sensing by radar;robot vision;surface roughness;terrain mapping","multimodal local terrain maps;LiDAR;autonomous vehicle;spatiotemporal data fusion;complementary sensors;grid map;obstacle probabilities;infrared reflectivity;terrain slopes;surface roughness;elevation;color;map memory;international robotics competitions;vision","","11","","38","IEEE","31 Jul 2017","","","IEEE","IEEE Conferences"
"Analyzing the Cross-Sensor Portability of Neural Network Architectures for LiDAR-based Semantic Labeling","F. Piewak; P. Pinggera; M. Zöllner","Daimler AG, R&D, Stuttgart, Germany; Daimler AG, R&D, Stuttgart, Germany; Karlsruhe Institute of Technology, Karlsruhe, Germany","2019 IEEE Intelligent Transportation Systems Conference (ITSC)","28 Nov 2019","2019","","","3419","3426","State-of-the-art approaches for the semantic labeling of LiDAR point clouds heavily rely on the use of deep Convolutional Neural Networks (CNNs). However, transferring network architectures across different LiDAR sensor types represents a significant challenge, especially due to sensor specific design choices with regard to network architecture as well as data representation. In this paper we propose a new CNN architecture for the point-wise semantic labeling of LiDAR data which achieves state-of-the-art results while increasing portability across sensor types. This represents a significant advantage given the fast-paced development of LiDAR hardware technology. We perform a thorough quantitative cross-sensor analysis of semantic labeling performance in comparison to a state-of-the-art reference method. Our evaluation shows that the proposed architecture is indeed highly portable, yielding an improvement of 10 percentage points in the Intersectionover-Union (IoU) score when compared to the reference approach. Further, the results indicate that the proposed network architecture can provide an efficient way for the automated generation of large-scale training data for novel LiDAR sensor types without the need for extensive manual annotation or multi-modal label transfer.","","978-1-5386-7024-8","10.1109/ITSC.2019.8917412","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8917412","","Semantics;Feature extraction;Three-dimensional displays;Laser radar;Labeling;Computer architecture;Task analysis","convolutional neural nets;geophysical image processing;image fusion;image segmentation;learning (artificial intelligence);optical radar;remote sensing by laser beam;remote sensing by radar","large-scale training data;novel LiDAR sensor types;multimodal label transfer;cross-sensor portability;LiDAR-based semantic labeling;LiDAR point;deep convolutional neural networks;transferring network architectures;different LiDAR sensor types;specific design choices;network architecture;data representation;CNN architecture;point-wise semantic labeling;LiDAR data;LiDAR hardware technology;quantitative cross-sensor analysis;semantic labeling performance;state-of-the-art reference method;10 percentage points;reference approach","","4","","34","IEEE","28 Nov 2019","","","IEEE","IEEE Conferences"
"Superpixel-Based Feature Extraction and Fusion Method for Hyperspectral and LiDAR Classification","S. Jia; M. Zhang; J. Xian; J. Zhuang; Q. Huang","College of Computer Science and Software Engineering, Shenzhen University; College of Computer Science and Software Engineering, Shenzhen University; College of Computer Science and Software Engineering, Shenzhen University; College of Computer Science and Software Engineering, Shenzhen University; College of Computer Science and Software Engineering, Shenzhen University","2018 24th International Conference on Pattern Recognition (ICPR)","29 Nov 2018","2018","","","764","769","In this paper, we propose a new efficient superpixel-based feature extraction and fusion method on hyperspectral and LiDAR data. Such important factor that the adjacent pixels belong to the same class with high probability is taken into consideration in our method, which means each superpixel can be regarded as a small region consisting of a number of pixels with similar spectral characteristics. In order to represent each superpixel well, we use our Gabor-wavelet-based feature extraction approach instead of morphological APs. A feature selection and fusion process has also been used to reduce the redundancy among Gabor features and make the fused feature more discriminative. The results on the several real dataset indicate that the proposed method provides state-of-the-art classification results, respectively, even when only few samples, i.e., only three samples per class, are labeled.","1051-4651","978-1-5386-3788-3","10.1109/ICPR.2018.8545238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8545238","","Feature extraction;Hyperspectral imaging;Laser radar;Wavelet domain;Entropy;Image segmentation","feature extraction;geophysical image processing;image classification;image fusion;image representation;optical radar;remote sensing by laser beam;remote sensing by radar;wavelet transforms","fusion method;LiDAR classification;LiDAR data;adjacent pixels;similar spectral characteristics;Gabor-wavelet-based feature extraction approach;feature selection;fusion process;Gabor features;hyperspectral classification;superpixel-based feature extraction","","2","","14","IEEE","29 Nov 2018","","","IEEE","IEEE Conferences"
"MLFFNet: Multi-level Feature Fusion Net for Underwater Image Enhancement","X. Liu; Z. Gao; B. M. Chen","School of Electrical and Computer Engineering, National University of Singapore, Singapore; School of Remote Sensing and Information Engieering, Wuhan University, Wuhan, China; Department of Mechanical and Automation Engineering, Chinese University of Hong Kong, Shatin, N.T.","OCEANS 2019 MTS/IEEE SEATTLE","20 Jan 2020","2019","","","1","6","Underwater image enhancement has been attracting much attention recently since many underwater vision tasks are relying on the acquisition of clear underwater images. Inspired by the success of deep convolutional neural networks (CNNs) for many high-level vision tasks, in this paper, we propose a multi-scale feature fusion based neural network for underwater image enhancement. First, multi-scale features, including the local features and global features, are extracted. Then, we propose to fuse the global feature with local feature at each scale dynamically. Considering the global features encodes the high-level semantic information and local features holds the structure details at different scales, this fusion strategy is beneficial to underwater image enhancement. Extensive experiments are conducted and the comparisons are made among the state of the art methods as well, where we show great improvements.","0197-7385","978-0-578-57618-3","10.23919/OCEANS40490.2019.8962577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8962577","","","computer vision;convolutional neural nets;feature extraction;image enhancement;image fusion","multilevel feature fusion net;underwater image enhancement;underwater vision tasks;clear underwater images;high-level vision tasks;multiscale feature fusion;deep convolutional neural networks;CNN;high-level semantic information","","2","","18","","20 Jan 2020","","","IEEE","IEEE Conferences"
"SAR target recognition method of MSTAR data set based on multi-feature fusion","J. Shi","Beijing Institute of Remote Sensing Equipment, The Second Research Institute of China Aerospace Science and Industry Corporation, Beijing, China","2022 International Conference on Big Data, Information and Computer Network (BDICN)","20 Apr 2022","2022","","","626","632","To solve the problem of low recognition rate of synthetic aperture radar (SAR) target based on feature recognition, a target recognition method of SAR image based on multi-feature fusion is proposed, which combines Hu moment, Harris corner point and Gabor feature. The three kinds of features describe the target's geometric shape feature, corner feature and image texture feature respectively, which can improve the accuracy of SAR target recognition from the aspect of feature extraction. Based on the MSTAR data set, the experiment is carried out under standard and extended operating conditions. The results show that the proposed method can effectively overcome the deficiency of insufficient single feature description information and improve the SAR target recognition rate to a certain extent.","","978-1-6654-8476-3","10.1109/BDICN55575.2022.00120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9758449","component;synthetic aperture radar;target recognition;multi-feature fusion;feature extraction","Training;Image texture;Image recognition;Target recognition;Shape;Feature extraction;Radar polarimetry","feature extraction;image fusion;image texture;radar imaging;radar target recognition;synthetic aperture radar","low recognition rate;synthetic aperture radar target;feature recognition;SAR image;multifeature fusion;Harris corner point;Gabor feature;corner feature;image texture feature;feature extraction;MSTAR data;insufficient single feature description information;SAR target recognition rate;Hu moment","","","","21","IEEE","20 Apr 2022","","","IEEE","IEEE Conferences"
"Spectral Quality Evaluation of Reconstructed Hyperspectral Images","S. Tang; Z. Chen; M. Zhang","Doctoral Students, Department of Computer Science and Electrical Engineering, University of Missouri, Kansas City; Associate Professor, Department of Civil and Mechanical Engineering, University of Missouri, Kansas City; Doctoral Students, Department of Computer Science and Electrical Engineering, University of Missouri, Kansas City","2021 11th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Jul 2021","2021","","","1","5","With the advance in imaging optics, hyperspectral images (or cubes) have become low-cost and real-time for acquiring images in the field, specifically thanks to the recent development of different 'snapshot' hyperspectral imaging systems. However, cameras producing high resolutions in both the spectral domains and the spatial domains are still rare or considered to be high-cost. Algorithm-based pansharpening, or in general image reconstruction methods, are often used to create high spatial-resolution cubes by fusing high-spatial gray or color images and low spatial-resolution hyperspectral images. Moreover, most of these methods emphasized achieving high visual quality in spatial resolution but not considering the spectral accuracy in the reconstructed images. This paper aims to evaluate the spectral quality of reconstructed images from multiple methods. A commercial hyperspectral camera (Cubert S185) was used to conduct the research. Important conclusions include that spectral information is lost to different degrees per different reconstruction methods when the spatial resolution is raised too high. The trade-off between spatial sharpening and retaining spectral information is important for machine learning tasks.","2158-6276","978-1-6654-3601-4","10.1109/WHISPERS52202.2021.9483970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9483970","Hyperspectral image;Super Resolution;Spectrum analysis","Support vector machines;Visualization;Vegetation mapping;Cameras;Spatial resolution;Task analysis;Image reconstruction","cameras;hyperspectral imaging;image colour analysis;image fusion;image reconstruction;image resolution;learning (artificial intelligence)","high spatial-resolution cubes;high-spatial gray image fusion;machine learning tasks;Cubert S185;low spatial-resolution hyperspectral images;snapshot hyperspectral imaging systems;algorithm-based pan sharpening;hyperspectral image reconstruction methods;spectral information;spatial sharpening;commercial hyperspectral camera;spectral accuracy;spatial resolution;high visual quality;color images;general image reconstruction methods;spatial domains;spectral domains;imaging optics;spectral quality evaluation","","","","18","IEEE","19 Jul 2021","","","IEEE","IEEE Conferences"
"A Triple-Double Convolutional Neural Network for Panchromatic Sharpening","T. -J. Zhang; L. -J. Deng; T. -Z. Huang; J. Chanussot; G. Vivone","Yingcai Honors College, University of Electronic Science and Technology of China, Chengdu, Sichuan 611731, China.; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, Sichuan 611731, China.; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, Sichuan 611731, China.; Laboratoire Jean Kuntzmann, Inria, CNRS, Grenoble INP, Université Grenoble Alpes, 38000 Grenoble, France.; National Research Council-Institute of Methodologies for Environmental Analysis, CNR-IMAA, 85050 Tito Scalo, Italy.","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","14","Pansharpening refers to the fusion of a panchromatic (PAN) image with a high spatial resolution and a multispectral (MS) image with a low spatial resolution, aiming to obtain a high spatial resolution MS (HRMS) image. In this article, we propose a novel deep neural network architecture with level-domain-based loss function for pansharpening by taking into account the following double-type structures, i.e., double-level, double-branch, and double-direction, called as triple-double network (TDNet). By using the structure of TDNet, the spatial details of the PAN image can be fully exploited and utilized to progressively inject into the low spatial resolution MS (LRMS) image, thus yielding the high spatial resolution output. The specific network design is motivated by the physical formula of the traditional multi-resolution analysis (MRA) methods. Hence, an effective MRA fusion module is also integrated into the TDNet. Besides, we adopt a few ResNet blocks and some multi-scale convolution kernels to deepen and widen the network to effectively enhance the feature extraction and the robustness of the proposed TDNet. Extensive experiments on reduced- and full-resolution datasets acquired by WorldView-3, QuickBird, and GaoFen-2 sensors demonstrate the superiority of the proposed TDNet compared with some recent state-of-the-art pansharpening approaches. An ablation study has also corroborated the effectiveness of the proposed approach. The code is available at https://github.com/liangjiandeng/TDNet.","2162-2388","","10.1109/TNNLS.2022.3155655","National Natural Science Foundation of China(grant numbers:12171072,61702083); Key Projects of Applied Basic Research in Sichuan Province(grant numbers:2020YJ0216); National Key Research and Development Program of China(grant numbers:2020YFA0714001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732243","Deep convolutional neural networks (CNNs);multi-resolution analysis (MRA);multi-scale feature extraction;multispectral (MS) image fusion;pansharpening;remote sensing;triple-double network (TDNet).","Pansharpening;Spatial resolution;Convolutional neural networks;Convolution;Kernel;Image resolution;Feature extraction","","","","9","","","IEEE","9 Mar 2022","","","IEEE","IEEE Early Access Articles"
"HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening","W. G. C. Bandara; V. M. Patel","Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA; Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","1757","1767","Pansharpening aims to fuse a registered high-resolution panchromatic image (PAN) with a low-resolution hyper-spectral image (LR-HSI) to generate an enhanced HSI with high spectral and spatial resolution. Existing pansharpening approaches neglect using an attention mechanism to transfer HR texture features from PAN to LR-HSI features, resulting in spatial and spectral distortions. In this paper, we present a novel attention mechanism for pansharpening called HyperTransformer, in which features of LR-HSI and PAN are formulated as queries and keys in a transformer, respectively. HyperTransformer consists of three main modules, namely two separate feature extractors for PAN and HSI, a multi-head feature soft-attention module, and a spatial-spectral feature fusion module. Such a network improves both spatial and spectral quality measures of the pansharpened HSI by learning cross-feature space dependencies and long-range details of PAN and LR-HSI. Furthermore, HyperTransformer can be utilized across multiple spatial scales at the backbone for obtaining improved performance. Extensive experiments conducted on three widely used datasets demonstrate that HyperTransformer achieves significant improvement over the state-of-the-art methods on both spatial and spectral quality measures. Implementation code and pretrained weights can be accessed at https://github.com/wgcban/HyperTransformer.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880014","Photogrammetry and remote sensing","Computer vision;Codes;Fuses;Pansharpening;Feature extraction;Transformers;Extraterrestrial measurements","feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;image resolution;image texture;learning (artificial intelligence);object detection;remote sensing;spectral analysis","pansharpening aims;registered high-resolution panchromatic image;PAN;low-resolution hyper-spectral image;enhanced HSI;high spectral resolution;spatial resolution;pansharpening approaches neglect;attention mechanism;HR texture features;LR-HSI features;spatial distortions;spectral distortions;separate feature extractors;multihead feature soft-attention module;spatial-spectral feature fusion module;spatial quality measures;spectral quality measures;pansharpened HSI;cross-feature space dependencies;multiple spatial scales","","3","","61","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Self-Supervised Super-Resolution for Multi-Exposure Push-Frame Satellites","N. L. Nguyen; J. Anger; A. Davy; P. Arias; G. Facciolo","CNRS, ENS Paris-Saclay, Centre Borelli, Universite Paris-Saclay, France; Kayrros SAS; CNRS, ENS Paris-Saclay, Centre Borelli, Universite Paris-Saclay, France; CNRS, ENS Paris-Saclay, Centre Borelli, Universite Paris-Saclay, France; CNRS, ENS Paris-Saclay, Centre Borelli, Universite Paris-Saclay, France","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","1848","1858","Modern Earth observation satellites capture multi-exposure bursts of push-frame images that can be super-resolved via computational means. In this work, we propose a super-resolution method for such multi-exposure sequences, a problem that has received very little attention in the literature. The proposed method can handle the signal-dependent noise in the inputs, process sequences of any length, and be robust to inaccuracies in the exposure times. Furthermore, it can be trained end-to-end with self-supervision, without requiring ground truth high resolution frames, which makes it especially suited to handle real data. Central to our method are three key contributions: i) a base-detail decomposition for handling errors in the exposure times, ii) a noise-level-aware feature encoding for improved fusion of frames with varying signal-to-noise ratio and iii) a permutation invariant fusion strategy by temporal pooling operators. We evaluate the proposed method on synthetic and real data and show that it outperforms by a significant margin existing single-exposure approaches that we adapted to the multi-exposure case.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00190","GENCI-IDRIS(grant numbers:2022-AD011012453RI,2022-ADO11012458RI); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880235","Photogrammetry and remote sensing; Computational photography; Low-level vision; Self-& semi-& meta- & unsupervised learning","Photography;Earth;Satellites;Superresolution;Encoding;Pattern recognition;Signal resolution","cameras;geophysical image processing;image classification;image fusion;image reconstruction;image representation;image resolution;image sensors;image sequences;learning (artificial intelligence);remote sensing;terrain mapping","process sequences;exposure times;self-supervision;ground truth high resolution frames;noise-level-aware feature encoding;signal-to-noise ratio;single-exposure;multiexposure case;self-supervised super-resolution;multiexposure push-frame satellites;Modern Earth observation;capture multiexposure bursts;push-frame images;computational means;super-resolution method;multiexposure sequences;signal-dependent noise","","3","","58","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Transfer Learning for Airport Detection Based on Saliency Fusion of Parallel Lines and Regions of Interest","W. Huang; J. Xia; M. Xu; J. Huang","AI Research Center, Wuhan Digital, Engineering Institute, Wuhan, China; AI Research Center, Wuhan Digital, Engineering Institute, Wuhan, China; AI Research Center, Wuhan Digital, Engineering Institute, Wuhan, China; AI Research Center, Wuhan Digital, Engineering Institute, Wuhan, China","2019 IEEE 7th International Conference on Computer Science and Network Technology (ICCSNT)","20 Jan 2020","2019","","","200","207","Most existing airport detection methods for remote sensing image utilizes linear features of the airport runway insufficiently, and the computational complexity is high due to multi-scale anchor matching and global searching within full image. To solve this problem, an airport detection method based on saliency fusion of parallel lines and regions of interest is presented in this paper. Firstly the parallelism feature of the airport runway is extracted based on the prior knowledge of airport, and then regions of interest (ROI) is obtained according to the improved graph based visual saliency (GBVS). The airport is then located through the saliency fusion of parallel lines and regions of interest. Finally airport detection is achieved by transfer learning. The experimental results demonstrate that the proposed method is more advantageous than the comparison algorithms in terms of detection accuracy, processing speed, and false alarm rate. Moreover, the method only requires a small amount of samples for model training.","","978-1-7281-3299-0","10.1109/ICCSNT47585.2019.8962470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8962470","airport detection;parallel line;graph based visual saliency;saliency fusion;transfer learning","Airports;Feature extraction;Image segmentation;Remote sensing;Visualization;Atmospheric modeling;Computational modeling","aerospace engineering;airports;edge detection;feature extraction;graph theory;image classification;image fusion;learning (artificial intelligence);object detection","transfer learning;saliency fusion;parallel lines;existing airport detection methods;remote sensing image utilizes;airport runway;multiscale anchor matching;airport detection method;parallelism feature;visual saliency;detection accuracy","","","","22","IEEE","20 Jan 2020","","","IEEE","IEEE Conferences"
"Fever detection for dynamic human environment using sensor fusion","H. Fallah-Haghmohammadi; D. -S. Necsulecsu","Department of Mechanical Engineering, University of Ottawa, Ottawa, ON, Canada; Department of Mechanical Engineering, University of Ottawa, Ottawa, ON, Canada","2017 International Conference on Optimization of Electrical and Electronic Equipment (OPTIM) & 2017 Intl Aegean Conference on Electrical Machines and Power Electronics (ACEMP)","13 Jul 2017","2017","","","881","886","The objective of this paper is to present an algorithm for processing infrared images and accomplishing automatic detection and path tracking of moving subjects with fever. The detection is based on two main features: the distinction between the geometry of a human face and other objects in the field of view of the camera and the temperature of the radiating object. These features are used for tracking the identified person with fever. The position of camera with respect to direction of motion the walkers appeared to be critical in this process. Infrared thermography is a remote sensing technique used to measure temperatures based on emitted infrared radiation. This application may be used for fever screening in major public places such as airports and hospitals. For this study, we first look at human body and objects in a line of view with different temperatures that would be higher than the normal human body temperature (37.8C at morning and 38.3C at evening). As a part of the experimental study, two humans with different body temperatures walking a path were subjected to automatic fever detection applied for tracking the detected human with fever. The algorithm consists of image processing to threshold objects based on the temperature and template matching used for fever detection in a dynamic human environment.","","978-1-5090-4489-4","10.1109/OPTIM.2017.7975081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975081","","Face;Heuristic algorithms;Temperature measurement;Cameras;Temperature sensors;Shape;Object recognition","biomedical optical imaging;biothermics;cameras;diseases;image fusion;image matching;infrared imaging;medical image processing;temperature measurement","dynamic human environment;sensor fusion;infrared image processing;path tracking;moving subject;human face;camera;radiating object temperature;infrared thermography;remote sensing technique;temperature measurement;emitted infrared radiation;fever screening;human body temperature;body temperatures walking;automatic fever detection;template matching","","8","","19","IEEE","13 Jul 2017","","","IEEE","IEEE Conferences"
"A Tasseled Cap Transformation for GF-2 Fused Multispectral Images","Q. Liu","State Key Lab. of Resources and Environmental Information System, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, China","2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","23 Jan 2020","2019","","","1","5","Compared with spectral bands and vegetation indices, the tasseled cap transformation (TCT) components can associate remote sensing spectral information with physical characteristics of vegetation classes, and can yield slightly better classification results. However, the TCT parameters must be recalculated for a new sensor or a different spectral feature data (digital number (DN), reflectance factor, or at-satellite reflectance). In this paper, the TCT parameters appropriate for the spectral reflectance of fused high spatial resolution multispectral images from Gaofen 2 (GF-2) were derived based on the optimal seasonal image (April 30, 2016) for classifying the quasi-circular vegetation patches sparsely distributed in the Yellow River Delta, China. The tasseled cap parameters from this study will enhance the applications of GF-2 images in detecting vegetation patch pattern, classifying crop types, mapping the dieback of forest and forest health, and monitoring land use and land cover changes.","","978-1-7281-4852-6","10.1109/CISP-BMEI48845.2019.8966053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8966053","Gaofen 2 spectral reflectance;tasseled cap parameters;pan-sharpening;Yellow River Delta;vegetation patch","","crops;geophysical image processing;image classification;image fusion;image resolution;rivers;vegetation mapping","GF-2 Fused Multispectral Images;spectral bands;vegetation indices;tasseled cap transformation components;remote sensing spectral information;physical characteristics;vegetation classes;TCT parameters;reflectance factor;at-satellite reflectance;spectral reflectance;fused high spatial resolution multispectral images;Gaofen 2;optimal seasonal image;quasicircular vegetation patches;tasseled cap parameters;GF-2 images;vegetation patch pattern;classification results;spectral feature data;digital number;AD 2016 04 30;Yellow River Delta;China;crop types;forest dieback mapping;forest health;land use change;land cover change","","1","","35","IEEE","23 Jan 2020","","","IEEE","IEEE Conferences"
"Combination of spectral unmixing algorithms for the fusion of the hyperspectral and multispectral data with unknown spectral response function","B. M. Amine","URD-SE, Sidi Bel Abbès, Algeria","2017 5th International Conference on Electrical Engineering - Boumerdes (ICEE-B)","14 Dec 2017","2017","","","1","6","In this paper Least Square-Non Negative Matrix Factorization Spectral Unmixing Combination (LS-NNMF-SUC) is presented for the fusion of hyperspectral (HS) and multispectral (MS) data. The observed HS and MS images are respectively the spatial and the spectral degradations according to the sensor characteristics of the High spatial-resolution HS (HHS) image, which is reconstructed based on the high spectral information of Low spatial resolution HS (LHS) image represented by endmembers and high spatial information of High spatial-resolution MS (HMS) image represented by abundances. In this work, the proposed algorithm deals with practical remote sensing situation, where the spectral relationship between the observed HMS image and the estimated HHS image is unknown. As a result, a Spectral Unmixing Combination (SUC) diagram based on Least square (LS) and Non-negative Matrix Factorization (NNMF) Spectral Unmixing is developed, in which the one loop NNMF and LS Spectral Unmixing is performed on the MS and HS images sequentially. The spatial spread transform matrix of the sensor observation model is used to produce the matched abundances of the LHS image, in order to unmix the later. Simulation results performed on HYDICE and AVIRIS data demonstrate the efficiency of the proposed fusion algorithm.","","978-1-5386-0686-5","10.1109/ICEE-B.2017.8192020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8192020","component;unmixing;fusion;hyperspectral;multispectral","Hyperspectral imaging;Matrices;Spatial resolution;Transforms;Image reconstruction","geophysical image processing;image fusion;image resolution;matrix decomposition","Least Square-NonNegative Matrix Factorization Spectral Unmixing Combination;unknown spectral response function;multispectral data;hyperspectral data;spectral unmixing algorithms;fusion algorithm;LHS image;sensor observation model;spatial spread;Spectral Unmixing Combination diagram;estimated HHS image;observed HMS image;spectral relationship;practical remote sensing situation;High spatial-resolution MS image;high spatial information;Low spatial resolution HS image;high spectral information;High spatial-resolution HS image;spectral degradations","","1","","14","IEEE","14 Dec 2017","","","IEEE","IEEE Conferences"
"Joint Learning from Earth Observation and OpenStreetMap Data to Get Faster Better Semantic Maps","N. Audebert; B. Le Saux; S. Lefèvre","ONERA, The French Aerospace Lab, Palaiseau, France; Office National d'Etudes et de Recherches Aerospatiales, Chatillon, ÃŽle-de-France, FR; Univ. Bretagne-Sud, IRISA, Vannes, France","2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","24 Aug 2017","2017","","","1552","1560","In this work, we investigate the use of OpenStreetMap data for semantic labeling of Earth Observation images. Deep neural networks have been used in the past for remote sensing data classification from various sensors, including multispectral, hyperspectral, SAR and LiDAR data. While OpenStreetMap has already been used as ground truth data for training such networks, this abundant data source remains rarely exploited as an input information layer. In this paper, we study different use cases and deep network architectures to leverage OpenStreetMap data for semantic labeling of aerial and satellite images. Especially, we look into fusion based architectures and coarseto- fine segmentation to include the OpenStreetMap layer into multispectral-based deep fully convolutional networks. We illustrate how these methods can be successfully used on two public datasets: ISPRS Potsdam and DFC2017. We show that OpenStreetMap data can efficiently be integrated into the vision-based deep learning models and that it significantly improves both the accuracy performance and the convergence speed of the networks.","2160-7516","978-1-5386-0733-6","10.1109/CVPRW.2017.199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014933","","Semantics;Roads;Training;Buildings;Labeling;Sensors;Optical imaging","feedforward neural nets;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);neural net architecture;remote sensing","convergence speed;accuracy performance;vision-based deep learning models;DFC2017 datasets;ISPRS Potsdam datasets;multispectral-based deep fully convolutional networks;OpenStreetMap layer;coarse-to-fine segmentation;fusion based architectures;satellite images;aerial images;input information layer;data source;multispectral data;hyperspectral data;SAR data;LiDAR data;remote sensing data classification;deep neural network architectures;Earth Observation images;semantic labeling;OpenStreetMap data","","47","","34","IEEE","24 Aug 2017","","","IEEE","IEEE Conferences"
"FusAtNet: Dual Attention based SpectroSpatial Multimodal Fusion Network for Hyperspectral and LiDAR Classification","S. Mohla; S. Pande; B. Banerjee; S. Chaudhuri","Deptt. of Electrical Engineering, IIT Bombay, Mumbai, India; Centre of Studies in Resources Engineering, IIT Bombay, Mumbai, India; Centre of Studies in Resources Engineering, IIT Bombay, Mumbai, India; Deptt. of Electrical Engineering, IIT Bombay, Mumbai, India","2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","28 Jul 2020","2020","","","416","425","With recent advances in sensing, multimodal data is becoming easily available for various applications, especially in remote sensing (RS), where many data types like multispectral imagery (MSI), hyperspectral imagery (HSI), LiDAR etc. are available. Effective fusion of these multisource datasets is becoming important, for these multimodality features have been shown to generate highly accurate land-cover maps. However, fusion in the context of RS is non-trivial considering the redundancy involved in the data and the large domain differences among multiple modalities. In addition, the feature extraction modules for different modalities hardly interact among themselves, which further limits their semantic relatedness. As a remedy, we propose a feature fusion and extraction framework, namely FusAtNet, for collective land-cover classification of HSIs and LiDAR data in this paper. The proposed framework effectively utilizses HSI modality to generate an attention map using ""self-attention"" mechanism that highlights its own spectral features. Similarly, a ""cross-attention"" approach is simultaneously used to harness the LiDAR derived attention map that accentuates the spatial features of HSI. These attentive spectral and spatial representations are then explored further along with the original data to obtain modality-specific feature embeddings. The modality oriented joint spectro-spatial information thus obtained, is subsequently utilized to carry out the land-cover classification task. Experimental evaluations on three HSILiDAR datasets show that the proposed method achieves the state-of-the-art classification performance, including on the largest HSI-LiDAR dataset available, University of Houston (Data Fusion Contest - 2013), opening new avenues in multimodal feature fusion for classification.","2160-7516","978-1-7281-9360-1","10.1109/CVPRW50498.2020.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9150738","","Feature extraction;Laser radar;Task analysis;Hyperspectral sensors;Sensors;Machine learning","feature extraction;geophysical image processing;image classification;image fusion;optical radar;remote sensing by laser beam;sensor fusion;terrain mapping","SpectroSpatial multimodal Fusion network;LiDAR classification;multimodal data;remote sensing;hyperspectral imagery;effective fusion;multisource datasets;highly accurate land-cover maps;feature extraction modules;semantic relatedness;extraction framework;FusAtNet;collective land-cover classification;HSI modality;attention map;self-attention mechanism;spectral features;spatial features;attentive spectral representations;spatial representations;modality-specific feature embeddings;joint spectro-spatial information;land-cover classification task;state-of-the-art classification performance;data fusion contest;multimodal feature fusion;HSI-LiDAR dataset","","45","","48","IEEE","28 Jul 2020","","","IEEE","IEEE Conferences"
"A Survey of Multimodal Sensor Fusion for Passive RF and EO Information Integration","A. Vakil; J. Liu; P. Zulch; E. Blasch; R. Ewing; J. Li","Oakland University, Rochester, MI, USA; Oakland University, Rochester, MI, USA; Information Directorate, Air Force Research Laboratory, Rome, NY, USA; Air Force Office of Scientific Research, Arlington, VA, USA; Sensors Directorate, Air Force Research Laboratory, Dayton, OH, USA; Oakland University, Rochester, MI, USA","IEEE Aerospace and Electronic Systems Magazine","7 Jul 2021","2021","36","7","44","61","Integrating information collected by different types of sensors observing the same or related phenomenon can lead to more accurate and robust decision making. The purpose of this article is to review sensor fusion approaches to achieve passive radio frequency (RF) and electro-optical (EO) sensor fusion and to present the proposed fusion of EO/RF neural network (FERNN). While research has been conducted to integrate complementary data collected by EO and RF modalities, the processing of RF data usually applies traditional features, such as Doppler. This article explores the viability of using the histogram of I/Q (in-phase and quadrature) data for the purposes of augmenting the detection accuracy that EO input alone is incapable of achieving. Specifically, by processing the histogram of I/Q data via deep learning and enhancing feature input for neural network fusion. Using the simulated data from the Digital Imaging and Remote Sensing Image Generation dataset, FERNN can achieve 95% accuracy in vehicle detection and scenario categorization, which is a 23% improvement over the accuracy achieved by a stand-alone EO sensor.","1557-959X","","10.1109/MAES.2020.3006410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9475915","","Radio frequency;Histograms;Multimodal sensors;Machine vision;Vehicle detection;Heuristic algorithms;Hidden Markov models","data integration;decision making;deep learning (artificial intelligence);image enhancement;image fusion;neural nets;optical information processing;radiofrequency imaging;remote sensing","FERNN;RF data processing;deep learning;multimodal sensor fusion;passive RF;EO information integration;decision making;complementary data integration;feature input enhancement;remote sensing image generation dataset;digital imaging dataset;fusion of EO/RF neural network;electro-optical sensor fusion;I/Q data processing;passive radio frequency","","13","","59","IEEE","7 Jul 2021","","","IEEE","IEEE Magazines"
"Overcoming Missing and Incomplete Modalities with Generative Adversarial Networks for Building Footprint Segmentation","B. Bischke; P. Helber; F. Koenig; D. Borth; A. Dengel","TU Kaiserslautern, Germany; TU Kaiserslautern, Germany; TU Kaiserslautern, Germany; German Research Center for Artificial Intelligence (DFKI), Germany; TU Kaiserslautern, Germany","2018 International Conference on Content-Based Multimedia Indexing (CBMI)","1 Nov 2018","2018","","","1","6","The integration of information acquired with different modalities, spatial resolution and spectral bands has shown to improve predictive accuracies. Data fusion is therefore one of the key challenges in remote sensing. Most prior work focusing on multi-modal fusion, assumes that modalities are always available during inference. This assumption limits the applications of multi-modal models since in practice the data collection process is likely to generate data with missing, incomplete or corrupted modalities. In this paper, we show that Generative Adversarial Networks can be effectively used to overcome the problems that arise when modalities are missing or incomplete. Focusing on semantic segmentation of building footprints with missing modalities, our approach achieves an improvement of about 2% on the Intersection over Union (IoU) against the same network that relies only on the available modality.","","978-1-5386-7021-7","10.1109/CBMI.2018.8516271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516271","Generative Adversarial Networks;Semantic Segmentation;Missing Modalities","Generators;Image segmentation;Gallium nitride;Generative adversarial networks;Training;Buildings;Feature extraction","image fusion;image segmentation;remote sensing","missing modalities;incomplete modalities;spatial resolution;spectral bands;data fusion;remote sensing;multimodal fusion;data collection process;corrupted modalities;semantic segmentation;generative adversarial networks;footprint segmentation;intersection over union;IoU","","7","","30","IEEE","1 Nov 2018","","","IEEE","IEEE Conferences"
"Manifold learning algorithms for sensor fusion of image and radio-frequency data","D. Shen; P. Zulch; M. Disasio; E. Blasch; G. Chen; Z. Wang; J. Lu; R. Niu","Intelligent Fusion Technology, Inc., MD; Information Directorate, Air Force Research Lab., Rome, NY; Information Directorate, Air Force Research Lab., Rome, NY; Air Force Office Scientific Research (AFOSR), Arlington, VA; Intelligent Fusion Technology, Inc., MD; Intelligent Fusion Technology, Inc., MD; Intelligent Fusion Technology, Inc., MD; Virginia Commonwealth University, Richmond, VA","2018 IEEE Aerospace Conference","28 Jun 2018","2018","","","1","9","This paper presents a joint manifold learning based heterogenous data fusion approach for image and radio frequency (RF) data. A typic scenario includes several objects (with RF emitters), which are observed by Medium Wavelength Infrared (MWIR) cameras and RF Doppler sensors. The sensor modalities of images and Doppler effects are analyzed in a way that joint manifolds can be formed by stacking up the image and Doppler data. The image data provide the aerial position and velocities of objects while the Doppler data represent the radial speeds of the objects. The proposed fusion approach exploits the manifold learning algorithms for fast and accurate sensor fusion solutions. The fusion framework has two phases: training and testing. In the training phase, the various manifold learning algorithms are applied to extract the intrinsic information via dimension reduction. Then, the raw manifold learning results (i.e., the dimension reduction results) are mapped to object trajectories of interest. The fusion results are compared with the ground truth data to evaluate the performance, based on which optimal manifold learning algorithm is selected. After the training phase, the manifold learning matrices and linear regression matrices are fixed. These matrices are used in the testing phase for multiple sensor data applications. Eight manifold learning algorithms are implemented and evaluated on Digital Imaging and Remote Sensing Image Generation (DIRSIG) scenes with MWIR data as well as distributed radiofrequency (RF) Doppler data from the same scene.","","978-1-5386-2014-4","10.1109/AERO.2018.8396395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8396395","","Manifolds;Radio frequency;Doppler effect;Dimensionality reduction;Sensor fusion;Training;Cameras","cameras;Doppler effect;image fusion;learning (artificial intelligence);matrix algebra;regression analysis;remote sensing","heterogenous data fusion approach;image data;raw manifold learning results;optimal manifold learning algorithm;manifold learning matrices;multiple sensor data applications;MWIR data;RF data;medium wavelength infrared cameras;RF doppler sensors;sensor fusion;radio frequency doppler data;aerial position;objects velocity;radial speeds;intrinsic information;dimension reduction;object trajectories;linear regression matrices;digital imaging and remote sensing image generation;DIRSIG;doppler effects","","5","","23","IEEE","28 Jun 2018","","","IEEE","IEEE Conferences"
"Fine-Grained Road Mining from Satellite Images with Bilateral Xception and DeepLab","L. Cao","King Digital Entertainment, Activision Blizzard Group, Stockholm, Sweden","2019 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2019","2019","","","1","8","With the recent development of remote sensing and deep learning techniques, automatic and robust road extraction from satellite imaging data has become one of the most popular topics in both fields of Geographic Information System (GIS) and Computer Vision. Despite of the superior performance of Convolutional Neural Networks (DCNNs), a common problem of choosing between the classification and segmentation DCNNs still remains. By comparing two state-of-the-art baseline classification/segmentation DCNNs in several industrial application scenarios, we illustrate that their relative performance may vary, leading to different choices. Based on that observation, we propose a general fusion strategy that conveniently combines the strength of both classification and segmentation DCNNs using an end-to-end network architecture; this paradigm only requires pre-train segmentation/classification DCNNs once, which then can be reused in different road feature mining tasks. The task-specific experiments show that our fusion strategy guarantees superior results in all tested industrial scenarios.","2161-4407","978-1-7281-1985-4","10.1109/IJCNN.2019.8852009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8852009","road extraction;satellite images;convolutional neural network;fusion strategy;classification;segmentation","Roads;Task analysis;Image segmentation;Feature extraction;Satellites;Data mining;Geographic information systems","computer vision;convolutional neural nets;data mining;feature extraction;geographic information systems;geophysics computing;image classification;image fusion;image segmentation;learning (artificial intelligence);neural net architecture;remote sensing","road mining;satellite images;remote sensing;geographic information system;computer vision;convolutional neural networks;road feature mining;classification DCNN;segmentation DCNN;DeepLab;Bilateral Xception;end-to-end network architecture;satellite imaging data;automatic road extraction;deep learning techniques","","2","","43","IEEE","30 Sep 2019","","","IEEE","IEEE Conferences"
"Enhanced Hyperspectral Image Super-Resolution via RGB Fusion and TV-TV Minimization","M. Vella; B. Zhang; W. Chen; J. F. C. Mota","Institute of Sensors, Signals and Systems, Heriot-Watt University, Edinburgh EH14 4AS, UK; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, China; Institute of Sensors, Signals and Systems, Heriot-Watt University, Edinburgh EH14 4AS, UK","2021 IEEE International Conference on Image Processing (ICIP)","23 Aug 2021","2021","","","3837","3841","Hyperspectral (HS) images contain detailed spectral information that has proven crucial in applications like remote sensing, surveillance, and astronomy. However, because of hardware limitations of HS cameras, the captured images have low spatial resolution. To improve them, the low-resolution hyperspectral images are fused with conventional high-resolution RGB images via a technique known as fusion based HS image super-resolution. Currently, the best performance in this task is achieved by deep learning (DL) methods. Such methods, however, cannot guarantee that the input measurements are satisfied in the recovered image, since the learned parameters by the network are applied to every test image. Conversely, model-based algorithms can typically guarantee such measurement consistency. Inspired by these observations, we propose a framework that integrates learning and model based methods. Experimental results show that our method produces images of superior spatial and spectral resolution compared to the current leading methods, whether model-or DL-based.","2381-8549","978-1-6654-4115-5","10.1109/ICIP42928.2021.9506715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506715","Deep learning;super-resolution;hyper-spectral imaging;optimization;total variation","Current measurement;Surveillance;Superresolution;Neural networks;Extraterrestrial measurements;Minimization;Cameras","geophysical image processing;hyperspectral imaging;image fusion;image resolution;remote sensing","current leading methods;enhanced hyperspectral image super-resolution;RGB fusion;TV-TV minimization;detailed spectral information;remote sensing;hardware limitations;HS cameras;captured images;low spatial resolution;low-resolution hyperspectral images;conventional high-resolution RGB images;fusion based HS image super-resolution;deep learning methods;recovered image;test image;model-based algorithms;model based methods;superior spatial resolution;spectral resolution","","2","","29","IEEE","23 Aug 2021","","","IEEE","IEEE Conferences"
"An Adaptive Multi-Scale and Multi-Level Features Fusion Network with Perceptual Loss for Change Detection","J. Xu; Y. Luo; X. Chen; C. Luo","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","2275","2279","Change detection plays a vital role in monitoring and analyzing temporal changes in Earth observation tasks. This paper proposes a novel adaptive multi-scale and multi-level features fusion network for change detection in very-high-resolution bi-temporal remote sensing images. The proposed approach has three advantages. Firstly, it excels in abstracting high-level representations empowered by a highly effective feature extraction module. Secondly, an elaborate feature fusion module incorporated with the channel and spatial attention mechanism is proposed to provide efficient fusion strategies for multi-scale and multi-level features from bi-temporal images and multiple convolutional layers. Finally, a novel perceptual auxiliary component is designed to capture the perceptual loss of the global perceptual and structural differences and address the optimization problem caused by only using per-pixel loss function in change detection. Comprehensive experiments on two benchmark datasets confirm that our proposed framework outperforms state-of-the-art algorithms in both quantitative assessment and visual interpretation.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414394","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414394","Change detection;perceptual loss;feature fusion;deep learning","Visualization;Adaptive systems;Signal processing algorithms;Feature extraction;Finite element analysis;Task analysis;Speech processing","feature extraction;geophysical image processing;image classification;image fusion;object detection;remote sensing;sensor fusion","highly effective feature extraction module;elaborate feature fusion module;efficient fusion strategies;bi-temporal images;perceptual loss;change detection;multilevel features fusion network;temporal changes;novel adaptive multiscale;very-high-resolution bi-temporal remote sensing images;high-level representations","","1","","26","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Real-Time Ship detection from Infrared Images Through Multi-feature Fusion","R. Miao; H. Jiang; F. Tian; X. Li; Y. Zhang; D. Dong","Hangzhou Innovation Institute of Beihang University, Beijing Key Laboratory of Digital Media of Beihang University, Beijing; Hangzhou Innovation Institute of Beihang University, Beijing Key Laboratory of Digital Media of Beihang University, Beijing; Hangzhou Innovation Institute of Beihang University, Beijing Key Laboratory of Digital Media of Beihang University, Beijing; Hangzhou Innovation Institute of Beihang University, Beijing Key Laboratory of Digital Media of Beihang University, Beijing; Hangzhou Innovation Institute of Beihang University, Beijing Key Laboratory of Digital Media of Beihang University, Beijing; Hangzhou Innovation Institute of Beihang University, Beijing Key Laboratory of Digital Media of Beihang University, Beijing","2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)","22 Dec 2021","2021","","","533","538","Real-time ship detection from remote sensing imagery it is a great challenge due to the complex scene, the changeable characteristics of ship target, and the uncontrollable interference factors. In this letter, an infrared ship detection algorithm based on multi-feature fusion is proposed. Based on the fully analysis of the target features, the proposed algorithm combines the cascade rejection mechanism with multi-features through a cascade linear classifier from simple to complex, and uses fine features to accurately distinguish the target from the complex background. Large number of experimental results show that the proposed method can achieve better results in detection performance and real-time processing.","","978-1-6654-3574-1","10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9644827","Image Processing;Real-time;Multi-feature Fusion","Buildings;Object detection;Interference;Market research;Real-time systems;Classification algorithms;Marine vehicles","feature extraction;geophysical image processing;image fusion;infrared imaging;object detection;remote sensing;ships","real-time ship detection;infrared images;multifeature fusion;remote sensing imagery;complex scene;ship target;uncontrollable interference factors;infrared ship detection algorithm;target features;cascade rejection mechanism;multifeatures;fine features;detection performance;real-time processing","","","","22","IEEE","22 Dec 2021","","","IEEE","IEEE Conferences"
"Multi-scale Feature Fusion in Wireless Propagation Model Optimization Algorithms","L. Zheng; J. Min; C. Guo; T. Yan","Gansu Radio Monitoring and Positioning Industry Technology Center, Lanzhou, China; Gansu Radio Monitoring and Positioning Industry Technology Center, Lanzhou, China; Gansu Radio Monitoring and Positioning Industry Technology Center, Lanzhou, China; Gansu Radio Monitoring and Positioning Industry Technology Center, Lanzhou, China","2021 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)","3 Sep 2021","2021","","","692","696","In order to address the effects of various environmental variables on radio propagation and improve the quality of radio propagation. Inspired by the traditional empirical classification method and based on an artificial environment, this paper improves the experiments on the path well prediction results of the wireless propagation model according to the optimization algorithm with satellite remote sensing images as the input data of the network, and selects RESNet50 with the highest recognition efficiency by comparing the experimental data of eight deep learning networks such as VGG16 and RESNet50 under the self-built wireless transmission environment. By adding a feature pyramid to the network, the recognition accuracy of environmental variables reaches 96.4%, and the recognition time is 1.29 seconds per 1000 samples.","","978-1-6654-4854-3","10.1109/ICITBS53129.2021.00174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9526056","Deep Learning;Radio Propagation Model;Characteristic Gold Tower","Wireless communication;Wireless sensor networks;Smart cities;Satellite broadcasting;Radio propagation;Data models;Convolutional neural networks","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing","multiscale feature fusion;wireless propagation model optimization algorithms;environmental variables;radio propagation;traditional empirical classification method;artificial environment;path well prediction results;optimization algorithm;satellite remote sensing images;highest recognition efficiency;deep learning networks;transmission environment;feature pyramid","","","","13","IEEE","3 Sep 2021","","","IEEE","IEEE Conferences"
"Deep Image Interpolation: A Unified Unsupervised Framework for Pansharpening","J. Gao; J. Li; X. Su; M. Jiang; Q. Yuan",Wuhan University; Wuhan University; Wuhan University; Wuhan University; Wuhan University,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","23 Aug 2022","2022","","","608","617","Pansharpening, whose aim is to acquire high resolution multispectral data (HRMS) by the fusion of low resolution multispectral data (LRMS) and panchromatic data (PAN), is a specific mission of spatial-spectral fusion in remote sensing field. In recent years, deep learning methods have proved the most feasible methods for pansharpening task. However, these deep learning methods have difficulty in training in an unsupervised manner and become useless when it comes to the condition where no training dataset is available. In this paper, we propose a universal algorithm called deep image interpolation for pansharpening task. The main idea is achieving high-quality fusion results by interpolating two low-quality multispectral images in a deep neural network. We apply it to two conditions: 1) unsupervised training a network when there are enough datasets; 2) directly optimizing the fusion result where no training datasets are available. Simulation and real-data experiments are conducted on various kinds of satellite data. Quantitative and qualitative evaluation results illustrate that the proposed method outperforms traditional pansharpening methods and even catch up with those supervised methods to some extent.","2160-7516","978-1-6654-8739-9","10.1109/CVPRW56347.2022.00076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857173","","Training;Deep learning;Interpolation;Satellites;Neural networks;Data integration;Pansharpening","geophysical image processing;image fusion;image resolution;interpolation;learning (artificial intelligence);neural nets;remote sensing","deep image interpolation;pansharpening task;high-quality fusion results;low-quality multispectral images;deep neural network;fusion result;training dataset;real-data experiments;satellite data;traditional pansharpening methods;supervised methods;unified unsupervised framework;high resolution multispectral data;panchromatic data;spatial-spectral fusion;remote sensing field;deep learning methods;feasible methods;unsupervised manner","","","","33","IEEE","23 Aug 2022","","","IEEE","IEEE Conferences"
"Hyperspectral Image Visualization Using Multiscale Fusion","T. P. Jamshiyath; R. Shayini","Department of ECE, College Of Engineering, Kannur, India; Department of ECE, College Of Engineering, Kannur, India","2020 International Conference on Electronics and Sustainable Communication Systems (ICESC)","4 Aug 2020","2020","","","127","132","Hyperspectral satellite sensors yield images of excessive spectral resolution that contains an abundance of spectral channels enclosing infrared and visible wavelengths. Hyperspectral imaging can supply very high spectral resolution enabling in-depth research of the scene in comparison with an ordinary image. The advanced techniques of spectral imaging find use in the area of remote sensing, agriculture, medical diagnosis, forensic medicine, etc. The operations on hyperspectral data are generally completed via assuming it as a three-dimensional data which is highly complex. So the efficient dimension reduction technique is an essential research place, due to the wealth of information HSI provides. The visualization technique targets constructing an efficient RGB image which enables us to drain the complete data effortlessly via employing band segmentation and average fusion-based dimension reduction with the aid of the multi-scale fusion-based decolorization algorithm to produce perceptually accurate versions of the dimension reduced image.","","978-1-7281-4108-4","10.1109/ICESC48915.2020.9155758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9155758","Hyperspectral imaging;Laplacian pyramid;Multiscale fusion;Saliency weight map","Hyperspectral imaging;Image segmentation;Dimensionality reduction;Data visualization;Image resolution;Image color analysis","data visualisation;geophysical image processing;geophysical techniques;hyperspectral imaging;image colour analysis;image fusion;image resolution;remote sensing","visible wavelengths;spectral channels;excessive spectral resolution;hyperspectral satellite sensors;hyperspectral image visualization;dimension reduced image;multiscale fusion-based decolorization algorithm;average fusion-based dimension reduction;band segmentation;RGB image;visualization technique;dimension reduction technique;three-dimensional data;hyperspectral data;remote sensing;spectral imaging;high spectral resolution;hyperspectral imaging","","","","15","IEEE","4 Aug 2020","","","IEEE","IEEE Conferences"
"Multispectral Image Super Resolution with Auto-Encoder Model and Fusion Technique","K. A. Reddy; P. S. V. S. P. Teja; G. K. Teja; K. Divya; J. Aravinth.","Department of Electronics and Communication Engineering, Amrita School of Engineering, Coimbatore, Amrita Vishwa Vidyapeetham, India; Department of Electronics and Communication Engineering, Amrita School of Engineering, Coimbatore, Amrita Vishwa Vidyapeetham, India; Department of Electronics and Communication Engineering, Amrita School of Engineering, Coimbatore, Amrita Vishwa Vidyapeetham, India; Department of Electronics and Communication Engineering, Amrita School of Engineering, Coimbatore, Amrita Vishwa Vidyapeetham, India; Department of Electronics and Communication Engineering, Amrita School of Engineering, Coimbatore, Amrita Vishwa Vidyapeetham, India","2022 7th International Conference on Communication and Electronics Systems (ICCES)","29 Jul 2022","2022","","","1485","1490","Obtaining High Resolution(HR) Multispectral Images which are not readily available is one of the more critical objectives in remote sensing applications as these images can be used for various agricultural applications and previously various other methods like pansharpening have been introduced. This paper proposes a novel convolutional auto-encoder for training the multispectral images obtained from Sentinel -2A Satellite and then pass the degraded multispectral image to obtain the reconstructed Multispectral Image which is spectrally enhanced and then fuse the image obtained from reconstruction with the original degraded image to obtain a spatial HR Multispectral Image. This fusion is done using various state of the art methods like Principal Component Analysis(PCA), Discrete Wavelet Transform Level-l(DWT) and Stationary Wavelet Transform Level-l(SWT) and the performance metrics.","","978-1-6654-9634-6","10.1109/ICCES54183.2022.9835943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9835943","Convolutional Autoencoder;Multispectral Images;Principal Component Analysis(PCA);Discrete Wavelet Transform(DWT);Stationary Wavelet Transform(SWT)","Training;Measurement;Analytical models;Satellites;Pansharpening;Wavelet analysis;Discrete wavelet transforms","discrete wavelet transforms;geophysical image processing;image coding;image fusion;image reconstruction;image resolution;principal component analysis;remote sensing;wavelet transforms","Multispectral Image super Resolution;auto-encoder model;fusion technique;critical objectives;remote sensing applications;agricultural applications;novel convolutional auto-encoder;degraded multispectral image;reconstructed Multispectral Image;original degraded image;spatial HR Multispectral Image;wavelength 2.0 A","","","","20","IEEE","29 Jul 2022","","","IEEE","IEEE Conferences"
"Multi-level Fusion Based 3D Object Detection from Monocular Images","B. Xu; Z. Chen","Sch. of Remote Sensing & Inf. Eng., Wuhan Univ., Wuhan, China; Sch. of Remote Sensing & Inf. Eng., Wuhan Univ., Wuhan, China","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","16 Dec 2018","2018","","","2345","2353","In this paper, we present an end-to-end multi-level fusion based framework for 3D object detection from a single monocular image. The whole network is composed of two parts: one for 2D region proposal generation and another for simultaneously predictions of objects' 2D locations, orientations, dimensions, and 3D locations. With the help of a stand-alone module to estimate the disparity and compute the 3D point cloud, we introduce the multi-level fusion scheme. First, we encode the disparity information with a front view feature representation and fuse it with the RGB image to enhance the input. Second, features extracted from the original input and the point cloud are combined to boost the object detection. For 3D localization, we introduce an extra stream to predict the location information from point cloud directly and add it to the aforementioned location prediction. The proposed algorithm can directly output both 2D and 3D object detection results in an end-to-end fashion with only a single RGB image as the input. The experimental results on the challenging KITTI benchmark demonstrate that our algorithm significantly outperforms monocular state-of-the-art methods.","2575-7075","978-1-5386-6420-9","10.1109/CVPR.2018.00249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578347","","Three-dimensional displays;Two dimensional displays;Object detection;Proposals;Detectors;Feature extraction;Estimation","feature extraction;image coding;image colour analysis;image enhancement;image fusion;image representation;object detection","2D region proposal generation;3D object detection;single monocular imaging;end-to-end multilevel fusion scheme;2D location object prediction;3D point cloud computation;disparity information coding;front view feature representation;feature extraction;single RGB imaging;KITTI benchmark;end-to-end multilevel fusion based framework;single RGB image","","158","","39","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"AerialMPTNet: Multi-Pedestrian Tracking in Aerial Imagery Using Temporal and Graphical Features","M. Kraus; S. M. Azimi; E. Ercelik; R. Bahmanyar; P. Reinartz; A. Knoll","Department of Informatics, Technical University of Munich, Munich, Germany; Department of Aerospace, Aeronautics and Geodesy, Technical University of Munich, Munich, Germany; Department of Informatics, Technical University of Munich, Munich, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Wessling, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Wessling, Germany; Department of Informatics, Technical University of Munich, Munich, Germany","2020 25th International Conference on Pattern Recognition (ICPR)","5 May 2021","2021","","","2454","2461","Multi-pedestrian tracking in aerial imagery has several applications such as large-scale event monitoring, disaster management, search-and-rescue missions, and as input into predictive crowd dynamic models. Due to the challenges such as the large number and the tiny size of the pedestrians (e.g., 4 × 4 pixels) with their similar appearances as well as different scales and atmospheric conditions of the images with their extremely low frame rates (e.g., 2 fps), current state-of-the-art algorithms including the deep learning-based ones are unable to perform well. In this paper, we propose AerialMPTNet, a novel approach for multi-pedestrian tracking in geo-referenced aerial imagery by fusing appearance features from a Siamese Neural Network, movement predictions from a Long Short-Term Memory, and pedestrian interconnections from a GraphCNN. In addition, to address the lack of diverse aerial pedestrian tracking datasets, we introduce the Aerial Multi-Pedestrian Tracking (AerialMPT) dataset consisting of 307 frames and 44,740 pedestrians annotated. We believe that AerialMPT is the largest and most diverse dataset to this date and will be released publicly. We evaluate AerialMPTNet on AerialMPT and KIT AIS, and benchmark with several state-of-the-art tracking methods. Results indicate that AerialMPTNet significantly outperforms other methods on accuracy and time-efficiency.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9413031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413031","","Image quality;Tracking;Atmospheric modeling;Neural networks;Predictive models;Prediction algorithms;Trajectory","convolutional neural nets;feature extraction;image classification;image fusion;learning (artificial intelligence);object detection;object tracking;pedestrians;traffic engineering computing","Aerial MultiPedestrian Tracking dataset;large-scale event monitoring;predictive crowd dynamic models;geo-referenced aerial imagery;pedestrian interconnections;aerial pedestrian tracking datasets;KIT AIS;Siamese neural network;GraphCNN;long short-term memory","","2","","32","IEEE","5 May 2021","","","IEEE","IEEE Conferences"
"Refined building change detection in satellite stereo imagery based on belief functions and reliabilities","J. Tian; J. Dezert; P. Reinartz","Remote Sensing Technology Institute, German Aerospace Center, Oberpfaffenhofen, Germany; French Aerospace Lab, Palaiseau, France; Remote Sensing Technology Institute, German Aerospace Center, Oberpfaffenhofen, Germany","2015 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)","12 Oct 2015","2015","","","160","165","Digital Surface Models (DSMs) generated from satellite stereo imagery provide valuable but not comprehensive information for building change detection. Therefore, belief functions have been introduced to solve this problem by fusing DSM information with changes extracted from images. However, miss-detection can not be avoided if the DSMs are containing large region of wrong height values. A refined workflow is thereby proposed by adopting the initial disparity map to generate a reliability map. This reliability map is then built in the fusion model. The reliability map has been tested in both Dempster-Shafer Theory (DST), and Dezert-Smarandache Theory (DSmT) frameworks. The results have been validated by comparing to the manually extracted change reference mask.","","978-1-4799-7772-7","10.1109/MFI.2015.7295802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7295802","","Buildings;Indexes;Satellites;Reliability theory;Accuracy;Image color analysis","belief networks;feature extraction;geophysical image processing;geophysical techniques;image fusion;reliability;stereo image processing","refined building change detection;satellite stereoimagery;belief function;reliability;digital surface model;DSM information fusing;image extraction;change detection;reliability map;Dempster-Shafer theory;Dezert-Smarandache theory;extracted change reference mask;DST;DSmT","","2","","16","IEEE","12 Oct 2015","","","IEEE","IEEE Conferences"
"Deep Convolutional Sparse Coding Network For Pansharpening With Guidance Of Side Information","S. Xu; J. Zhang; K. Sun; Z. Zhao; L. Huang; J. Liu; C. Zhang","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China","2021 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2021","2021","","","1","6","Pansharpening is a fundamental issue in remote sensing field. This paper proposes a side information partially guided convolutional sparse coding (SCSC) model for pansharpening. The key idea is to split the low resolution multispectral image into a panchromatic image related feature map and a panchromatic image irrelated feature map, where the former one is regularized by the side information from panchromatic images. With the principle of algorithm unrolling techniques, the proposed model is generalized as a deep neural network, called as SCSC pansharpening neural network (SCSC-PNN). Compared with 13 classic and state-of-the-art methods on three satellites, the numerical experiments show that SCSC-PNN is superior to others. The codes are available at https://github.com/xsxjtu/SCSC-PNN.","1945-788X","978-1-6654-3864-3","10.1109/ICME51207.2021.9428131","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428131","Pan-sharpening;convolutional sparse coding;algorithm unrolling;image fusion","Convolutional codes;Measurement;Earth;Satellites;Image resolution;Neural networks;Pansharpening","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image coding;image resolution","panchromatic images;algorithm unrolling techniques;deep neural network;SCSC pansharpening neural network;convolutional sparse coding network;remote sensing field;convolutional sparse coding model;low resolution multispectral image;panchromatic image related feature map;panchromatic image irrelated feature map;deep convolutional sparse coding network;side information;https://github.com/xsxjtu/SCSC-PNN","","3","","15","IEEE","9 Jun 2021","","","IEEE","IEEE Conferences"
"Flood monitoring and change detection based on unsupervised image segmentation and fusion in multitemporal SAR imagery","J. Avendano; S. F. Mora; J. E. Vera; J. A. Torres; F. A. Prieto","Grupo de Investigacion y Desarrollo TECnologico Aplicado (INDETECA), Universidad Escuela Colombiana de Carreras Industriales, Bogota, Colombia; Grupo de Investigacion y Desarrollo TECnologico Aplicado (INDETECA), Universidad Escuela Colombiana de Carreras Industriales Bogota, Colombia; Grupo de Investigacion y Desarrollo TECnologico Aplicado (INDETECA), Universidad Escuela Colombiana de Carreras Industriales, Bogota, Colombia; Grupo de Investigacion y Desarrollo TECnologico Aplicado (INDETECA), Universidad Escuela Colombiana de Carreras Industriales, Bogota, Colombia; Grupo de Automatica (GAUNAL), Universidad Nacional de Colombia, Bogota, Colombia","2015 12th International Conference on Electrical Engineering, Computing Science and Automatic Control (CCE)","17 Dec 2015","2015","","","1","6","This paper presents an unsupervised method for change detection and analysis of the behavior of floods using multitemporal SAR (Synthetic Aperture Radar) images. First, images were filtered using the Enhanced Frost Filter in order to reduce the effect of speckle noise. Fuzzy Clustering Means (FCM) and k-means algorithms were used for unsupervised segmentation, and both results were fused using PCA. Subsequently, a Boolean image was created from the change information using a thresholding algorithm. Finally, the area of changes in the scene was calculated with spatial resolution information from the images. For the experiment phase, synthetic images were first created with varying levels of speckle noise, making it possible to evaluate the performance of the proposed method. The results showed an overall accuracy of approximately 99% and a kappa index of 0.76 for images whose Equivalent Number of Looks (ENL) equals 0.7. This shows that the proposed method is efficient in detecting changes in SAR images with an ENL greater than or equal to 0.7. Finally, two SAR images were tested, one before and one after a flood that covered an area of the Magdalena River in Colombia called Plato-Magdalena. Our method found that the river flooded approximately 131.51 hectares of terrain in the case of the studied images.","","978-1-4673-7839-0","10.1109/ICEEE.2015.7357982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357982","SAR;Unsupervised Segmentation;Multitemporal Data Fusion;Change Detection;Flood Analysis","Image segmentation;Speckle;Change detection algorithms;Synthetic aperture radar;Histograms;Principal component analysis;Fuses","floods;fuzzy set theory;geophysical image processing;hydrological techniques;image fusion;image resolution;image segmentation;principal component analysis;radar imaging;remote sensing by radar;rivers;speckle;synthetic aperture radar","unsupervised method;change detection;change analysis;flood monitoring;multitemporal SAR images;synthetic aperture radar;enhanced frost filter;speckle noise;fuzzy clustering means algorithm;k-means algorithm;unsupervised segmentation;PCA;Boolean image;thresholding algorithm;spatial resolution information;experiment phase;synthetic images;kappa index;equivalent number of looks;Magdalena River;Colombia;Plato-Magdalena;unsupervised image segmentation;image fusion;multitemporal SAR imagery","","3","","21","IEEE","17 Dec 2015","","","IEEE","IEEE Conferences"
"Fusion and classification of aerial images from MAVS and airplanes for local information enrichment","X. Zhuo; S. Cui; F. Kurz; P. Reinartz","German Aerospace Center, Wessling, Germany; German Aerospace Center, Wessling, Germany; German Aerospace Center, Wessling, Germany; German Aerospace Center, Wessling, Germany","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","3567","3570","Despite the existence of various matching algorithms, matching of images from Micro Aerial Vehicles (MAVs) and airplanes is still a tough problem due to the substantial differences in scale and rotation. This paper investigates the fusion of MAV imagery and airplane imagery and proposes a new robust image matching method with self-adaption to differences in scale and viewing direction. This method is further applied to register a MAV image block with reference to the orthophoto and DSM of a previously-geolocalized aerial image dataset. After registration, a fused 3D point cloud is generated and then combined with images as inputs for land cover (here roofs) classification. Experiments show that the proposed matching method outperforms SIFT/ASIFT methods in both quantity and reliability of matching results, while the registration of MAV imagery achieves decimeter-level accuracy without using any onboard GPS/IMU data. Besides, the pixel-level classification that integrates information of point clouds and images achieves significantly higher accuracy than simply image-based classification.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729924","Micro Aerial Vehicle;Multi-scale;Image Registration;Fusion;Point Cloud;Roof Classification","Three-dimensional displays;Robustness;Feature extraction;Image resolution;Airplanes","geophysical image processing;geophysical techniques;Global Positioning System;image classification;image matching;image registration","aerial image classification;aerial image fusion;MAVS imagery;local information enrichment;aerial image matching algorithm;microaerial vehicle image;airplane imagery;geolocalized aerial image dataset;proposed matching method;SIFT method;ASIFT method;MAV imagery registration;MAV imagery matching;onboard GPS data;onboard IMU data;point cloud information;pixel-level classification","","","","5","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Subspace-Based Feature Fusion from Hyperspectral and Multispectral Images for Land Cover Classification","J. Ramírez; H. Vargas; J. I. Martinez; H. Arguello","Universidad Rey Juan Carlos, Móstoles, Comunidad de Madrid, Spain; Universidad Manuela Beltrán, Bogotá, Colombia; Universidad Rey Juan Carlos, Móstoles, Comunidad de Madrid, Spain; Universidad Industrial de Santander, Bucaramanga, Colombia","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","3003","3006","In remote sensing, hyperspectral (HS) and multispectral (MS) image fusion have emerged as a synthesis tool to improve the data set resolution. However, conventional image fusion methods typically degrade the performance of the land cover classification. In this paper, a feature fusion method from HS and MS images for pixel-based classification is proposed. More precisely, the proposed method first extracts spatial features from the MS image using morphological profiles. Then, the feature fusion model assumes that both the extracted morphological profiles and the HS image can be described as a feature matrix lying in different subspaces. An algorithm based on combining alternating optimization (AO) and the alternating direction method of multipliers (ADMM) is developed to solve efficiently the feature fusion problem. Finally, extensive simulations were run to evaluate the performance of the proposed feature fusion approach for two data sets. In general, the proposed approach exhibits a competitive performance compared to other feature extraction methods.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554465","European Union's(grant numbers:754382); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554465","feature fusion;subspace methods;pixel-based classification;morphological profiles","Degradation;Tools;Feature extraction;Convex functions;Classification algorithms;Labeling;Spatial resolution","","","","1","","15","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"SCRDet: Towards More Robust Detection for Small, Cluttered and Rotated Objects","X. Yang; J. Yang; J. Yan; Y. Zhang; T. Zhang; Z. Guo; X. Sun; K. Fu","MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University; University of Chinese Academy of Sciences, Beijing, China; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University; NIST, Institute of Electronics, Beijing (Suzhou), China; University of Chinese Academy of Sciences, Beijing, China; NIST, Institute of Electronics, Beijing (Suzhou), China; NIST, Institute of Electronics, Beijing (Suzhou), China; University of Chinese Academy of Sciences, Beijing, China","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","8231","8240","Object detection has been a building block in computer vision. Though considerable progress has been made, there still exist challenges for objects with small size, arbitrary direction, and dense distribution. Apart from natural images, such issues are especially pronounced for aerial images of great importance. This paper presents a novel multi-category rotation detector for small, cluttered and rotated objects, namely SCRDet. Specifically, a sampling fusion network is devised which fuses multi-layer feature with effective anchor sampling, to improve the sensitivity to small objects. Meanwhile, the supervised pixel attention network and the channel attention network are jointly explored for small and cluttered object detection by suppressing the noise and highlighting the objects feature. For more accurate rotation estimation, the IoU constant factor is added to the smooth L1 loss to address the boundary problem for the rotating bounding box. Extensive experiments on two remote sensing public datasets DOTA, NWPU VHR-10 as well as natural image datasets COCO, VOC2007 and scene text data ICDAR2015 show the state-of-the-art performance of our detector. The code and models will be available at https://github.com/DetectionTeamUCAS.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00832","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008772","","Detectors;Feature extraction;Object detection;Remote sensing;Training;Convolution;Semantics","computer vision;feature extraction;image fusion;object detection","SCRDet;robust detection;natural images;aerial images;novel multicategory rotation detector;small objects;rotated objects;sampling fusion network;supervised pixel attention network;channel attention network;cluttered object detection;natural image datasets;COCO;computer vision","","265","","45","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"A refined automatic co-registration method for high-resolution optical and sar images by maximizing mutual information","F. Saidi; J. Chen; P. Wang","School of Electronics and Information Engineering, Beihang University, Beijing, China; School of Electronics and Information Engineering, Beihang University, Beijing, China; School of Electronics and Information Engineering, Beihang University, Beijing, China","2016 IEEE International Conference on Signal and Image Processing (ICSIP)","30 Mar 2017","2016","","","231","235","The use of multisensors images for different applications, like change detection and image fusion, require an image to image registration. Actually, the image registration becomes a crucial task with the continuous increase in image resolution, especially in urban area. In this paper, a refined automatic Mutual Information based registration approach of spaceborne Synthetic Aperture radar (SAR) and optical image is proposed. The data set include TerraSar-X (1m) and Worldview-2 (0.5m) images and cover urban area of San Francisco. Our approach is divided into two steps. First, we use Discrete Canny edge detector to extract contours from the optical image, and Gabor-Wavelet filter to extract edge from the SAR image. Second, a rigid transformation (rotation and translation) is applied to the contours image obtained by Canny detector. Then, for each value of transformation, the Mutual Information is computed between this transformed image and the feature image outcome of application of Gabor-wavelet to the SAR image. The best transformation parameters are obtained when the Mutual Information is maximal. Also, in this work, we compared the proposed approach with the basic Mutual Information intensity based registration method. The results obtained demonstrated that our approach improve the registration accuracy comparatively to the basic Mutual Information intensity based registration method.","","978-1-5090-2377-6","10.1109/SIPROCESS.2016.7888258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7888258","Image registration;mutual information;Gabor-wavelet;SAR image;canny edge detector","Optical filters;Optical imaging;Adaptive optics;Image registration;Feature extraction;Optical sensors;Synthetic aperture radar","edge detection;Gabor filters;image registration;radar imaging;remote sensing by laser beam;remote sensing by radar;synthetic aperture radar;wavelet transforms","refined automatic coregistration method;high-resolution optical images;SAR images;mutual information;multisensors images;image registration;image resolution;urban area;refined automatic Mutual Information;spaceborne Synthetic Aperture radar;TerraSar-X (1m) images;Worldview-2 (0.5m) images;San Francisco;Discrete Canny edge detector;Gabor-Wavelet filter;Gabor-wavelet","","3","","26","IEEE","30 Mar 2017","","","IEEE","IEEE Conferences"
"Deep Gradient Projection Networks for Pan-sharpening","S. Xu; J. Zhang; Z. Zhao; K. Sun; J. Liu; C. Zhang","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","1366","1375","Pan-sharpening is an important technique for remote sensing imaging systems to obtain high resolution multi-spectral images. Recently, deep learning has become the most popular tool for pan-sharpening. This paper develops a model-based deep pan-sharpening approach. Specifically, two optimization problems regularized by the deep prior are formulated, and they are separately responsible for the generative models for panchromatic images and low resolution multispectral images. Then, the two problems are solved by a gradient projection algorithm, and the iterative steps are generalized into two network blocks. By alternatively stacking the two blocks, a novel network, called gradient projection based pan-sharpening neural network, is constructed. The experimental results on different kinds of satellite datasets demonstrate that the new network out-performs state-of-the-art methods both visually and quantitatively. The codes are available at https://github.com/xsxjtu/GPPNN.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00142","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578619","","Image resolution;Satellites;Stacking;Neural networks;Tools;Iterative algorithms;Pattern recognition","correlation methods;filtering theory;geophysical image processing;gradient methods;image enhancement;image fusion;image resolution;learning (artificial intelligence);remote sensing","deep learning;model-based deep pan-sharpening approach;panchromatic images;low resolution multispectral images;gradient projection algorithm;network blocks;called gradient projection;pan-sharpening neural network;network out-performs state-of-the-art methods;gradient projection networks;high resolution multispectral images","","39","","41","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Machine Learning based Image Processing Techniques for Satellite Image Analysis -A Survey","A. Asokan; J. Anitha","ECE Department, Karunya Institute of Technology and Sciences, Coimbatore, India; ECE Department, Karunya Institute of Technology and Sciences, Coimbatore, India","2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)","11 Oct 2019","2019","","","119","124","This paper presents the detailed comparison of various image processing techniques for analyzing satellite images. The satellite images are large in size, acquired from long distances and are affected by noise and other environmental conditions. Hence it is necessary to process them so that they can be used by the researchers for analysis. Satellite images are widely used in many real time applications such as in agriculture land detection, navigation and in geographical information systems. In this paper, a review of some popular machine learning based image processing techniques is presented. Also a detailed comparison of various techniques is performed. Limitations in each image processing method are also described. In addition to reviewing of different methods, different metrics for performance evaluation in each of the image processing areas is studied.","","978-1-7281-0211-5","10.1109/COMITCon.2019.8862452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8862452","Remote Sensing. Machine Learning;Segmentation;Enhancement;Feature Extraction","Feature extraction;Satellites;Image segmentation;Classification algorithms;Image enhancement;Image fusion","agriculture;geographic information systems;geophysical image processing;learning (artificial intelligence)","image processing areas;satellite image analysis;machine learning based image processing techniques;agriculture land detection;geographical information systems","","13","","25","IEEE","11 Oct 2019","","","IEEE","IEEE Conferences"
"VRHAZE: The Simulation of Synthetic Haze Based on Visibility Range for Dehazing Method in Single Image","N. A. Husain; M. S. M. Rahim; S. Kari; H. Chaudhry","School of Computing, Faculty of Engineering, Universiti Teknologi Malaysia; School of Computing, Faculty of Engineering, Universiti Teknologi Malaysia; Razak Faculty of Technology & Informatics, Advanced Informatics School, Universiti Teknologi Malaysia; College of Engineering and Science, Victoria University, Melbourne, Australia","2020 6th International Conference on Interactive Digital Media (ICIDM)","4 Feb 2021","2020","","","1","7","Outdoor images are typically degraded by light scattering and absorption from aerosols, such as dust, mist, and smoke in the atmosphere. Because of poor visibility, dimmed brightness, low contrast, and colour distortion, these phenomena affect the captured image. Therefore, it is a critical challenge to recover pictures taken in a haze condition, which is called image dehazing. The primary aim of image dehazing is to improve details on visibility, edge, and texture and retain the image structure and colours without data loss. There are no proven benchmarks for their assessment, despite the many algorithms suggested for single image dehazing. In previous publications, arbitrary comparisons were mostly focused on a small number of images, with different publications using different sets of images. This paper presents VRHAZE, a new dataset that includes eight image pairs of hazy and corresponding outdoor images that are haze-free (ground-truth). Most of the current hazy database presented in a single image simulated synthetic haze indicated complicated calculation of the depth map. Unlike most of the current dehazing databases, a synthetic haze, which is determined by the atmospheric scattering algorithm derived from the actual distance from the camera to the scene object, has simulated hazy images. In the separate range, the synthetic haze derivation referred from the meteorological range explicitly based on haze conditions. On a clear day as referred to as a low Air Pollutant Index, this experiment simulated synthetic haze in the Malaysian outdoor scene. The haze simulation illustrates how this VRHAZE approach can lead to better outcomes in the measurement of image quality than the current state-of-the-art dehazing method.","","978-1-7281-4928-8","10.1109/ICIDM51048.2020.9339638","Ministry of Higher Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9339638","haze;atmospheric scattering model;dehazing;synthetic haze","Image color analysis;Databases;Atmospheric modeling;Computational modeling;Surveillance;Pollution measurement;Remote sensing","aerosols;air pollution;brightness;image colour analysis;image enhancement;image fusion;image reconstruction;image restoration;light scattering;visibility","haze condition;image structure;single image dehazing;VRHAZE;image pairs;hazy images;corresponding outdoor images;haze-free;current hazy database;current dehazing databases;atmospheric scattering;synthetic haze derivation;haze simulation;image quality;visibility range;light scattering;absorption;poor visibility;colour distortion;captured image","","2","","39","IEEE","4 Feb 2021","","","IEEE","IEEE Conferences"
"Multi-ratio fusion change detection","P. C. Hytla; E. J. Balster; J. R. Vasquez; R. M. Neuroth","Sensor APEX Office, University of Dayton Research Institute, Dayton, OH; Dept. of Electrical and Computer Engineering, Kettering Laboratory, Dayton, OH; Layered Sensing Exploitation Division, United States Air Force Research Laboratory, OH; Layered Sensing Exploitation Division, United States Air Force Research Laboratory, OH","2016 IEEE National Aerospace and Electronics Conference (NAECON) and Ohio Innovation Summit (OIS)","16 Feb 2017","2016","","","54","61","In this paper three ratio-based change detection algorithms, dual ratio (DR), multi-ratio (MR) and multi-ratio fusion (MRF), are tested with full motion video data collected from an unmanned aerial vehicle (UAV) platform. The dataset suffers from several practical issues that generally hinder change detection utility and performance: including image registration error, changes in perspective and significant illumination changes. The ratio-based approaches are compared to change detection methods from literature and are found to be more robust to these practical issues. MRF is found to be the top performing method exhibiting a 10% average performance advantage over the next best performing method across all false alarm regions. MRF also outperforms the next best performing method by 22% at low false alarms rates that are critical in many applications.","2379-2027","978-1-5090-3441-3","10.1109/NAECON.2016.7856774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7856774","multi-ratio fusion;dual ratio;multi-ratio;change detection","Image registration;Sensitivity;Lighting;Standards;Shape;Unmanned aerial vehicles;Remote sensing","autonomous aerial vehicles;image fusion;image motion analysis;image registration","dual ratio-based change detection algorithm;DR-based change detection algorithm;multiratio-based change detection algorithm;MR-based change detection algorithm;multiratio fusion-based change detection algorithm;MRF-based change detection algorithm;full motion video data collection;unmanned aerial vehicle platform;UAV platform;image registration error","","","","25","IEEE","16 Feb 2017","","","IEEE","IEEE Conferences"
"A Variational Pan-Sharpening With Local Gradient Constraints","X. Fu; Z. Lin; Y. Huang; X. Ding","School of Information Science and Technology, University of Science and Technology of China, China; Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, China; Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, China; Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, China","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","10257","10266","Pan-sharpening aims at fusing spectral and spatial information, which are respectively contained in the multispectral (MS) image and panchromatic (PAN) image, to produce a high resolution multi-spectral (HRMS) image. In this paper, a new variational model based on a local gradient constraint for pan-sharpening is proposed. Different with previous methods that only use global constraints to preserve spatial information, we first consider gradient difference of PAN and HRMS images in different local patches and bands. Then a more accurate spatial preservation based on local gradient constraints is incorporated into the objective to fully utilize spatial information contained in the PAN image. The objective is formulated as a convex optimization problem which minimizes two leastsquares terms and thus very simple and easy to implement. A fast algorithm is also designed to improve efficiency. Experiments show that our method outperforms previous variational algorithms and achieves better generalization than recent deep learning methods.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953524","Low-level Vision","","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);optimisation;remote sensing;variational techniques","variational pan-sharpening;local gradient constraint;pan-sharpening aims;spatial information;multispectral image;panchromatic image;high resolution multispectral image;variational model;use global constraints;gradient difference;different local patches;accurate spatial preservation;PAN image;variational algorithms;HRMS image;convex optimization problem;least squares terms","","61","","33","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Semantic Depth Map Fusion for Moving Vehicle Detection in Aerial Video","M. Poostchi; H. Aliakbarpour; R. Viguier; F. Bunyak; K. Palaniappan; G. Seetharaman","Computer Science Department, University of Missouri-Columbia, Columbia, MO, USA; Computer Science Department, University of Missouri-Columbia, Columbia, MO, USA; Computer Science Department, University of Missouri-Columbia, Columbia, MO, USA; Computer Science Department, University of Missouri-Columbia, Columbia, MO, USA; Computer Science Department, University of Missouri-Columbia, Columbia, MO, USA; US Naval Research Laboratory, Washington, D.C., USA","2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","19 Dec 2016","2016","","","1575","1583","Wide area motion imagery from an aerial platform offers a compelling advantage in providing a global picture of traffic flows for transportation and urban planning that is complementary to the information from a network of ground-based sensors and instrumented vehicles. We propose an automatic moving vehicle detection system for wide area aerial video based on semantic fusion of motion information with projected building footprint information to significantly reduce the false alarm rate in urban scenes with many tall structures. Motion detections are obtained using the flux tensor and combined with a scene level depth mask to identify tall structures using height information derived from a dense 3D point cloud estimated using multiview stereo from the same source imagery or a prior model. The trace of the flux tensor provides robust spatio-temporal information of moving edges including the motion of tall structures caused by parallax effects. The parallax induced motions are filtered out by incorporating building depth maps obtained from dense urban 3D point clouds. Using a level-set based geodesic active contours framework, the coarse thresholded tall structures depth masks evolved and stopped at the actual building boundaries. Experiments are carried out on a cropped 2k × 2k region of interest for 200 frames from Albuquerque urban aerial imagery. An average precision of 83% and recall of 76% have been reported using an object-level detection performance evaluation method.","2160-7516","978-1-5090-1437-8","10.1109/CVPRW.2016.196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789686","","Three-dimensional displays;Tensile stress;Vehicles;Buildings;Cameras;Semantics;Vehicle detection","differential geometry;image fusion;image motion analysis;object detection;remote sensing;stereo image processing;video signal processing","semantic depth map fusion;automatic moving vehicle detection system;wide area aerial video;building footprint information;false alarm rate;flux tensor;scene level depth mask;3D point cloud;multiview stereo;parallax effects;building depth maps;level-set based geodesic active contours framework;object-level detection performance evaluation method","","12","","46","IEEE","19 Dec 2016","","","IEEE","IEEE Conferences"
"Probabilistic Surfel Fusion for Dense LiDAR Mapping","C. Park; S. Kim; P. Moghadam; C. Fookes; S. Sridharan","CSIRO Data61, Autonomous Systems Laboratory, Brisbane, Australia; CSIRO Data61, Autonomous Systems Laboratory, Brisbane, Australia; CSIRO Data61, Autonomous Systems Laboratory, Brisbane, Australia; Queensland University of Technology, Brisbane, Australia; Queensland University of Technology, Brisbane, Australia","2017 IEEE International Conference on Computer Vision Workshops (ICCVW)","22 Jan 2018","2017","","","2418","2426","With the recent development of high-end LiDARs, more and more systems are able to continuously map the environment while moving and producing spatially redundant information. However, none of the previous approaches were able to effectively exploit this redundancy in a dense LiDAR mapping problem. In this paper, we present a new approach for dense LiDAR mapping using probabilistic surfel fusion. The proposed system is capable of reconstructing a high-quality dense surface element (surfel) map from spatially redundant multiple views. This is achieved by a proposed probabilistic surfel fusion along with a geometry considered data association. The proposed surfel data association method considers surface resolution as well as high measurement uncertainty along its beam direction which enables the mapping system to be able to control surface resolution without introducing spatial digitization. The proposed fusion method successfully suppresses the map noise level by considering measurement noise caused by laser beam incident angle and depth distance in a Bayesian filtering framework. Experimental results with simulated and real data for the dense surfel mapping prove the ability of the proposed method to accurately find the canonical form of the environment without further post-processing.","2473-9944","978-1-5386-1034-3","10.1109/ICCVW.2017.285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8265495","","Three-dimensional displays;Laser radar;Uncertainty;Ellipsoids;Simultaneous localization and mapping;Estimation","Bayes methods;image filtering;image fusion;image resolution;measurement errors;object detection;optical radar;probability;remote sensing by laser beam;target tracking","probabilistic surfel fusion;high-end LiDARs;spatially redundant information;dense LiDAR mapping problem;spatially redundant multiple views;surfel data association method;surface resolution;high measurement uncertainty;fusion method;dense surfel mapping;high-quality dense surface element map reconstruction;map noise level suppression;measurement noise;laser beam incident angle;depth distance;Bayesian filtering framework","","12","","24","IEEE","22 Jan 2018","","","IEEE","IEEE Conferences"
"A Cascaded LiDAR-Camera Fusion Network for Road Detection","S. Gu; J. Yang; H. Kong","PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","13308","13314","Most of the existing road detection methods are either single-modal based, e.g., based on LiDAR or camera, or multi-modal based with LiDAR-camera fusion. The algorithms are designed for a specific data type, and cannot cope with input data changes. In addition, the LiDAR-camera based methods can only work in day time with enough light. In this paper, we develop a novel LiDAR-camera fusion strategy, which combines the LiDAR point clouds and the camera images in a cascaded way. The proposed network has two working modes, the single-modal mode with LiDAR point clouds only and the multimodal mode with both LiDAR and camera data, so it can be used in all day scenes. The whole network consists of three parts: 1) LiDAR segmentation module, which segments road points in the LiDAR’s imagery view. 2) Sparse-to-dense module, which upsamples the sparse LiDAR feature maps to dense road detection results. 3) LiDAR-camera fusion module, which fuses the dense LiDAR feature maps with the dense camera images to obtain accurate road estimations. Experiments on the KITTI-Road dataset show that the proposed cascaded LiDAR-camera fusion network can obtain very competitive road detection performance, with a MaxF value of 96.38%, and achieve the state-of-the-art in the single-modal mode among all LiDAR-only methods.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561935","","Image segmentation;Laser radar;Automation;Fuses;Roads;Conferences;Estimation","cameras;driver information systems;feature extraction;image fusion;image segmentation;object detection;optical radar;radar imaging;remote sensing by laser beam;road vehicles;roads;sensor fusion","cascaded LiDAR-camera fusion network;existing road detection methods;LiDAR-camera based methods;novel LiDAR-camera fusion strategy;LiDAR point clouds;single-modal mode;camera data;segments road points;LiDAR's imagery view;sparse LiDAR feature maps;dense road detection results;dense LiDAR feature maps;dense camera images;competitive road detection performance;LiDAR-only methods","","9","","20","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Late or Earlier Information Fusion From Depth and Spectral Data? Large-Scale Digital Surface Model Refinement by Hybrid-CGAN","K. Bittner; P. Reinartz; M. Körner","German Aerospace Center (DLR), Munich, Germany; German Aerospace Center (DLR), Munich, Germany; Technical University of Munich, Munich, Germany","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","9 Apr 2020","2019","","","1471","1478","We present the workflow of a digital surface model (DSM) refinement methodology using a Hybrid-cGAN where the generative part consists of two encoders and a common decoder which blends the spectral and height information within one network. The inputs to the Hybrid-cGAN are single-channel photogrammetric DSMs with continuous values and single-channel pan-chromatic (PAN) half-meter resolution satellite images. Experimental results demonstrate that the earlier information fusion from data with different physical meanings helps to propagate fine details and complete an inaccurate or missing 3D information about building forms. Moreover, it improves the building boundaries making them more rectilinear.","2160-7516","978-1-7281-2506-0","10.1109/CVPRW.2019.00188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025611","","Buildings;Shape;Training;Generators;Decoding;Satellites;Linear programming","feature extraction;geophysical image processing;image fusion;image resolution;photogrammetry;remote sensing;solid modelling;terrain mapping","spectral data;large-scale digital surface model refinement;digital surface model refinement methodology;generative part;common decoder;spectral height information;single-channel photogrammetric DSMs;single-channel pan-chromatic half-meter resolution satellite images;hybrid-cGAN;PAN half-meter resolution satellite images;information fusion","","3","","21","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"An Efficient Pansharpening Method Based On Conditional Random Fields","Y. Yang; H. Lu; S. Huang; W. Tu","School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Software and Internet of Things Engineering, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China","2020 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2020","2020","","","1","6","Pansharpening is to fuse the existing low spatial resolution multi-spectral (MS) image with high spatial resolution panchromatic (PAN) image, so as to obtain high spatial resolution MS (HRMS) image. An efficient pansharpening model based on conditional random fields (CRFs) is proposed in this paper. In the model, a state feature function is designed to force the blurred HRMS image in accordance with the upsampled MS (UPMS) image to keep the spectral fidelity. Meanwhile, a transition feature function is defined to keep the sharpness of fused image. Besides, a new Gaussian filter acquisition algorithm is proposed to effectively satisfy the blur function in the model. To improve the efficiency of algorithm, a new initialization method based on fitting normal distribution is presented. Experiments are conducted on both reduced-scale images and full-scale images. Compared with some classical and state-of-art pansharpening methods, the proposed method achieves the best results in terms of fusion quality and efficiency.","1945-788X","978-1-7281-1331-9","10.1109/ICME46284.2020.9102885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9102885","pansharpening;CRFs;filter acquisition;feature function","Fuses;Computational modeling;Pansharpening;Transforms;Filtering algorithms;Gaussian distribution;Spatial resolution","filtering theory;geophysical image processing;image enhancement;image fusion;image resolution;remote sensing;wavelet transforms","conditional random fields;existing low spatial resolution multispectral;high spatial resolution panchromatic image;high spatial resolution MS image;efficient pansharpening model;state feature function;blurred HRMS image;upsampled MS image;spectral fidelity;transition feature function;fused image;Gaussian filter acquisition algorithm;blur function;initialization method;reduced-scale images;full-scale images;classical state-of-art pansharpening methods;fusion quality","","2","","24","IEEE","9 Jun 2020","","","IEEE","IEEE Conferences"
"Pansharpening and Semantic Segmentation of Satellite Imagery","J. Nishchal; S. Reddy; N. Navya Priya; V. R. Jenni; R. Hebbar; B. S. Babu","Computer Science, RV College of Engineering, Bangalore, India; Computer Science, RV College of Engineering, Bangalore, India; Computer Science, RV College of Engineering, Bangalore, India; Computer Science, RV College of Engineering, Bangalore, India; RRSC-South NRSC/ISRO, Bangalore, India; Computer Science, RV College of Engineering, Bangalore, India","2021 Asian Conference on Innovation in Technology (ASIANCON)","4 Oct 2021","2021","","","1","9","The fusion of high spatial resolution panchromatic (PAN) data with simultaneously acquired lower spatial resolution multispectral (MS) data is called pansharpening. The conventional techniques like Brovey and IHS to perform the same are lengthy and time consuming. Hence, this paper proposes a Convolutional Neural Network for pansharpening which gives better results than the conventional methods. This paper also looks at binary and multiclass semantic segmentation using U-Net and its variations. Finally, a model based on 3D convolutions is introduced which performs semantic segmentation on Hyperspectral Imagery with high accuracy and computational efficiency.","","978-1-7281-8402-9","10.1109/ASIANCON51346.2021.9544725","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9544725","Multispectral imagery;panchromatic images;pansharpening;PansharpNet;Convolutional Neural Network (CNN);Swish activation;semantic segmentation;U-Net;Hyperspectral imagery;3D-Hyper UNET;3D Convolutions","Image segmentation;Solid modeling;Three-dimensional displays;Computational modeling;Semantics;Vegetation mapping;Pansharpening","convolutional neural nets;feature extraction;geophysical image processing;image fusion;image resolution;image segmentation;remote sensing","pansharpening;convolutional neural network;binary segmentation;multiclass semantic segmentation;hyperspectral imagery;satellite Imagery;lower spatial resolution multispectral data;U-Net;3D convolutions;high spatial resolution panchromatic data;PAN","","1","","13","IEEE","4 Oct 2021","","","IEEE","IEEE Conferences"
"Data and Feature Fusion Approaches for Anomaly Detection in Polarimetric Hyperspectral Imagery","T. J. Bihl; J. A. Martin; K. C. Gross; K. W. Bauer","Sensors Directorate, Air Force Research Laboratory, Dayton, OH, USA; Sensors Directorate, Air Force Research Laboratory, Dayton, OH, USA; Air Force Institute of Technology, OH, USA; Air Force Institute of Technology, OH, USA","NAECON 2021 - IEEE National Aerospace and Electronics Conference","7 Feb 2022","2021","","","157","163","Hyperspectral imaging (HSI) generally ignores the polarimetric information inherent in electromagnetic waves. However, by incorporating such polarimetric data, Polarimetric HSI (P-HSI) offers the chance to increase the separation between classes of objects and thereby improve general anomaly detection accuracy. However, P-HSI further expands on the multitude of data inherent in HSI due there now being multiple images of the same scene, albeit at different polarization angles. Thus, selecting appropriate and useful subsets of this data for anomaly detection is of interest. This paper considers this issue in P-HSI by considering fusion/ensemble methodologies at the data and feature level for effective combination of the available P-HSI data with an emphasis on anomaly detection. Data fusion is considered by combining multiple Stokes Vectors into a larger image cube, from which features are extracted. Feature fusion is considered by computing features from individual Stokes Vectors and then combining retained features for anomaly detection performance. Three anomaly detection algorithms and two experimentally collected P-HSI images are considered; full factorial experiments of all combinations of data/features are considered for evaluation. The results show that the appropriate selection of data/features is critical for anomaly detection performance in P-HSI with both the level of fusion and subset being critical to accuracy.","2379-2027","978-1-6654-4859-8","10.1109/NAECON49338.2021.9696422","U.S. Air Force Research Laboratory, Sensors Directorate (AFRL/RY)(grant numbers:AFRL-2021-2767); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9696422","Anomaly detection;fusion;global iterative principal component analysis;HSI;hyperspectral imaging;object detection;PCA;polarimetric hyperspectral;support vector data description","Learning systems;Conferences;Electromagnetic scattering;Data integration;Aerospace electronics;Feature extraction;Anomaly detection","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;object detection;remote sensing","anomaly detection performance;feature fusion approaches;hyperspectral imaging;polarimetric information;polarimetric data;Polarimetric HSI;detection accuracy;feature level;P-HSI data;data fusion;multiple Stokes Vectors;anomaly detection algorithms;P-HSI images;polarimetric hyperspectral imagery","","1","","26","USGov","7 Feb 2022","","","IEEE","IEEE Conferences"
"Improving Lidar-Based Semantic Segmentation of Top-View Grid Maps by Learning Features in Complementary Representations","F. Bieder; M. Link; S. Romanski; H. Hu; C. Stiller","Institute of Measurement and Control Systems, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Understand.ai, Karlsruhe; Understand.ai, Karlsruhe; Institute of Measurement and Control Systems, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Institute of Measurement and Control Systems, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany","2021 IEEE 24th International Conference on Information Fusion (FUSION)","2 Dec 2021","2021","","","1","7","In this paper we introduce a novel way to predict semantic information from sparse, single-shot LiDAR measurements in the context of autonomous driving. In particular, we fuse learned features from complementary representations. The approach is aimed specifically at improving the semantic segmentation of top-view grid maps. Towards this goal the 3D LiDAR point cloud is projected onto two orthogonal 2D representations. For each representation a tailored deep learning architecture is developed to effectively extract semantic information which are fused by a superordinate deep neural network. The contribution of this work is threefold: (1) We examine different stages within the segmentation network for fusion. (2) We quantify the impact of embedding different features. (3) We use the findings of this survey to design a tailored deep neural network architecture leveraging respective advantages of different representations. Our method is evaluated using the SemanticKITTI dataset which provides a point-wise semantic annotation of more than 23.000 LiDAR measurements.","","978-1-7377497-1-4","10.23919/FUSION49465.2021.9627069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9627069","Semantic Segmentation;Sensor Fusion;Multi-Representation Encoding;Deep Learning;Automated Driving","Deep learning;Point cloud compression;Laser radar;Three-dimensional displays;Fuses;Conferences;Semantics","feature extraction;image classification;image fusion;image segmentation;learning (artificial intelligence);neural nets;optical radar;remote sensing by laser beam","semantic information;superordinate deep neural network;segmentation network;tailored deep neural network architecture leveraging respective advantages;point-wise semantic annotation;23.000 LiDAR measurements;lidar-based semantic segmentation;top-view grid maps;complementary representations;single-shot LiDAR measurements;autonomous driving;3D LiDAR point cloud;orthogonal 2D representations;tailored deep learning architecture","","1","","28","","2 Dec 2021","","","IEEE","IEEE Conferences"
"Seagrass Propeller Scar Detection using Deep Convolutional Neural Network","M. R. Ul Hoque; K. A. Islam; D. Perez; V. Hill; B. Schaeffer; R. Zimmerman; J. Li","Department of Electrical & Computer Engineering, Old Dominion University, Norfolk, Virginia; Department of Electrical & Computer Engineering, Old Dominion University, Norfolk, Virginia; Department of Modeling, Old Dominion University, Norfolk, Virginia; Department of Ocean, Old Dominion Univ., Norfolk, VA, USA; Office of Research and Development, U.S. Environmental Protection Agency; Department of Ocean, Old Dominion Univ., Norfolk, VA, USA; Department of Electrical & Computer Engineering, Old Dominion University, Norfolk, Virginia","2018 9th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)","15 Aug 2019","2018","","","659","665","Seagrass habitats are becoming extremely vulnerable due to human intrusion to seagrass meadows, which results in unbalanced marine ecosystems and extinction of marine animals. Traditionally, manual scarring has been used to identify and quantify seagrass propeller scars. However, this method requires site visitation and it is cost ineffective. In this paper, we propose deep learning method to automatically detect propeller seagrass scars in multispectral satellite images. Our proposed algorithm is more computationally efficient than our previous sparse coding detection model and can accurately detect seagrass scars. Additionally, we explored two pan-sharpening methods for obtaining high-resolution multispectral satellite images for scar detection. We evaluated our methods on four multispectral images collected in Florida and experimental results show that the proposed deep learning model combined with the Gram-Smith (GS) pan-sharpening approach achieved the best sensitivities in seagrass scar detection and this combination is also the most computational efficient method, requiring only 7 minutes for a testing image with a size of 1000×800 in testing phase.","","978-1-5386-7693-6","10.1109/UEMCON.2018.8796636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8796636","Seagrass;Pan-sharpening;Convolutional Neural Network","","convolutional neural nets;encoding;geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);object detection;remote sensing","propeller seagrass scars;pan-sharpening methods;high-resolution multispectral satellite images;sparse coding detection model;deep learning method;seagrass propeller scars;manual scarring;marine animals;unbalanced marine ecosystems;human intrusion;seagrass habitats;deep convolutional neural network;seagrass propeller scar detection;computational efficient method;seagrass scar detection;Gram-Smith pan-sharpening approach;deep learning model;time 7.0 min","","","","35","IEEE","15 Aug 2019","","","IEEE","IEEE Conferences"
"Mutual Information-driven Pan-sharpening","M. Zhou; K. Yan; J. Huang; Z. Yang; X. Fu; F. Zhao","University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","1788","1798","Pan-sharpening aims to integrate the complementary information of texture-rich PAN images and multi-spectral (MS) images to produce the texture-rich MS images. Despite the remarkable progress, existing state-of-the-art Pansharpening methods don't explicitly enforce the complementary information learning between two modalities of PAN and MS images. This leads to information redundancy not being handled well, which further limits the performance of these methods. To address the above issue, we propose a novel mutual information-driven Pan-sharpening framework in this paper. To be specific, we first project the PAN and MS image into modality-aware feature space independently, and then impose the mutual information minimization over them to explicitly encourage the complementary information learning. Such operation is capable of reducing the information redundancy and improving the model performance. Extensive experimental results over multiple satellite datasets demonstrate that the proposed algorithm outperforms other state-of-the-art methods qualitatively and quantitatively with great generalization ability to real-world scenes.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00184","National Natural Science Foundation of China (NSFC)(grant numbers:61901433); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879770","Photogrammetry and remote sensing","Computer vision;Satellites;Redundancy;Pansharpening;Minimization;Pattern recognition;Mutual information","correlation methods;filtering theory;geophysical image processing;image enhancement;image fusion;image resolution;learning (artificial intelligence);minimisation","Pan-sharpening aims;texture-rich PAN images;multispectral images;texture-rich MS images;state-of-the-art Pansharpening methods;complementary information learning;information redundancy;novel mutual information-driven Pan-sharpening framework;modality-aware feature space;mutual information minimization","","3","","58","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Real-time algorithm for video fusion evaluation: Application to surveillance system based on UAV platform","A. Masini; M. Maffei; A. Bracci; P. Catella; S. Benco","Flyby S. r. l, Torino, Italy; Selex ES - A Finmeccanica Company, Engineering BU, Italy; Flyby S. r. l, Torino, Italy; Selex ES - A Finmeccanica Company, Engineering BU, Italy; Consulting&Development, Info Solution S.p.A., Vimodrone, MI, Italy","2015 IEEE 1st International Forum on Research and Technologies for Society and Industry Leveraging a better tomorrow (RTSI)","12 Nov 2015","2015","","","326","333","A novel technique to evaluate in real time the video fusion algorithm has been developed and is proposed in this work.","","978-1-4673-8167-3","10.1109/RTSI.2015.7325119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325119","video fusion-quality index;video features extraction;remote sensing","Cameras;Monitoring;Real-time systems;Image resolution;Retina","autonomous aerial vehicles;image fusion;mobile robots;real-time systems;telerobotics;video signal processing","real-time algorithm;video fusion evaluation;surveillance system;UAV platform;video fusion algorithm","","","","14","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Feature cross fusion - Attention mechanism for intelligent semantic segmentation of surface features around railway","X. Fu; C. Wang; J. Wang; Z. Han; C. Qi","National Engineering Research Center for Digital Construction and Evaluation of Urban Rail Transit, Tianjin, China; National Engineering Research Center for Digital Construction and Evaluation of Urban Rail Transit, Tianjin, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; National Engineering Research Center for Digital Construction and Evaluation of Urban Rail Transit, Tianjin, China; National Engineering Research Center for Digital Construction and Evaluation of Urban Rail Transit, Tianjin, China","2022 16th IEEE International Conference on Signal Processing (ICSP)","2 Dec 2022","2022","1","","359","364","The surface features around railway can provide necessary environment information for railway survey and design, construction management, operation and maintenance, which provide information support for railway design and route selection, land demolition and land acquisition, and foreign object intrusion restrictions. However, the traditional artificial method has problems such as large manpower occupation, long time consumption and high detection missed rate. Aiming at the above problems, we proposed the feature cross fusion-attention mechanism for intelligent semantic segmentation of surface features around railway. On the basis of the hyperseg, the features of adjacent layers are fused by cross-convolution, which improves the information sharing between features of different scales. The attention module is constructed between the same layers of upsampling and downsampling. The local features in the multi-scale fusion feature map are weighted by the attention module and fused with the upsampling features to enrich the detailed local features in the upsampling feature maps and improve the accuracy of target segmentation. The experimental results show that the segmentation accuracy of the color steel houses and buildings of proposed method can reach 0.9235 and 0.7921, and the mIOU can reach 0. S761 and 0.6852.","2164-5221","978-1-6654-6056-9","10.1109/ICSP56322.2022.9965311","China Railway; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9965311","Railway life cycle;Semantic segmentation of remote sensing images;Convolutional Neural Networks","Semantic segmentation;Neural networks;Information sharing;Signal processing;Maintenance engineering;Feature extraction;Rail transportation","feature extraction;image fusion;image segmentation;railways","attention mechanism;color steel houses;construction management;feature cross fusion;foreign object intrusion restrictions;information sharing;information support;intelligent semantic segmentation;land acquisition;land demolition;local features;multiscale fusion feature map;railway design;route selection;surface features;target segmentation;upsampling feature maps","","","","21","IEEE","2 Dec 2022","","","IEEE","IEEE Conferences"
"Machine learning versus ray-tracing to forecast irradiance for an edge-computing SkyImager","W. Richardson; H. Krishnaswami; L. Shephard; R. Vega","Hariharan Krishnaswami MIEEE and Les Shephard, The University of Texas at San Antonio, San Antonio, Texas, USA; University of Texas at San Antonio, San Antonio, TX, US; University of Texas at San Antonio, San Antonio, TX, US; CPS Energy, San Antonio, Texas, USA","2017 19th International Conference on Intelligent System Application to Power Systems (ISAP)","19 Oct 2017","2017","","","1","6","Increasing penetration of photovoltaics (PV) into the electric grid necessitates accurate intra-hour solar irra-diance forecasting. The authors have previously developed a low cost all-sky imaging system at the University of Texas at San Antonio. The SkyImager hardware is designed around a Raspberry Pi single board computer (SBC) with a fully programmable, high resolution Pi Camera, housed in an all-weather enclosure. The software to process the images and to make minutes-ahead forecasts is written in Python 2.7 and utilizes the open source computer vision package OpenCV. A two-step approach for the forecasts uses optical flow to track low-level cumulus clouds and ray-tracing to predict the location of cloud shadows. This paper proposes to replace the ray-tracing approach which is traditionally known to be an ill-posed inverse problem with a machine learning strategy that utilizes a novel multi-layer perceptron (MLP) to classify cloud-cover in sub-images of the circumsolar region. The developed SkyImager was deployed at the National Renewable Energy Laboratory (NREL) in 2015, where it successfully collected months of all-sky image data. In 2016 a second SkyImager was integrated into a microgrid management system at Joint Base San Antonio. Results are presented to validate the proposed methodology.","","978-1-5090-4000-1","10.1109/ISAP.2017.8071425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071425","Image Fusion;Microgrids;Multilayer per-ceptron;Photovoltaic systems;Power system analysis computing","Clouds;Forecasting;Cameras;Optical imaging;Software;Adaptive optics;Ray tracing","cameras;clouds;geophysical image processing;image classification;learning (artificial intelligence);multilayer perceptrons;ray tracing;remote sensing;sunlight;weather forecasting","SkyImager hardware;Raspberry Pi single board computer;fully programmable resolution Pi Camera;low-level cumulus clouds;cloud shadows;ray-tracing approach;machine learning strategy;cloud-cover;all-sky image data;Joint Base San Antonio;edge-computing SkyImager;multilayer perceptron;high-resolution Pi Camera;OpenCV;open source computer vision package;Texas;intrahour solar irradiance forecasting;microgrid management system;AD 2016","","5","1","16","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"Coupled Tensor Models Accounting for Inter-image Variability","R. A. Borsoi; C. Prévost; K. Usevich; D. Brie; J. C. M. Bermudez; C. Richard","Centre de Recherche en Automatique de Nancy, Université de Lorraine, Vandoeuvre-lès-Nancy, France; Centre de Recherche en Automatique de Nancy, Université de Lorraine, Vandoeuvre-lès-Nancy, France; Centre de Recherche en Automatique de Nancy, Université de Lorraine, Vandoeuvre-lès-Nancy, France; Centre de Recherche en Automatique de Nancy, Université de Lorraine, Vandoeuvre-lès-Nancy, France; Department of Electrical Engineering, Federal University of Santa Catarina, Florianópolis, SC, Brazil; Lagrange Laboratory, Université Côte d’Azur, Nice, France","2021 55th Asilomar Conference on Signals, Systems, and Computers","4 Mar 2022","2021","","","1586","1590","Coupled tensor approximation has recently emerged as a promising approach for the fusion of hyperspectral and multispectral images (respectively HSI and MSI). This problem is referred to as hyperspectral super-resolution, and consists in recovering a super-resolution image (SRI). Previously proposed tensor-based approaches share a common limitation: they assume that the observed images are acquired under exactly the same conditions. In practice, there exist very few optical satellites that carry both hyperspectral and multispectral sensors: thus, combining an HSI and an MSI acquired on board different missions has become a task of prime interest. Since the HSI and MSI are acquired at different time instants, they can differ by, e.g., illumination, atmospheric or seasonal changes. In this work, we address the problem of hyperspectral super-resolution accounting for inter-image variability. We propose a tensor degradation model accounting for variability between the observed HSI and MSI. After introducing noiseless recovery guarantees for the target SRI, we propose two algorithms based on low-rank tensor approximations. We illustrate the performance of the proposed approach for a set of synthetic and real datasets accounting for inter-image variability.","2576-2303","978-1-6654-5828-3","10.1109/IEEECONF53345.2021.9723178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9723178","Hyperspectral;multispectral;inter-image variability;super-resolution;image fusion;tensor decomposition","Tensors;Satellites;Atmospheric modeling;Superresolution;Lighting;Optical imaging;Optical sensors","geophysical image processing;hyperspectral imaging;image colour analysis;image resolution;remote sensing;tensors","inter-image variability;coupled tensor approximation;hyperspectral images;multispectral images;respectively HSI;MSI;hyperspectral super-resolution;super-resolution image;tensor-based approaches;observed images;hyperspectral sensors;multispectral sensors;board different missions;different time instants;super-resolution accounting;tensor degradation model;observed HSI;low-rank tensor approximations;coupled tensor models","","","","22","IEEE","4 Mar 2022","","","IEEE","IEEE Conferences"
"Fast Spatio-Temporal Residual Network for Video Super-Resolution","S. Li; F. He; B. Du; L. Zhang; Y. Xu; D. Tao","School of Computer Science, Wuhan University, China; UBTECH Sydney AI Centre, SCS, FEIT, the University of Sydney, Australia; School of Computer Science, Wuhan University, China; School of Computer Science, Wuhan University, China; The State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, China; UBTECH Sydney AI Centre, SCS, FEIT, the University of Sydney, Australia","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","10514","10523","Recently, deep learning based video super-resolution (SR) methods have achieved promising performance. To simultaneously exploit the spatial and temporal information of videos, employing 3-dimensional (3D) convolutions is a natural approach. However, straight utilizing 3D convolutions may lead to an excessively high computational complexity which restricts the depth of video SR models and thus undermine the performance. In this paper, we present a novel fast spatio-temporal residual network (FSTRN) to adopt 3D convolutions for the video SR task in order to enhance the performance while maintaining a low computational load. Specifically, we propose a fast spatio-temporal residual block (FRB) that divide each 3D filter to the product of two 3D filters, which have considerably lower dimensions. Furthermore, we design a cross-space residual learning that directly links the low-resolution space and the high-resolution space, which can greatly relieve the computational burden on the feature fusion and up-scaling parts. Extensive evaluations and comparisons on benchmark datasets validate the strengths of the proposed approach and demonstrate that the proposed network significantly outperforms the current state-of-the-art methods.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953815","Vision Applications and Systems;Computational Photography","","convolutional neural nets;feature extraction;image filtering;image fusion;image resolution;learning (artificial intelligence);spatiotemporal phenomena;stereo image processing;video signal processing","video SR models;fast spatio-temporal residual network;cross-space residual learning;deep learning;video super-resolution methods;3-dimensional convolutions;high computational complexity;FRB;FSTRN;fast spatio-temporal residual block;3D filter;feature fusion;up-scaling parts","","67","","47","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Research on An Autonomous Tunnel Inspection UAV based on Visual Feature Extraction and Multi-sensor Fusion Indoor Navigation System","S. Ge; F. Pan; D. Wang; P. Ning","School of Automation, Beijing Institute of Technology, Beijing, China; Kunming-BIT Industry Technology Research Institute INC, Kunming, China; School of Automation, Beijing Institute of Technology, Beijing, China; Yunnan Remote Sensing Center, Kunming, China","2021 33rd Chinese Control and Decision Conference (CCDC)","30 Nov 2021","2021","","","6082","6089","UAVs have been widely used in line erection traction and inspection of outdoor high-voltage power grids due to its flexibility and low cost. Most of the inspection systems inside underground cable tunnels now adopt track-type and trolley-type robots, which are stable and safe, but also have disadvantages of high laying cost, low utilization efficiency, sensitive to terrain and so on. A UAV system for autonomous tunnel inspection based on visual feature extraction and multi-sensor fusion in GPS-denied environments is proposed in this paper. Kalman filter and GCC1 fusion method is the algorithm utilized for UAV indoor positioning using the optical flow sensor and the UAV inertial sensor. And then the flight stability was further enhanced by attitude decoupling. For visual feature extraction, this paper used the LAB color gamut threshold segmentation method to extract the foreground, the Hough transform matching straight line to extract the features of the horizontal deviation and forward direction, and designed a method to separate straight lines, curves and corners, thereby improved the UAV's perception of the tunnel environment. A series of inspection processes such as steering, height change, hovering and return voyage mission with vision feedforward PID controller were designed as tunnel inspection tasks. The on-site flight verification was conducted in the power transmission underground tunnel. Experimental results show that this system has the advantages of light weight, low cost, reliability, highly efficiency, low operation and maintenance costs. It can be deployed quickly and efficiently, and has certain engineering application prospect in the autonomous tunnel inspection.","1948-9447","978-1-6654-4089-9","10.1109/CCDC52312.2021.9602626","National Natural Science Foundation of China(grant numbers:61973036); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9602626","UAV;Cable tunnel inspection;Multi-sensor Fusion;Visual feature extraction","Visualization;Costs;Process control;Transforms;Inspection;Feature extraction;Robot sensing systems","autonomous aerial vehicles;feature extraction;feedforward;Global Positioning System;image colour analysis;image fusion;image segmentation;image sequences;inspection;Kalman filters;mobile robots;path planning;robot vision;three-term control;trolleys;tunnels;underground cables","visual feature extraction;multisensor fusion indoor navigation system;UAVs;high-voltage power grids;underground cable tunnels;trolley-type robots;high laying cost;low utilization efficiency;GCC1 fusion method;indoor positioning;optical flow sensor;inertial sensor;LAB color gamut threshold segmentation method;tunnel environment;tunnel inspection tasks;power transmission underground tunnel;maintenance costs;autonomous tunnel inspection;Kalman filter;vision feedforward PID controller;track-type robots","","3","","15","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Insulator Defect Detection Based on Feature Fusion and Attention Mechanism","Y. Zhang; B. Wei; L. Zhao; J. Liu; Z. Hao; L. Li; X. Li","School of Electronic Information, Northwestern Polytechnical University, Xi’an, China; School of Electronic Information, Northwestern Polytechnical University, Xi’an, China; School of Electronic Information, Northwestern Polytechnical University, Xi’an, China; School of Electronic Information, Northwestern Polytechnical University, Xi’an, China; Aero Photogrammetry and Remote Sensing Bureau of China Coal, Xi’an, China; School of Electronic Information, Northwestern Polytechnical University, Xi’an, China; School of Electronic Information, Northwestern Polytechnical University, Xi’an, China","2022 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","23 Dec 2022","2022","","","1","6","The performance of insulator defect detection model is not satisfactory due to the small object size, imbalanced and insufficient data. In this paper, based on YOLOv5 model, we propose an insulator defect detection method incorporating feature fusion and attention mechanism. Firstly, multi-scale feature fusion is introduced to strengthen the ability to extract minute features from images. Secondly, an attention mechanism based on SE-C module is proposed to improve the detection of defective objects. In addition, K-means++ is used to customize anchor boxes to meet the actual requirements and avoid mismatches. The experimental results show that the proposed model achieves 92.4% precision on the public insulator dataset, which demonstrates the applicability of the auto-detection system for insulator defects significantly.","","978-1-6654-6972-2","10.1109/ICSPCC55723.2022.9984418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9984418","insulator;defect detection;attention mechanism;object detection","Power transmission lines;Object detection;Signal processing;Insulators;Feature extraction;Generative adversarial networks;Data models","feature extraction;flaw detection;image fusion;image recognition;insulators;neural nets;power engineering computing","anchor boxes;attention mechanism;feature fusion;insulator defect detection model;K-means++;public insulator dataset;small object size;YOLOv5 model","","","","19","IEEE","23 Dec 2022","","","IEEE","IEEE Conferences"
"Research on Multi-target Tracking Algorithm Based on Classified Data Association","M. Cai; B. Wei; Z. Hao; Y. Wang; X. Li; L. Li","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, P.R. China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, P.R. China; Aerophotogrammetry and Remote Sensing Bureau of China Coal (ARSC), Xi’an, P.R. China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, P.R. China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, P.R. China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, P.R. China","2022 IEEE 17th Conference on Industrial Electronics and Applications (ICIEA)","12 Jan 2023","2022","","","468","473","Data association, aiming at matching multiple targets between frames, is a key step in multi-target tracking. In this paper, we propose a multi-target tracking algorithm based on classified data association. We first use the RetinaNet detection algorithm to quickly obtain highly accurate detection results to provide reliable data for subsequent data association. Then, we build a model for the motion of the target and obtain the candidate association values of the target in the current frame according to the established model, thus eliminating the influence of interference detection values and reducing the possibility of false matches in data association. Finally, a classified data association method is proposed. The association scenarios are classified according to the association strength and number of target candidate associations, and specific data association methods are proposed for different association scenarios. The effectiveness of the proposed algorithm is confirmed by extensive experiments, and the experimental results show that the algorithm has high tracking accuracy and precision.","2158-2297","978-1-6654-0984-1","10.1109/ICIEA54703.2022.10005936","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10005936","multi-target tracking;classified data association;target detection;motion model","Target tracking;Video sequences;Switches;Interference;Predictive models;Prediction algorithms;Data models","image fusion;image matching;target tracking","association strength;candidate association values;classified data association method;different association scenarios;high tracking accuracy;highly accurate detection results;interference detection values;multiple targets;multitarget tracking algorithm;RetinaNet detection algorithm;specific data association methods;subsequent data association;target candidate associations","","","","22","IEEE","12 Jan 2023","","","IEEE","IEEE Conferences"
"Multi-Hyperedge Hypergraph for Group Activity Recognition","W. Li; W. Xie; Z. Tu; W. Wang; L. Jin","School of Computer, Central China Normal University, Wuhan, China; School of Computer, Central China Normal University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote sensing (LIESMARS), Wuhan University, Wuhan, China; School of Computer, Central China Normal University, Wuhan, China; School of Computer, Central China Normal University, Wuhan, China","2022 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2022","2022","","","01","07","Group activity recognition aims to identify group activities from the videos. Most of the previous methods focus on modeling between individuals (one-to-one), which ignores the fact that a single individual's behavior may be jointly determined by multiple individual behaviors (many-to-one). For this reason, we propose a Multi-Hyperedge Hypergraph (MHH) to capture high-order relationships between multiple people. Specifically, we build three different types of hyperedges on the hypergraph structure. Each hyperedge can accommodate the characteristics of multiple nodes to capture different types of high-order relationships between nodes. Then, we use the late fusion method to fuse the three features to further enhance the overall behavioral representation. Finally, we perform a series of experiments on two of the most widely used benchmarks in group activity recognition, which have proved the effectiveness of MHH. More importantly, as far as we know, this is the first case of using a hypergraph structure for group activity recognition.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892494","Multi-hyperedge hypergraph;group activity recognition;high-order relationships","Visualization;Fuses;Neural networks;Activity recognition;Benchmark testing;Behavioral sciences;Task analysis","graph theory;image fusion;image motion analysis;image recognition;sensor fusion;video signal processing","MultiHyperedge Hypergraph;group activity recognition;group activities;single individual;multiple individual behaviors;high-order relationships;hyperedge;hypergraph structure","","","","34","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Hyperspectral image super-resolution extending: An effective fusion based method without knowing the spatial transformation matrix","Y. Li; L. Zhang; C. Tian; C. Ding; Y. Zhang; W. Wei","Northwestern Polytechnical University, School of Computer Science, Xi'an, China; Northwestern Polytechnical University, School of Computer Science, Xi'an, China; Xidian University, School of Electronic Engineering, Xi'an, China; Northwestern Polytechnical University, School of Computer Science, Xi'an, China; Northwestern Polytechnical University, School of Computer Science, Xi'an, China; Northwestern Polytechnical University, School of Computer Science, Xi'an, China","2017 IEEE International Conference on Multimedia and Expo (ICME)","31 Aug 2017","2017","","","1117","1122","Hyperspectral image (HSI) super-resolution, a technique to obtain higher (often spatial) resolution image from the original image, has been extensively studied and applied to lots of fields such as computer vision, remote sensing, etc. Though fusion based method has achieved state-of-the-art result, it always assume the spatial transformation matrix is given in advance, whereas such a matrix is actually unknown in reality. An unsuitable given matrix will deteriorate the superresolution result greatly. To address this issue, we propose a novel fusion based HSI super-resolution method without knowing the spatial transformation matrix. Specifically, we incorporate super-resolution and spatial transformation matrix estimation into a unified framework. We alternately estimate the matrix and the higher spatial resolution HSI. We find that without given the spatial transformation matrix, the proposed method can obtain more accurate reconstruction result compared with other competing methods. Experimental results demonstrate the effectiveness of the proposed method.","1945-788X","978-1-5090-6067-2","10.1109/ICME.2017.8019510","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019510","Hyperspectral;super-resolution;spatial transformation estimation","Spatial resolution;Image reconstruction;Databases;Hyperspectral imaging;Dictionaries;Optimization","hyperspectral imaging;image fusion;image resolution;matrix algebra","hyperspectral image super-resolution;fusion based HSI super-resolution;spatial transformation matrix estimation","","7","","21","IEEE","31 Aug 2017","","","IEEE","IEEE Conferences"
"Fusion of Convolutional Neural Network and Statistical Features for Texture classification","M. Jbene; A. D. el Maliani; M. El Hassouni","LRIT-CNRST URAC 29, Rabat IT Center Faculty of Sciences Mohammed V University, Rabat, Morocco; LRIT-CNRST URAC 29, Rabat IT Center Faculty of Sciences Mohammed V University, Rabat, Morocco; LRIT-CNRST URAC 29, Rabat IT Center, FLSH Mohammed V University, Rabat, Morocco","2019 International Conference on Wireless Networks and Mobile Communications (WINCOM)","27 Dec 2019","2019","","","1","4","Texture is a fundamental characteristic of many types of images, especially those with significant rotation, scale illumination, and viewpoint change. Texture image classification is one of the challenging problems that have various applications such as remote sensing, material recognition, and computer-aided medical diagnosis, etc. Various Computer vision techniques have been used. More recently, Deep learning architectures demonstrated impressive results. This paper aims to investigate combining two feature extraction methods: Handcrafted-based and CNN-based in a two-stream neural network architecture. We believe that Statistical features could enhance the performance of the CNN architecture, especially in the case of small datasets. To test our approach we used two challenging datasets, the Describable Textures Dataset (DTD) and Flicker Material Database (FMD). Results showed that our two-stream neural network which has an image as a first stream and a statistical feature vector as a second stream achieve better results than a Convolutional neural network achieved with just the RGB image as input. The Xception network [9] combined with SIFT-FV demonstrated an accuracy superiority for both datasets.","","978-1-7281-2625-8","10.1109/WINCOM47513.2019.8942469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8942469","Texture classification and retrieval;Deep learning;Computer vision;Feature extraction;CNN;visual attributes","Feature extraction;Discrete wavelet transforms;Convolutional neural networks;Computer architecture;Streaming media;Visualization","computer vision;feature extraction;image classification;image colour analysis;image fusion;image texture;learning (artificial intelligence);neural net architecture;statistical analysis","convolutional neural network fusion;Statistical features;texture classification;texture image classification;computer vision;Deep learning architectures;feature extraction;CNN-based;two-stream neural network architecture;CNN architecture;Describable Textures Dataset;Flicker Material Database;statistical feature vector;RGB image;Xception network;Handcrafted-based","","5","","15","IEEE","27 Dec 2019","","","IEEE","IEEE Conferences"
"Image registration with artificial neural networks using spatial and frequency features","P. Gadde; X. -H. Yu","Department of Electrical Engineering, California Polytechnic State University, San Luis Obispo, San Luis Obispo, CA, USA; Department of Electrical Engineering, California Polytechnic State University, San Luis Obispo, San Luis Obispo, CA, USA","2016 International Joint Conference on Neural Networks (IJCNN)","3 Nov 2016","2016","","","4643","4649","Image registration has been widely used in many fields such as medical imaging, remote sensing, and computer vision. It transforms multiple images of the same subject that are taken either at different times or from different points of view to the same coordinate system. In this study, two novel neural network based approaches are investigated for the registration of magnetic resonance images (MRI). They combine features in spatial domain (with scale invariant feature transform) and frequency domain (with discrete cosine transform or discrete wavelet transform) together to provide more robust feature extraction methods for image registration. Besides, the learning ability and nonlinear mapping ability of artificial neural network provide a flexible and intelligent tool for data fusion on feature matching and parameter estimation. The performances of the proposed approaches are studied and compared with other methods via computer simulations.","2161-4407","978-1-5090-0620-5","10.1109/IJCNN.2016.7727809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727809","Image registration;Artificial neural networks;Scale invariant feature transform;Discrete wavelet transform;Discrete cosine transform","Feature extraction;Image registration;Discrete cosine transforms;Discrete wavelet transforms;Biological neural networks","biomedical MRI;feature extraction;image fusion;image matching;image registration;medical image processing;neural nets;parameter estimation","magnetic resonance images;spatial domain features;frequency domain features;feature extraction methods;image registration;artificial neural network;learning ability;nonlinear mapping ability;data fusion;feature matching;parameter estimation;MRI","","1","","20","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"An Improved YOLOv3 Airplane Target Detection Algorithm Based on Multi-Scale Feature Fusion","J. Li; R. Chen; H. Cai","College of Computer Science and Technology, Jilin University, Changchun, China; College of Photonic and Electronic Engineering, Fujian Normal University, Fuzhou, China; College of Computer Science and Technology, Jilin University, Changchun, China","2022 IEEE 4th International Conference on Civil Aviation Safety and Information Technology (ICCASIT)","27 Dec 2022","2022","","","1","6","The detection and identification of airplane targets images have essential application value in remote sensing. Being aimed at the problem of unbalanced accuracy in the current detection of small-scale airplane targets, an improved airplane target detection algorithm based on YOLOv3 is proposed. First, with the feature pyramid network of the YOLOv3 model redesigned, a prediction feature layer for small targets was added, and the features of the external network and the deep network were fused to improve the model’s detection ability small targets. Then, the K-means clustering algorithm is utilized to adjust the a priori box scale of the prediction feature layer. Lastly, experimental works based on the airplane target detection show: that the average precision of the YOLOv3 algorithm of the proposed method achieves 95.3%, which is 22.9% higher than the original algorithm. The precision and recall rates are improved, and the PFS is 30.86. These results prove the effectiveness of the method proposed in this letter.","","978-1-6654-6766-7","10.1109/ICCASIT55263.2022.9986890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9986890","airplane target detection;deep learning;YOLOv3;multi-scale feature fusion","Airplanes;Atmospheric modeling;Clustering algorithms;Object detection;Predictive models;Feature extraction;Prediction algorithms","aircraft;feature extraction;image fusion;object detection;pattern clustering","airplane target detection algorithm;feature pyramid network;improved YOLOv3 airplane target detection algorithm;K-means clustering algorithm;multiscale feature fusion;prediction feature layer;small-scale airplane targets;YOLOv3 model","","","","5","IEEE","27 Dec 2022","","","IEEE","IEEE Conferences"
"Dual Low Identification Target Recognition in Complex Environment based on Neural Network","Y. Wu; K. Geng; P. Xue; G. Yin; W. Zou; S. Liu; Y. Yan","School of Mechanical Engineering, Southeast University, Nanjing; School of Mechanical Engineering, Southeast University, Nanjing; School of Mechanical Engineering, Southeast University, Nanjing; School of Mechanical Engineering, Southeast University, Nanjing; School of Mechanical Engineering, Southeast University, Nanjing; School of Mechanical Engineering, Southeast University, Nanjing; School of Mechanical Engineering, Southeast University, Nanjing","2019 Chinese Control Conference (CCC)","17 Oct 2019","2019","","","8653","8657","In order to solve the problem that the traditional neural network has low recognition rate in the complex environment, this paper designed a low recognition target recognition system based on Faster-RCNN and YOLO neural network. Firstly, a synchronous acquisition system for collecting RGB and thermal dual datasets was designed. Then, a Dual Faster-RCNN and a Dual YOLO neural network were established, and three sets of information feature fusion experiments were carried out to establish the optimal network parameters. Finally, the two networks that were trained and tested using a separate test sets. The test data shows that the average recognition rate of personnel and vehicles in the Dual Faster-RCNN neural network is about 82%, and the recognition rate of the Dual YOLO neural network is about 80.3%. The low-recognition target recognition system studied in this paper can accurately identify the key information in low-recognition pictures, which has important research significance and practical application value for future development of intelligent transportation, remote sensing image analysis and security system protection.","1934-1768","978-9-8815-6397-2","10.23919/ChiCC.2019.8866577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8866577","Low-recognition;Target Recognition;Dual-modal Dataset;Dual Faster-RCNN;Dual YOLO","Neural networks;Target recognition;Object detection;Feature extraction;Meteorology;Proposals;Thermal expansion","feature extraction;image fusion;image recognition;neural nets;object detection","complex environment;thermal dual datasets;Dual YOLO neural network;Dual Faster-RCNN neural network;dual low identification target recognition","","","","16","","17 Oct 2019","","","IEEE","IEEE Conferences"
"Feature Level Sensor Fusion for Passive RF and EO Information Integration","A. Vakil; J. Liu; P. Zulch; E. Blasch; R. Ewing; J. Li","Department of Electrical and Computer Engineering, Oakland University, Rochester, MI; Department of Electrical and Computer Engineering, Oakland University, Rochester, MI; Air Force Research Laboratory, Rome, NY; Air Force Office of Scientific Research, Arlington, VA; Air Force Research Laboratory, Wright-Patterson Air Force Base, Dayton, OH; Department of Electrical and Computer Engineering, Oakland University, Rochester, MI","2020 IEEE Aerospace Conference","21 Aug 2020","2020","","","1","9","Many different sensing modalities across the spectrum exist for collecting and processing data for the purposes of target detection, tracking and differentiation. However, each of these individual modalities from the electromagnetic spectrum contain benefits, limitations, and sources of uncertainty. While research has been conducted to integrate complementary data collected by electro-optical (EO) and radio frequency (RF) modalities, the processing of RF data usually applies traditional methods, such as Doppler. This paper explores the viability of using histogram of I/Q (in-phase and quadrature) data for the purposes of augmenting the detection accuracy that EO input alone is incapable of achieving. Processing the histogram of I/Q data via deep learning, enhances feature resolution for neural network fusion. Using the simulated data from the Digital Imaging and Remote Sensing Image Generation (DIRSIG) dataset, the resulting fusion of EO/RF neural network (FERNN) can achieve 95% accuracy in vehicle detection and scenario categorization, which is a 23% improvement over the accuracy achieved by a standalone EO sensor.","1095-323X","978-1-7281-2734-7","10.1109/AERO47225.2020.9172254","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9172254","Heterogeneous Sensor Fusion;Deep Learning;Feature Level Fusion;Histogram of In-phase and Quadrature Components","","feature extraction;image fusion;image sensors;learning (artificial intelligence);neural nets;object detection","neural network fusion;vehicle detection;standalone EO sensor;feature level sensor fusion;EO information integration;target detection;tracking;electromagnetic spectrum;RF data processing;passive RF information integration;electro-optical;radio frequency modalities;deep learning;Digital Imaging and Remote Sensing Image Generation dataset","","5","","21","IEEE","21 Aug 2020","","","IEEE","IEEE Conferences"
"Quality Enhancement of Foggy Images Comprising of Large Sky Region on SAMEER TU Dataset","T. Pal; M. K. Bhowmik","Department of Computer Science and Engineering, National Institute of Technology, Agartala, India; Department of Computer Science and Engineering, Tripura University (A Central University), Tripura, India","2018 9th International Conference on Computing, Communication and Networking Technologies (ICCCNT)","18 Oct 2018","2018","","","1","7","Images acquired by a visual system are critically degraded under the hazy and foggy weather, which will exert bad influence on several computer vision applications like remote sensing, intelligent vehicles, and visual surveillance detection, tracking, and recognition of objects. Thus, reconstructing the true scene from such a foggy image is very important. In this paper, we have adopted a segmentation method on fog degraded images containing large sky region as restoring the foggy images comprising of sky region is a very difficult task. Here, we presented different issues related to poor visibility in foggy weather condition. Then, we investigated existing image defogging algorithms based on atmospheric scattering model, fusion based strategy, dark channel prior and filtering approach with relative advantages and shortcomings. Most of the existing techniques fail to enhance image from fog degraded input image comprising the sky areas. This challenge is solved in our approach by adopting an FCM clustering based segmentation method. The experimental results demonstrate that by using this segmentation method, the foggy image with the sky region shows better results than other existing defogging algorithms that make it suitable to use in many computer vision based real-time applications.","","978-1-5386-4430-0","10.1109/ICCCNT.2018.8493832","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8493832","foggy image classification;reasons for poor visibility;sky region segmentation;visibility restoration","Meteorology;Atmospheric modeling;Image color analysis;Scattering;Lighting;Image restoration;Visualization","computer vision;filtering theory;image colour analysis;image denoising;image enhancement;image fusion;image restoration;image segmentation;object detection;pattern clustering;surveillance","quality enhancement;sky region;SAMEER TU dataset;visual system;hazy weather;computer vision applications;visual surveillance detection;foggy image;foggy weather condition;image defogging algorithms;sky areas;FCM clustering based segmentation method","","1","","23","IEEE","18 Oct 2018","","","IEEE","IEEE Conferences"
"Computational Speed and Qualitative Assessment of Real-Time Image Stitching Algorithm","D. Kumareswaran; N. Nasir; M. Z. A. Rahman; A. S. M. Supa’at","School of Mechanical Engineering, Faculty of Engineering, Universiti Teknologi, Malaysia; School of Mechanical Engineering, Faculty of Engineering, Universiti Teknologi, Malaysia; Department of Geoinformation, Faculty of Built Environment and Surveying, Universiti Teknologi, Malaysia; School of Electrical Engineering, Faculty of Engineering, Universiti Teknologi, Malaysia","2021 International Conference on Communication, Control and Information Sciences (ICCISc)","20 Jul 2021","2021","1","","1","6","In remote sensing and environmental mapping, Unmanned Aerial Vehicle (UAV) has been used extensively to capture images. For many years, digital maps are generated by using a method called image stitching. It is a method of combining multiple images to produce a segmented panorama. Since this method has been commonly used, many users produce an accurate map by using commercial software. However, a downside of this commercial software is a long computational time which is not appropriate for immediate mapping activities at chaos areas in particularly during rescue missions. This paper proposes a method to speed up the process of map generation by using a revised real-time image stitching algorithm including the qualitative assessments. In this research, the images are extracted from a video taken by a drone that flies in a dedicated flight path. These videos are then immediately transmitted to a ground station for further image processing and computation. The overlapping images are stitched and later undergoes features extraction process to identify the common features between the images. These common features are used to compute homography matrix which beneficial for image wrapping. The finding of this study suggests that ORB and AKAZE are the most suitable descriptors to be used in real-time image stitching because of their fast computational speed at adequate level quality. For instance, at 5 number of skip frame, ORB is at least 2-fold faster than AKAZE and goes up to 10-fold faster than BRISK.","","978-1-6654-0295-8","10.1109/ICCISc52257.2021.9484887","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9484887","image stitching;UAV;detection;real-time","Image segmentation;Software algorithms;Streaming media;Parallel processing;Feature extraction;Real-time systems;Software","autonomous aerial vehicles;feature extraction;image fusion;image processing;image resolution;image segmentation;remotely operated vehicles","immediate mapping activities;map generation;real-time image stitching algorithm;qualitative assessment;image processing;overlapping images;features extraction process;image wrapping;fast computational speed;environmental mapping;Unmanned Aerial Vehicle;digital maps;method called image stitching;multiple images;accurate map;commercial software;long computational time","","","","27","IEEE","20 Jul 2021","","","IEEE","IEEE Conferences"
"Study of pansharpening methods applied to hyperspectral images of sediment cores (Poster)","K. Jacq; D. Coquin; B. Fanget; Y. Perrette; M. Debret","Laboratoire des Environnements, DYnamiques, TErritoires de la Montagne Université Savoie Mont-Blanc, UMR CNRS 5204, Le Bourget du Lac, France; Laboratoire d'Informatique, Systemes, Traitement de l‘Information et de la Connaissance Université Savoie Mont-Blanc, Annecy Le Vieux Cedex, France; Laboratoire des Environnements, DYnamiques, TErritoires de la Montagne Université Savoie Mont-Blanc, UMR CNRS 5204, Le Bourget du Lac, France; Laboratoire des Environnements, DYnamiques, TErritoires de la Montagne Université Savoie Mont-Blanc, UMR CNRS 5204, Le Bourget du Lac, France; Laboratoire de Morphodynamique Continentale et Côtière Université de Rouen, UMR CNRS 6143, Mont-Saint-Aignan, France","2019 22th International Conference on Information Fusion (FUSION)","27 Feb 2020","2019","","","1","6","Spectroscopic and imaging sensors are very useful methods in analytical chemistry because they are fast, cost effective, very informative analysis. Recent works search to fused them to create a new sensor with different spectral range to increase spectral and thus chemical information to create robust and precise prediction models. Remote sensing already used fusion methods to increase spatial resolution for spectral sensors. In this paper, we propose to use pixel level data fusion methods on laboratory sensors to check their availability to increase spatial information with low effect on both dimensions (spectral and spatial). The methodology presents two steps, first the registration to fit spatially the sensors and then the fusion step to estimate each sensor at the optimal resolution. The proposed method was used on three sediment cores, that are living sample which can move, crack. They are imaged sequentially with two sensors that do not overlap spectrally: visible near infrared VNIR (400–1000 nm, pixel size: 60 μm), short wave infrared SWIR (1000–2500 nm, pixel size: 190 μm). The registration step allows to have a correlation above 0.9 with still spatial defect bring by the samples that cannot be removed. The twenty-one state of the art pixel level data fusion methods seems to be less relevant than a bicubic interpolation for the case of the laboratory hyperspectral images of sediment cores.","","978-0-9964527-8-6","10.23919/FUSION43075.2019.9011365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9011365","hyperspectral imaging;data fusion;hypersharpening;image registration;sediment core;visible and near infrared spectroscopy","Sensors;Sediments;Hyperspectral sensors;Spatial resolution;Data integration;Chemical sensors","chemistry computing;hyperspectral imaging;image fusion;image registration;image resolution;image sensors;sediments;spectral analysis","data fusion methods;hyperspectral images;sediment cores;analytical chemistry;chemical information;spatial resolution;spectral sensors;laboratory sensors;spatial information;registration step;pansharpening methods;size 190.0 mum;size 60.0 mum","","","","43","","27 Feb 2020","","","IEEE","IEEE Conferences"
"GuidedNet: A General CNN Fusion Framework via High-Resolution Guidance for Hyperspectral Image Super-Resolution","R. Ran; L. -J. Deng; T. -X. Jiang; J. -F. Hu; J. Chanussot; G. Vivone","School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; Financial Intelligence and Financial Engineering Research Key Laboratory of Sichuan Province, School of Economic Information Engineering, FinTech Innovation Center, Southwestern University of Finance and Economics, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; Inria, CNRS, Grenoble INP, LJK, Univ. Grenoble Alpes, Grenoble, France; National Research Council-Institute of Methodologies for Environmental Analysis, CNR-IMAA, Tito Scalo, Italy","IEEE Transactions on Cybernetics","","2023","PP","99","1","14","Hyperspectral image super-resolution (HISR) is about fusing a low-resolution hyperspectral image (LR-HSI) and a high-resolution multispectral image (HR-MSI) to generate a high-resolution hyperspectral image (HR-HSI). Recently, convolutional neural network (CNN)-based techniques have been extensively investigated for HISR yielding competitive outcomes. However, existing CNN-based methods often require a huge amount of network parameters leading to a heavy computational burden, thus, limiting the generalization ability. In this article, we fully consider the characteristic of the HISR, proposing a general CNN fusion framework with high-resolution guidance, called GuidedNet. This framework consists of two branches, including 1) the high-resolution guidance branch (HGB) that can decompose the high-resolution guidance image into several scales and 2) the feature reconstruction branch (FRB) that takes the low-resolution image and the multiscaled high-resolution guidance images from the HGB to reconstruct the high-resolution fused image. GuidedNet can effectively predict the high-resolution residual details that are added to the upsampled HSI to simultaneously improve spatial quality and preserve spectral information. The proposed framework is implemented using recursive and progressive strategies, which can promote high performance with a significant network parameter reduction, even ensuring network stability by supervising several intermediate outputs. Additionally, the proposed approach is also suitable for other resolution enhancement tasks, such as remote sensing pansharpening and single-image super-resolution (SISR). Extensive experiments on simulated and real datasets demonstrate that the proposed framework generates state-of-the-art outcomes for several applications (i.e., HISR, pansharpening, and SISR). Finally, an ablation study and more discussions assessing, for example, the network generalization, the low computational cost, and the fewer network parameters, are provided to the readers. The code link is: https://github.com/Evangelion09/GuidedNet.","2168-2275","","10.1109/TCYB.2023.3238200","NSFC(grant numbers:12271083,62203089,12001446); Natural Science Foundation of Sichuan Province(grant numbers:2022NSFSC0501,2022NSFSC0507,2022NSFSC1798); Key Projects of Applied Basic Research in Sichuan Province(grant numbers:2020YJ0216); National Key Research and Development Program of China(grant numbers:2020YFA0714001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035506","Convolutional neural network (CNN);high-resolution guidance;hyperspectral image super-resolution (HISR);image fusion;pansharpening;single-image super-resolution (SISR)","Image reconstruction;Task analysis;Superresolution;Pansharpening;Hyperspectral imaging;Spatial resolution;Training","","","","","","","IEEE","2 Feb 2023","","","IEEE","IEEE Early Access Articles"
"Geometric and Polarimetric Sharpening of SAR Images by Kennaugh- and Schmittlet-based Multi-frequency Data Fusion","A. Schmitt; A. Wendleder",NA; NA,"Proceedings of EUSAR 2016: 11th European Conference on Synthetic Aperture Radar","5 Sep 2016","2016","","","1","4","The joint use of diverse sensors is of major interest in the remote sensing community. As each sensor has its own characteristics in terms of geometric, radiometric and polarimetric resolution, it provides certain advantages and disadvantages for special applications. This contribution presents a way of image sharpening both in the geometric and polarimetric domain by joining the acquisitions of TerraSAR-X and RADARSAT-2. The image fusion is performed by the help of the Kennaugh element framework and the Schmittlet image enhancement. An unexpected high richness of detail characterizes the combined image and facilitates the following image interpretation like land cover classification.","","978-3-8007-4228-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7559483","","","","","","","","","","5 Sep 2016","","","VDE","VDE Conferences"
"[Title page]","",,"2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","i","i","The following topics are dealt with: SAR tomography of distributed media; hyperspectral band selection and dimensionality reduction; SAR image feature extraction and filtering; image processing techniques of detecting; ship detection; observations by the NASA Soil Moisture Active Passive Mission; multi-source remote sensing approaches to vegetation monitoring; Earth observing data science; remote sensing of land surface evapotranspiration; international spaceborne imaging spectroscopy missions; L-band microwave radiometry; radio frequency interference in microwave remote sensing and radio astronomy; TANDEM-X; atmosphere remote sensing and its application in air pollution; status and development of chinese meteorological and oceanographic series satellites; aperture synthesis radiometry; building features detection; classification of hyperspectral image; clouds and precipitation; data management and systems; detection with high resolution images; disaster and anomaly detection; feature extraction and detection algorithm; forest monitoring by radar and lidar; high resolution images classification; land cover mapping; land targets; microwave and optical calibration; microwave radiometer calibration and emerging techniques; microwave radiometry; multi-source images fusion and classification; object detection and recognition with SAR images; region based image classification; SAR and sonar image analysis and classification; SAR imaging techniques; satellite missions; urban targets and roads; vegetation monitoring by MODIS; vehicle and aircraft detection; advances on spaceborne SAR imaging; advanced interferometric processing and multidimensional SAR imaging techniques; lidar feature extraction and analysis; learning based image classification; SMAP soil moisture; remote sensing of vegetation traits and function; radar forestry; remote sensors and sensing of urban areas; geographic information science; IEEE GRSS data fusion contest; student paper contest finalists; physical models in microwave remote sensing; calibration, validation and related topics in support of spaceborne imaging spectroscopy missions; thermal and hyperspectral sensors and mapping; remote sensing using GNSS-like signals and other sensors; radiometer cross-calibration; COSMO-SKYMED mission; bistatic and digital beamforming SAR; numerical weather prediction and data assimilation; sar remote sensing for ocean applications; remote sensing of high winds; ocean surface winds; aerial images analysis and applications; analysis of multitemporal optical images; change detection applications; coastal zones; data fusion techniques; electromagnetic theory; multitemporal InSAR analysis; noise reduction techniques; optical sensors and calibration; soil properties; vegetation and tree remote sensing; pseudo pixel based image classification; Water Cycle Observation Mission (WCOM); remote sensing for ecology; advancing interoperability for geoscience information systems; lunar-based earth observation; advances in bathymetric and oceanographic lidar studies; DRAGON 3 cooperation; the Mexican perspective to the understanding of our planet through remote sensing; ALOS-2; SENTINEL-1 constellation mission; atmospheric sounding sensors; ocean waves and currents; ocean temperature and salinity; aerosols and atmospheric chemistry; air pollution; disaster assessment; droughts; earthquake; estimation and regression applications; floods; geographic information science; hazards and disasters; land cover dynamics; land use applications; ocean altimetry; POL and POLINSAR; sea ice; snow cover; neural network based classification; high resolution image analysis; global change study; forest biomass; digital terrain models; minerals and hydrocarbons; remote sensing data and policy decisions; earth remote sensing with small satellites; the China France Oceanography Satellite; agricultural parameters; big data in geoscience; differential SAR interferometry; ice sheets and glaciers; inland waters; interdisciplinary topics; remote sensing for crop yield and classification; remote sensing in mining; subsurface sensing; spectral unmixing techniques; UAV systems and sensors; super-resolution; oil spill, and; urbanization and environmental change.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7728982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7728982","","","aerosols;air pollution;atmospheric chemistry;autonomous underwater vehicles;bathymetry;crops;disasters;earthquakes;ecology;evaporation;feature extraction;floods;geophysical image processing;geophysical signal processing;glaciology;hydrology;ice;image classification;image denoising;image fusion;land cover;marine pollution;meteorology;minerals;mining;object detection;optical radar;radiometry;rain;remote sensing by laser beam;remote sensing by radar;ships;snow;sonar;synthetic aperture radar;transpiration;vegetation mapping","IEEE International Geoscience and Remote Sensing Symposium;precipitation;disaster detection;anomaly detection;radar forest monitoring;lidar forest monitoring;land cover mapping;land targets;calibration;multisource images fusion;multisource images classification;object detection;object recognition;sonar image analysis;urban targets;urban roads;MODIS;vehicle detection;aircraft detection;SMAP soil moisture;forestry;geographic information science;GNSS like signals;COSMO-SKYMED mission;change detection;coastal zones;data fusion;noise reduction;Water Cycle Observation Mission;ecology;bathymetric lidar;oceanographic lidar;DRAGON 3 cooperation;ALOS-2;clouds;hyperspectral image classification;building features detection;aperture synthesis radiometry;air pollution;atmosphere remote sensing;TANDEM-X;radio astronomy;radio frequency interference;microwave radiometry;international spaceborne imaging spectroscopy missions;land surface evapotranspiration;Earth observing data science;vegetation monitoring;multisource remote sensing;NASA Soil Moisture Active Passive Mission;ship detection;image processing techniques;SAR image filtering;SAR image feature extraction;dimensionality reduction;hyperspectral band selection;SAR tomography;IGARSS;SENTINEL-1 constellation mission;atmospheric sounding sensors;ocean waves;ocean currents;ocean temperature;ocean salinity;aerosols;atmospheric chemistry;disaster assessment;droughts;earthquake;floods;hazards;disasters;land cover dynamics;land use;ocean altimetry;POLINSAR;sea ice;snow cover;neural network based classification;forest biomass;digital terrain models;minerals;hydrocarbons;China France Oceanography Satellite;differential SAR interferometry;ice sheets;glaciers;crop yield;oil spill","","","","","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
