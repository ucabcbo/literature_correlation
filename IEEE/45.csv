"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"RTC-GAN: REAL-TIME CLASSIFICATION OF SATELLITE IMAGERY USING DEEP GENERATIVE ADVERSARIAL NETWORKS WITH INFUSED SPECTRAL INFORMATION","R. Gandikota; R. K. K; A. Sharma; M. M; V. M. Bothale","National Remote Sensing Center (NRSC), Indian Space Research Organisation (ISRO), Hyderabad, India; National Remote Sensing Center (NRSC), Indian Space Research Organisation (ISRO), Hyderabad, India; National Remote Sensing Center (NRSC), Indian Space Research Organisation (ISRO), Hyderabad, India; National Remote Sensing Center (NRSC), Indian Space Research Organisation (ISRO), Hyderabad, India; National Remote Sensing Center (NRSC), Indian Space Research Organisation (ISRO), Hyderabad, India","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","6993","6996","This paper implements a deep learning-based Convolutional Neural Network (CNN) with adversarial training and infused pixel information to classify multi-spectral data into 4 LULC classes and cloud. The network is capable of classifying the image on a real-time basis at acquisition time in pixel level by considering the various spectral band values at the pixel and a spatial region around the pixel to collect the spatial features. This way, both spatial information, and spectral information are considered to classify the image. This novel GAN architecture named RTC-GAN is generalized over all the satellites that have their sensors in and around standard NIR, R and G spectral bands while being able to classify the images in realtime. This network is realized and tested on data obtained from satellites Landsat 8 Sentinel2 and Indian Remote Sensing (IRS) satellites like Cartosat-2S, Resourcesat-2/2A. The dataset is not open-sourced and hence very minimal information is provided regarding the IRS data.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323363","Generative Adversarial Networks;Real-Time Analysis;Satellite Imagery;Landsat8;Cartosat;LULC","Satellites;Feature extraction;Remote sensing;Generative adversarial networks;Gallium nitride;Training;Real-time systems","convolutional neural nets;geophysical image processing;image classification;learning (artificial intelligence);remote sensing","RTC-GAN;real-time classification;satellite imagery;deep generative adversarial networks;infused spectral information;deep learning-based Convolutional Neural Network;CNN;adversarial training;pixel information;multispectral data;cloud;real-time basis;acquisition time;pixel level;spectral band values;spatial region;spatial features;spatial information;novel GAN architecture;satellites Landsat 8 Sentinel2;open-sourced information;IRS data;standard NIR;Indian Remote Sensing satellites;Cartosat-2S;Resourcesat-2/2A","","1","","5","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Semi-Supervised Representation Learning for Remote Sensing Image Classification Based on Generative Adversarial Networks","P. Yan; F. He; Y. Yang; F. Hu","National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, Wuhan, China","IEEE Access","24 Mar 2020","2020","8","","54135","54144","In the existing studies on remote sensing image scene classification, the supervised learning methods which are fine-tuned from pre-trained model require a large amount of labeled training data and parameters, while unsupervised learning methods do not make full use of label information, and the classification performance could be improved. In this paper, we introduced semi-supervised learning into generative adversarial network (GAN), so the discriminator learned more discriminative features from labeled data and unlabeled data. Moreover, the mixup data augmentation method was introduced into our classification model to augment the data and stabilized the training process. We carried out extensive experiments for both UC-Merced and NWPU-RESISC45 datasets with a 5-fold cross-validation protocol using a linear SVM as classifier. We trained the proposed method on UC-Merced dataset and achieve an average overall accuracy of 94.05% under 80% training ratio. When trained on NWPU-RESISC45 dataset, the proposed method reached an average overall accuracy of 83.12% and 92.78% under the training ratios of 20% and 80% respectively, which achieves the state-of-the-art deep learning methods without pre-training.","2169-3536","","10.1109/ACCESS.2020.2981358","Fundamental Research Funds for the Central Universities(grant numbers:HUST-2018KFYYXJJ141); National Natural Science Foundation of China(grant numbers:61871438); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9039665","Remote sensing scene classification;semi-supervised learning;generative adversarial networks;data augmentation","Training;Gallium nitride;Remote sensing;Feature extraction;Generative adversarial networks;Generators;Data models","geophysical image processing;image classification;learning (artificial intelligence);remote sensing;support vector machines","semisupervised representation learning;remote sensing image classification;generative adversarial network;remote sensing image scene classification;supervised learning methods;labeled training data;unsupervised learning methods;label information;classification performance;semisupervised learning;discriminative features;unlabeled data;mixup data augmentation method;classification model;training process;NWPU-RESISC45 dataset;cross-validation protocol;UC-Merced dataset;average overall accuracy;training ratios;deep learning methods","","23","","29","CCBY","17 Mar 2020","","","IEEE","IEEE Journals"
"A Semisupervised GAN-Based Multiple Change Detection Framework in Multi-Spectral Images","F. Jiang; M. Gong; T. Zhan; X. Fan","School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","23 Jun 2020","2020","17","7","1223","1227","Effectively highlighting multiple changes in the earth surface from multi-temporal remote sensing images is a meaningful but challenging task. In order to reduce costs and ensure the performance, it is advisable to employ a semisupervised strategy to achieve this goal. As a discriminative joint classification task, semisupervised change detection aims to extract useful and discriminative features from a large amount of unlabeled data in addition to limited labeled samples. The discriminator of a well-trained generative adversarial network (GAN) is just right for this. Therefore, in this letter, we proposed a semisupervised GAN-based multiple change detection framework for multi-spectral images. First, the GAN is trained by all data without any prior information. Then, we combine two identical trained discriminators to construct a dual-pipeline joint classifier. Finally, the classifier is fine-tuned by a very small amount of labeled data to detect multiple changes. The superior performance of the proposed model over both real multi-spectral data sets demonstrates its robustness and effectiveness.","1558-0571","","10.1109/LGRS.2019.2941318","National Natural Science Foundation of China(grant numbers:61772393); Key Research and Development Program of Shaanxi Province(grant numbers:2018ZDXM-GY-045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854295","Generative adversarial network (GAN);multi-spectral images;multiple change detection;semisupervised","Feature extraction;Gallium nitride;Task analysis;Training;Remote sensing;Data mining;Generative adversarial networks","feature extraction;geophysical image processing;image classification;neural nets;object detection;remote sensing","multispectral images;multitemporal remote sensing images;discriminative joint classification task;semisupervised change detection;discriminative features;identical trained discriminators;multispectral data sets;semisupervised GAN-based multiple change detection","","14","","19","IEEE","1 Oct 2019","","","IEEE","IEEE Journals"
"Single Satellite Optical Imagery Dehazing using SAR Image Prior Based on conditional Generative Adversarial Networks","B. Huang; Z. Li; C. Yang; F. Sun; Y. Song","Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Tsinghua University; Shanghai University Of Engineering Science; Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Tsinghua University; Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Tsinghua University; Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Tsinghua University","2020 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 May 2020","2020","","","1795","1802","Satellite image dehazing aims at precisely retrieving the real situations of the obscured parts from the hazy remote sensing (RS) images, which is a challenging task since the hazy regions contain both ground features and haze components. Many approaches of removing haze focus on processing multi-spectral or RGB images, whereas few of them utilize multi-sensor data. The multi-sensor data fusion is significant to provide auxiliary information since RGB images are sensitive to atmospheric conditions. In this paper, a dataset called SateHaze1k is established and composed of 1200 pairs clear Synthetic Aperture Radar (SAR), hazy RGB, and corresponding ground truth images, which are divided into three degrees of the haze, i.e. thin, moderate, and thick fog. Moreover, we propose a novel fusion dehazing method to directly restore the haze-free RS images by using an end-to-end conditional generative adversarial network(cGAN). The proposed network combines the information of both RGB and SAR images to eliminate the image blurring. Besides, the dilated residual blocks of the generator can also sufficiently improve the dehazing effects. Our experiments demonstrate that the proposed method, which fuses the information of different sensors applied to the cloudy conditions, can achieve more precise results than other baseline models.","2642-9381","978-1-7281-6553-0","10.1109/WACV45572.2020.9093471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093471","","Synthetic aperture radar;Satellites;Remote sensing;Gallium nitride;Task analysis;Optical imaging;Optical sensors","image colour analysis;image enhancement;image restoration;neural nets;radar imaging;remote sensing by radar;sensor fusion;spaceborne radar;synthetic aperture radar","synthetic aperture radar;hazy RGB;fusion dehazing method;haze-free RS images;image blurring;dehazing effects;cloudy conditions;single satellite optical imagery dehazing;SAR image;conditional generative adversarial networks;hazy remote sensing images;ground features;RGB images;multisensor data fusion;ground truth images;cGAN;SateHaze1k;multispectral images","","14","","39","IEEE","14 May 2020","","","IEEE","IEEE Conferences"
"FGF-GAN: A Lightweight Generative Adversarial Network for Pansharpening via Fast Guided Filter","Z. Zhao; J. Zhan; S. Xu; K. Sun; L. Huang; J. Liu; C. Zhang","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China","2021 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2021","2021","","","1","6","Pansharpening is a widely used image enhancement technique for remote sensing. Its principle is to fuse the input high-resolution single-channel panchromatic (PAN) image and low-resolution multi-spectral image and to obtain a high-resolution multi-spectral (HRMS) image. The existing deep learning pansharpening method has two shortcomings. First, features of two input images need to be concatenated along the channel dimension to reconstruct the HRMS image, which makes the importance of PAN images not prominent, and also leads to high computational cost. Second, the implicit information of features is difficult to extract through the manually designed loss function. To this end, we propose a generative adversarial network via the fast guided filter (FGF) for pansharpening. In generator, traditional channel concatenation is replaced by FGF to better retain the spatial information while reducing the number of parameters. Meanwhile, the fusion objects can be highlighted by the spatial attention module. In addition, the latent information of features can be preserved effectively through adversarial training. Numerous experiments illustrate that our network generates high-quality HRMS images that can surpass existing methods, and with fewer parameters.","1945-788X","978-1-6654-3864-3","10.1109/ICME51207.2021.9428272","National Key Research and Development Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428272","Pansharpening;Fast guided filter;Generative adversarial network;Image fusion","Training;Earth;Fuses;Pansharpening;Feature extraction;Information filters;Generative adversarial networks","deep learning (artificial intelligence);geophysical image processing;image enhancement;image fusion;image resolution;remote sensing","lightweight generative adversarial network;fast guided filter;image enhancement technique;remote sensing;input high-resolution single-channel panchromatic image;low-resolution multispectral image;high-resolution multispectral image;channel dimension;PAN images;high computational cost;channel concatenation;adversarial training;high-quality HRMS images;FGF-GAN;deep learning pansharpening method","","3","","20","IEEE","9 Jun 2021","","","IEEE","IEEE Conferences"
"A Conditional Generative Adversarial Network to Fuse Sar And Multispectral Optical Data For Cloud Removal From Sentinel-2 Images","C. Grohnfeldt; M. Schmitt; X. Zhu","Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1726","1729","In this paper, we present the first conditional generative adversarial network (cGAN) architecture that is specifically designed to fuse synthetic aperture radar (SAR) and optical multi-spectral (MS) image data to generate cloud- and haze-free MS optical data from a cloud-corrupted MS input and an auxiliary SAR image. Experiments on Sentinel-2 MS and Sentinel-l SAR data confirm that our extended SAR-Opt-cGAN model utilizes the auxiliary SAR information to better reconstruct MS images than an equivalent model which uses the same architecture but only single-sensor MS data as input.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519215","SAR;optical remote sensing;data fusion;deep learning;generative adversarial network (GAN);cloud-removal","Synthetic aperture radar;Optical sensors;Optical imaging;Clouds;Remote sensing;Adaptive optics;Generative adversarial networks","geophysical image processing;image fusion;radar imaging;synthetic aperture radar","cloud-free MS optical data;Sentinel-l SAR data;single-sensor MS data;reconstruct MS images;auxiliary SAR information;extended SAR-Opt-cGAN model;auxiliary SAR image;cloud-corrupted MS input;haze-free MS optical data;multispectral image data;synthetic aperture radar;conditional generative adversarial network architecture;Sentinel-2;cloud removal;multispectral optical data","","51","3","12","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Multi-Discriminator Generative Adversarial Network for High Resolution Gray-Scale Satellite Image Colorization","F. Li; L. Ma; J. Cai","Institute of Automation, Chinese Academy of Science; Institute of Automation, Chinese Academy of Science; Institute of Automation, Chinese Academy of Science","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","3489","3492","Automatic colorization for grayscale satellite images can help with eliminating lighting differences between multi-spectral captures, and provides strong prior information for ground type classification and object detection. In this paper, we introduced a novel generative adversarial network with multiple discriminators for colorizing gray-scale satellite images with pseudo-natural appearances. Although being powerful, deep generative model in its common form with a single discriminator could be unstable for achieving spatial consistency on local textured regions, especially highly textured ones. To address this issue, the generator in our proposed structure produces a group of colored outputs from feature maps at different scale levels of the network, each being supervised by an independent discriminator to fit the original colored training input in discrete Lab color space. The final colored output is a cascaded ensemble of these preceding by-products via summation, thus the fitting errors are reduced by a geometric series form. Quantitative and qualitative comparisons with the sole-discriminator version have been performed on high-resolution satellite images in experiments, where significant reductions in prediction errors have been observed.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517930","Pseudo-natural colorization;gray-scale satellite images;generative adversarial network;multiple discriminators","Image color analysis;Generative adversarial networks;Gallium nitride;Training;Satellites;Generators;Visualization","image colour analysis;image resolution;image segmentation;image texture;object detection","scale levels;object detection;ground type classification;strong prior information;multispectral captures;lighting differences;grayscale satellite images;automatic colorization;high resolution gray-scale satellite image colorization;multidiscriminator generative adversarial network;high-resolution satellite images;sole-discriminator version;final colored output;discrete Lab color space;original colored training input;independent discriminator;colored outputs;local textured regions;single discriminator;deep generative model;pseudonatural appearances;gray-scale satellite images","","5","","8","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"UFN-GAN: An unsupervised generative adversarial network for remote sensing image fusion","H. Dai; X. Liu; Y. Qiao; K. Zheng; X. Xiao; Z. Cai","School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Computer Science, China University of Geosciences, Whuhan, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","1803","1808","Different sensors acquire different images in the same area, such as multi-spectral (MS) images and panchromatic (PAN) images. Normally, the MS images possess high spectral resolution but low spatial resolution, while PAN images are opposite in the distribution of spectral and spatial information. Image fusion is a common method to obtain the information of PAN and MS images simultaneously. To generate clearer fusion image with abundant information, we design an unsupervised fusion net based on generative adversarial network (GAN), named UFNGAN for remote sensing image fusion. In our proposed UFNGAN, an adversarial net is designed between our generator and two discriminators to adequately retain the spectral and spatial information of original images without supervision. MS images and PAN images are fused by our generator, which consists of an encoder and a decoder. Our encoder is used to extract deeper feature maps of the original images, and the decoder is applied to rebuild images. Furthermore, the Spatial-Information-Enhancement (SIE) model is utilized to obtain spatial information of MS images for enhancing PAN image, and the Edge-Detection-Registration (EDR) method is applied to register the original images to avoid fused images distortion. At last, experiments are performed on QuickBird and GaoFen-2 datasets.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727490","Unsupervised learning;Image fusion;Generative adversarial network;Deep learning;Remote sensing images","Image edge detection;Generative adversarial networks;Feature extraction;Distortion;Generators;Decoding;Sensors","geophysical image processing;image fusion;image resolution;neural nets;remote sensing;unsupervised learning","remote sensing image fusion;multispectral images;panchromatic images;MS images;high spectral resolution;PAN images;spatial information;fusion image;unsupervised fusion net;spectral information;original images;fused images distortion;unsupervised generative adversarial network;spatial-information-enhancement model;UFNGAN;QuickBird;GaoFen-2 datasets;encoder","","","","10","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Infrared and Visible Image Registration Using Transformer Adversarial Network","L. Wang; C. Gao; Y. Zhao; T. Song; Q. Feng","Chongqing Key Laboratory of Signal and Information Processing, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Signal and Information Processing, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Signal and Information Processing, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Signal and Information Processing, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Signal and Information Processing, Chongqing University of Posts and Telecommunications, Chongqing, China","2018 25th IEEE International Conference on Image Processing (ICIP)","6 Sep 2018","2018","","","1248","1252","In this paper we address the task of infrared and visible image registration in complex scenes. Due to the difference of infrared and visible images, it is neither easy to reliably find features nor suitable for directly training in deep learning architecture. Thus, we propose a two-stage adversarial network, which first conducts a multi-spectral image transfer to obtain a mapped image. And then the proposed network incorporate a transformer module into the conditional adversarial network architecture to get the refined warped image. Our method can back propagate the multi-spectral registration loss and achieve end-to-end training. Experiments on our multi -spectral dataset demonstrate that this approach is effective and robust, which outperforms other state-of-the-art methods.","2381-8549","978-1-4799-7061-2","10.1109/ICIP.2018.8451370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451370","Image Registration;Multi-spectrum;Generative Adversarial Networks;Infrared Image","Image registration;Task analysis;Training;Gallium nitride;Feature extraction;Imaging;Transforms","image registration;learning (artificial intelligence)","conditional adversarial network architecture;refined warped image;multispectral registration loss;transformer adversarial network;infrared image registration;visible image registration;infrared images;visible images;deep learning architecture;two-stage adversarial network;multispectral image transfer;mapped image","","3","","26","IEEE","6 Sep 2018","","","IEEE","IEEE Conferences"
"Learning to Find Unpaired Cross-Spectral Correspondences","S. Jeong; S. Kim; K. Park; K. Sohn","School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Computer and Communication Sciences, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea","IEEE Transactions on Image Processing","22 Aug 2019","2019","28","11","5394","5406","We present a deep architecture and learning framework for establishing correspondences across cross-spectral visible and infrared images in an unpaired setting. To overcome the unpaired cross-spectral data problem, we design the unified image translation and feature extraction modules to be learned in a joint and boosting manner. Concretely, the image translation module is learned only with the unpaired cross-spectral data, and the feature extraction module is learned with an input image and its translated image. By learning two modules simultaneously, the image translation module generates the translated image that preserves not only the domain-specific attributes with separate latent spaces but also the domain-agnostic contents with feature consistency constraint. In an inference phase, the cross-spectral feature similarity is augmented by intra-spectral similarities between the features extracted from the translated images. Experimental results show that this model outperforms the state-of-the-art unpaired image translation methods and cross-spectral feature descriptors on various visible and infrared benchmarks.","1941-0042","","10.1109/TIP.2019.2917864","National Research Foundation of Korea(grant numbers:NRF-2018M3E3A1057289); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8721725","Image-to-image translation;feature extraction;multi-spectral;unpaired setting;infrared","Feature extraction;Training;Benchmark testing;Computer vision;Lighting;Generative adversarial networks;Task analysis","feature extraction;infrared imaging;learning (artificial intelligence)","unpaired cross-spectral correspondences;deep architecture;learning framework;infrared images;unpaired setting;unpaired cross-spectral data problem;unified image translation;feature extraction modules;image translation module;feature extraction module;input image;translated image;feature consistency constraint;cross-spectral feature similarity;intra-spectral similarities;cross-spectral feature descriptors;cross-spectral visible images","","5","","45","IEEE","24 May 2019","","","IEEE","IEEE Journals"
"RGNET: A Two-stage Low-light Image Enhancement Network Without Paired Supervision","R. Chang; Q. Song; Y. Wang","School of Automation, Huazhong University of Science and Technology, Wuhan, China; School of computer science, Northeast electric power university, Jilin, China; National Key Lab of Science and Technology on Multi-spectral Information Processing, Wuhan, China","2021 International Joint Conference on Neural Networks (IJCNN)","20 Sep 2021","2021","","","1","7","Deep learning-based methods have achieved remarkable success in low-light image enhancement. However, in the absence of a large number of low/normal light image pairs, it is still a challenge to train the enhancement network with good generalization ability. In this paper, we propose a highly effective unsupervised network for low-light image enhancement (named RGNET). We divide the enhancement task into two stages, complete from coarse to precise. At the first stage, we roughly amplify the input image nonlinearly using an unsupervised network. At the second stage, we build a two-path network to restore image details, one is uesed for residual restoration and the other is used for contextual attention. With the combination of reconstruction and adversarial loss, our enhancement effects are more consistant and natural than other GAN-based methods. Both quantitative and qualitative experiments on challenging datasets demonstrate the advantages of our method in comparison with state-of-the-art methods.","2161-4407","978-1-6654-3900-8","10.1109/IJCNN52387.2021.9533511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533511","GAN;unpaired learing;retinex;two-path network;low-light image enhancement","Learning systems;Neural networks;Lighting;Estimation;Feature extraction;Generative adversarial networks;Image restoration","deep learning (artificial intelligence);image enhancement;image reconstruction;image restoration;tree data structures","input image;two-path network;image details;enhancement effects;RGNET;two-stage low-light image enhancement network;deep learning-based methods;light image pairs;unsupervised network;enhancement task","","","","36","IEEE","20 Sep 2021","","","IEEE","IEEE Conferences"
"Effects of Demographics and Photometric Normalization on Image Translation GANs for Cross-Spectral Face Recognition","S. R. Mokalla; T. Bourlai","College of Electrical and Computer Engineering, University of Georgia, Athens, GA, USA; College of Electrical and Computer Engineering, University of Georgia, Athens, GA, USA","2021 IEEE International Conference on Big Data (Big Data)","13 Jan 2022","2021","","","2109","2118","This paper focuses on thermal-to-visible face matching through image synthesis. Most of the legacy face image datasets are composed of visible band data. Thermal band as well as dual band, i.e. visible and thermal face datasets, are limited. Operating in the thermal band and therefore working on visible thermal face recognition (FR) systems can be beneficial in various scenarios. The challenge is cross-spectral matching, i.e. matching gallery, visible band, face images against thermal ones. To address this problem, we train and test two of the most popular image-to-image translation Generative Adversarial Networks (GANs). These are Pix2pix and StarGAN2. In this work, the two aforementioned GAN trained models are tested, and the visible face images generated are matched against the ground truth visible faces using one of the most powerful visible-to-visible face matching algorithms, namely Facenet. We also perform an ablation study where the original thermal and visible images are photometrically normalized before training the image synthesis-specific models. The main outcomes of our study are that the FR accuracy from the pix2pix model did not vary significantly; when using StarGAN2, the original face images yield much higher accuracy compared to the photometrically normalized ones; finally, we observe that, when using the pix2pix model for image synthesis, bearded and non-Caucasian generated face images suffer the most from different noise factors. Specifically, the FR accuracy when using pix2pix after image synthesis yields a face verification area under curve (AUC) of 58.3%, while the same models when tested on data excluding bearded and non-Caucasian faces yields an accuracy of 68.6%.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671292","Image synthesis;Thermal-to-visible;Pix2pix;StarGAN2;Photometric normalization;Thermal spectrum;Visible spectrum;Multi-spectral;Cross-spectral;Face recognition","Training;Image recognition;Image synthesis;Face recognition;Conferences;Dual band;Big Data","demography;face recognition;feature extraction;image matching;infrared imaging;neural nets;photometry","photometric normalization;cross-spectral face recognition;thermal-to-visible face;legacy face image datasets;visible band data;thermal band;dual band;visible face datasets;thermal face datasets;visible thermal face recognition systems;cross-spectral matching;matching gallery;image-to-image translation;StarGAN2;aforementioned GAN trained models;visible face images;ground truth visible faces;visible-to-visible face matching algorithms;original thermal images;visible images;image synthesis-specific models;FR accuracy;pix2pix model;original face images;photometrically normalized ones;image synthesis yields;face verification area;nonCaucasian faces;generative adversarial networks;image translation GAN;demographics normalization;thermal-to-visible face matching;image synthesis;visible band;face images;face verification;area under curve","","","","46","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"SAR-to-Optical Image Translation Using Supervised Cycle-Consistent Adversarial Networks","L. Wang; X. Xu; Y. Yu; R. Yang; R. Gui; Z. Xu; F. Pu","Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan, China; Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan, China; Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan, China; Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan, China; Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan, China; Electrical Engineering Department, Stanford University, Stanford, CA, USA; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China","IEEE Access","19 Sep 2019","2019","7","","129136","129149","Optical remote sensing (RS) data suffer from the limitation of bad weather and cloud contamination, whereas synthetic aperture radar (SAR) can work under all weather conditions and overcome this disadvantage of optical RS data. However, due to the imaging mechanism of SAR and the speckle noise, untrained people are difficult to recognize the land cover types visually from SAR images. Inspired by the excellent image-to-image translation performance of Generative Adversarial Networks (GANs), a supervised Cycle-Consistent Adversarial Network (S-CycleGAN) was proposed to generate large optical images from the SAR images. When the optical RS data are unavailable or partly unavailable, the generated optical images can be alternative data that aid in land cover visual recognition for untrained people. The main steps of SAR-to-optical image translation were as follows. First, the large SAR image was split to small patches. Then S-CycleGAN was used to translate the SAR patches to optical image patches. Finally, the optical image patches were stitched to generate the large optical image. A paired SAR-optical image dataset which covered 32 Chinese cities was published to evaluate the proposed method. The dataset was generated from Sentinel-1 (SEN-1) SAR images and Sentinel-2 (SEN-2) multi-spectral images. S-CycleGAN was applied to two experiments, which were SAR-to-optical image translation and cloud removal, and the results showed that S-CycleGAN could keep both the land cover and structure information well, and its performance was superior to some famous image-to-image translation models.","2169-3536","","10.1109/ACCESS.2019.2939649","National Basic Research Program of China (973 Program)(grant numbers:2016YFB0502600); Thirteen-Five Civil Aerospace Planning Project — Integration of Communication, Navigation and Remote Sensing Comprehensive Application Technology; Chinese Technology Research and Development of the Major Project of High-Resolution Earth Observation System(grant numbers:03-Y20A10-9001-15/16); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825802","SAR-to-optical image translation;visualization;GAN;Sentinel;cloud removal","Radar polarimetry;Optical imaging;Clouds;Optical sensors;Adaptive optics;Optical polarization;Gallium nitride","geophysical image processing;optical images;radar imaging;remote sensing by radar;synthetic aperture radar;terrain mapping","SAR-to-optical image translation;supervised Cycle-Consistent Adversarial Networks;optical remote sensing data;optical RS data;imaging mechanism;generated optical images;SAR patches;optical image patches;paired SAR-optical image dataset;Sentinel-1 SAR images;multispectral images;image-to-image translation models;S-CycleGAN;Chinese cities;land cover;Generative Adversarial Networks;weather conditions;cloud contamination;bad weather","","59","","58","CCBY","5 Sep 2019","","","IEEE","IEEE Journals"
"Domain-Aware Unsupervised Hyperspectral Reconstruction for Aerial Image Dehazing","A. Mehta; H. Sinha; M. Mandal; P. Narang","Department of CSIS, BITS, Pilani, India; Department of CSIS, BITS, Pilani, India; Department of CSE, IIIT, Kota, India; Department of CSIS, BITS, Pilani, India","2021 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 Jun 2021","2021","","","413","422","Haze removal in aerial images is a challenging problem due to considerable variation in spatial details and varying contrast. Changes in particulate matter density often lead to degradation in visibility. Therefore, several approaches utilize multi-spectral data as auxiliary information for haze removal. In this paper, we propose SkyGAN for haze removal in aerial images. SkyGAN consists of 1) a domain-aware hazy-to-hyperspectral (H2H) module, and 2) a conditional GAN (cGAN) based multi-cue image-to-image translation module (I2I) for dehazing. The proposed H2H module reconstructs several visual bands from RGB images in an unsupervised manner, which overcomes the lack of hazy hyperspectral aerial image datasets. The module utilizes task supervision and domain adaptation in order to create a ""hyperspectral catalyst"" for image dehazing. The I2I module uses the hyperspectral catalyst along with a 12-channel multi-cue input and performs effective image de-hazing by utilizing the entire visual spectrum. In addition, this work introduces a new dataset, called Hazy Aerial-Image (HAI) dataset, that contains more than 65,000 pairs of hazy and ground truth aerial images with realistic, non- homogeneous haze of varying density. The performance of SkyGAN is evaluated on the recent SateHaze1k dataset as well as the HAI dataset. We also present a comprehensive evaluation of HAI dataset with a representative set of state-of-the-art techniques in terms of PSNR and SSIM.","2642-9381","978-1-6654-0477-8","10.1109/WACV48630.2021.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9423159","","Degradation;Visualization;Image color analysis;Catalysts;Conferences;Generative adversarial networks;Task analysis","feature extraction;image classification;image colour analysis;image denoising;image enhancement;image fusion;image reconstruction;neural nets;unsupervised learning","multispectral data;haze removal;SkyGAN;domain-aware hazy-to-hyperspectral module;H2H;conditional GAN based multicue image-to-image translation module;RGB images;hazy hyperspectral aerial image datasets;task supervision;domain adaptation;hyperspectral catalyst;12-channel multicue input;hazy aerial-image dataset;hazy truth aerial images;ground truth aerial images;HAI dataset;domain-aware unsupervised hyperspectral reconstruction;aerial image dehazing;particulate matter density;image dehazing;I2I module","","11","","53","IEEE","14 Jun 2021","","","IEEE","IEEE Conferences"
"Interpretable Multi-Modal Image Registration Network Based on Disentangled Convolutional Sparse Coding","X. Deng; E. Liu; S. Li; Y. Duan; M. Xu","School of Cyber Science and Technology, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China","IEEE Transactions on Image Processing","7 Feb 2023","2023","32","","1078","1091","Multi-modal image registration aims to spatially align two images from different modalities to make their feature points match with each other. Captured by different sensors, the images from different modalities often contain many distinct features, which makes it challenging to find their accurate correspondences. With the success of deep learning, many deep networks have been proposed to align multi-modal images, however, they are mostly lack of interpretability. In this paper, we first model the multi-modal image registration problem as a disentangled convolutional sparse coding (DCSC) model. In this model, the multi-modal features that are responsible for alignment (RA features) are well separated from the features that are not responsible for alignment (nRA features). By only allowing the RA features to participate in the deformation field prediction, we can eliminate the interference of the nRA features to improve the registration accuracy and efficiency. The optimization process of the DCSC model to separate the RA and nRA features is then turned into a deep network, namely Interpretable Multi-modal Image Registration Network (InMIR-Net). To ensure the accurate separation of RA and nRA features, we further design an accompanying guidance network (AG-Net) to supervise the extraction of RA features in InMIR-Net. The advantage of InMIR-Net is that it provides a universal framework to tackle both rigid and non-rigid multi-modal image registration tasks. Extensive experimental results verify the effectiveness of our method on both rigid and non-rigid registrations on various multi-modal image datasets, including RGB/depth images, RGB/near-infrared (NIR) images, RGB/multi-spectral images, T1/T2 weighted magnetic resonance (MR) images and computed tomography (CT)/MR images. The codes are available at https://github.com/lep990816/Interpretable-Multi-modal-Image-Registration.","1941-0042","","10.1109/TIP.2023.3240024","National Natural Science Foundation of China(grant numbers:62001016,62231002,62250001); Beijing Natural Science Foundation(grant numbers:JQ20020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10034541","Multi-modal image registration;convolutional sparse coding;interpretable network","Image registration;Task analysis;Measurement;Convolutional codes;Strain;Image coding;Generative adversarial networks","","","","","","67","IEEE","31 Jan 2023","","","IEEE","IEEE Journals"
