"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Convolutional Neural Network Approach for Mapping Arctic Vegetation Using Multi-Sensor Remote Sensing Fusion","Z. L. Langford; J. Kumar; F. M. Hoffman","University of Tennessee, Knoxville, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA","2017 IEEE International Conference on Data Mining Workshops (ICDMW)","18 Dec 2017","2017","","","322","331","Accurate and high-resolution maps of vegetation are critical for projects seeking to understand the terrestrial ecosystem processes and land-atmosphere interactions in Arctic ecosystems, such as U.S. Department of Energy's Next Generation Ecosystem Experiment (NGEE) Arctic. However, most existing Arctic vegetation maps are at a coarse resolution and with a varying degree of detail and accuracy. Remote sensing-based approaches for mapping vegetation, while promising, are challenging in high latitude environments due to frequent cloud cover, polar darkness, and limited availability of high-resolution remote sensing datasets (e.g., ~ 5 m). This study proposes a new remote sensing based multi-sensor data fusion approach for developing high-resolution maps of vegetation in the Seward Peninsula, Alaska. We focus detailed analysis and validation study around the Kougarok river, located in the central Seward Peninsula of Alaska. We seek to evaluate the integration of hyper-spectral, multi-spectral, radar, and terrain datasets using unsupervised and supervised classification techniques over a ~343.72 km2 area for generating vegetation classifications at a variety of resolutions (5 m and 12.5 m). We fist applied a quantitative goodness-of-fit method, called Mapcurves, that shows the degree of spatial concordance between the public coarse resolution maps and k-means clustering values and relabels the k values based on the best overlap. We develop a convolutional neural network (CNN) approach for developing high resolution vegetation maps for our study region in Arctic. We compare two CNN approaches: (1) breaking up the images into small patches (e.g., 6 Ã— 6) and predict the vegetation class for entire patch and (2) semantic segmentation and predict the vegetation class for every pixel. We also perform accuracy assessments of the developed data products and evaluate varying CNN architectures. The fusion of hyperspectral and optical datasets performed the best, with accuracy values increased from 0.64 to 0.96-0.97 when using a training map produced by unsupervised clustering and Mapcurves labeling for both CNN models.","2375-9259","978-1-5386-3800-2","10.1109/ICDMW.2017.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8215680","Multi-Sensor;Fusion;Remote Sensing;Vegetation Classificaiton","Vegetation mapping;Remote sensing;Arctic;Earth;Biological system modeling;Artificial satellites;Meteorology","geophysical image processing;image classification;image fusion;image segmentation;neural nets;pattern clustering;sensor fusion;terrain mapping;vegetation;vegetation mapping","US Department of Energy;Arctic vegetation mapping;Next Generation Ecosystem Experiment;Mapcurves labeling;unsupervised clustering;hyperspectral-optical dataset fusion;k values;training map;developed data products;vegetation class;CNN approaches;high resolution vegetation maps;public coarse resolution maps;vegetation classifications;unsupervised classification techniques;central Seward Peninsula;validation study;Alaska;multisensor data fusion approach;high-resolution remote sensing datasets;frequent cloud cover;high latitude environments;existing Arctic vegetation maps;Arctic ecosystems;land-atmosphere interactions;terrestrial ecosystem processes;multisensor remote sensing fusion;convolutional neural network approach;size 343.72 km","","5","","44","IEEE","18 Dec 2017","","","IEEE","IEEE Conferences"
"Toward Autonomous Driving in Arctic Areas","L. Ruotsalainen; V. Renaudin; L. Pei; M. Piras; J. Marais; E. Cavalheri; S. Kaasalainen","Department of Computer Science, University of Helsinki, and the Department of Navigation and Positioning, Finnish Geospatial Research Institute, Finland; IFSTTAR, France; Shanghai Key Laboratory of Navigation and Location-Based Services, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, China; Department of Environment, Land and Infrastructure Engineering, Politecnico di Torino, Italy; IFSTTAR, France; Department of Geodesy and Geomatics Engineering, University of New Brunswick, Canada; Department of Navigation and Positioning, Finnish Geospatial Research Institute, Finland","IEEE Intelligent Transportation Systems Magazine","22 Jul 2020","2020","12","3","10","24","This article provides an overview of the use of inertial and visual sensors and discusses their prospects in the Arctic navigation of autonomous vehicles. We also examine the fusion algorithms used thus far for integrating vehicle localization measurements as well as the map-matching (MM) algorithms relating position coordinates with road infrastructure. Our review reveals that conventional fusion and MM methods are not enough for navigation in challenging environments, like urban areas and Arctic environments. We also offer new results from testing inertial and optical sensors in vehicle positioning in snowy conditions. We find that the fusion of Global Navigation Satellite System (GNSS) and inertial navigation systems (INSs) does not provide the accuracy required for automated driving, and the use of optical sensors is challenged by snow covering the road markings. Although extensive further research is needed to solve these problems, the fusion of GNSS, INSs, and optical sensors seems to be the best option due to their complementary nature.","1941-1197","","10.1109/MITS.2020.2994014","Science and Technology Commission of Shanghai Municipality(grant numbers:17DZ1100803 and NSFC No. 61873163); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9115644","","Global navigation satellite system;Cameras;Autonomous vehicles;Arctic;Optical sensors","image fusion;image matching;image sensors;inertial navigation;mobile robots;robot vision;satellite navigation;SLAM (robots);snow","vehicle localization measurements;inertial navigation systems;vehicle positioning;Arctic environments;urban areas;road infrastructure;map-matching;fusion algorithms;autonomous vehicles;Arctic navigation;visual sensors;inertial sensors;Arctic areas;autonomous driving;GNSS;road markings;optical sensors;automated driving","","6","","84","IEEE","12 Jun 2020","","","IEEE","IEEE Magazines"
"Adaptive Filtering and Temporal Alignment Based Fusion Algorithm for Navigation Systems in the Arctic Region","J. Sun; Z. Wu; Z. Yin; Z. Yang","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Systems Journal","31 May 2019","2019","13","2","2022","2033","The navigation systems of vehicles face notable challenges in the Arctic region. To overcome the problems of single-sensor solutions, such as inertial navigation systems and global navigation satellite systems, hybrid navigation solutions with multiple complementary sensors are required. This paper proposes a new decentralized architecture to adapt the practical measurement noise of sensors and improve the robustness of the entire system. Previous studies have examined such techniques as Kalman filters that are primarily used to address stationary Gaussian noise. An approach named interacting dual model based adaptive filter (IDM-AF) is proposed in this study to overcome nonstationary noise and certain non-Gaussian factors, and a method that dynamically estimates the noise covariance matrix is derived in this paper. In accordance with practical requirements of fusing asynchronous sensors, we propose a new information fusion scheme based on the so-called degradation indicators of subsystems and a temporal alignment algorithm. Subsystem level and system level simulations and evaluations demonstrate that the proposed architecture and algorithms enhance navigational performance in the Arctic. Compared to a current method, the proposed architecture is shown to reduce the positioning error by 33.3-46.9% for different scenarios.","1937-9234","","10.1109/JSYST.2018.2853083","National Natural Science Foundation of China(grant numbers:61571167,61471142,61102084); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8416697","Arctic region;asynchronous information fusion;interacting dual model based adaptive filter (IDM-AF);nonstationary noise;temporal alignment","Arctic;Adaptation models;Global navigation satellite system;Mathematical model;Noise measurement","adaptive filters;covariance matrices;Gaussian noise;Gaussian processes;image fusion;inertial navigation;Kalman filters;satellite navigation;sensor fusion","Arctic region;single-sensor solutions;inertial navigation systems;global navigation satellite systems;hybrid navigation solutions;multiple complementary sensors;Kalman filters;stationary Gaussian noise;nonstationary noise;nonGaussian factors;noise covariance matrix;practical requirements;asynchronous sensors;information fusion scheme;temporal alignment algorithm;system level simulations;navigational performance;adaptive filtering","","5","","41","IEEE","20 Jul 2018","","","IEEE","IEEE Journals"
"A Multimodal Feature Selection Method for Remote Sensing Data Analysis Based on Double Graph Laplacian Diagonalization","E. Khachatrian; S. Chlaily; T. Eltoft; A. Marinoni","Department of Physics and Technology, University of TromsÃ¸â€”The Arctic University of Norway, N-9037 TromsÃ¸, Norway; Department of Physics and Technology, University of TromsÃ¸â€”The Arctic University of Norway, N-9037 TromsÃ¸, Norway; Department of Physics and Technology, University of TromsÃ¸â€”The Arctic University of Norway, N-9037 TromsÃ¸, Norway; Department of Physics and Technology, University of TromsÃ¸â€”The Arctic University of Norway, N-9037 TromsÃ¸, Norway","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","22 Nov 2021","2021","14","","11546","11566","When dealing with multivariate remotely sensed records collected by multiple sensors, an accurate selection of information at the data, feature, or decision level is instrumental in improving the scenesâ€™ characterization. This will also enhance the systemâ€™s efficiency and provide more details on modeling the physical phenomena occurring on the Earthâ€™s surface. In this article, we introduce a flexible and efficient method based on graph Laplacians for information selection at different levels of data fusion. The proposed approach combines data structure and information content to address the limitations of existing graph-Laplacian-based methods in dealing with heterogeneous datasets. Moreover, it adapts the selection to each homogenous area of the considered images according to their underlying properties. Experimental tests carried out on several multivariate remote sensing datasets show the consistency of the proposed approach.","2151-1535","","10.1109/JSTARS.2021.3124308","Centre for Integrated Remote Sensing and Forecasting for Arctic Operations; Norges ForskningsrÃ¥d(grant numbers:237906); Automatic Multi-sensor Remote Sensing for Sea Ice Characterization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9599440","Gaussian kernel (GK);graph Laplacians;multimodal remote sensing;mutual information (MI);unsupervised information selection","Laplace equations;Dimensionality reduction;Sensor phenomena and characterization;Feature extraction;Earth;Data analysis","data analysis;data structures;feature selection;geophysical image processing;graph theory;image fusion;remote sensing","Earth's surface;information selection;data fusion;data structure;information content;multivariate remote sensing datasets;multimodal feature selection;remote sensing data analysis;double graph Laplacian diagonalization","","1","","82","CCBYNCND","2 Nov 2021","","","IEEE","IEEE Journals"
"Dense Dilated Convolutions Merging Network for Semantic Mapping of Remote Sensing Images","Q. liu; M. Kampffmeyer; R. Jenssen; A. -B. Salberg","SAMBA and Machine Learning Group, Norwegian Computing Center and UiT, Oslo, Norway; Machine Learning Group UiT, the Arctic University of Norway, TromsÃ¸, Norway; Machine Learning Group UiT, the Arctic University of Norway, TromsÃ¸, Norway; dept. SAMBA of NR, Norwegian Computing Center, Oslo, Norway","2019 Joint Urban Remote Sensing Event (JURSE)","22 Aug 2019","2019","","","1","4","We propose a network for semantic mapping called the Dense Dilated Convolutions Merging Network (DDCM-Net) to provide a deep learning approach that can recognize multi-scale and complex shaped objects with similar color and textures, such as buildings, surfaces/roads, and trees in very high resolution remote sensing images. The proposed DDCM-Net consists of dense dilated convolutions merged with varying dilation rates. This can effectively enlarge the kernels' receptive fields, and, more importantly, obtain fused local and global context information to promote surrounding discriminative capability. We demonstrate the effectiveness of the proposed DDCM-Net on the publicly available ISPRS Potsdam dataset and achieve a performance of 92.3% F1-score and 86.0% mean intersection over union accuracy by only using the RGB bands, without any post-processing. We also show results on the ISPRS Vaihingen dataset, where the DDCM-Net trained with IRRG bands, also obtained better mapping accuracy (89.8% F1-score) than previous state-of-the-art approaches.","2642-9535","978-1-7281-0009-8","10.1109/JURSE.2019.8809046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809046","Dense Dilated Convolutions Merging (DDCM);deep learning;semantic mapping;remote sensing","Semantics;Merging;Remote sensing;Buildings;Computer architecture;Training;Automobiles","feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;image resolution;image segmentation;image texture;learning (artificial intelligence);object detection;remote sensing","dilation rates;kernels;local context information;global context information;publicly available ISPRS Potsdam dataset;mapping accuracy;semantic mapping;Dense Dilated Convolutions Merging Network;deep learning approach;complex shaped objects;high resolution remote sensing images;multiscale objects;DDCM-Net","","5","","29","IEEE","22 Aug 2019","","","IEEE","IEEE Conferences"
"GMSRF-Net: An Improved generalizability with Global Multi-Scale Residual Fusion Network for Polyp Segmentation","A. Srivastava; S. Chanda; D. Jha; U. Pal; S. Ali","Computer Vision and Pattern Recognition Unit, Indian Statistical Institute, Kolkata, India; Department of Computer Science and Communication, Ã˜stfold University College, Halden, Norway; UiT The Arctic University of Norway, TromsÃ¸, Norway; Computer Vision and Pattern Recognition Unit, Indian Statistical Institute, Kolkata, India; School of Computing, University of Leeds, Leeds, UK","2022 26th International Conference on Pattern Recognition (ICPR)","29 Nov 2022","2022","","","4321","4327","Colonoscopy is a gold standard procedure but is highly operator-dependent. Efforts have been made to automate the detection and segmentation of polyps, a precancerous precursor, to effectively minimize missed rate. Widely used computer-aided polyp segmentation systems actuated by encoder-decoder have achieved high performance in terms of accuracy. However, polyp segmentation datasets collected from varied centers can follow different imaging protocols leading to difference in data distribution. As a result, most methods suffer from performance drop when trained and tested on different distributions and therefore, require re-training for each specific dataset. We address this generalizability issue by proposing a global multi-scale residual fusion network (GMSRF-Net). Our proposed network maintains high-resolution representations by performing multi-scale fusion operations across all resolution scales through dense connections while preserving low-level information. To further leverage scale information, we design cross multi-scale attention (CMSA) module that uses multi-scale features to identify, keep, and propagate informative features. Additionally, we introduce multi-scale feature selection (MSFS) modules to perform channel-wise attention that gates irrelevant features gathered through global multi-scale fusion within the GMSRF-Net. The repeated fusion operations gated by CMSA and MSFS demonstrate improved generalizability of our network.Experiments conducted on two different polyp segmentation datasets show that our proposed GMSRF-Net outperforms the previous top-performing state-of-the-art method by 8.34% and 10.31% on unseen CVC-ClinicDB and on unseen Kvasir-SEG, in terms of dice coefficient. Additionally, when tested on unseen CVC-ColonDB, we surpass the state-of-the-art method by 9.38% and 4.04% in terms of dice coefficient, when source dataset is Kvasir-SEG and CVC-ClinicDB, respectively.","2831-7475","978-1-6654-9062-7","10.1109/ICPR56361.2022.9956726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956726","","Training;Image segmentation;Protocols;Supervised learning;Logic gates;Performance gain;Pattern recognition","cancer;feature extraction;image fusion;image segmentation;medical image processing;minimisation","CMSA;computer-aided polyp segmentation systems;cross multiscale attention module design;cross multiscale attention module design;CVC-ColonDB;global multiscale fusion;global multiscale residual fusion network;GMSRF-Net;high-resolution representations;highly operator-dependent;imaging protocols;improved generalizability;leverage scale information;multiscale attention module;multiscale feature selection;multiscale fusion operations;polyp segmentation datasets;repeated fusion operations;resolution scales","","3","","25","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining","X. Dong; X. Zhan; Y. Wu; Y. Wei; M. C. Kampffmeyer; X. Wei; M. Lu; Y. Wang; X. Liang",Sun Yat-sen University; Shenzhen Campus of Sun Yat-sen University; Sun Yat-sen University; Beijing Jiaotong University; UiT The Arctic University of Norway; PengCheng Laboratory; Alibaba Group; PengCheng Laboratory; Shenzhen Campus of Sun Yat-sen University,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","21220","21230","Despite the potential of multi-modal pre-training to learn highly discriminative feature representations from complementary data modalities, current progress is being slowed by the lack of large-scale modality-diverse datasets. By leveraging the natural suitability of E-commerce, where different modalities capture complementary semantic information, we contribute a large-scale multi-modal pretraining dataset M5Product. The dataset comprises 5 modalities (image, text, table, video, and audio), covers over 6,000 categories and 5,000 attributes, and is 500Ã— larger than the largest publicly available dataset with a similar number of modalities. Furthermore, M5Product contains incomplete modality pairs and noise while also having a long-tailed distribution, resembling most real-world problems. We further propose Self-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework that integrates the different modalities into a unified model through an adaptive feature fusion mechanism, where the importance of each modality is learned directly from the modality embeddings and impacts the inter-modality contrastive learning and masked tasks within a multi-modal transformer model. We evaluate the current multi-modal pre-training state-of-the-art approaches and benchmark their ability to learn from unlabeled data when faced with the large number of modalities in the M5Product dataset. We conduct extensive experiments on four downstream tasks and demonstrate the superiority of our SCALE model, providing insights into the importance of dataset scale and diversity. Dataset and codes are available at 11https://xiaodongsuper.github.io/M5Product_dataset/.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.02057","Shenzhen Fundamental Research Program(grant numbers:RCYX20200714114642083,JCYJ20190807154211365); National Key RD Program of China(grant numbers:2021ZD0112100); CAAI-Huawei MindSpore Open Fund; Guangdong Provincial Key Laboratory of Fire Science and Intelligent Emergency Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878412","Datasets and evaluation; Representation learning; Vision + language","Representation learning;Adaptation models;Computer vision;Codes;Computational modeling;Semantics;Transformers","feature extraction;image classification;image fusion;image registration;image representation;learning (artificial intelligence);medical image processing;text analysis","inter-modality contrastive learning;multimodal transformer model;current multimodal pre-training state-of-the-art;M5Product dataset;SCALE model;dataset scale;E-commercial multimodal pretraining;highly discriminative feature representations;complementary data modalities;large-scale modality-diverse datasets;different modalities capture complementary semantic information;large-scale multimodal pretraining dataset;largest publicly available dataset;incomplete modality pairs;noise while;novel pretraining framework;adaptive feature fusion mechanism;modality embeddings","","2","","59","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
