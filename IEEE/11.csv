"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Cloud-Gan: Cloud Removal for Sentinel-2 Imagery Using a Cyclic Consistent Generative Adversarial Networks","P. Singh; N. Komodakis","Ecole des Ponts ParisTech & Université Paris Est, France; Ecole des Ponts ParisTech & Université Paris Est, France","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1772","1775","Cloud cover is a serious impediment in land surface analysis from Remote Sensing images either causing complete obstruction (thick clouds) with loss of information or blurry effects when being semi-transparent (thin clouds). While thick clouds require complete pixel replacement, thin cloud removal is fairly challenging as the atmospheric and land-cover information is inter-twined. In this paper, we address this problem and propose a Cloud-GAN to learn the mapping between cloudy images and cloud-free images. The adver-sarialloss in the proposed method constrains the distribution of generated images to be close enough to the underlying distribution of the non-cloudy images. An additional cycle consistency loss is used to further restrain the generator to predict cloud-free images only of the same scene as reflected in the cloudy images. Our method not only rejects the necessity of any paired (cloud/cloud-free) training dataset but also avoids the need of any additional (expensive) spectral source of information such as Synthetic Aperture Radar imagery which is cloud penetrable. Lastly, we demonstrate the efficacy of our technique by training on an openly available and fairly new Sentinel-2 Imagery dataset consisting of real clouds. We also show significant improvement in PSNR values after removing clouds on synthetic images thus validating the competency of our methodology.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519033","Cloud Removal;Generative Adversarial Networks;Deep Learning;Sentinel-2 Imagery","Clouds;Training;Generators;Remote sensing;Synthetic aperture radar;Gallium nitride;Generative adversarial networks","clouds;geophysical image processing;image restoration;radar imaging;remote sensing by radar;synthetic aperture radar","cloud removal;cyclic consistent generative adversarial networks;Remote Sensing images;thick clouds;atmospheric land-cover information;Cloud-GAN;cloudy images;cloud-free images;generated images;noncloudy images;cloud cover;synthetic aperture radar imagery;thin cloud;Sentinel-2 imagery;land surface analysis","","73","","13","EU","4 Nov 2018","","","IEEE","IEEE Conferences"
"Super Resolution Approach for the Satellite Data Based on the Generative Adversarial Networks","M. Lavreniuk; N. Kussul; A. Shelestov; A. Lavrenyuk; L. Shumilo","Space Research Institute NASU-SSAU, Kyiv, Ukraine; National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”, Kyiv, Ukraine; National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”, Kyiv, Ukraine; National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”, Kyiv, Ukraine; National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”, Kyiv, Ukraine","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1095","1098","In the past few years, medium and high-resolution data became freely available for downloading. It provides great opportunity for researchers not to select between solving the task with high-resolution data on small territory or on global scale, but with low-resolution satellite images. Due to high spectral and spatial resolution of the data, Sentinel-1 and Sentinel-2 are very popular sources of information. Nevertheless, in practice if we would like to receive final product in 10 m resolution we should use bands with 10 m resolution. Sentinel-2 has four such bands, but also has other bands, especially red-edge 20 m resolution bands that are useful for vegetation analysis and often are omitted due to lower resolution. Thus, in this study we propose methodology for enhancing resolution (super-resolution) of the existing low-resolution images to higher resolution images. The main idea is to use advanced methods of deep learning - Generative Adversarial Networks (GAN) and train it to increase the resolution for the satellite images. Experimental results for the Sentinel-2 data showed that this approach is efficient and could be used for creating high resolution products.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884460","deep learning;Generative Adversarial Networks;GAN;super-resolution;Sentinel-2","Deep learning;Image resolution;Satellites;Superresolution;Vegetation mapping;Generative adversarial networks;Spatial resolution","geophysical image processing;image resolution;remote sensing;vegetation","high-resolution data;low-resolution satellite images;high spectral resolution;spatial resolution;Sentinel-1;enhancing resolution;existing low-resolution images;higher resolution images;Generative Adversarial Networks;Sentinel-2 data;high resolution products;super resolution approach;satellite data;size 10.0 m;size 20.0 m","","","","23","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"An Approach To Super-Resolution Of Sentinel-2 Images Based On Generative Adversarial Networks","K. Zhang; G. Sumbul; B. Demir",Shanghai Jiao Tong University; Technische Universität Berlin; Technische Universität Berlin,"2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium (M2GARSS)","2 Jun 2020","2020","","","69","72","This paper presents a generative adversarial network based super-resolution (SR) approach (which is called as S2GAN) to enhance the spatial resolution of Sentinel-2 spectral bands. The proposed approach consists of two main steps. The first step aims to increase the spatial resolution of the bands with 20m and 60m spatial resolutions by the scaling factors of 2 and 6, respectively. To this end, we introduce a generator network that performs SR on the lower resolution bands with the guidance of the bands associated to 10m spatial resolution by utilizing the convolutional layers with residual connections and a long skip-connection between inputs and outputs. The second step aims to distinguish SR bands from their ground truth bands. This is achieved by the proposed discriminator network, which alternately characterizes the high level features of the two sets of bands and applying binary classification on the extracted features. Then, we formulate the adversarial learning of the generator and discriminator networks as a min-max game. In this learning procedure, the generator aims to produce realistic SR bands as much as possible so that the discriminator incorrectly classifies SR bands. Experimental results obtained on different Sentinel-2 images show the effectiveness of the proposed approach compared to both conventional and deep learning based SR approaches.","","978-1-7281-2190-1","10.1109/M2GARSS47143.2020.9105165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9105165","Sentinel-2 images;super-resolution;generative adversarial network;remote sensing","Image color analysis;Superresolution;Neural networks;Geoscience and remote sensing;Games;Generative adversarial networks;Feature extraction","convolutional neural nets;feature extraction;game theory;geophysical image processing;image classification;image resolution;learning (artificial intelligence);minimax techniques","generative adversarial network;super-resolution approach;S2GAN;spatial resolution;Sentinel-2 spectral bands;generator network;lower resolution bands;ground truth bands;discriminator network;binary classification;adversarial learning;realistic SR bands;Sentinel-2 images;convolutional layers;feature extraction;min-max game;size 20.0 m;size 10.0 m;size 60.0 m","","3","","10","IEEE","2 Jun 2020","","","IEEE","IEEE Conferences"
"Automatic Area-Based Registration of Optical and SAR Images Through Generative Adversarial Networks and a Correlation-Type Metric","L. Maggiolo; D. Solarna; G. Moser; S. B. Serpico","University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2089","2092","The automatic registration of multisensor remote sensing images is a highly challenging task due to the inherently different physical, statistical, and textural properties of the input data. In the present paper, this problem is addressed in the case of optical-SAR images by proposing a novel method based on deep learning and area-based registration concepts. The method integrates a conditional generative adversarial network (cGAN), an area-based cross-correlation-type l2 similarity metric, and the COBYLA constrained maximization algorithm. Whereas correlation-type metrics are typically ineffective in the application to multisensor registration, the proposed approach allows exploiting the image translation capabilities of cGAN architectures to enable the use of an l2 similarity metric, which favors high computational efficiency. Experiments with Sentinel-1 and Sentinel-2 data suggest the effectiveness of this strategy and the capability of the proposed method to achieve accurate registration.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323235","European Space Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323235","Multisensor image registration;conditional generative adversarial network;$\ell^{2}$ similarity;COBYLA","Radar polarimetry;Optical imaging;Measurement;Optical sensors;Feature extraction;Training;Generative adversarial networks","geophysical image processing;geophysical signal processing;image classification;image fusion;image registration;radar imaging;remote sensing;remote sensing by radar;sensor fusion;synthetic aperture radar","automatic area-based registration;generative adversarial networks;correlation-type metric;automatic registration;multisensor remote sensing images;textural properties;optical-SAR images;deep learning;area-based registration concepts;conditional generative adversarial network;area-based cross-correlation-type l;image translation capabilities;Sentinel-2 data","","4","","14","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A Conditional Generative Adversarial Network to Fuse Sar And Multispectral Optical Data For Cloud Removal From Sentinel-2 Images","C. Grohnfeldt; M. Schmitt; X. Zhu","Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1726","1729","In this paper, we present the first conditional generative adversarial network (cGAN) architecture that is specifically designed to fuse synthetic aperture radar (SAR) and optical multi-spectral (MS) image data to generate cloud- and haze-free MS optical data from a cloud-corrupted MS input and an auxiliary SAR image. Experiments on Sentinel-2 MS and Sentinel-l SAR data confirm that our extended SAR-Opt-cGAN model utilizes the auxiliary SAR information to better reconstruct MS images than an equivalent model which uses the same architecture but only single-sensor MS data as input.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519215","SAR;optical remote sensing;data fusion;deep learning;generative adversarial network (GAN);cloud-removal","Synthetic aperture radar;Optical sensors;Optical imaging;Clouds;Remote sensing;Adaptive optics;Generative adversarial networks","geophysical image processing;image fusion;radar imaging;synthetic aperture radar","cloud-free MS optical data;Sentinel-l SAR data;single-sensor MS data;reconstruct MS images;auxiliary SAR information;extended SAR-Opt-cGAN model;auxiliary SAR image;cloud-corrupted MS input;haze-free MS optical data;multispectral image data;synthetic aperture radar;conditional generative adversarial network architecture;Sentinel-2;cloud removal;multispectral optical data","","51","3","12","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Cloud Removal in Unpaired Sentinel-2 Imagery Using Cycle-Consistent GAN and SAR-Optical Data Fusion","P. Ebel; M. Schmitt; X. X. Zhu","Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Wessling, Germany","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2065","2068","The majority of optical images acquired via spaceborne remote sensing are affected by clouds. Recent advances in cloud removal combine multimodal data with deep neural networks recovering the affected areas. To relax the requirements on the data the network is trained on previous approaches utilized generative models no longer necessitating strict pixel-wise correspondences between cloudy input and cloud-free target images. However, such models are often-times prone to fiction, i.e. the generation of content systematically differing from the structure of the target images. In this work we combine the fusion of optical and radar imagery with the advantages of generative models trainable on unpaired optical data, while reducing fiction by reconstructing optical information only where it need be-over cloud-covered areas. We evaluate our approach qualitatively and quantitatively and demonstrate its effectiveness.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324060","synthetic aperture radar (SAR);optical imagery;cloud removal;data fusion;deep learning","Clouds;Optical imaging;Optical sensors;Adaptive optics;Generative adversarial networks;Synthetic aperture radar;Optical fiber networks","geophysical image processing;image reconstruction;neural nets;optical images;radar imaging;remote sensing;sensor fusion;synthetic aperture radar","generative models;longer necessitating strict pixel-wise correspondences;cloudy input;cloud-free target images;fiction;content systematically differing;optical radar imagery;unpaired optical data;optical information;cloud-covered areas;unpaired sentinel-2 imagery;cycle-consistent GAN;SAR-optical data fusion;optical images;spaceborne remote sensing;cloud removal combine multimodal data;deep neural networks","","8","","15","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A High-Performance Multispectral Adaptation GAN for Harmonizing Dense Time Series of Landsat-8 and Sentinel-2 Images","R. Sedona; C. Paris; G. Cavallaro; L. Bruzzone; M. Riedel","University of Iceland, Reykjavik, Iceland; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Jülich Supercomputing Centre, Jülich, Germany; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; University of Iceland, Reykjavik, Iceland","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Oct 2021","2021","14","","10134","10146","The combination of data acquired by Landsat-8 and Sentinel-2 earth observation missions produces dense time series (TSs) of multispectral images that are essential for monitoring the dynamics of land-cover and land-use classes across the earth's surface with high temporal resolution. However, the optical sensors of the two missions have different spectral and spatial properties, thus they require a harmonization processing step before they can be exploited in remote sensing applications. In this work, we propose a workflow-based on a deep learning approach to harmonize these two products developed and deployed on an high-performance computing environment. In particular, we use a multispectral generative adversarial network with a U-Net generator and a PatchGan discriminator to integrate existing Landsat-8 TSs with data sensed by the Sentinel-2 mission. We show a qualitative and quantitative comparison with an existing physical method [National Aeronautics and Space Administration (NASA) Harmonized Landsat and Sentinel (HLS)] and analyze original and generated data in different experimental setups with the support of spectral distortion metrics. To demonstrate the effectiveness of the proposed approach, a crop type mapping task is addressed using the harmonized dense TS of images, which achieved an overall accuracy of 87.83% compared to 81.66% of the state-of-the-art method.","2151-1535","","10.1109/JSTARS.2021.3115604","CoE RAISE project; European Union's Horizon 2020 Research and Innovation Framework Programme(grant numbers:951733); ADMIRE project; European Union's Horizon 2020 JTI-EuroHPC research and innovation programme(grant numbers:956748); DEEP-EST project; European Union's Horizon 2020 research and innovation programme(grant numbers:754304); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9548804","Deep learning (DL);dense time series (TSs);generative adversarial network (GAN);harmonization;high performance computing (HPC);Landsat-8;remote sensing (RS);sentinel-2;virtual constellation","Remote sensing;Earth;Artificial satellites;Spatial resolution;Optical sensors;Generators;Generative adversarial networks","deep learning (artificial intelligence);geophysical image processing;land cover;land use;remote sensing by laser beam;time series","spectral distortion metrics;high-performance multispectral adaptation GAN;dense time series;Sentinel-2 earth observation missions;multispectral images;land-cover;land-use classes;optical sensors;spatial properties;harmonization processing step;remote sensing applications;deep learning approach;multispectral generative adversarial network;U-Net generator;Sentinel-2 mission;qualitative comparison;Landsat-8 images","","3","","63","CCBY","27 Sep 2021","","","IEEE","IEEE Journals"
"Thin Cloud Removal Fusing Full Spectral and Spatial Features for Sentinel-2 Imagery","J. Li; Y. Zhang; Q. Sheng; Z. Wu; B. Wang; Z. Hu; G. Shen; M. Schmitt; M. Molinier","College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, China; MNR Key Laboratory for Geo-Environmental Monitoring of Great Bay Area, Shenzhen University, Shenzhen, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Department of Aerospace Engineering, University of the Bundeswehr Munich, Neubiberg, Germany; VTT Technical Research Centre of Finland, Ltd., Espoo, Finland","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 Oct 2022","2022","15","","8759","8775","Multispectral remote sensing images are widely used for monitoring the globe. Although thin clouds can affect all optical bands, the influences of thin clouds differ with band wavelength. When processing multispectral bands at different resolutions, many methods only remove thin clouds in visible/near-infrared bands or rescale multiresolution bands to the same resolution and then process them together. The former cannot make full use of multispectral information, and in the latter, the rescaling process will introduce noise. In this article, a deep-learning-based thin cloud removal method that fuses full spectral and spatial features in original Sentinel-2 bands is proposed, named CR4S2. A multi-input and output architecture is designed for better fusing information in all bands and reconstructing the background at original resolutions. In addition, two parallel downsampling residual blocks are designed to transfer features extracted from different depths to the bottom of the network. Experiments were conducted on a new globally distributed Sentinel-2 thin cloud removal dataset called WHUS2-CRv. The results show that the best averaged peak signal-to-noise ratio, structural similarity index measurement, normalized root-mean-square error, and spectral angle mapper of the proposed method over 12 bands in all 20 testing images were 39.55, 0.9443, 0.0245, and 2.5676°, respectively. Compared with baseline methods, the proposed CR4S2 method can better restore not only the spatial features but also spectral features. This indicates that the proposed method is very promising for removing thin clouds in multispectral remote sensing images at different resolutions.","2151-1535","","10.1109/JSTARS.2022.3211857","Natural Science Foundation of Jiangsu Province(grant numbers:BK20220888); National Key Laboratory Foundation of China(grant numbers:2021-JCJQ-LB-006,8676142411442120); National Key Laboratory of Science and Technology on Space Microwave(grant numbers:HTKJ2022KL504018); Academy of Finland; Finnish Flagship Programme FCAI: Finnish Center for Artificial Intelligence(grant numbers:320183); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910392","Deep learning (DL);multifeature fusion;parallel downsample residual block (PDRB);Sentinel-2;thin cloud removal","Clouds;Cloud computing;Remote sensing;Spatial resolution;Feature extraction;Vegetation mapping;Generative adversarial networks","feature extraction;geophysical image processing;image resolution;learning (artificial intelligence);mean square error methods;remote sensing","thin cloud removal fusing;spatial features;Sentinel-2 imagery;multispectral remote sensing images;optical bands;band wavelength;multispectral bands;rescale multiresolution bands;multispectral information;rescaling process;cloud removal method;original Sentinel-2 bands;named CR4S2;output architecture;fusing information;original resolutions;cloud removal dataset;WHUS2-CRv;peak signal-to-noise ratio;spectral angle mapper;baseline methods;spectral features","","","","63","CCBY","4 Oct 2022","","","IEEE","IEEE Journals"
"Gan-based SAR to Optical Image Translation in Fire-Disturbed Regions","X. Hu; P. Zhang; Y. Ban","Division of Geoinformatics, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Geoinformatics, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Geoinformatics, KTH Royal Institute of Technology, Stockholm, Sweden","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1456","1459","Climate change by anthropogenic warming leads to increases in dry fuels and promotes forest fires. Multispectral images' quality is easily affected by poor atmospheric conditions. SAR satellite sensors can penetrate through clouds and image day and night. However, the burned area mapping methods widely used for optical data are not feasible to be applied for SAR data owing to the differences in imaging mechanisms. Recent advances in deep image translation can fill this gap by using Generative Adversarial Networks (GAN). In this research, we apply a GAN-based model for SAR to optical image translation over fire-disturbed regions. Specifically, Sentinel-1 SAR images are translated into Sentinel-2 images using the ResNet-based Pix2Pix model, which is trained on 281 large fire events and tested on the other 23 events in Canada. The generated images preserve the spectral characteristics well and show high similarity to the real images with Structure Similarity Index Measure (SSIM) over 0.59.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884234","Sentinel-2 MSI;Sentinel-1 C band SAR;image Translation;Generative Adversarial Network (GAN)","Optical losses;Visualization;Satellites;Soil moisture;Forestry;Optical imaging;Generative adversarial networks","fires;geophysical image processing;optical images;radar imaging;remote sensing by radar;synthetic aperture radar","gan-based SAR;optical image translation;fire-disturbed regions;climate change;anthropogenic warming;dry fuels;promotes forest fires;multispectral images;poor atmospheric conditions;SAR satellite sensors;image day;burned area mapping methods;optical data;SAR data;imaging mechanisms;deep image translation;GAN-based model;Sentinel-1 SAR images;Sentinel-2 images;ResNet-based Pix2Pix model;281 large fire events","","","","12","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Single Sensor Image Fusion Using A Deep Convolutional Generative Adversarial Network","F. Palsson; J. R. Sveinsson; M. O. Ulfarsson","Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland","2018 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","27 Jun 2019","2018","","","1","5","Recently deployed multispectral sensors can acquire multispectral images where different bands have different spatial resolution depending on wavelength. An example is the Sentinel-2 constellation which can acquire multispectral bands of 10 m, 20 m, and 60 m resolution, covering the visible, near-infrared (NIR) and short-wave infrared (SWIR) parts of the electromagnetic spectrum. In this paper, a method to perform image fusion of the fine and coarse spatial resolution bands to increase the resolution of the coarser bands is proposed. The method is based on a so-called Generative Adversarial Network (GAN) and uses a deep convolutional design for both the generator and the discriminator. In experiments, it is demonstrated that the proposed method gives good results when compared to state-of-the-art single sensor image fusion methods using both simulated and real Sentinel-2 datasets.","2158-6276","978-1-7281-1581-8","10.1109/WHISPERS.2018.8747268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747268","Image fusion;generative adversarial network;convolutional network;Sentinel-2","Image resolution;Image fusion;Generators;Gallium nitride;Training;Generative adversarial networks;Signal resolution","geophysical image processing;geophysical techniques;image fusion;image resolution;image sensors;remote sensing","deep convolutional Generative Adversarial Network;multispectral images;Sentinel-2 constellation;multispectral bands;electromagnetic spectrum;fine resolution bands;coarse spatial resolution bands;deep convolutional design;Sentinel-2 datasets;multispectral sensors;visible band;short-wave infrared band;single sensor image fusion methods;near-infrared band;wavelength 10.0 m;wavelength 60.0 m;wavelength 20.0 m","","7","","16","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"Unsupervised Deep Transfer Learning-Based Change Detection for HR Multispectral Images","S. Saha; Y. T. Solano-Correa; F. Bovolo; L. Bruzzone","Department of Information Engineering and Computer science, University of Trento, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; Department of Information Engineering and Computer science, University of Trento, Trento, Italy","IEEE Geoscience and Remote Sensing Letters","22 Apr 2021","2021","18","5","856","860","To overcome the limited capability of most state-of-the-art change detection (CD) methods in modeling spatial context of multispectral high spatial resolution (HR) images and exploiting all spectral bands jointly, this letter presents a novel unsupervised deep-learning-based CD method that can effectively model contextual information and handle the large number of bands in multispectral HR images. This is achieved by exploiting all spectral bands after grouping them into spectral-dedicated band groups. To eliminate the necessity of multitemporal training data, the proposed method exploits a data set targeted for image classification to train spectral-dedicated Auxiliary Classifier Generative Adversarial Networks (ACGANs). They are used to obtain pixelwise deep change hypervector from multitemporal images. Each feature in deep change hypervector is analyzed based on the magnitude to identify changed pixels. An ensemble decision fusion strategy is used to combine change information from different features. Experimental results on the urban, Alpine, and agricultural Sentinel-2 data sets confirm the effectiveness of the proposed method.","1558-0571","","10.1109/LGRS.2020.2990284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9089195","Change detection (CD);deep learning;generative adversarial network;high resolution;Sentinel-2","Feature extraction;Spatial resolution;Training;Gallium nitride;Generative adversarial networks;Generators","feature extraction;geophysical image processing;geophysical signal processing;image classification;image segmentation;learning (artificial intelligence);pattern classification;remote sensing;unsupervised learning","unsupervised deep transfer learning-based change detection;HR multispectral;state-of-the-art change detection methods;modeling spatial context;multispectral high spatial resolution images;spectral bands;CD method;contextual information;multispectral HR images;spectral-dedicated band groups;multitemporal training data;data set;image classification;Auxiliary Classifier Generative Adversarial Networks;pixelwise deep change hypervector;multitemporal images;changed pixels;change information","","29","","12","IEEE","7 May 2020","","","IEEE","IEEE Journals"
"Biophysical Parameter Estimation Using Earth Observation Data in a Multi-Sensor Data Fusion Approach: CycleGAN","N. Efremova; E. Erten","University of Oxford Deepplanet, Oxford, UK; Department of Geomatics Engineering, Istanbul Technical University, Istanbul, Turkey","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","5965","5968","Water management and up-to-date soil moisture (SM) information are crucial to ensure agricultural activities in dry-land farming regions. In this context, remote sensing imagery coupled with machine learning techniques can provide large scale SM information if there is enough data for training, which is really limited in reality. In this paper, we explored the potential of cycle-consistent Generative Adversarial Network (GAN) for data augmentation for training machine learning algorithms, which try to model spatial and temporal dependencies between the SM prediction (output) and the remote sensing imagery (input features). Specifically, the freely available SAR (Sentinel-1) and optical (Sentinel-2) time series data were evaluated together to predict SM using GANs. The experiments demonstrate that the proposed methodology outperforms the compared state-of-the-art methods if there is not enough data to train a regression convolutional neural networks (CNN) to predict SM content.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553561","Research Fund of the Istanbul Technical University(grant numbers:43018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553561","soil moisture;Sentinel-1;Sentinel-2;cycleGAN;CNN;PCA;autoencoders;support vector regression;ridge regression","Training;Time series analysis;Soil moisture;Pipelines;Estimation;Predictive models;Generative adversarial networks","","","","1","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"CTGAN : Cloud Transformer Generative Adversarial Network","G. -L. Huang; P. -Y. Wu","Graduate Institute of Communication Engineering, Electrical Engineering, National Taiwan University, Taiwan; Graduate Institute of Communication Engineering, Electrical Engineering, National Taiwan University, Taiwan","2022 IEEE International Conference on Image Processing (ICIP)","18 Oct 2022","2022","","","511","515","Cloud occlusions obstruct some applications of remote sensing imagery, such as environment monitoring, land cover classification, and poverty prediction. In this paper, we propose the Cloud Transformer Generative Adversarial Network (CTGAN), taking three temporal cloudy images as input and generating a corresponding cloud-free image. Unlike previous work using generative networks, we design the feature extractor to maintain the weight of the cloudless region while reducing the weight of the cloudy region, and we pass the extracted features to a conformer module to find the most critical representations. Mean-while, to address the lack of datasets, we collected a new dataset named Sen2_MTC from the Sentinel-2 satellite and manually labeled each cloudy and cloud-free image. Finally, we conducted extensive experiments on FS-2, the STGAN dataset, and Sen2_MTC. Our proposed CTGAN demonstrates higher qualitative and quantitative performance than the previous work and achieves state-of-the-art performance on these three datasets. The code is available at https://github.com/come880412/CTGAN","2381-8549","978-1-6654-9620-9","10.1109/ICIP46576.2022.9897229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9897229","Cloud removal for multi-temporal cloudy images;generative adversarial network;conformer;Sentinel-2 satellite;FormoSat-2 satellite","Satellites;Codes;Clouds;Feature extraction;Transformers;Generative adversarial networks;Remote sensing","","","","","","21","IEEE","18 Oct 2022","","","IEEE","IEEE Conferences"
"Agriculture Land Appraisal with Use of Remote Sensing and Infrastructure Data","N. Kussul; A. Shelestov; H. Yailymova; L. Shumilo; S. Drozd","Space Research Institute NASU-SSAU, Kyiv, Ukraine; Space Research Institute NASU-SSAU, Kyiv, Ukraine; Space Research Institute NASU-SSAU, Kyiv, Ukraine; Department of Geographical Sciences, University of Maryland, College Park, MD, USA; National Technical University of Ukraine “Igor Sikorsky Kiev Polytechnic Institute”, Kyiv, Ukraine","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2785","2788","1st July 2021 the law on the creation of land market start effect in Ukraine. As a result, land appraisal became cornerstone task in Ukrainian agriculture sector. The official methodology on land appraisal includes use of soil fertility characteristics combined with coefficients related to the distance to the infrastructure objects or settlements and placing of field in specific functional areas, like recreational, or areas with high level of radiation pollution. In this study we collected open source infostructure geospatial information and characteristics of fields obtained from remote sensing data - crop types and Normalized Difference Vegetation Index to build land price predictive model trained on the official land market information. This work designed to investigate potential of geo-informational technologies and remote sensing in the land appraisal use. We separated all available ground truth land price data into three groups by fields size - very small, small, medium and big. We found different relationships between field characteristics and prices. For very small fields the most important features are area, altitude, slope, bonitet and distances to elevators, villages and roads. For small fields the most important are bonitet, altitude, area and distances to cities and roads. For medium and big field's area, slope, distance to cities, roads and historical NDVI.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884045","deep learning;Generative Adversarial Networks;GAN;super-resolution;Sentinel-2","Pollution;Roads;Urban areas;Vegetation mapping;Soil;Predictive models;Agriculture","agriculture;crops;geographic information systems;land use planning;pricing;remote sensing;soil;vegetation;vegetation mapping","cornerstone task;Ukrainian agriculture sector;official methodology;soil fertility characteristics;infrastructure objects;settlements;specific functional areas;open source infostructure geospatial information;remote sensing data - crop types;Normalized Difference Vegetation Index;land price predictive model;official land market information;geo-informational technologies;land appraisal use;available ground truth land price data;fields size;field characteristics;bonitet;big field;agriculture land appraisal;infrastructure data;st July;land market start effect","","","","15","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"SIGAN: Spectral Index Generative Adversarial Network for Data Augmentation in Multispectral Remote Sensing Images","A. Singh; L. Bruzzone","Department of Information Engineering and Computer Science, Remote Sensing Laboratory (RSLab), University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, Remote Sensing Laboratory (RSLab), University of Trento, Trento, Italy","IEEE Geoscience and Remote Sensing Letters","24 Dec 2021","2022","19","","1","5","Generative models are typically employed to approximate the distribution of deep features. Recently, these state-of-the-art methods have been applied to estimate image transformations by an unsupervised learning approach. In this letter, a novel spectral index generative adversarial network (SIGAN) is proposed for the generation of multispectral (MS) remote sensing images. This network is defined to effectively perform data augmentation starting from a limited number of training samples in the MS remote sensing domain for training deep learning models. The SIGAN model is able to capture class-specific properties in data augmentation, by incorporating the task-specific normalized spectral indices to model class-by-class properties of MS images. Experimental results obtained on a Sentinel 2 dataset show that the proposed model provides better performance than other generative adversarial networks (GANs) in MS data generation.","1558-0571","","10.1109/LGRS.2021.3093238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9537161","Data augmentation;labeled sample;multispectral (MS) images;remote sensing","Generators;Training;Indexes;Data models;Remote sensing;Convolution;Generative adversarial networks","","","","5","","14","IEEE","14 Sep 2021","","","IEEE","IEEE Journals"
"Cycle-Consistent Adversarial Networks for Realistic Pervasive Change Generation in Remote Sensing Imagery","C. X. Ren; A. Ziemann; J. Theiler; A. M. S. Durieux","Intelligence and Space Research Division, Los Alamos National Laboratory, Los Alamos, NM, USA; Intelligence and Space Research Division, Los Alamos National Laboratory, Los Alamos, NM, USA; Intelligence and Space Research Division, Los Alamos National Laboratory, Los Alamos, NM, USA; Descartes Labs, Inc., Santa Fe, NM, USA","2020 IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI)","18 May 2020","2020","","","42","45","This paper introduces a new method of generating realistic pervasive changes in the context of evaluating the effectiveness of change detection algorithms in controlled settings. The method - a cycle-consistent adversarial network (CycleGAN) - requires low quantities of training data to generate realistic changes. Here we show an application of CycleGAN in creating realistic snow-covered scenes of multispectral Sentinel-2 imagery, and demonstrate how these images can be used as a test bed for anomalous change detection algorithms.","2473-3598","978-1-7281-5745-0","10.1109/SSIAI49293.2020.9094603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9094603","remote sensing;multispectral;generative adversarial networks;deep learning;change detection;image analysis","Remote sensing;Change detection algorithms;Robustness;Gallium nitride;Terrain factors;Detection algorithms;Data models","geophysical image processing;remote sensing;snow","CycleGAN;realistic snow-covered scenes;multispectral Sentinel-2 imagery;anomalous change detection algorithms;cycle-consistent adversarial network;remote sensing imagery;pervasive change generation","","5","","15","IEEE","18 May 2020","","","IEEE","IEEE Conferences"
"A Cycle Gan Approach for Heterogeneous Domain Adaptation in Land Use Classification","C. Voreiter; J. -C. Burnel; P. Lassalle; M. Spigai; R. Hugues; N. Courty","Université de Bretagne Sud - IRISA, Vannes, France; Université de Bretagne Sud - IRISA, Vannes, France; CNES, Toulouse Cedex 9, France; Thales Alenia Space, Toulouse, France; Thales Alenia Space, Toulouse, France; Université de Bretagne Sud - IRISA, Vannes, France","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1961","1964","In the field of remote sensing and more specifically in Earth Observation, new data are available every day, coming from different sensors. Leveraging on those data in classification tasks comes at the price of intense labelling tasks that are not realistic in operational settings. While domain adaptation could be useful to counterbalance this problem, most of the usual methods assume that the data to adapt are comparable (they belong to the same metric space), which is not the case when multiple sensors are at stake. Heterogeneous domain adaptation methods are a particular solution to this problem. We present a novel method to deal with such cases, based on a modified cycleGAN version that incorporates classification losses and a metric space alignment term. We demonstrate its power on a land use classification tasks, with images from both Google Earth and Sentinel-2.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324264","Centre National d’Études Spatiales (CNES); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324264","Heterogeneous Domain Adaptation;Cycle GAN;Land Use Classification","Gallium nitride;Measurement;Generators;Task analysis;Generative adversarial networks;Agriculture;Earth","geophysical image processing;image classification;learning (artificial intelligence);pattern classification;remote sensing","cycle gan approach;land use classification;remote sensing;Earth Observation;classification tasks;intense labelling tasks;operational settings;usual methods;multiple sensors;heterogeneous domain adaptation methods;classification losses;metric space alignment term;Google Earth","","3","","13","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Data Augmentation Through Spectrally Controlled Adversarial Networks for Classification of Multispectral Remote Sensing Images","A. Singh; L. Bruzzone","Department of Information Engineering and Computer Science, Remote Sensing Laboratory (RSLab), University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, Remote Sensing Laboratory (RSLab), University of Trento, Trento, Italy","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","651","654","Availability of limited training remote sensing datasets is one of the problems in deep learning, as deep architectures require a large number of training samples for proper training. In this paper, we present a technique for data augmentation based on a spectral indexed generative adversarial network to train deep convolutional neural networks. This technique uses the spectral characteristic of multispectral (MS) images to support data augmentation in order to generate realistic training samples with respect to each land-use and land-cover class. The impact of multispectral remote sensing data generated through the spectral indexed GAN are evaluated through classification experiments. Experimental results obtained on the classification of the Sentinel-2 Eurosatallband datasets show that data augmentation through spectral indexed GAN enhances the main accuracy metrics.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884928","Training Data;Data Augmentation;Generative Model;Remote Sensing Image Analysis;Classification Accuracy","Training;Measurement;Deep learning;Analytical models;Generative adversarial networks;Sensors;Convolutional neural networks","geophysical image processing;image classification;learning (artificial intelligence);neural nets;remote sensing","data augmentation;spectrally controlled adversarial networks;training remote sensing datasets;deep learning;deep architectures;proper training;spectral indexed generative adversarial network;deep convolutional neural networks;spectral characteristic;multispectral images;realistic training samples;multispectral remote sensing data;spectral indexed GAN","","1","","14","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Improving Satellite Image Fusion via Generative Adversarial Training","X. Luo; X. Tong; Z. Hu","College of Life Sciences and Oceanography, Shenzhen University, Shenzhen, China; College of Surveying and Geo-informatics, Tongji University, Shanghai, China; MNR Key Laboratory for Geo-Environmental Monitoring of Great Bay Area & Guangdong Key Laboratory of Urban Informatics & Shenzhen Key Laboratory of Spatial Smart Sensing and Services, Shenzhen University, Shenzhen, China","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2021","2021","59","8","6969","6982","The optical images acquired from satellite platforms are commonly multiresolution images, and converting multiresolution satellite images into full higher-resolution (HR) images has been a critical technique for improving the image quality. In this study, we introduced the generative adversarial network (GAN) and proposed a new fusion GAN (FusGAN) approach for solving the remote sensing image fusion problem. Specifically, we developed a new adversarial training strategy: 1) downscaled multiresolution images are adopted for generative network (G-Net) training, and 2) the discriminative network (D-Net) is used to adversarially train the G-Net by discriminating whether the original multiresolution images have been fused well enough. To further improve the capability of the network, we structured our G-Net with residual dense blocks by combining state-of-the-art residual and dense connection ideas. Our proposed FusGAN approach is evaluated both visually and quantitatively on Sentinel-2 and Landsat Operational Land Imager (OLI) multiresolution images. As demonstrated by the results, the proposed FusGAN approach outperforms the selected benchmark methods and both perfectly preserves spectral information and reconstructs spatial information in image fusion. Considering the common resolution disparities among intra- and intersatellite images, the proposed FusGAN approach can contribute to the quality improvement of satellite images and thus improve remote sensing applications.","1558-0644","","10.1109/TGRS.2020.3025821","National Natural Science Foundation of China(grant numbers:41631178); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9212572","Deep learning;generative adversarial networks (GANs);Landsat 8;remote sensing image fusion;residual dense blocks;Sentinel-2","Image fusion;Satellites;Training;Spatial resolution;Remote sensing","geophysical image processing;image fusion;image resolution;optical images;remote sensing","satellite image fusion;generative adversarial training;optical images;satellite platforms;multiresolution satellite images;higher-resolution images;image quality;generative adversarial network;fusion GAN approach;remote sensing image fusion problem;adversarial training strategy;generative network;G-Net;discriminative network;original multiresolution images;residual dense blocks;FusGAN approach;Landsat Operational Land Imager multiresolution images;common resolution disparities;intersatellite images;quality improvement","","7","","74","IEEE","5 Oct 2020","","","IEEE","IEEE Journals"
"Soil Moisture Estimation Using Sentinel-1/-2 Imagery Coupled With CycleGAN for Time-Series Gap Filing","N. Efremova; M. E. A. Seddik; E. Erten","School of Business and Management, University of Queen Mary of London, London, U.K.; Huawei Paris Research Centre, Paris, France; Faculty of Civil Engineering, Istanbul Technical University, Istanbul, Turkey","IEEE Transactions on Geoscience and Remote Sensing","1 Mar 2022","2022","60","","1","11","Fast soil moisture content (SMC) mapping is necessary to support water resource management and to understand crop growth, quality, and yield. Therefore, earth observation (EO) plays a key role due to its ability of almost real-time monitoring of large areas at a low cost. This study aimed to explore the possibility of taking advantage of freely available Sentinel-1 (S1) and Sentinel-2 (S2) EO data for the simultaneous prediction of SMC with cycle-consistent adversarial network (CycleGAN) for time-series gap filling. The proposed methodology, first, learns latent low-dimensional representation of the satellite images, then learns a simple machine learning (ML) model on top of these representations. To evaluate the methodology, a series of vineyards, located in South Australia ’s Eden valley are chosen. Specifically, we presented an efficient framework for extracting latent features from S1 and S2 imagery. We showed how one could use S1 to S2 feature translation based on CycleGAN using S1 and S2 time series when there are missing images acquired over an area of interest. The resulting data in our study is then used to fill gaps in time-series data. We used the resulting latent representations to predict SMC with various ML tools. In the experiments, CycleGAN and the autoencoders were trained with data randomly chosen around the site of interest, so we could augment the existing dataset. The best performance was demonstrated with random forest (RF) algorithm, whereas linear regression model demonstrated significant overfitting. The experiments demonstrate that the proposed methodology outperforms the compared state-of-the-art methods if there are missing optical and synthetic-aperture radar (SAR) images.","1558-0644","","10.1109/TGRS.2021.3134127","Space Research and Innovation Network for Technology (SPRINT)(grant numbers:1243832); Research Fund of the Istanbul Technical University(grant numbers:MGA-2021-43018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9643416","Agriculture;generative adversarial networks (GANs);machine learning (ML);Sentinel-1;Sentinel-2;soil moisture (SM);unsupervised domain adaptation","Soil moisture;Feature extraction;Satellites;Data models;Predictive models;Training;Soil measurements","crops;feature extraction;geophysical image processing;hydrological techniques;image representation;learning (artificial intelligence);moisture;radar imaging;regression analysis;remote sensing by radar;soil;synthetic aperture radar;time series","SMC;cycle-consistent adversarial network;CycleGAN;time-series gap filling;low-dimensional representation;simple machine learning model;extracting latent features;S2 feature translation;S2 time series;time-series data;resulting latent representations;soil moisture estimation;time-series gap filing;fast soil moisture content mapping;water resource management;real-time monitoring","","2","","45","CCBY","10 Dec 2021","","","IEEE","IEEE Journals"
"The Added Value of Cycle-GAN for Agriculture Studies","E. Şener; E. Çolak; E. Erten; G. Taşkin","Department of Geomatics Engineering, Istanbul Technical University, Istanbul, Turkey; Department of Microwave Engineering and Electromagnetic Theory, Chemnitz University of Technology, Chemnitz, Germany; Department of Geomatics Engineering, Istanbul Technical University, Istanbul, Turkey; Istanbul Technical University, Earthquake Engineering and Disaster Management Institute, Istanbul, Turkey","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","7039","7042","It is significant to monitor the phenological stages of agricultural crops with accurate and up-to-date information. In monitoring the phenological phases of some crops, optical remote sensing data offers significant spectral information and outstanding feature identification. However, a continuous time series of optical remote sensing data is difficult to obtain due to the weather dependency of optical acquisitions. In this paper, the feasibility of transfer learning between the features of Sentinel-1 and Sentinel-2 is evaluated to reduce these difficulties. A feature translation based on deep learning (DL) method, namely Cycle-Consistent Generative Adversarial Networks (cycle-GAN), was applied between Sentinel-1 and Sentinel-2 data. In order to evaluate the effect of the cycle-GAN method on crop type mapping and identification, Random Forest classification was applied to four different cases (Real SAR, Fake Optical + Real SAR, Real Optical, and Real Optical + Real SAR).","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553876","Agriculture;Sentinel-1;Sentinel-2;cycle-GAN;Consistent Adversarial Networks;Random Forest Classification","Transfer learning;Time series analysis;Crops;Optical imaging;Optical sensors;Synthetic aperture radar;Remote sensing","","","","","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"SAR-to-Optical Image Translation Using Supervised Cycle-Consistent Adversarial Networks","L. Wang; X. Xu; Y. Yu; R. Yang; R. Gui; Z. Xu; F. Pu","Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan, China; Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan, China; Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan, China; Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan, China; Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan, China; Electrical Engineering Department, Stanford University, Stanford, CA, USA; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China","IEEE Access","19 Sep 2019","2019","7","","129136","129149","Optical remote sensing (RS) data suffer from the limitation of bad weather and cloud contamination, whereas synthetic aperture radar (SAR) can work under all weather conditions and overcome this disadvantage of optical RS data. However, due to the imaging mechanism of SAR and the speckle noise, untrained people are difficult to recognize the land cover types visually from SAR images. Inspired by the excellent image-to-image translation performance of Generative Adversarial Networks (GANs), a supervised Cycle-Consistent Adversarial Network (S-CycleGAN) was proposed to generate large optical images from the SAR images. When the optical RS data are unavailable or partly unavailable, the generated optical images can be alternative data that aid in land cover visual recognition for untrained people. The main steps of SAR-to-optical image translation were as follows. First, the large SAR image was split to small patches. Then S-CycleGAN was used to translate the SAR patches to optical image patches. Finally, the optical image patches were stitched to generate the large optical image. A paired SAR-optical image dataset which covered 32 Chinese cities was published to evaluate the proposed method. The dataset was generated from Sentinel-1 (SEN-1) SAR images and Sentinel-2 (SEN-2) multi-spectral images. S-CycleGAN was applied to two experiments, which were SAR-to-optical image translation and cloud removal, and the results showed that S-CycleGAN could keep both the land cover and structure information well, and its performance was superior to some famous image-to-image translation models.","2169-3536","","10.1109/ACCESS.2019.2939649","National Basic Research Program of China (973 Program)(grant numbers:2016YFB0502600); Thirteen-Five Civil Aerospace Planning Project — Integration of Communication, Navigation and Remote Sensing Comprehensive Application Technology; Chinese Technology Research and Development of the Major Project of High-Resolution Earth Observation System(grant numbers:03-Y20A10-9001-15/16); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825802","SAR-to-optical image translation;visualization;GAN;Sentinel;cloud removal","Radar polarimetry;Optical imaging;Clouds;Optical sensors;Adaptive optics;Optical polarization;Gallium nitride","geophysical image processing;optical images;radar imaging;remote sensing by radar;synthetic aperture radar;terrain mapping","SAR-to-optical image translation;supervised Cycle-Consistent Adversarial Networks;optical remote sensing data;optical RS data;imaging mechanism;generated optical images;SAR patches;optical image patches;paired SAR-optical image dataset;Sentinel-1 SAR images;multispectral images;image-to-image translation models;S-CycleGAN;Chinese cities;land cover;Generative Adversarial Networks;weather conditions;cloud contamination;bad weather","","59","","58","CCBY","5 Sep 2019","","","IEEE","IEEE Journals"
"PLFM: Pixel-Level Merging of Intermediate Feature Maps by Disentangling and Fusing Spatial and Temporal Data for Cloud Removal","A. Sebastianelli; E. Puglisi; M. P. Del Rosso; J. Mifdal; A. Nowakowski; P. P. Mathieu; F. Pirri; S. L. Ullo","Department of Engineering, University of Sannio, Benevento, Italy; Department of Automation Robotics and Control for Aerospace, La Sapienza University, Rome, Italy; Department of Engineering, University of Sannio, Benevento, Italy; Φ-Lab, European Space Agency, Frascati, Italy; Department of Mathematics and Computer Science, Warsaw University of Technology, Warsaw, Poland; Φ-Lab, European Space Agency, Frascati, Italy; Department of Automation Robotics and Control for Aerospace, La Sapienza University, Rome, Italy; Department of Engineering, University of Sannio, Benevento, Italy","IEEE Transactions on Geoscience and Remote Sensing","13 Oct 2022","2022","60","","1","16","Cloud removal is a relevant topic in remote sensing, fostering medium- and high-resolution optical (OPT) image usability for Earth monitoring and study. Recent applications of deep generative models and sequence-to-sequence-based models have proved their capability to advance the field significantly. Nevertheless, there are still some gaps: the amount of cloud coverage, the landscape temporal changes, and the density and thickness of clouds need further investigation. We fill some of these gaps in this work by introducing an innovative deep model. The proposed model is multimodal, relying on both spatial and temporal sources of information to restore the whole optical scene of interest. We use the outcomes of both temporal-sequence blending and direct translation from synthetic aperture radar (SAR) to optical images to obtain a pixel-wise restoration of the whole scene. The reconstructed images preserve scene details without resorting to a considerable portion of a clean image. Our approach’s advantage is demonstrated across various atmospheric conditions tested on different datasets. Quantitative and qualitative results prove that the proposed method obtains cloud-free images coping with landscape changes.","1558-0644","","10.1109/TGRS.2022.3208694","ongoing Open Space Innovation Platform (OSIP) Project “Al Powered Cross-Modal Adaptation Techniques Applied to Sentinel-1 and Sentinel-2 Data,” started in June 2020 under a joint collaboration between the Φ-Lab, European Space Agency, and the University of Sannio, Benevento, Italy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9899477","Cloud removal (CR);conditional generative adversarial networks (cGANs);convolutional long short-term memory (ConvLSTM);deep hierarchical model;multitemporal remote sensing (RS) images;synthetic aperture radar (SAR)-optical (OPT) data fusion","Clouds;Optical imaging;Optical sensors;Image reconstruction;Radar polarimetry;Adaptation models;Image restoration","geophysical image processing;image reconstruction;image resolution;optical images;radar imaging;remote sensing;synthetic aperture radar","PLFM;pixel-level merging;intermediate feature maps;fusing spatial;temporal data;cloud removal;relevant topic;remote sensing;high-resolution optical image usability;deep generative models;sequence-to-sequence-based models;cloud coverage;landscape temporal changes;density;innovative deep model;optical scene;temporal-sequence blending;optical images;pixel-wise restoration;reconstructed images;scene details;clean image;method obtains cloud-free images;landscape changes","","3","","90","IEEE","22 Sep 2022","","","IEEE","IEEE Journals"
"Deep-Learning-Based Spatio-Temporal-Spectral Integrated Fusion of Heterogeneous Remote Sensing Images","M. Jiang; H. Shen; J. Li","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences and the Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2022","2022","60","","1","15","It is a challenging task to integrate the spatial, temporal, and spectral information of multisource remote sensing images, especially in the case of heterogeneous images. To this end, for the first time, this article proposes a heterogeneous integrated framework based on a novel deep residual cycle generative adversarial network (GAN). The proposed network consists of a forward fusion part and a backward degeneration feedback part. The forward part generates the desired fusion result from the various observations; the backward degeneration feedback part considers the imaging degradation process and regenerates the observations inversely from the fusion result. The heterogeneous integrated fusion framework supported by the proposed network can simultaneously merge the complementary spatial, temporal, and spectral information of multisource heterogeneous observations to achieve heterogeneous spatiospectral fusion, spatiotemporal fusion, and heterogeneous spatiotemporal–spectral fusion. Furthermore, the proposed heterogeneous integrated fusion framework can be leveraged to relieve the two bottlenecks of land-cover change and thick cloud cover. Thus, the inapparent and unobserved variation trends of surface features, which are caused by the low-resolution imaging and cloud contamination, can be detected and reconstructed well. Images from many different remote sensing satellites, i.e., Moderate Resolution Imaging Spectroradiometer (MODIS), Landsat 8, Sentinel-1, and Sentinel-2, were utilized in the experiments conducted in this study, and both the qualitative and quantitative evaluations confirmed the effectiveness of the proposed image fusion method.","1558-0644","","10.1109/TGRS.2022.3188998","National Natural Science Foundation of China(grant numbers:42130108,62071341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829539","Deep residual cycle generative adversarial network (GAN);heterogeneous integrated framework;land-cover change;thick cloud cover","Generators;Remote sensing;Spatial resolution;Generative adversarial networks;Feature extraction;Image fusion;Optical sensors","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);remote sensing;sensor fusion;spatiotemporal phenomena","imaging degradation process;heterogeneous integrated fusion framework;complementary spatial information;temporal, information;spectral information;multisource heterogeneous observations;heterogeneous spatiospectral fusion;spatiotemporal fusion;heterogeneous spatiotemporal-spectral fusion;low-resolution imaging;cloud contamination;different remote sensing satellites;Moderate Resolution Imaging Spectroradiometer;image fusion method;spatio-temporal-spectral integrated fusion;heterogeneous remote sensing images;multisource remote sensing images;heterogeneous images;heterogeneous integrated framework;deep residual cycle generative adversarial network;forward fusion part;backward degeneration feedback part;forward part;desired fusion result","","","","54","IEEE","14 Jul 2022","","","IEEE","IEEE Journals"
"Mind the Gap: Generating Imputations for Satellite Data Collections at Myriad Spatiotemporal Scopes","P. Khandelwal; D. Rammer; S. Pallickara; S. L. Pallickara","Department of Computer Science, Colorado State University, Fort Collins, Colorado; Department of Computer Science, Colorado State University, Fort Collins, Colorado; Department of Computer Science, Colorado State University, Fort Collins, Colorado; Department of Computer Science, Colorado State University, Fort Collins, Colorado","2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","2 Aug 2021","2021","","","92","102","Hyperspectral satellite data collections have been successfully leveraged in many domains such as meteorology, agriculture, forestry, and disaster management. There is also a collection of publicly available satellite observation networks. However, gaps in scanning frequencies and inadequate spatial resolutions limit the capabilities of geoscience applications. In this study, we target the temporal sparsity of high-resolution satellite images. In particular, we propose a novel methodology to estimate high-resolution images between scheduled scans. Our model SATnet, falls broadly within the class of Generative Adversarial Networks. SATnet allows us to generate accurate high-resolution, high-frequency satellite data at diverse spatial extents. SATnet achieves this by learning relations between a sequence of high-resolution/low-frequency satellite imageries (from Sentinel-2) and an ancillary satellite image that is high-frequency/low-resolution (from MODIS). Our benchmarks demonstrate that SATnet outperforms existing approaches such as ConvLSTMs, Dynamic Filter Network, and TrajGRU with a PSNR accuracy of 31.82. We trained and deployed SATnet over a distributed storage cluster to support the high-throughput generation of imputed satellite imagery via query evaluations. Our methodology preserves geospatial proximity and facilitates the dynamic construction of satellite imagery at a particular timestamp for arbitrary spatial scopes.","","978-1-7281-9586-5","10.1109/CCGrid51090.2021.00019","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9499692","spatial data;time series analysis;deep learning;high-resolution imaging","Satellites;Query processing;Imaging;Data collection;Spatial databases;Spatiotemporal phenomena;Spatial resolution","forestry;geophysical image processing;geophysical signal processing;geophysical techniques;image classification;image resolution;remote sensing","generating imputations;myriad spatiotemporal scopes;hyperspectral satellite data collections;disaster management;publicly available satellite observation networks;scanning frequencies;inadequate spatial resolutions;high-resolution satellite images;high-resolution images;scheduled scans;model SATnet;Generative Adversarial Networks;accurate high-resolution;high-frequency satellite data;diverse spatial extents;ancillary satellite image;Dynamic Filter Network;imputed satellite imagery;arbitrary spatial scopes","","2","","42","IEEE","2 Aug 2021","","","IEEE","IEEE Conferences"
