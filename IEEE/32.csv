"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"A Full-Resolution Training Framework for Sentinel-2 Image Fusion","M. Ciotola; M. Ragosta; G. Poggi; G. Scarpa","Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione, Università Federico II, Napoli, (I); Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione, Università Federico II, Napoli, (I); Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione, Università Federico II, Napoli, (I); Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione, Università Federico II, Napoli, (I)","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","1260","1263","This work presents a new unsupervised framework for training deep learning models for super-resolution of Sentinel-2 images by fusion of its 10-m and 20-m bands. The proposed scheme avoids the resolution downgrade process needed to generate training data in the supervised case. On the other hand, a proper loss that accounts for cycle-consistency between the network prediction and the input components to be fused is proposed. Despite its unsupervised nature, in our preliminary experiments the proposed scheme has shown promising results in comparison to the supervised approach. Besides, by construction of the proposed loss, the resulting trained network can be ascribed to the class of multi-resolution analysis methods.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553199","Super-resolution;data-fusion;convolutional neural network;machine learning;Sentinel-2","Training;Deep learning;Superresolution;Training data;Geoscience and remote sensing;Image fusion","geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);neural nets","full-resolution training framework;Sentinel-2 image fusion;unsupervised framework;deep learning models;super-resolution;Sentinel-2 images;resolution downgrade process;training data;supervised case;proper loss;cycle-consistency;network prediction;input components;unsupervised nature;supervised approach;resulting trained network;multiresolution analysis methods","","6","","20","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Single Sensor Image Fusion Using A Deep Convolutional Generative Adversarial Network","F. Palsson; J. R. Sveinsson; M. O. Ulfarsson","Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland","2018 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","27 Jun 2019","2018","","","1","5","Recently deployed multispectral sensors can acquire multispectral images where different bands have different spatial resolution depending on wavelength. An example is the Sentinel-2 constellation which can acquire multispectral bands of 10 m, 20 m, and 60 m resolution, covering the visible, near-infrared (NIR) and short-wave infrared (SWIR) parts of the electromagnetic spectrum. In this paper, a method to perform image fusion of the fine and coarse spatial resolution bands to increase the resolution of the coarser bands is proposed. The method is based on a so-called Generative Adversarial Network (GAN) and uses a deep convolutional design for both the generator and the discriminator. In experiments, it is demonstrated that the proposed method gives good results when compared to state-of-the-art single sensor image fusion methods using both simulated and real Sentinel-2 datasets.","2158-6276","978-1-7281-1581-8","10.1109/WHISPERS.2018.8747268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747268","Image fusion;generative adversarial network;convolutional network;Sentinel-2","Image resolution;Image fusion;Generators;Gallium nitride;Training;Generative adversarial networks;Signal resolution","geophysical image processing;geophysical techniques;image fusion;image resolution;image sensors;remote sensing","deep convolutional Generative Adversarial Network;multispectral images;Sentinel-2 constellation;multispectral bands;electromagnetic spectrum;fine resolution bands;coarse spatial resolution bands;deep convolutional design;Sentinel-2 datasets;multispectral sensors;visible band;short-wave infrared band;single sensor image fusion methods;near-infrared band;wavelength 10.0 m;wavelength 60.0 m;wavelength 20.0 m","","7","","16","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"Single Sensor Image Fusion Using a Deep Residual Network","F. Palsson; J. R. Sveinsson; M. O. Ulfarsson","Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland","2018 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","27 Jun 2019","2018","","","1","5","Single sensor fusion is the fusion of two or more spectrally disjoint reflectance bands that have different spatial resolution and have been acquired by the same sensor. An example is the Sentinel-2 constellation which can acquire multispectral bands of 10 m, 20 m and 60 m resolution from the visible to short-wave infrared (SWIR) regions of the electromagnetic spectrum. In this paper, we present a method based on a deep residual convolutional network to fuse the fine and coarse spatial resolution bands to obtain finer spatial resolution versions of the coarse bands. The benefits of the residual design are primarily that the network converges faster and it allows for deeper networks. Also, it improves the spectral consistency of the fused image. Using a real Sentinel-2 dataset, it is demonstrated that the proposed method gives good results when compared to state-of-the-art single sensor image fusion methods.","2158-6276","978-1-7281-1581-8","10.1109/WHISPERS.2018.8747059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747059","Image fusion;deep residual network;convolutional network;Sentinel-2","Training;Spatial resolution;Image fusion;Sensor fusion;MODIS","convolutional neural nets;geophysical image processing;geophysical techniques;image fusion;image resolution;image sensors","deep residual network;single sensor fusion;spectrally disjoint reflectance bands;Sentinel-2 constellation;multispectral bands;short-wave infrared regions;electromagnetic spectrum;deep residual convolutional network;fine resolution bands;coarse spatial resolution bands;finer spatial resolution versions;coarse bands;residual design;spectral consistency;fused image;Sentinel-2 dataset;single sensor image fusion methods","","","","15","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"Multimodal Probabilistic Latent Semantic Analysis for Sentinel-1 and Sentinel-2 Image Fusion","R. Fernandez-Beltran; J. M. Haut; M. E. Paoletti; J. Plaza; A. Plaza; F. Pla","Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain","IEEE Geoscience and Remote Sensing Letters","26 Aug 2018","2018","15","9","1347","1351","Probabilistic topic models have recently shown a great potential in the remote sensing image fusion field, which is particularly helpful in land-cover categorization tasks. This letter first studies the application of probabilistic latent semantic analysis (pLSA) and latent Dirichlet allocation to remote sensing synthetic aperture radar (SAR) and multispectral imaging (MSI) unsupervised land-cover categorization. Then, a novel pLSA-based image fusion approach is presented, which pursues to uncover multimodal feature patterns from SAR and MSI data in order to effectively fuse and categorize Sentinel-1 and Sentinel-2 remotely sensed data. Experiments conducted over two different data sets reveal the advantages of the proposed approach for unsupervised land-cover categorization tasks.","1558-0571","","10.1109/LGRS.2018.2843886","Generalitat Valenciana(grant numbers:APOSTD/2017/007); Ministerio de Educación, Cultura y Deporte(grant numbers:FPU14/02012-FPU15/02090,ESP2016-79503-C2-2-P); Consejería de Educación y Empleo, Junta de Extremadura(grant numbers:GR15005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8392415","Image fusion;land-cover categorization;probabilistic latent semantic analysis (pLSA);Sentinel-1;Sentinel-2","Synthetic aperture radar;Remote sensing;Data models;Image fusion;Semantics;Feature extraction;Probabilistic logic","feature extraction;geophysical image processing;geophysical techniques;image fusion;land cover;remote sensing;synthetic aperture radar","MSI data;SAR data;multispectral imaging unsupervised land-cover categorization;remote sensing synthetic aperture radar;latent Dirichlet allocation;remote sensing image fusion field;probabilistic topic models;Sentinel-2 image fusion;multimodal probabilistic latent semantic analysis;unsupervised land-cover categorization tasks;Sentinel-2 remotely sensed data;Sentinel-1;multimodal feature patterns;novel pLSA-based image fusion approach","","28","","18","IEEE","21 Jun 2018","","","IEEE","IEEE Journals"
"Deepsen3: Deep Multi-Scale Learning Model For Spatial-Spectral Fusion Of Sentinel-2 And Sentinel-3 Remote Sensing Images","A. Alboody; M. Puigt; G. Roussel; V. Vantrepotte; C. Jamet; T. -K. Tran","LISIC – UR 4491, Univ. Littoral Côte d’Opale, Longuenesse, France; LISIC – UR 4491, Univ. Littoral Côte d’Opale, Longuenesse, France; LISIC – UR 4491, Univ. Littoral Côte d’Opale, Longuenesse, France; CNRS, LOG – UMR 8187, Univ. Littoral Côte d’Opale, Wimereux, France; CNRS, LOG – UMR 8187, Univ. Littoral Côte d’Opale, Wimereux, France; CNRS, LOG – UMR 8187, Univ. Littoral Côte d’Opale, Wimereux, France","2022 12th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","22 Nov 2022","2022","","","1","5","Recently, deep learning methods that integrate image features gradually became a hot development trend in fusion of multispectral and hyperspectral remote sensing images, aka multi-sharpening. Fusion of a low spatial resolution hyperspectral image (LR-HSI datacube) with its corresponding high spatial resolution multispectral image (HR-MSI datacube) to reconstruct a high spatial resolution hyperspectral image (HR-HSI) has been a significant subject in recent years. Nevertheless, it is still difficult to achieve a high quality of spatial and spectral information fusion. In this paper, we propose a Deep Multi-Scale Learning Model (called DeepSen3) of spatial-spectral information fusion based on multi-scale inception residual convolutional neural network (CNN) for more efficient hyperspectral and multispectral image fusion from ESA remote sensing satellite missions (Sentinel-2 and Sentinel-3 images). The proposed DeepSen3 fusion network was applied to Sentinel-2 MSI (13 spectral bands with a spatial resolution ranging from 10, 20 to 60 m) and Sentinel-3 OLCI (21 spectral bands with a spatial resolution of 300 m) images. Extensive experiments demonstrate that the proposed DeepSen3 network achieves the best performance (both qualitatively and quantitatively) compared with recent state-of-the-art deep learning approaches.","2158-6276","978-1-6654-7069-8","10.1109/WHISPERS56178.2022.9955139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955139","Deep Learning;Residual Convolutional Neural Network (ResNet-CNN);Multi-Scale Inception;Feature Extraction;Spatial-Spectral Image Fusion;Sentinel-2 and Sentinel-3 Remote Sensing Images;HyperSpectral Images (HSI);Multi-Spectral Images (MSI)","Deep learning;Satellites;Signal processing;Market research;Distance measurement;Convolutional neural networks;Spatial resolution","artificial satellites;convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;image fusion;remote sensing","deep multiscale learning model;DeepSen3 fusion network;ESA remote sensing satellite missions;high spatial resolution hyperspectral image;low spatial resolution hyperspectral image fusion;multiscale inception residual convolutional neural network;multisharpening;multispectral remote sensing image fusion;Sentinel-2 MSI images;Sentinel-3 OLCI images;Sentinel-3 remote sensing images;spatial-spectral information fusion","","","","20","IEEE","22 Nov 2022","","","IEEE","IEEE Conferences"
"Sentinel-1 and Sentinel-2 Data Fusion for Urban Change Detection","A. Benedetti; M. Picchiani; F. Del Frate","University of Tor Vergata, Rome, Italy; University of Tor Vergata, Rome, Italy; University of Tor Vergata, Rome, Italy","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1962","1965","In this paper a new approach based on the fusion of Sentinel-1 and Sentinel-2 products to map urban change detection and to observe suburb's development is presented. The algorithm developed can process data in a fast, automatic and accurate way. To reach this goal, the processing chain uses an iterative multitemporal approach based, for each iteration, on three procedures. The first and second ones are based on Pulse Coupled Neural Network (PCNN) applied to SAR and optical images, respectively, while the third processing is an optical multiband filter, implementing the spectral difference computation. The three outputs of each iteration are fused together by means of a weighted average formulation. The algorithm may deal with multitemporal acquisitions to improve the overall accuracy in the detection of urban changes by the integration of the outputs at different time intervals.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517586","Sentinel-1;Sentinel-2;image fusion;change detection;global monitoring urbanization","Optical filters;Optical imaging;Synthetic aperture radar;Change detection algorithms;Remote sensing;Data integration;Adaptive optics","geophysical image processing;geophysical techniques;image fusion;neural nets;remote sensing by radar;synthetic aperture radar;terrain mapping","Sentinel-1;Sentinel-2 data fusion;Sentinel-2 products;map urban change detection;suburb;processing chain;iterative multitemporal approach;Pulse Coupled Neural Network;optical images;optical multiband filter;urban changes","","13","","13","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Inter-Sensor Remote Sensing Image Enhancement for Operational Sentinel-2 and Sentinel-3 Data Products","R. Fernandez; R. Fernandez-Beltran; F. Pla","Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1504","1507","The recent availability of operational data from the Sentinel-2 and Sentinel-3 missions provides widespread opportunities to generate diverse high-level remote sensing products. However, the synergies between both multi-spectral instruments are often difficult to exploit from an operational perspective. Standard pansharpening algorithms may encounter important disadvantages due to the limited intersensor data availability in actual production environments. Moreover, the lack of a real high-resolution ground-truth for super-resolution techniques may affect the radiometric quality of the final result. In this scenario, this work investigates the viability of using the Multi-Spectral Instrument of Sentinel-2 for super-resolving data products acquired by the Ocean and Land Colour Instrument of Sentinel-3. Specifically, we define an inter-sensor image enhancement framework which combines a PCA-based component substitution pansharpening scheme with a CNN-based spatial enhancing super-resolution mapping. The conducted experiments reveal the suitability of the proposed approach for generating Level-4 data products within the Copernicus programme context.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324071","Sentinel-2 (S2);Sentinel-3 (S3);super-resolution (SR);pansharpening;image fusion","Spatial resolution;Pansharpening;Principal component analysis;Instruments;Remote sensing;Image enhancement;Superresolution","geophysical image processing;image enhancement;image fusion;image resolution;remote sensing;terrain mapping","inter-sensor remote sensing image enhancement;operational Sentinel-2;Sentinel-3 data products;recent availability;operational data;Sentinel-3 missions;diverse high-level remote sensing products;multispectral instruments;operational perspective;standard pansharpening algorithms;intersensor data availability;actual production environments;high-resolution ground-truth;super-resolution techniques;MultiSpectral Instrument;inter-sensor image enhancement framework;super-resolution mapping;Level-4 data products","","","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Deep SURE for Unsupervised Remote Sensing Image Fusion","H. V. Nguyen; M. O. Ulfarsson; J. R. Sveinsson; M. Dalla Mura","Department of Electrical and Electronic Engineering, Nha Trang University, Nha Trang, Vietnam; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavík, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavík, Iceland; Institut Universitaire de France (IUF), Paris, France","IEEE Transactions on Geoscience and Remote Sensing","8 Nov 2022","2022","60","","1","13","Image fusion is utilized in remote sensing (RS) due to the limitation of the imaging sensor and the high cost of simultaneously acquiring high spatial and spectral resolution images. Optical RS imaging systems usually provide images of high spatial resolution but low spectral resolution and vice versa. Therefore, fusing those images to obtain a fused image having both high spectral and spatial resolution is desirable in many applications. This article proposes a fusion framework using an unsupervised convolutional neural network (CNN) and Stein’s unbiased risk estimate (SURE). We derive a new loss function for a CNN that incorporates a backprojection mean square error (MSE) with SURE to estimate the projected mse between the fused image and the ground truth. The main motivation is that training a CNN with this SURE loss function is unsupervised and avoids overfitting. Experimental results for two fusion examples, multispectral and hyperspectral (MS–HS) image fusion and multispectral and multispectral (MS–MS) image fusion, show that the proposed method yields high-quality fused images and outperforms the competitive methods. Codes are available at https://github.com/hvn2/Deep-SURE-Fusion.","1558-0644","","10.1109/TGRS.2022.3215902","Icelandic Research Fund(grant numbers:207233-051); University of Iceland Doctoral Fund(grant numbers:1547-154305); ANR FuMultiSPOC(grant numbers:ANR-20-ASTR-0006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9924190","Image fusion;multispectral and hyperspectral (MS–HS) image fusion;pansharpening;remote sensing (RS);Sentinel 2 sharpening;Stein’s unbiased risk estimate (SURE);unsupervised convolutional neural networks (CNNs)","Spatial resolution;Image fusion;Convolutional neural networks;Electronics packaging;Training;Pansharpening;Optical sensors","convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;hyperspectral imaging;image fusion;image resolution;mean square error methods;remote sensing;unsupervised learning","CNN;loss function;high-quality fused images;imaging sensor;spatial resolution images;spectral resolution images;unsupervised convolutional neural network;Stein unbiased risk estimate;deep SURE;unsupervised remote sensing image fusion;optical RS imaging systems;backprojection mean square error;MSE;multispectral image fusion;hyperspectral image fusion","","","","75","IEEE","19 Oct 2022","","","IEEE","IEEE Journals"
"Fusion of Sentinel-2 Data with High Resolution Open Access Planet Basemaps for Grazing Lawn Detection in Southern African Savannahs","K. T. Awuah; P. Aplin","Department of Geography and Geology, Edge Hill University, Ormskirk, UK; Department of Geography and Geology, Edge Hill University, Ormskirk, UK","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","1409","1412","Short grass grazing lawn patches are significant components of habitat heterogeneity in southern African savannah ecosystems. Accurate maps of grazing lawn distribution is essential to enhance understanding of important ecosystem processes such as mega-herbivore population dynamics, nutrient cycling and plant community composition. The inherent heterogeneity of savannah landscapes however creates significant challenges for accurate discrimination of vegetation components and thus grazing lawn detection. Recent studies favour very high spatial resolution (VHR) multi-spectral imagery for dealing with this challenge. However, such data are costly for use in operational management. Planet Labs, through Norway's International Climate and Forests Initiative (NICFI), now grant free access to high-resolution, analysis-ready mosaics over the tropics, with great potential for fine-scale vegetation mapping. However, the spectral characteristics of these data are limited and fail to resolve the spectral similarity of different savannah vegetation components. We address these issues using Gram-Schmidt transformation to fuse Planet Basemaps and Sentinel-2A images for grazing lawn detection within the Lower Sabie region of Kruger National Park, South Africa. The original and fused images were classified using a random forest approach. Overall, the fused image achieved the best grazing lawn detection accuracy (0.85) and general map accuracy (0.72) results compared to Sentinel-2 (0.67 and 0.62) and Planet basemap (0.64 and 0.62 respectively). Our findings provide a foundation for cost-effective and accurate high spatial resolution vegetation mapping in heterogenous savannah landscapes. Further studies will investigate the potential of multi-temporal fused data and object-based approaches for enhanced savannah vegetation mapping","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554156","Edge Hill University; Royal Geographical Society; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554156","Image fusion;Sentinel-2;Planet basemaps;Random forest;savannah;grazing lawn","Planets;Open Access;Fuses;Ecosystems;Sociology;Vegetation mapping;Forestry","ecology;geophysical image processing;image fusion;image resolution;vegetation;vegetation mapping","Sentinel-2 data;high resolution open access Planet Basemaps;southern African savannahs;lawn patches;southern African savannah ecosystems;grazing lawn distribution;ecosystem processes;nutrient cycling;plant community composition;high spatial resolution multispectral imagery;fine-scale vegetation mapping;grazing lawn detection accuracy;Planet basemap;heterogenous savannah landscapes;multitemporal fused data;enhanced savannah vegetation mapping;high spatial resolution vegetation mapping;savannah vegetation components;International Climate and Forests Initiative;Kruger National Park","","1","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Zero-Shot Sentinel-2 Sharpening Using a Symmetric Skipped Connection Convolutional Neural Network","H. V. Nguyen; M. O. Ulfarsson; J. R. Sveinsson; J. Sigurdsson","Faculty of Electrical and Computer Engineering, University of Iceland; Faculty of Electrical and Computer Engineering, University of Iceland; Faculty of Electrical and Computer Engineering, University of Iceland; Faculty of Electrical and Computer Engineering, University of Iceland","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","613","616","Sentinel-2 (S2) satellite constellations can provide multispectral images of 10 m, 20 m, and 60 m resolution for visible, near-infrared (NIR) and short-wave infrared (SWIR) in the electromagnetic spectrum. In this paper, we present a sharpening method based on a symmetric skipped connection convolutional neural network, called SSC-CNN, to sharpen 20 m bands using 10 m bands. The main advantage of SSC-CNN architecture is that it brings the features of the input branch to the output, thus improving convergence without using too many deep layers. The proposed method uses the reduced-scale combination of 10 m bands and 20 m bands, and the observed 20 m bands as the training pairs. The experimental results using two Sentinel-2 datasets show that our method outperforms competitive methods in quantitative metrics and visualization.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323614","Sentinel-2;image fusion;image sharpening;super resolution;convolutional neural network","Image resolution;Spatial resolution;Training;Sea measurements;Remote sensing;Pansharpening;Measurement","convolutional neural nets;image classification;image resolution;image sensors;learning (artificial intelligence)","SSC-CNN architecture;Sentinel-2 datasets;zero-shot Sentinel-2 sharpening;symmetric skipped connection convolutional neural network;Sentinel-2 satellite constellations;near-infrared;sharpening method;size 20.0 m;size 60.0 m;size 10.0 m","","5","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Fusing Multiseasonal Sentinel-2 Imagery for Urban Land Cover Classification With Multibranch Residual Convolutional Neural Networks","C. Qiu; L. Mou; M. Schmitt; X. X. Zhu","Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, German; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, German; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, German; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Weßling, Germany","IEEE Geoscience and Remote Sensing Letters","24 Sep 2020","2020","17","10","1787","1791","Exploiting multitemporal Sentinel-2 images for urban land cover classification has become an important research topic, since these images have become globally available at relatively fine temporal resolution, thus offering great potential for large-scale land cover mapping. However, appropriate exploitation of the images needs to address problems such as cloud cover inherent to optical satellite imagery. To this end, we propose a simple yet effective decision-level fusion approach for urban land cover prediction from multiseasonal Sentinel-2 images, using the state-of-the-art residual convolutional neural networks (ResNet). We extensively tested the approach in a cross-validation manner over a seven-city study area in central Europe. Both quantitative and qualitative results demonstrated the superior performance of the proposed fusion approach over several baseline approaches, including observation- and feature-level fusion.","1558-0571","","10.1109/LGRS.2019.2953497","China Scholarship Council (CSC); European Research Council (ERC) under the European Union’s Horizon 2020 Research and Innovation Program (So2Sat: Big Data for 4D Global Urban Mapping–1016 Bytes from Social Media to EO Satellites)(grant numbers:ERC-2016-StG-714087); Helmholtz Association under the Framework of the Young Investigators Group Signal Processing in Earth Observation (SiPEO)(grant numbers:VH-NG-1018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951229","Classification;fusion;long short-term memory (LSTM);multitemporal;nonlocal;residual convolutional neural network (ResNet);Sentinel-2;urban land cover","Urban areas;Earth;Convolutional neural networks;Satellites;Europe;Remote sensing;Task analysis","geophysical image processing;geophysical techniques;image classification;image fusion;land cover;neural nets;terrain mapping","multiseasonal Sentinel-2 imagery;urban land cover classification;multibranch residual convolutional neural networks;multitemporal Sentinel-2 images;relatively fine temporal resolution;large-scale land cover mapping;cloud cover;optical satellite imagery;urban land cover prediction;multiseasonal Sentinel-2 images;state-of-the-art residual convolutional neural networks;effective decision-level fusion approach;feature-level fusion","","15","","15","CCBY","7 Jan 2020","","","IEEE","IEEE Journals"
"The Effect of SAR Speckle Removal in SAR-Optical Image Fusion","S. Gençay; C. Özcan","Bilgisayar Teknolojileri, Manisa Celal Bayar Üniversitesi, Manisa, Türkiye; Yazılım Mühendisliği, Karabük Üniversitesi, Karabük, Türkiye","2022 30th Signal Processing and Communications Applications Conference (SIU)","29 Aug 2022","2022","","","1","4","Due to the imaging mechanism of Synthetic Aperture Radar (SAR) and the noise in the images, visual identification of objects in the scene is not as easy as in optical images. SAR images have limited color information and cannot reflect the spectral information of objects. Optical images, on the other hand, have rich spectral information. SAR-Optical image fusion is an important area of study so that SAR data can be easily evaluated by anyone, but it is difficult to find a matching SAR and optical image of the same scene. In order to overcome this difficulty, Sentinel-1 and Sentinel-2 datasets have been published and image fusion studies have been carried out with various methods. However, it has been observed that the effect of SAR noise removal before merging on image fusion methods has not been investigated. In the studies conducted to investigate this effect, five different fusion algorithms used in the literature were tested with twenty different image groups using different noise reduction ratios. The success of the fusion results obtained was compared with five different metrics that are widely used in the literature. The images and metric results obtained as a result of the tests showed that the removal of speckle noise in the SAR data has a positive effect on the fusion results.","2165-0608","978-1-6654-5092-8","10.1109/SIU55565.2022.9864861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864861","optical image;SAR image;image fusion;remote sensing","Measurement;Visualization;Speckle;Optical imaging;Adaptive optics;Optical sensors;Optical reflection","image fusion;optical images;radar imaging;speckle;synthetic aperture radar","SAR speckle removal;SAR-Optical image fusion;imaging mechanism;SAR images;Optical images;rich spectral information;SAR data;matching SAR;image fusion studies;SAR noise removal;image fusion methods;different fusion algorithms;different image groups;fusion results","","","","0","IEEE","29 Aug 2022","","","IEEE","IEEE Conferences"
"Mangrove Species Mapping and Above-Ground Biomass Estimation in Suriname Based on Fused Sentinel-1 and Sentinel-2 Imagery and National Forest Inventory Data","J. Feyen; G. Wip; S. Crabbe; V. Wortel; S. P. Sari; F. Van Coillie","Department of Environment, Remote Sensing | Spatial Analysis Lab (REMOSA), Ghent University, Ghent, Belgium; Foundation for Forest Management and Production Control, Paramaribo, Suriname; Foundation for Forest Management and Production Control, Paramaribo, Suriname; Department of Forest Management, Centre for Agricultural Research in Suriname (CELOS), Paramaribo, Suriname; Department of Environment, Remote Sensing | Spatial Analysis Lab (REMOSA), Ghent University, Ghent, Belgium; Department of Environment, Remote Sensing | Spatial Analysis Lab (REMOSA), Ghent University, Ghent, Belgium","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","6072","6075","Obtaining state-of-the art data on the Mangrove cover extent is important to monitor possible responses to environmental changes such as land use change and mangrove ecosystem degradation caused by climate change. In this study, we examined the possibility of species-specific mapping within the mangrove area in Suriname based on the fusion of Sentinel-1 and Sentinel-2 data using the Google Earth Engine platform and a Random Forest classifier. To do this, a 2-level classification scheme was developed. In the first level, the mangrove cover was discriminated from mangrove graveyards and other land cover classes (kappa index of 80.65%). In the second level, the dominating mangrove species were successfully classified within the living mangrove cover (kappa index of 75.21 %). Secondly mangrove above-ground biomass (AGB) was estimated on a national scale, based on fused Sentinel-1 and Sentinel- 2 data and national mangrove forest inventory data by using a Support Vector Regression (SVR) machine learning technique, resulting in a root mean square error (RMSE) of 32.181 Mg.ha−1 and a R2 of 0.542.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555037","European Union; UNDP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555037","mangroves;species mapping;above-ground biomass;sentinel-1;sentinel- 2;support vector regression","Support vector machines;Geoscience and remote sensing;Estimation;Forestry;Biomass;Climate change;Root mean square","forestry;geophysical image processing;image classification;image fusion;land cover;random forests;regression analysis;support vector machines;vegetation;vegetation mapping","land use change;climate change;species-specific mapping;Sentinel-2 data;Google Earth Engine;random forest classifier;mangrove graveyards;land cover classes;mangrove species;national mangrove forest inventory data;above-ground biomass estimation;Sentinel-2 imagery;Sentinel-1 imagery;support vector regression;Suriname","","1","","15","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Exploring the Fusion of Sentinel-1 SAR and Sentinel-2 MSI Data for Built-Up Area Mapping Using Deep Learning","S. Hafner; Y. Ban; A. Nascetti","Division of Geoinformatics, KTH Royal Institute of Technology; Division of Geoinformatics, KTH Royal Institute of Technology; Division of Geoinformatics, KTH Royal Institute of Technology","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4720","4723","This research explores the potential of combining Sentinel-1 C-band Synthetic Aperture Radar (SAR) and Sentinel-2 MultiSpectral Instrument (MSI) data for Built-Up Area (BUA) mapping using deep learning. A lightweight U-Net model is trained using openly available building footprint reference data in North America and tested in four cities across three additional continents. The best test performance in terms of F1 score was achieved by the joint use of SAR and multispectral data (0.676), followed by multi-spectral (0.611) and SAR data (0.601). The developed fusion approach is particularly promising to distinguish BUA in low-density residential neighborhoods. Furthermore, our fusion approach compares favorably to the state-of-the-art in BUA mapping in the selected cities. However, associated with the diverse characteristics of human settlements around the world, considerable differences in accuracy among the test cities were observed. This indicates the need for more sophisticated fusion techniques to improve CNN model generalization and for adding more diverse training data.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553448","Swedish National Space Agency; ESA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553448","Sentinel-1;Sentinel-2;Built-up area mapping;data fusion;deep learning","Deep learning;Instruments;Urban areas;Training data;Optical imaging;Data models;Internet","convolutional neural nets;geophysical image processing;image fusion;learning (artificial intelligence);radar imaging;remote sensing by radar;synthetic aperture radar","Sentinel-1 SAR;Sentinel-2 MSI data;Built-Up Area mapping;deep learning;SAR data;BUA mapping;fusion techniques;diverse training data;building footprint reference data;Sentinel-1 C-band synthetic aperture radar;lightweight U-Net model;Sentinel-2 multispectral instrument data;F1 score","","1","","8","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Optical SAR Fusion of Sentinel-2 Images for Mapping High Resolution Land Cover","Yuhendra; E. Yulianti; J. Na'am","Informatics Engineering Department, Padang Institute of Technology, Padang-West Sumatera, Indonesia; Informatics Engineering Department, Padang Institute of Technology, Padang-West Sumatera, Indonesia; Informatics Engineering Department, Padang Institute of Technology, Padang-West Sumatera, Indonesia","2018 International Conference on System Science and Engineering (ICSSE)","4 Nov 2018","2018","","","1","4","Sentinel-2 is a very new programme of the European Space Agency (ESA) that is designed for fine spatial resolution global monitoring. Land cover-land use (LCLU) classification tasks can take advantage of the fusion of radar and optical remote sensing data, leading generally to increase mapping accuracy. Here we propose a methodological approach to fuse information from the new European Space Agency Sentinel-1 and Sentinel-2 imagery for accurate land cover mapping of a portion of the South Solok region, West Sumatera. Data pre-processing was carried out using the European Space Agency's Sentinel Application Platform and the SEN2COR toolboxes. The two main objectives of this study are to evaluate the potential use and synergetic effects of ESA Sentinel-1A C-band SAR and Sentinel-2A Optical data for classification and mapping of LCLU. As a result of the research, two main advantages. First, the pre-processing chain supported by sensor-specific toolboxes developed by ESA represents a reliable and fast approach for the preparation of ready-to-process imagery. Second, investigation to derive a methodological framework to integrate Sentinel-1 and Sentinel-2 imagery for land cover mapping by integrating of radar and optical imagery have been set up and tested.","2325-0925","978-1-5386-6285-4","10.1109/ICSSE.2018.8520099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8520099","Sentinel-1;Sentinel-2;SAR;land cover mapping;data fusion;segmentation;South Solok","Remote sensing;Optical imaging;Spatial resolution;Laser radar;Image segmentation;Optical sensors;Earth","geophysical image processing;geophysical techniques;image classification;image fusion;land cover;radar imaging;remote sensing by radar;synthetic aperture radar;terrain mapping","land cover mapping;South Solok region;West Sumatera;optical imagery;ready-to-process imagery;pre-processing chain;Optical data;Sentinel-2A;ESA Sentinel-1A;SEN2COR toolboxes;European Space Agency's Sentinel Application Platform;data pre-processing;Sentinel-2 imagery;European Space Agency Sentinel-1;mapping accuracy;optical remote sensing data;land cover-land use classification tasks;fine spatial resolution global monitoring;mapping high resolution land cover;Optical SAR fusion","","","","6","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Tuning Parameter Selection for Sentinel-2 Sharpening Using Wald's Protocol","S. E. Armannsson; J. Sigurdsson; J. R. Sveinsson; M. O. Ulfarsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2871","2874","In recent years numerous model-based methods for super-resolution of Sentinel-2 (S2) multispectral images have been suggested. Super-resolution aims to enhance the resolution of a captured image by upscaling and enhancing the details. The performance of model-based methods relies on carefully selecting regularizers and tuning parameters. This paper investigates whether using Wald's protocol, i.e., selecting tuning parameters at reduced-resolution, translates to a good performance at a full-scale. To investigate this, we use the recently proposed S2Sharp method and show that selecting its tuning parameters using Wald's protocol improves its performance.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553346","University of Iceland Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553346","Image fusion;image sharpening;multispectral (MS) multiresolution images;parameter selection;scale invariance;Sentinel-2 constellation;super-resolution","Measurement;Image quality;Protocols;Parameter estimation;Superresolution;Geoscience and remote sensing;Bayes methods","geophysical image processing;image enhancement;image resolution","Wald's protocol;Sentinel-2 multispectral images;super-resolution;captured image;reduced-resolution;S2Sharp;parameter selection tuning;Sentinel-2 sharpening;image upscaling;image enhancement;model-based methods","","1","","13","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Sharpening the 20 M Bands of SENTINEL-2 Image Using an Unsupervised Convolutional Neural Network","H. V. Nguyen; M. O. Ulfarsson; J. R. Sveinsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2875","2878","This paper proposes a novel method for sharpening the 20 m bands of the multispectral images acquired by the Sentinel-2 (S2) constellation. We formulate the S2 sharpening as an inverse problem and solve it using an unsupervised convolutional neural network (CNN), called S2UCNN. The proposed method extends the deep image prior provided by a CNN structure with S2 domain knowledge. We incorporate a modulation transfer function-based degradation model as a network layer. We add the 10 m bands to both the network input and output to take advantage of the multitask learning. Experimental results with a real S2 dataset show that the proposed method outperforms the competitive methods on reduced-resolution data and gives very high quality sharpened image on full-resolution data.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555082","Remote sensing;Sentinel-2;image fusion;sharpening;super-resolution;unsupervised convolutional neural network","Degradation;Inverse problems;Modulation;Convolutional neural networks;Remote sensing;Electronics packaging","convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;image resolution;inverse problems;optical transfer function;unsupervised learning","unsupervised convolutional neural network;multispectral images;Sentinel-2 constellation;deep image;CNN structure;S2 domain knowledge;network layer;high quality sharpened image;modulation transfer function-based degradation;S2UCNN;Sentinel-2 image sharpening;S2 sharpening;inverse problem;multitask learning;S2 dataset;full-resolution data","","","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Improving Satellite Image Fusion via Generative Adversarial Training","X. Luo; X. Tong; Z. Hu","College of Life Sciences and Oceanography, Shenzhen University, Shenzhen, China; College of Surveying and Geo-informatics, Tongji University, Shanghai, China; MNR Key Laboratory for Geo-Environmental Monitoring of Great Bay Area & Guangdong Key Laboratory of Urban Informatics & Shenzhen Key Laboratory of Spatial Smart Sensing and Services, Shenzhen University, Shenzhen, China","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2021","2021","59","8","6969","6982","The optical images acquired from satellite platforms are commonly multiresolution images, and converting multiresolution satellite images into full higher-resolution (HR) images has been a critical technique for improving the image quality. In this study, we introduced the generative adversarial network (GAN) and proposed a new fusion GAN (FusGAN) approach for solving the remote sensing image fusion problem. Specifically, we developed a new adversarial training strategy: 1) downscaled multiresolution images are adopted for generative network (G-Net) training, and 2) the discriminative network (D-Net) is used to adversarially train the G-Net by discriminating whether the original multiresolution images have been fused well enough. To further improve the capability of the network, we structured our G-Net with residual dense blocks by combining state-of-the-art residual and dense connection ideas. Our proposed FusGAN approach is evaluated both visually and quantitatively on Sentinel-2 and Landsat Operational Land Imager (OLI) multiresolution images. As demonstrated by the results, the proposed FusGAN approach outperforms the selected benchmark methods and both perfectly preserves spectral information and reconstructs spatial information in image fusion. Considering the common resolution disparities among intra- and intersatellite images, the proposed FusGAN approach can contribute to the quality improvement of satellite images and thus improve remote sensing applications.","1558-0644","","10.1109/TGRS.2020.3025821","National Natural Science Foundation of China(grant numbers:41631178); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9212572","Deep learning;generative adversarial networks (GANs);Landsat 8;remote sensing image fusion;residual dense blocks;Sentinel-2","Image fusion;Satellites;Training;Spatial resolution;Remote sensing","geophysical image processing;image fusion;image resolution;optical images;remote sensing","satellite image fusion;generative adversarial training;optical images;satellite platforms;multiresolution satellite images;higher-resolution images;image quality;generative adversarial network;fusion GAN approach;remote sensing image fusion problem;adversarial training strategy;generative network;G-Net;discriminative network;original multiresolution images;residual dense blocks;FusGAN approach;Landsat Operational Land Imager multiresolution images;common resolution disparities;intersatellite images;quality improvement","","7","","74","IEEE","5 Oct 2020","","","IEEE","IEEE Journals"
"Fusion of Landsat 8 OLI and Sentinel-2 MSI Data","Q. Wang; G. A. Blackburn; A. O. Onojeghuo; J. Dash; L. Zhou; Y. Zhang; P. M. Atkinson","Lancaster Environment Centre, Lancaster University, Lancaster, U.K.; Lancaster Environment Centre, Lancaster University, Lancaster, U.K.; Department of Surveying and Geoinformatics, Nnamdi Azikiwe University, Awka, PMB, Nigeria; Geography and Environment, University of Southampton, Highfield, Southampton, U.K.; Geography and Environment, University of Southampton, Highfield, Southampton, U.K.; University of Chinese Academy of Sciences, Beijing, China; Faculty of Science and Technology, Lancaster University, Lancaster, U.K.","IEEE Transactions on Geoscience and Remote Sensing","22 Jun 2017","2017","55","7","3885","3899","Sentinel-2 is a wide-swath and fine spatial resolution satellite imaging mission designed for data continuity and enhancement of the Landsat and other missions. The Sentinel-2 data are freely available at the global scale, and have similar wavelengths and the same geographic coordinate system as the Landsat data, which provides an excellent opportunity to fuse these two types of satellite sensor data together. In this paper, a new approach is presented for the fusion of Landsat 8 Operational Land Imager and Sentinel-2 Multispectral Imager data to coordinate their spatial resolutions for continuous global monitoring. The 30 m spatial resolution Landsat 8 bands are downscaled to 10 m using available 10 m Sentinel-2 bands. To account for the land-cover/land-use (LCLU) changes that may have occurred between the Landsat 8 and Sentinel-2 images, the Landsat 8 panchromatic (PAN) band was also incorporated in the fusion process. The experimental results showed that the proposed approach is effective for fusing Landsat 8 with Sentinel-2 data, and the use of the PAN band can decrease the errors introduced by LCLU changes. By fusion of Landsat 8 and Sentinel-2 data, more frequent observations can be produced for continuous monitoring (this is particularly valuable for areas that can be covered easily by clouds, thereby, contaminating some Landsat or Sentinel-2 observations), and the observations are at a consistent fine spatial resolution of 10 m. The products have great potential for timely monitoring of rapid changes.","1558-0644","","10.1109/TGRS.2017.2683444","UK-China STFC Newton Agri-Tech Program “Remote Sensing for Sustainable Intensification in China through Improved Farm Decision-Making.”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7894218","Downscaling;global monitoring;image fusion;Landsat 8 Operational Land Imager (OLI);Sentinel-2 Multispectral Imager (MSI)","Satellites;Remote sensing;Earth;Spatial resolution;Monitoring;Image fusion;MODIS","land cover;land use;remote sensing by radar","Landsat 8 OLI;Sentinel-2 MSI Data;fine spatial resolution satellite imaging mission;geographic coordinate system;Landsat 8 Operational Land Imager;Sentinel-2 Multispectral Imager data;land-cover/land-use;Landsat 8 panchromatic","","101","","50","IEEE","7 Apr 2017","","","IEEE","IEEE Journals"
"Fusing Multi-Seasonal Sentinel-2 Images with Residual Convolutional Neural Networks for Local Climate Zone-Derived Urban Land Cover Classification","C. Qiu; M. Schmitt; X. X. Zhu","Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","5037","5040","This paper proposes a framework to fuse multi-seasonal Sentinel-2 images, with application on LCZ-derived urban land cover classification. Cross-validation over a seven-city study area in central Europe demonstrates its consistently better performance over several previous approaches, with the same experimental setup. Based on our previous work, we can conclude that decision-level fusion is better than feature-level fusion for similar tasks at similar scale with multi-seasonal Sentinel-2 images. With the framework, urban land cover maps of several cities are produced. The visualization of two exemplary areas shows urban structures that are consistent with existing datasets. This framework can be also generally beneficial for other types of urban mapping.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898223","Sentinel-2;Classification;residual convolutional neural network (ResNet);urban land cover;long short-term memory (LSTM)","Meteorology;Earth;Urban areas;Google;Remote sensing;Europe;Meters","geophysical image processing;image classification;image fusion;land cover;neural nets;terrain mapping","local climate zone-derived urban land cover classification;multiseasonal Sentinel-2 images;LCZ-derived urban land cover classification;urban land cover maps;urban structures;residual convolutional neural networks;central Europe;decision-level fusion","","4","","11","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"A CNN-Based Fusion Method for Super-Resolution of Sentinel-2 Data","M. Gargiulo; A. Mazza; R. Gaetano; G. Ruello; G. Scarpa","DIETI, University Federico II, Naples, Italy; DIETI, University Federico II, Naples, Italy; CIRAD, UMR-TETIS Laboratory, Montpellier, France; DIETI, University Federico II, Naples, Italy; DIETI, University Federico II, Naples, Italy","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","4713","4716","Sentinel-2 data represent a rich source of information for the community due to the free access and to the temporal-spatial coverage assured. However, some of the spectral bands are sensed at reduced resolution due to a compromise between technological limitations and Copernicus program's objectives. For this reason in this work we present a new super-resolution method based on Convolutional Neural Networks (CNNs) to rise the resolution of the short wave infra-red (SWIR) band from 20 to 10 meters, that is the highest resolution provided. This is accomplished by fusing the target band with the finer-resolution ones. The proposed solution compares favourably against several alternative methods according to different quality indexes. In addition we have also tested the use of the super-resolved band from an applicative perspective by detecting water basins through the Modified Normalized Difference Water Index (MNDWI).","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518447","Deep learning;convolutional neural network;normalized difference water index;Sentinel-2;pansharpening","Spatial resolution;Training;Indexes;Convolutional neural networks;Meters","geophysical image processing;image fusion;image resolution;neural nets;remote sensing;water resources","short wave infrared band;Sentinel-2 data;water basins;Modified Normalized Difference Water Index;MNDWI;Convolutional Neural Networks;Copernicus program;spectral bands;CNN-based fusion method;super-resolved band","","12","","17","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"A high resolution burned area detector for Sentinel-2 and Landsat-8","M. Zanetti; D. Marinelli; M. Bertoluzza; S. Saha; F. Bovolo; L. Bruzzone; M. L. Magliozzi; M. Zavagli; M. Costantini","Department of Information Engineering and Computer Science, University of Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Italy; Fondazione Bruno Kessler, Center for Information and Communication Technology, Trento, Italy; Fondazione Bruno Kessler, Center for Information and Communication Technology, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Italy; e-GEOS, Rome, Italy; e-GEOS, Rome, Italy; e-GEOS, Rome, Italy","2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp)","14 Oct 2019","2019","","","1","4","Fires are disruptive events that should be carefully studied. To this date, the monitoring at large scale of fire events is mainly performed using low spatial resolution sensors such as MODIS and VIIRS and ancillary data like active fire maps. However, the data produced by the new generation of spaceborne multispectral missions can be used for burned area detection at a finer spatial and temporal scale with respect to existing methods. In this paper we present a method that analyzes time-series of optical images acquired by Landsat-8 and Sentinel-2 to detect burned areas in a fully automatic and unsupervised way. The method analyzes each image in the time-series to extract Candidate Burned Areas (CBAs). CBAs are analyzed accounting for the temporal correlation to reduce the number of false alarms. In particular, only pixels that have a temporal profile consistent with the physical event of a fire are selected. Moreover, the time variable is used to compute the confidence of the detection. The method has been tested on a time-series of the Attica region, Greece, which was affected by large fires in 2018. Preliminary experimental results showed that the method correctly identified the burned areas of the region.","","978-1-7281-4615-7","10.1109/Multi-Temp.2019.8866958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8866958","Burned Area Detection;Time-series;Multitemporal;Sentinel-2;Remote Sensing","Indexes;Vegetation mapping;Remote sensing;Spatial resolution;Artificial satellites;Earth;MODIS","fires;geophysical image processing;geophysical techniques;image classification;image fusion;image resolution;remote sensing;time series;vegetation mapping;wildfires","Landsat-8;Sentinel-2;Candidate Burned Areas;CBAs;temporal correlation;temporal profile;physical event;high resolution burned area detector;disruptive events;fire events;low spatial resolution sensors;ancillary data;active fire maps;spaceborne multispectral missions;burned area detection;temporal scale;time-series;optical images","","4","","12","IEEE","14 Oct 2019","","","IEEE","IEEE Conferences"
"Geostatistical Data Fusion: Application to Red Edge Bands of Sentinel 2","M. J. Pereira; A. Ramos; R. Nunes; L. Azevedo; A. Soares","CERENA/DECivil, Instituto Superior Técnico U. Lisboa, Lisboa, Portugal; CERENA/DECivil, Instituto Superior Técnico U. Lisboa, Lisboa, Portugal; CERENA/DECivil, Instituto Superior Técnico U. Lisboa, Lisboa, Portugal; CERENA/DECivil, Instituto Superior Técnico U. Lisboa, Lisboa, Portugal; CERENA/DECivil, Instituto Superior Técnico U. Lisboa, Lisboa, Portugal","2016 International Conference on Computational Science and Computational Intelligence (CSCI)","20 Mar 2017","2016","","","758","761","The new ESA Sentinel-2 satellite delivers images of the red-edge band at a spatial resolution of 20m. These bands are particular useful for vegetation monitoring in general and present high potential of application in precision agriculture. For this, we propose a data fusion methodology for downscaling rededge bands to 10 m spatial resolution using the information of visible and near infrared band. The methodology is based on inverse modelling by combining geostatistical stochastic simulation methods and genetic programming. A case study case presents the preliminary results.","","978-1-5090-5510-4","10.1109/CSCI.2016.0147","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881440","inverse modeling;data fusion;genetic programming;direct sequential cosimulation;sentinel 2;red-edge bands","Correlation coefficient;Spatial resolution;Genetic programming;Histograms;Stochastic processes;Data integration","genetic algorithms;geophysical image processing;image fusion;image resolution;remote sensing;stochastic processes","geostatistical data fusion;red edge bands;ESA Sentinel-2 satellite;spatial resolution;near infrared band;visible band;inverse modelling;geostatistical stochastic simulation methods;genetic programming","","3","","6","IEEE","20 Mar 2017","","","IEEE","IEEE Conferences"
"Fusing Sentinel-2 Satellite Images and Aerial RGB Images","J. Sigurdsson; M. O. Ulfarsson; J. R. Sveinsson","Dept. Electrical Eng., University of Iceland, Reykjavik, ICELAND; Dept. Electrical Eng., University of Iceland, Reykjavik, ICELAND; Dept. Electrical Eng., University of Iceland, Reykjavik, ICELAND","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4444","4447","Sentinel-2 (S2) is a constellation of two satellites that frequently acquire optical imagery over land and coastal waters. The S2 sensors have three spatial resolutions: 10, 20, and 60 m. Many remote sensing applications require the spatial resolution to be at the highest resolution, i.e., 10 m for S2. To address this demand, researchers have proposed various methods that exploit the spectral and spatial correlation in multispectral data to sharpen the S2 bands to 10 m. In this paper, we fuse S2 data with high-resolution aerial RGB images. A method called S2Sharp is modified to include the red, green, and blue bands of the aerial image and sharpen S2 data to the resolution of the RGB image. The method, termed S2PF, is evaluated using an S2 image and aerial photographs of Reykjavik, Iceland.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554406","Data fusion;image sharpening;Sentinel-2 (S2) constellation;superresolution;RGB images","Integrated optics;Satellites;Correlation;Fuses;Sea measurements;Optical imaging;Optical sensors","geophysical image processing;image fusion;image resolution;image sensors;remote sensing","Sentinel-2 satellite images;optical imagery;coastal waters;spatial resolution;remote sensing applications;highest resolution;spectral correlation;spatial correlation;multispectral data;high-resolution aerial RGB images;S2Sharp;red bands;green bands;blue bands;Reykjavik;Iceland","","1","","16","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Fusion of POLSAR and Multispectral Satellite Images: A New Insight for Image Fusion","J. Wang; J. Chen; Q. Wang","College of Computer and Information Engineering, Hohai University, Nanjing, China; College of Computer and Information Engineering, Hohai University, Nanjing, China; College of Computer and Information Engineering, Hohai University, Nanjing, China","2020 IEEE International Conference on Computational Electromagnetics (ICCEM)","12 Oct 2020","2020","","","83","84","Polarized synthetic aperture radar (POLSAR) and multispectral images have great complementarity in information volume. That is to say, POLSAR have high resolution but poor color information. Multispectral images have rich spectral channel information, but the resolution is low. Therefore, this work has explored the fusion problem of the two data source. A framework was proposed, which merged polarized channel fusion data and multispectral images based on the Sentinel-2 and GF-3 data. The experimental results showed that the fusion results greatly integrated the characteristics of each channel of POLSAR and optical image. Therefore, our work has great application potential in improving the accuracy of feature recognition.","","978-1-7281-3448-2","10.1109/ICCEM47450.2020.9219457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9219457","fusion;polarized synthetic aperture radar (POL-SAR);GF-3;Sentinel-2;feature recognition","Integrated optics;Visualization;Optical polarization;Satellites;Soft sensors;Optical imaging;Adaptive optics","geophysical image processing;image colour analysis;image fusion;radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","optical image;fusion results;channel fusion data;fusion problem;rich spectral channel information;poor color information;information volume;multispectral images;polarized synthetic aperture radar;image fusion;multispectral satellite images;POLSAR","","1","","7","IEEE","12 Oct 2020","","","IEEE","IEEE Conferences"
"Single Image Cloud Detection via Multi-Image Fusion","S. Workman; M. U. Rafique; H. Blanton; C. Greenwell; N. Jacobs",DZYNE Technologies; University of Kentucky; University of Kentucky; University of Kentucky; University of Kentucky,"IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1468","1471","Artifacts in imagery captured by remote sensing, such as clouds, snow, and shadows, present challenges for various tasks, including semantic segmentation and object detection. A primary challenge in developing algorithms for identifying such artifacts is the cost of collecting annotated training data. In this work, we explore how recent advances in multi-image fusion can be leveraged to bootstrap single image cloud detection. We demonstrate that a network optimized to estimate image quality also implicitly learns to detect clouds. To support the training and evaluation of our approach, we collect a large dataset of Sentinel-2 images along with a per-pixel semantic labelling for land cover. Through various experiments, we demonstrate that our method reduces the need for annotated training data and improves cloud detection performance.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323759","weakly-supervised learning;multi-image fusion;segmentation;clouds","Image segmentation;Clouds;Training;Cloud computing;Semantics;Remote sensing;Training data","clouds;geophysical image processing;image classification;image fusion;image segmentation;image sensors;learning (artificial intelligence);object detection;remote sensing","object detection;annotated training data;multiimage fusion;single image cloud detection;image quality;Sentinel-2 images;cloud detection performance;semantic segmentation","","2","","16","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Fusing Sentinel-2 and Landsat-8 Surface Reflectance Data via Pixel-Wise Local Normalization","Y. Li; Q. Shi; L. He; R. Cai; L. Meng; J. Li; A. Plaza","Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hyperspectral Computing Laboratory Department of Technology of Computers and Communications, Escuela Politécnica, University of Extremadura, Cáceres, Spain","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Sep 2022","2022","15","","7359","7374","Medium spatial resolution surface reflectance image series from the combination of Landsat-8 Operational Land Imager and Sentinel-2 Multispectral Imager observations have great importance to the land surface monitoring tasks, for which great efforts have been paid for blending the two data. However, most of the efforts focus on the image series with spatial resolution of 30 m, which cannot meet the data demand of some applications. Therefore, it is necessary to fuse Landsat-8 and Sentinel-2 images to provide 10-m image series. Currently, there are three means to achieve that, including the area-to-point regression kriging fusion approach (ATPRK), spatiotemporal fusion methods, and deep-learning-based fusion models. However, the ATPRK and spatiotemporal fusion methods suffer from the limited fusion performance, while the deep-learning-based fusion models are hardware dependent, i.e., requiring the graphics processing units, which may not be satisfied sometimes. To address these issues, in this article, we develop a new pixel-wise local normalization-based fusion method (LN-FM) for fusing Sentinel-2 and Landsat-8 images. The newly proposed LN-FM is compared to the ATPRK and three representative spatiotemporal fusion methods in experiments, which use imagery collected from both a rural area and an urban area. The experimental results demonstrate that the newly developed LN-FM exhibits excellent qualitative and quantitative performance, as well as remarkable spatial, spectral, and pixel distribution fidelity. Furthermore, this approach is fast, which may improve its applicability","2151-1535","","10.1109/JSTARS.2022.3200713","National Natural Science Foundation of China(grant numbers:T2225019,42030111,61976234,62071184); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2022A1515011615,2019A1515011057,202002030402); Guangzhou Science and Technology Program(grant numbers:202002030395); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864237","Landsat-8;local normalization;remote sensing image fusion;Sentinel-2","Remote sensing;Earth;Artificial satellites;Spatial resolution;Image resolution;Spatiotemporal phenomena;Task analysis","geophysical image processing;geophysical techniques;image fusion;image resolution;land cover;learning (artificial intelligence);radiometry;reflectivity;remote sensing;sensor fusion;spatiotemporal phenomena;statistical analysis;vegetation mapping","Landsat-8 surface reflectance data;medium spatial resolution surface reflectance image series;Landsat-8 Operational Land Imager;Sentinel-2 Multispectral Imager observations;land surface monitoring tasks;efforts focus;data demand;Sentinel-2 images;area-to-point regression;ATPRK;deep-learning-based fusion models;fusion performance;pixel-wise local normalization-based fusion method;Landsat-8 images;representative spatiotemporal fusion methods;newly developed LN-FM exhibits;remarkable spatial;size 30.0 m","","","","61","CCBYNCND","22 Aug 2022","","","IEEE","IEEE Journals"
"Sentinel-2 Sharpening Using a Single Unsupervised Convolutional Neural Network With MTF-Based Degradation Model","H. V. Nguyen; M. O. Ulfarsson; J. R. Sveinsson; M. D. Mura","Department of Electrical and Electronic Engineering, Nha Trang University, Nha Trang, Vietnam; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Institut Universitaire de France, Paris, France","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","21 Jul 2021","2021","14","","6882","6896","The Sentinel-2 (S2) constellation provides multispectral images at 10 m, 20 m, and 60 m resolution bands. Obtaining all bands at 10 m resolution would benefit many applications. Recently, many model-based and deep learning (DL)-based sharpening methods have been proposed. However, the downside of those methods is that the DL-based methods need to be trained separately for the 20 m and the 60 m bands in a supervised manner at reduced resolution, while the model-based methods heavily depend on the hand-crafted image priors. To break the gap, this article proposes a novel unsupervised DL-based S2 sharpening method using a single convolutional neural network (CNN) to sharpen the 20 and 60 m bands at the same time at full resolution. The proposed method replaces the hand-crafted image prior by the deep image prior (DIP) provided by a CNN structure whose parameters are easily optimized using a DL optimizer. We also incorporate the modulation transfer function-based degradation model as a network layer, and add all bands to both network input and output. This setting improves the DIP and exploits the advantage of multitask learning since all S2 bands are highly correlated. Extensive experiments with real S2 data show that our proposed method outperforms competitive methods for reduced-resolution evaluation and yields very high quality sharpened image for full-resolution evaluation.","2151-1535","","10.1109/JSTARS.2021.3092286","Icelandic Research Fund(grant numbers:207233-051); University of Iceland Doctoral(grant numbers:1547-154305); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9464640","Convolutional neural networks (CNNs);image fusion;MTF-based degradation;Sentinel-2 image sharpening;super-resolution;unsupervised deep learning","Spatial resolution;Pansharpening;Electronics packaging;Image fusion;Hyperspectral imaging;Degradation;Training","convolution;feature extraction;image classification;image resolution;learning (artificial intelligence);neural nets;optical transfer function;unsupervised learning","Sentinel-2 sharpening;single unsupervised convolutional neural network;MTF-based degradation model;Sentinel-2 constellation;multispectral images;deep learning-based sharpening methods;reduced resolution;model-based methods;hand-crafted image priors;novel unsupervised DL-based S2 sharpening method;single convolutional neural network;deep image;modulation transfer function-based degradation model;network layer;network input;competitive methods;reduced-resolution evaluation;yields very high quality sharpened image;full-resolution evaluation;size 10.0 m;size 20.0 m;size 60.0 m","","9","","58","CCBY","24 Jun 2021","","","IEEE","IEEE Journals"
"Fusing Landsat-8, Sentinel-1, and Sentinel-2 Data for River Water Mapping Using Multidimensional Weighted Fusion Method","Q. Liu; S. Zhang; N. Wang; Y. Ming; C. Huang","College of Urban and Environmental Sciences and the Shaanxi Key Laboratory of Earth Surface System and Environmental Carrying Capacity, Northwest University, Xi’an, China; Institute of Earth Surface System and Hazards, Northwest University, Xi’an, China; Institute of Earth Surface System and Hazards, Northwest University, Xi’an, China; College of Urban and Environmental Sciences and the Shaanxi Key Laboratory of Earth Surface System and Environmental Carrying Capacity, Northwest University, Xi’an, China; Institute of Earth Surface System and Hazards, Northwest University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","12 Jul 2022","2022","60","","1","12","River water extent is critical for understanding river discharge or its hydrological conditions. Although numerous methods have been proposed to map river water from either optical or synthetic aperture radar (SAR) remotely sensed images, uncertainties still exist broadly. In this study, we developed an image fusion method that integrates Landsat-8, Sentinel-1, and Sentinel-2 images simultaneously for river water mapping with two major steps. Firstly, a posterior probability support vector machine (SVM) model was adopted to generate water probability maps from each individual image; and second, a multidimensional weighted fusion method (MDWFM) was developed to fuse these probability maps. Four reaches with different characteristics were selected as case study sites. High-resolution aerial images were acquired and used as the reference to evaluate our results. We found that the fusion process not only improves the quality of river water mapping but also excludes the cloud interference. The fused river water maps become more reliable after the conflicts from difference images being solved by the proposed MDWFM method that contains a proportional conflict redistribution rule. The weighted root mean square difference was reduced to 0.066, and the area under the ROC curve reached up to 0.984. The critical success index (CSI), kappa coefficient (KC), and F-measure reached up to 0.810, 0.836, and 0.895, respectively. These stable and accurate river extent mapping results obtained through fusing multiple images with high spatial resolution (SR) (10 m) and short revisit interval (0.4–4.4 days) are of great significance for enriching the data and methodology of hydrological studies.","1558-0644","","10.1109/TGRS.2022.3187154","National Natural Science Foundation of China(grant numbers:U2243205); National Key Research and Development Program of China(grant numbers:2019YFC1510503); Shaanxi Natural Science Foundation(grant numbers:2021JM-314); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810288","Image fusion;multidimensional weighted fusion;posterior probability;support vector machine (SVM);synthetic aperture radar (SAR)","Rivers;Earth;Remote sensing;Artificial satellites;Uncertainty;Indexes;Fuses","geophysical image processing;hydrological techniques;image classification;image fusion;radar imaging;remote sensing by radar;rivers;support vector machines;synthetic aperture radar","Landsat-8;Sentinel-1;Sentinel-2 data;river water mapping;multidimensional weighted fusion method;river water extent;river discharge;image fusion method;Sentinel-2 images;posterior probability support vector machine model;water probability maps;high-resolution aerial images;fused river water maps;difference images;stable river extent;synthetic aperture radar remotely sensed images;support vector machine model;cloud interference;critical success index;kappa coefficient;F-measure;ROC curve;spatial resolution;hydrological studies;redistribution rule","","","","90","IEEE","29 Jun 2022","","","IEEE","IEEE Journals"
"Dynamic Wildfire Fuel Mapping Using Sentinel – 2 and Prisma Hyperspectral Imagery","R. U. Shaik; L. Giovanni; L. Fusilli","Dept. of Astronautical, Electrical and Energy Engineering (DIAEE), Sapienza University of Rome; School of Aerospace Engineering, Sapienza University of Rome, Italy; Dept. of Astronautical, Electrical and Energy Engineering (DIAEE), Sapienza University of Rome","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","5973","5976","Italy has witnessed a significant increase in wildfires in recent decades. Forest fire fuel maps play a vital role in the prevention, management and risk assessment of wildfires, and this paper presents the procedure implemented to develop a dynamic wildfire fuel map using PRISMA hyperspectral data and Sentinel-2 multispectral data. Freely available multispectral datasets are widely used for land cover and land use mapping, but they have limited utility for fuel mapping due to their coarse spectral resolution. So, in this study, hyperspectral imagery (HSI) from PRISMA has been used for fuel types classification. The feed-forward neural network showed an overall accuracy of 79% by validation. To convert the classification map into a dynamic fuel map, the knowledge of the proportion of live/dead herbaceous loads available in that area is essential. The Relative Greenness approach, which places the Normalized Difference Vegetation Index (NDVI) in the time series of measurements, was implemented using Sentinel - 2 multispectral data. By fusing the fuel types classification, relative greenness map and iso-bioclimatic map, a dynamic fuel map for the west of Latium in Italy was developed with reference to Scott/Burgan fuel models.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883095","Fuel Map;Hyperspectral;Image fusion;Multispectral;Wildfires","Neural networks;Time series analysis;Fires;Vegetation mapping;Time measurement;Fuels;Risk management","fires;geophysical image processing;geophysical techniques;geophysics computing;neural nets;remote sensing;time series;vegetation;vegetation mapping;wildfires","dynamic wildfire fuel mapping;Sentinel - 2;prisma hyperspectral imagery;wildfires;forest fire fuel maps;risk assessment;dynamic wildfire fuel map;PRISMA hyperspectral data;Sentinel-2 multispectral data;available multispectral datasets;land cover;land use mapping;fuel mapping due;fuel types classification;classification map;dynamic fuel map;relative greenness map;iso-bioclimatic map","","1","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"SEN23E: A Cloudless Geo-Referenced Multi-Spectral Sentinel-2/Sentinel-3 Dataset for Data Fusion Analysis","D. Ibañez; R. Fernandez-Beltran; F. Pla","Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1448","1451","The availability of geo-referenced coupled data of dif-ferent platforms is essential to train remote sensing (RS) multi-modal classification and bio-phyiscal parameter esti-mation learning methods. To properly develop a general-izing model different scenes and topographies are required. For this purpose, different multi-modal datasets have been published for the last years. Nevertheless, to our knowl-edge there is not any dataset composed of Sentinel-2 (S2) and Sentinel-3 (S3) geo-referenced images. In this paper we present SEN23, a dataset composed of 100 complete multi-spectral S2 and S3 paired images of different locations along Europe from the 2021 summer. The coupled images were obtained with a time difference of three or less days, containing less than a 1 % of cloud coverage and have a resolution difference of × 15. SEN23E is expected to help with the development of new multi-spectral, multi-resolution and multi-modal models for complex tasks which need con-text and complete images. SEN23E will be available at https://github.com/ibanezdf/SEN23E.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883867","Dataset;Data Fusion;Remote Sensing;Multi-spectral;Sentinel-2 (S2);Sentinel-3 (S3)","Learning systems;Image resolution;Biological system modeling;Europe;Data integration;Surfaces;Sensors","geophysical image processing;hydrological techniques;image classification;image fusion;learning (artificial intelligence);remote sensing;sensor fusion","data fusion analysis;geo-referenced coupled data;dif-ferent platforms;remote sensing multimodal classification;bio-phyiscal parameter esti-mation;model different scenes;topographies;different multimodal datasets;Sentinel-3 geo-referenced images;SEN23;100 complete multispectral;coupled images;time difference;resolution difference;multiresolution;multimodal models","","","","12","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"A CNN-based Super-resolution Technique for Active Fire Detection on Sentinel-2 Data","M. Gargiulo; D. A. G. Dell’ Aglio; A. Iodice; D. Riccio; G. Ruello","University of Naples “Federico II”, Italy; University of Naples “Federico II”, Italy; University of Naples “Federico II”, Italy; University of Naples “Federico II”, Italy; University of Naples “Federico II”, Italy","2019 PhotonIcs & Electromagnetics Research Symposium - Spring (PIERS-Spring)","2 Mar 2020","2019","","","418","426","Remote Sensing applications can benefit from a relatively fine spatial resolution multispectral (MS) images and a high revisit frequency ensured by the twin satellites Sentinel-2. Unfortunately, only four out of thirteen bands are provided at the highest resolution of 10 meters, and the others at 20 or 60 meters. For instance the Short-Wave Infrared (SWIR) bands, provided at 20 meters, are very useful to detect active fires. Aiming to a more detailed Active Fire Detection (AFD) maps, we propose a super-resolution data fusion method based on Convolutional Neural Network (CNN) to move towards the 10-m spatial resolution the SWIR bands. The proposed CNN-based solution is compared to alternative methods in terms of some accuracy metrics. Moreover we have tested the super-resolved bands from an application point of view by monitoring active fire through classic indices. Advantages and limits of our proposed approach are validated on specific geographical area (the mount Vesuvius, close to Naples) that was damaged by widespread fires during the summer of 2017.","1559-9450","978-1-7281-3403-1","10.1109/PIERS-Spring46901.2019.9017857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9017857","","Spatial resolution;Training;Springs;Measurement;Monitoring;Photonics","fires;geophysical image processing;geophysical signal processing;image fusion;image resolution;neural nets;remote sensing;sensor fusion;terrain mapping","CNN-based super-resolution technique;Sentinel-2 data;Remote Sensing applications;relatively fine spatial resolution multispectral images;high revisit frequency;twin satellites Sentinel-2;Short-Wave Infrared bands;super-resolution data fusion method;Convolutional Neural Network;SWIR bands;CNN-based solution;super-resolved bands;Active Fire Detection maps","","8","","26","IEEE","2 Mar 2020","","","IEEE","IEEE Conferences"
"Sentinel-3 Image Super-Resolution Using Data Fusion and Convolutional Neural Networks","R. Fernandez; R. Fernandez-Beltran; F. Pla","Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2867","2870","With the increasing availability of Sentinel-2 (S2) and Sentinel-3 (S3) data, developing higher-level data products becomes a very attractive option to relieve the spatial limitations of the Ocean and Land Colour Instrument (OLCI) of S3. In this context, this paper investigates the suitability of super-resolving operational OLCI products using the Multi-Spectral Instrument (MSI) of S2 as an offline spatial reference. Specifically, the proposed approach assembles a multi -spectral data fusion scheme together with a convolutinal neural network (CNN) mapping function to project the OLCI sensor onto its corresponding spatial reference which is synthetically generated by the OLCI/MSI fusion. In this way, the trained model is able to super-resolve operational OLCI products under demand without the need of using MSI data. The experimental part of the work shows the suitability of the proposed approach in the context of the Copernicus programme.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554826","Sentinel-2 (S2);Sentinel-3 (S3);image fusion;super-resolution (SR)","Image color analysis;Instruments;Oceans;Superresolution;Neural networks;Data integration;Geoscience and remote sensing","geophysical image processing;image fusion;image resolution;image sensors;neural nets;remote sensing;sensor fusion","corresponding spatial reference;super-resolve operational OLCI products;MSI data;Sentinel-3 image super-resolution;convolutional neural networks;Sentinel-2;Sentinel-3 data;higher-level data products;spatial limitations;MultiSpectral Instrument;offline spatial reference;multi-spectral data fusion scheme;convolutinal neural network mapping function;OLCI sensor","","","","16","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Experimental Comparison of Multi-Sharpening Methods Applied To Sentinel-2 MSI and Sentinel-3 OLCI Images","A. Alboody; M. Puigt; G. Roussel; V. Vantrepotte; C. Jamet; T. K. Tran","Univ. Littoral Côte d’Opale, Longuenesse, France; Univ. Littoral Côte d’Opale, Longuenesse, France; Univ. Littoral Côte d’Opale, Longuenesse, France; Univ. Littoral Côte d’Opale, Wimereux, France; Univ. Littoral Côte d’Opale, Wimereux, France; Univ. Littoral Côte d’Opale, Wimereux, France","2021 11th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Jul 2021","2021","","","1","5","Multi-spectral images are crucial to detect and to understand phenomena in marine observation. However, in coastal areas, these phenomena are complex and their analyze requires multi-spectral images with both a high spatial and spectral resolution. Unfortunately, no satellite is able to provide both at the same time. As a consequence, multi-sharpening techniques-a.k.a. fusion or super- resolution of multi-spectral and/or hyper-spectral images-were proposed and consist of combining information from at least two multi-spectral images with different spatial and spectral resolutions. The fused image then combines their best characteristics. Various methods-based on different strategies and tools-have been proposed to solve this problem. This article presents a comparative review of fusion methods applied to Sentinel-2 MSI (13 spectral bands with a spatial resolution ranging from 10 to 60 m) and Sentinel-3 OLCI (21 spectral bands with a spatial resolution of 300 m) images. Indeed, both satellites are extensively used in marine observation and, to the best of the authors' knowledge, the fusion of their data was partially investigated (and not in the way we aim to do in this paper). To that end, we provide both a quantitative analysis of the performance of some state-of-the-art methods on simulated images, and a qualitative analysis on real images.","2158-6276","978-1-6654-3601-4","10.1109/WHISPERS52202.2021.9484009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9484009","Image fusion;Remote sensing;Sentinel-2 MSI;Sentinel-3 OLCI;Simulations;Real data","Satellites;Statistical analysis;Conferences;Sea measurements;Distance measurement;Sensors;Spatial resolution","geophysical image processing;geophysical techniques;hyperspectral imaging;image fusion;image resolution;remote sensing","fusion methods;multi-sharpening techniques;multisharpening methods;multisharpening techniques;simulated images;Sentinel-3 OLCI;spectral bands;fused image;spectral resolutions;hyper-spectral images;spectral resolution;high spatial resolution;multispectral images;Sentinel-2 MSI","","2","","21","IEEE","19 Jul 2021","","","IEEE","IEEE Conferences"
"A Conditional Generative Adversarial Network to Fuse Sar And Multispectral Optical Data For Cloud Removal From Sentinel-2 Images","C. Grohnfeldt; M. Schmitt; X. Zhu","Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1726","1729","In this paper, we present the first conditional generative adversarial network (cGAN) architecture that is specifically designed to fuse synthetic aperture radar (SAR) and optical multi-spectral (MS) image data to generate cloud- and haze-free MS optical data from a cloud-corrupted MS input and an auxiliary SAR image. Experiments on Sentinel-2 MS and Sentinel-l SAR data confirm that our extended SAR-Opt-cGAN model utilizes the auxiliary SAR information to better reconstruct MS images than an equivalent model which uses the same architecture but only single-sensor MS data as input.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519215","SAR;optical remote sensing;data fusion;deep learning;generative adversarial network (GAN);cloud-removal","Synthetic aperture radar;Optical sensors;Optical imaging;Clouds;Remote sensing;Adaptive optics;Generative adversarial networks","geophysical image processing;image fusion;radar imaging;synthetic aperture radar","cloud-free MS optical data;Sentinel-l SAR data;single-sensor MS data;reconstruct MS images;auxiliary SAR information;extended SAR-Opt-cGAN model;auxiliary SAR image;cloud-corrupted MS input;haze-free MS optical data;multispectral image data;synthetic aperture radar;conditional generative adversarial network architecture;Sentinel-2;cloud removal;multispectral optical data","","51","3","12","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Water Pollution Detection in Acapulco Coasts Using Merged Data from the Sentinel-2 and Sentinel-3 Satellites","R. Lomelí-Huerta; H. Avila-George; J. P. Rivera-Caicedo; M. De-la-Torre","Departamento de Ciencias Computacionales e Ingenierías, Universidad de Guadalajara; Departamento de Ciencias Computacionales e Ingenierías, Universidad de Guadalajara; CONACyT-UAN, Secretaría de Investigation y Posgradom, Universidad Autónoma de Nayarit; Departamento de Ciencias Computacionales e Ingenierías, Universidad de Guadalajara","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","1518","1521","Acapulco coasts are occasionally contaminated by illegal discharges originated by temporary or permanent floods that disembogue to the pacific ocean. Plumes formed by contaminated water running through the ocean can be distinguished in satellite imagery, and their reflectance is related to the polluting elements. Although some spacial agencies provide data from diverse multispectral sensors, application-specific requirements are fulfilled by merging heterogeneous imagery (differences in spatial, temporal, and spectral resolutions). This paper proposes a continuous monitoring strategy to detect pollution in water discharges by combining data from Sentinel-2 and Sentinel-3 platforms. First, the region of interest to be monitored is detected using the bands with high spatial resolution. Then, distance-based supervised machine learning is employed to detect pixel-wise pollution in water. Finally, the historic detections over time are presented to detect recurrent discharges.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553929","Sentinel;satellite image fusion;contaminated water;monitoring system;remote sensing","Reflectivity;Image sensors;Satellites;Oceans;Merging;Machine learning;Water pollution","environmental monitoring (geophysics);floods;geophysical image processing;marine pollution;oceanographic regions;oceanographic techniques;remote sensing;supervised learning;water pollution measurement","water pollution detection;Sentinel-2 satellite;Sentinel-3 satellite;illegal discharges;temporary floods;permanent floods;Pacific Ocean;plumes;contaminated water;satellite imagery;diverse multispectral sensors;heterogeneous imagery;water discharges;distance-based supervised machine learning;pixel-wise pollution;Acapulco coasts;Mexico;permanent flood","","1","","11","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Sentinel-2 Sharpening Using a Reduced-Rank Method","M. O. Ulfarsson; F. Palsson; M. Dalla Mura; J. R. Sveinsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Tokyo Tech World Research Hub Initiative (WRHI), School of Computing, Tokyo Institute of Technology, Tokyo, Japan; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2019","2019","57","9","6408","6420","Recently, the Sentinel-2 (S2) satellite constellation was deployed for mapping and monitoring the Earth environment. Images acquired by the sensors mounted on the S2 platforms have three levels of spatial resolution: 10, 20, and 60 m. In many remote sensing applications, the availability of images at the highest spatial resolution (i.e., 10 m for S2) is often desirable. This can be achieved by generating a synthetic high-resolution image through data fusion. To this end, researchers have proposed techniques exploiting the spectral/spatial correlation inherent in multispectral data to sharpen the lower resolution S2 bands to 10 m. In this paper, we propose a novel method that formulates the sharpening process as a solution to an inverse problem. We develop a cyclic descent algorithm called S2Sharp and an associated tuning parameter selection algorithm based on generalized cross validation and Bayesian optimization. The tuning parameter selection method is evaluated on a simulated data set. The effectiveness of S2Sharp is assessed experimentally by comparisons to state-of-the-art methods using both simulated and real data sets.","1558-0644","","10.1109/TGRS.2019.2906048","Research Fund of the University of Iceland; Icelandic Research Fund(grant numbers:174075-05); Labex(grant numbers:PNTS-2016-03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8694937","Cyclic descent (CD);data fusion;image sharpening;Sentinel-2 (S2) constellation;superresolution","Spatial resolution;Image sensors;Optimization;Thermal sensors;Remote sensing","Bayes methods;geophysical image processing;gradient methods;image fusion;image resolution;inverse problems;remote sensing","reduced-rank method;Sentinel-2 satellite constellation;Earth environment;remote sensing applications;data fusion;multispectral data;S2Sharp;tuning parameter selection method;Sentinel-2 sharpening;spatial resolution;tuning parameter selection algorithm;inverse problem;cyclic descent algorithm;Bayesian optimization","","27","","48","IEEE","22 Apr 2019","","","IEEE","IEEE Journals"
"On the Fusion Strategies of Sentinel-1 and Sentinel-2 Data for Local Climate Zone Classification","J. Gawlikowski; M. Schmitt; A. Kruspe; X. X. Zhu","Institute of Data Science, German Aerospace Center (DLR), Jena, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Institute of Data Science, German Aerospace Center (DLR), Jena, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Germany","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2081","2084","Local Climate Zone (LCZ) classification is the most commonly used scheme to analyze how local urban morphology affects the climate of local areas. Classification methods are often based on remote sensing data or on a fusion of several data sources. In this study, the effects of different fusion strategies of optical and synthetic aperture radar (SAR) data on the accuracy of LCZ classifications are investigated. The data processing is implemented with a convolutional neural network (CNN), where until a fusion layer, separate data sources are processed separately on branches. Strategies of splitting the data into branches and the effects of different fusion stages are compared, together with approaches based on sums of independent classifiers. For our setting, the stage of fusion does not seem to have a big influence on the accuracy. The results of this study contribute to a better understanding of cooperative usage of multispectral and SAR data.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324234","Local Climate Zone Classification;Data Fusion;Fusion Network","Training;Meteorology;Synthetic aperture radar;Optical imaging;Task analysis;Remote sensing;Vegetation mapping","convolutional neural nets;geophysical image processing;image classification;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar","multispectral SAR data;fusion stages;data sources;fusion layer;convolutional neural network;data processing;LCZ classifications;fusion strategies;remote sensing data;classification methods;local urban morphology;local climate zone classification;sentinel-2 data;sentinel-1","","7","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Research on spatio-temporal fusion algorithm of remote sensing image based on GF-1 WFV and Sentinel-2 satellite data","S. Wang; X. Yang; G. Li; Y. Jin; C. Tian","Innovation Center, Hebei Aerospace Remote Sensing Information Technology, LangFang, China; Innovation Center, Hebei Aerospace Remote Sensing Information Technology, LangFang, China; Innovation Center, Hebei Aerospace Remote Sensing Information Technology, LangFang, China; Innovation Center, Hebei Aerospace Remote Sensing Information Technology, LangFang, China; Innovation Center, Hebei Aerospace Remote Sensing Information Technology, LangFang, China","2022 3rd International Conference on Geology, Mapping and Remote Sensing (ICGMRS)","18 Aug 2022","2022","","","667","678","The remote sensing data set with high spatial and temporal resolution is of great significance for monitoring surface change. However, the earth observation satellites at domestic and international cannot obtain high spatial resolution and high temporal resolution images at the same time. Remote sensing data spatio-temporal fusion technology is an effective means to solve this problem. In this paper, the multi-period GaoFen-1 WideField-View (GF-1 WFV) satellite images and Sentinel-2 satellite images of Beijing-Tianjin-Hebei region in 2020 are used to perform fusion simulation for the three spatio-temporal fusion algorithms of Spatial and Temporal Adaptive Reflectance Fusion Model (STARFM), Enhanced-STARFM (ESTARFM) and Enhanced Flexible Spatiotemporal Data Fusion Model (EFSDAF) and these simulation degree of the results are quantitatively evaluated. The results show that ESTARFM algorithm is more suitable for the construction of high spatio-temporal fusion data of GF-1 WFV and Sentinel-2 satellites in Beijing-Tianjin-Hebei region.","","978-1-6654-8595-1","10.1109/ICGMRS55602.2022.9849377","Natural Science Foundation of Hebei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9849377","STARFM;ESTARFM;EFSDAF;Temporal and spatial fusion;GF-1 WFV images","Adaptation models;Satellites;Time series analysis;Vegetation mapping;Lakes;Market research;Data models","geophysical image processing;image fusion;image resolution;remote sensing;sensor fusion;spatiotemporal phenomena","high spatio-temporal fusion data;GF-1 WFV;Sentinel-2 satellites;Beijing-Tianjin-Hebei region;spatio-temporal fusion algorithm;remote sensing image;Sentinel-2 satellite data;high spatial resolution;earth observation satellites;high temporal resolution images;remote sensing data spatio-temporal fusion technology;multiperiod GaoFen-1 WideField-View;Sentinel-2 satellite images;fusion simulation;Temporal Adaptive Reflectance Fusion Model;Enhanced Flexible Spatiotemporal Data Fusion Model;ESTARFM algorithm","","","","10","IEEE","18 Aug 2022","","","IEEE","IEEE Conferences"
"Constructing 10-m NDVI Time Series From Landsat 8 and Sentinel 2 Images Using Convolutional Neural Networks","Z. Ao; Y. Sun; Q. Xin","School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Key Laboratory of Geospatial Technology for the Middle and Lower Yellow River Regions (Henan University), Ministry of Education, Kaifeng, China; State Key Laboratory of Desert and Oasis Ecology, Research Center for Ecology and Environment of Central Asia, Chinese Academy of Sciences, Urumqi, China","IEEE Geoscience and Remote Sensing Letters","21 Jul 2021","2021","18","8","1461","1465","Normalized difference vegetation index (NDVI) carries valuable information related to the photosynthetic activity of vegetation and is essential for monitoring phenological changes and ecosystem dynamics. The medium to high spatial resolution satellite images from Landsat 8 and Sentinel 2 offer opportunities to generate dense NDVI time series at 10-m resolution to improve our understanding of the land surface processes. However, synergistic use of Landsat 8 and Sentinel 2 for generating frequent and consistent NDVI data remains challenging as they have different spatial resolutions and spectral response functions. In this letter, we developed an attentional super resolution convolutional neural network (ASRCNN) for producing 10-m NDVI time series through fusion of Landsat 8 and Sentinel 2 images. We evaluated its performance in two heterogeneous areas. Quantitative assessments indicated that the developed network outperforms five commonly used fusion methods [i.e., enhanced deep convolutional spatiotemporal fusion network (EDCSTFN), super resolution convolutional neural network (SRCNN), spatial and temporal adaptive reflectance fusion model (STARFM), enhanced STARFM (ESTARFM), and flexible spatiotemporal data fusion (FSDAF)]. The influence of the method selection on the fusion accuracy is much greater than that of the fusion strategy in blending Landsat-Sentinel NDVI. Our results illustrate the advantages and potentials of the deep learning approaches on satellite data fusion.","1558-0571","","10.1109/LGRS.2020.3003322","National Key Research and Development Program of China(grant numbers:2017YFA0604300); Natural Science Foundation of China(grant numbers:U1811464,41875122); Western Talents(grant numbers:2018XBYJRC004); Guangdong Top Young Talents(grant numbers:2017TQ04Z359); Open Foundation of Key Laboratory of Geospatial Technology for the Middle and Lower Yellow River Regions (Henan University), Ministry of Education(grant numbers:GTYR201810); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9125996","Convolutional neural network (CNN);data fusion;deep learning;remote sensing;spatiotemporal data","Remote sensing;Artificial satellites;Earth;Spatiotemporal phenomena;Spatial resolution;Time series analysis;Training","geophysical image processing;geophysical techniques;image fusion;image resolution;neural nets;phenology;remote sensing;sensor fusion;spatiotemporal phenomena;time series;vegetation;vegetation mapping","Landsat 8;Sentinel 2 images;convolutional neural networks;normalized difference vegetation index;high spatial resolution satellite images;Sentinel 2 offer opportunities;dense NDVI time series;10-m resolution;frequent data;consistent NDVI data;attentional super resolution convolutional neural network;developed network;temporal adaptive reflectance fusion model;flexible spatiotemporal data fusion;Landsat-Sentinel NDVI;satellite data fusion","","6","","32","IEEE","25 Jun 2020","","","IEEE","IEEE Journals"
"Multilevel Feature Fusion-Based CNN for Local Climate Zone Classification From Sentinel-2 Images: Benchmark Results on the So2Sat LCZ42 Dataset","C. Qiu; X. Tong; M. Schmitt; B. Bechtel; X. X. Zhu","Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Information Engineering University, Zhengzhou, China; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Institute of Geography, Ruhr-University Bochum, Bochum, Germany; German Aerospace Center (DLR) and Signal Processing in Earth Observation (SiPEO), Remote Sensing Technology Institute (IMF), Technical University of Munich, Munich, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","17 Jun 2020","2020","13","","2793","2806","As a unique classification scheme for urban forms and functions, the local climate zone (LCZ) system provides essential general information for any studies related to urban environments, especially on a large scale. Remote sensing data-based classification approaches are the key to large-scale mapping and monitoring of LCZs. The potential of deep learning-based approaches is not yet fully explored, even though advanced convolutional neural networks (CNNs) continue to push the frontiers for various computer vision tasks. One reason is that published studies are based on different datasets, usually at a regional scale, which makes it impossible to fairly and consistently compare the potential of different CNNs for real-world scenarios. This article is based on the big So2Sat LCZ42 benchmark dataset dedicated to LCZ classification. Using this dataset, we studied a range of CNNs of varying sizes. In addition, we proposed a CNN to classify LCZs from Sentinel-2 images, Sen2LCZ-Net. Using this base network, we propose fusing multilevel features using the extended Sen2LCZ-Net-MF. With this proposed simple network architecture, and the highly competitive benchmark dataset, we obtain results that are better than those obtained by the state-of-the-art CNNs, while requiring less computation with fewer layers and parameters. Large-scale LCZ classification examples of completely unseen areas are presented, demonstrating the potential of our proposed Sen2LCZ-Net-MF as well as the So2Sat LCZ42 dataset. We also intensively investigated the influence of network depth and width, and the effectiveness of the design choices made for Sen2LCZ-Net-MF. This article will provide important baselines for future CNN-based algorithm developments for both LCZ classification and other urban land cover land use classification. Code and pretrained models are available at https://github.com/ChunpingQiu/benchmark-on-So2SatLCZ42-dataset-a-simple-tour.","2151-1535","","10.1109/JSTARS.2020.2995711","China Scholarship Council; European Research Council; European Union’s Horizon 2020 Research and Innovation Programme(grant numbers:ERC-2016-StG-714087); Helmholtz Association; Framework of Helmholtz Artificial Intelligence; Munich Unit @Aeronautics, Space and Transport (MASTr); Helmholtz Excellent Professorship; Data Science in Earth Observation - Big Data Fusion for Urban Research; German Federal Ministry of Education and Research; International Future AI lab; AI4EO: Artificial Intelligence for Earth Observation: Reasoning, Uncertainties, Ethics and Beyond; National Key R&D Program of China(grant numbers:2018YFB0505304); National Natural Science Foundation of China(grant numbers:41671409); Benjamin Bechtel; DFG(grant numbers:437467569); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103196","Benchmark;convolutional neural networks (CNNs);local climate zones (LCZ);sentinel-2;urban land cover","Feature extraction;Benchmark testing;Remote sensing;Earth;Urban areas;Computer architecture;Meteorology","computer vision;convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing","LCZ classification;urban land cover land use classification;local climate zone classification;Sentinel-2 images;large-scale mapping;deep learning;convolutional neural networks;So2Sat LCZ42 benchmark dataset;remote sensing data-based classification;Sen2LCZ-Net-MF;multilevel feature fusion-based CNN","","26","","77","CCBY","28 May 2020","","","IEEE","IEEE Journals"
"Sentinel-2 and Sentinel-3 Intersensor Vegetation Estimation via Constrained Topic Modeling","R. Fernandez-Beltran; F. Pla; A. Plaza","Institute of New Imaging Technologies, University Jaume I, Castelló de la Plana, Spain; Institute of New Imaging Technologies, University Jaume I, Castelló de la Plana, Spain; Hyperspectral Computing Laboratory, University of Extremadura, Cáceres, Spain","IEEE Geoscience and Remote Sensing Letters","25 Sep 2019","2019","16","10","1531","1535","This letter presents a novel intersensor vegetation estimation framework, which aims at combining Sentinel-2 (S2) spatial resolution with Sentinel-3 (S3) spectral characteristics in order to generate fused vegetation maps. On the one hand, the multispectral instrument (MSI), carried by S2, provides high spatial resolution images. On the other hand, the Ocean and Land Color Instrument (OLCI), one of the instruments of S3, captures the Earth's surface at a substantially coarser spatial resolution but using smaller spectral bandwidths, which makes the OLCI data more convenient to highlight specific spectral features and motivates the development of synergetic fusion products. In this scenario, the approach presented here takes advantage of the proposed constrained probabilistic latent semantic analysis (CpLSA) model to produce intersensor vegetation estimations, which aim at synergically exploiting MSI's spatial resolution and OLCI's spectral characteristics. Initially, CpLSA is used to uncover the MSI reflectance patterns, which are able to represent the OLCI-derived vegetation. Then, the original MSI data are projected onto this higher abstraction-level representation space in order to generate a high-resolution version of the vegetation captured in the OLCI domain. Our experimental comparison, conducted using four data sets, three different regression algorithms, and two vegetation indices, reveals that the proposed framework is able to provide a competitive advantage in terms of quantitative and qualitative vegetation estimation results.","1558-0571","","10.1109/LGRS.2019.2903231","Generalitat Valenciana(grant numbers:APOSTD/2017/007); Ministerio de Economía y Competitividad(grant numbers:ESP2016-79503-C2-2-P,TIN2015-63646-C5-5-R); Junta de Extremadura(grant numbers:GR18060); European Union under the H2020 EOXPOSURE project(grant numbers:734541); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8674589","Constrained probabilistic latent semantic analysis (CpLSA);Sentinel-2;Sentinel-3;topic models;vegetation estimation","Vegetation mapping;Spatial resolution;Estimation;Analytical models;Instruments;Probabilistic logic;Semantics","feature extraction;geophysical image processing;image colour analysis;image fusion;image resolution;probability;regression analysis;vegetation;vegetation mapping","multispectral instrument;high spatial resolution images;MSI reflectance patterns;OLCI-derived vegetation;Sentinel-3 intersensor vegetation estimation;constrained topic modeling;fused vegetation maps;Sentinel-2 intersensor vegetation estimation;ocean and land color instrument;spectral features;constrained probabilistic latent semantic analysis;regression algorithms","","15","","21","IEEE","26 Mar 2019","","","IEEE","IEEE Journals"
"Comparative Performance Evaluation of Pixel-Level and Decision-Level Data Fusion of Landsat 8 OLI, Landsat 7 ETM+ and Sentinel-2 MSI for Crop Ensemble Classification","J. Useya; S. Chen","Department of Remote Sensing and GIS, College of GeoExploration Science and Technology, Jilin University, Changchun, China; Department of Remote Sensing and GIS, College of GeoExploration Science and Technology, Jilin University, Changchun, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","3 Dec 2018","2018","11","11","4441","4451","Crops mapping unequivocally becomes a daunting task in humid, tropical, or subtropical regions due to unattainability of adequate cloud-free optical imagery. Objective of this study is to evaluate the comparative performance between decisionand pixel-levels data fusion ensemble classified maps using Landsat 8, Landsat 7, and Sentinel-2 data. This research implements parallel and concatenation approach to ensemble classify the images. The multiclassifier system comprises of Maximum Likelihood, Support Vector Machines, and Spectral Information Divergence as base classifiers. Decision-level fusion is achieved by implementing plurality voting method. Pixel-level fusion is achieved by implementing fusion by mosaicking approach, thus appending cloud-free pixels from either Sentinel-2 or Landsat 7. The comparison is based on the assessment of classification accuracy. Overall accuracy results show that decision-level fusion achieved an accuracy of 85.4%, whereas pixel-level fusion classification attained 82.5%, but their respective kappa coefficients of 0.84 and 0.80 but are not significantly different according to Z-testat α = 0.05. F1-score values reveal that decision-level performed better on most individual classes than pixel-level. Regression coefficient between planted areas from both approaches is 0.99. However, Support Vector Machines performed the best of the three classifiers. The conclusion is that both decision-level and pixel-level fusion approaches produced comparable classification results. Therefore, either of the procedures can be adopted in areas with inescapable cloud problems for updating crop inventories and acreage estimation at regional scales. Future work can focus on performing more comparison tests on different areas, run tests using different multiclassifier systems, and use different imagery","2151-1535","","10.1109/JSTARS.2018.2870650","JLU Science and Technology Innovative Research Team(grant numbers:2017TD-26); Fundamental Research Funds for the Central Universities; Changbai Mountain Scholars of Jilin Province(grant numbers:JJLZ[2015]54); Jilin province and Jilin University co-building project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8528827","Data fusion;ensemble classifier;multiclassifier system;parallel and concatenation approach;plurality voting","Remote sensing;Agriculture;Earth;Artificial satellites;Cloud computing;Data integration;Security","crops;geophysical image processing;image classification;image fusion;regression analysis;remote sensing;support vector machines","Landsat 8 OLI;Landsat 7 ETM+;cloud-free optical imagery;multiclassifier system;maximum likelihood;spectral information divergence;regression coefficient;pixel-level fusion approaches;pixel-level fusion classification;classification accuracy;cloud-free pixels;decision-level fusion;Support Vector Machines;Sentinel-2 data;crops mapping;crop ensemble classification;Sentinel-2 MSI;decision-level data fusion","","19","","62","OAPA","9 Nov 2018","","","IEEE","IEEE Journals"
"HISTIF: A New Spatiotemporal Image Fusion Method for High-Resolution Monitoring of Crops at the Subfield Level","J. Jiang; Q. Zhang; X. Yao; Y. Tian; Y. Zhu; W. Cao; T. Cheng","National Engineering and Technology Center for Information Agriculture, MOE Engineering Research Center of Smart Agriculture, MARA Key Laboratory of Crop System Analysis and Decision Making, Jiangsu Key Laboratory for Information Agriculture, Institute of Smart Agriculture, Nanjing Agricultural University, Nanjing, China; National Engineering and Technology Center for Information Agriculture, MOE Engineering Research Center of Smart Agriculture, MARA Key Laboratory of Crop System Analysis and Decision Making, Jiangsu Key Laboratory for Information Agriculture, Institute of Smart Agriculture, Nanjing Agricultural University, Nanjing, China; National Engineering and Technology Center for Information Agriculture, MOE Engineering Research Center of Smart Agriculture, MARA Key Laboratory of Crop System Analysis and Decision Making, Jiangsu Key Laboratory for Information Agriculture, Institute of Smart Agriculture, Nanjing Agricultural University, Nanjing, China; National Engineering and Technology Center for Information Agriculture, MOE Engineering Research Center of Smart Agriculture, MARA Key Laboratory of Crop System Analysis and Decision Making, Jiangsu Key Laboratory for Information Agriculture, Institute of Smart Agriculture, Nanjing Agricultural University, Nanjing, China; National Engineering and Technology Center for Information Agriculture, MOE Engineering Research Center of Smart Agriculture, MARA Key Laboratory of Crop System Analysis and Decision Making, Jiangsu Key Laboratory for Information Agriculture, Institute of Smart Agriculture, Nanjing Agricultural University, Nanjing, China; National Engineering and Technology Center for Information Agriculture, MOE Engineering Research Center of Smart Agriculture, MARA Key Laboratory of Crop System Analysis and Decision Making, Jiangsu Key Laboratory for Information Agriculture, Institute of Smart Agriculture, Nanjing Agricultural University, Nanjing, China; National Engineering and Technology Center for Information Agriculture, MOE Engineering Research Center of Smart Agriculture, MARA Key Laboratory of Crop System Analysis and Decision Making, Jiangsu Key Laboratory for Information Agriculture, Institute of Smart Agriculture, Nanjing Agricultural University, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","25 Aug 2020","2020","13","","4607","4626","Satellite-based time-series crop monitoring at the subfield level is essential to the efficient implementation of precision crop management. Existing spatiotemporal image fusion techniques can be helpful, but they were often proposed to generate medium-resolution images. This study proposed a high-resolution spatiotemporal image fusion method (HISTIF) consisting of filtering for cross-scale spatial matching (FCSM) and multiplicative modulation of temporal change (MMTC). In FCSM, we considered both point spread function effect and geo-registration errors between fine and coarse resolution images. Subsequently, MMTC used pixel-based multiplicative factors to estimate the temporal change between reference and prediction dates without image classification. The performance of HISTIF was evaluated using both simulated and real datasets with one from real Gaofen-1 (GF-1) and simulated Landsat-like/Sentinel-like images, and the other from real GF-1 and real Landsat/Sentinel-2 data on two sites. HISTIF was compared with the existing methods spatial and temporal adaptive reflectance fusion model (STARFM), FSDAF, and Fit-FC. The results demonstrated that HISTIF produced substantial reduction in the fusion error from cross-scale spatial mismatch and accurate reconstruction in spatial details within fields, regardless of simulated or real data. The images predicted by STARFM exhibited pronounced blocky artifacts. While the images predicted by HISTIF and Fit-FC both showed clear within-field variability patterns, HISTIF was able to reduce the spectral distortion more significantly than Fit-FC. Furthermore, HISTIF exhibited the most stable performance across sensors. The findings suggest that HISTIF could be beneficial for the frequent and detailed monitoring of crop growth at the subfield level.","2151-1535","","10.1109/JSTARS.2020.3016135","National Key R&D Program(grant numbers:2016YFD0300601); Jiangsu Planned Projects for Postdoctoral Research Funds(grant numbers:2018K229C); National Natural Science Foundation of China(grant numbers:41871259,31725020); Jiangsu Collaborative Innovation Center for Modern Crop Production; Academic Program Development of Jiangsu Higher Education Institutions; Qinghai Project of Transformation of Scientific and Technological Achievements(grant numbers:2018-NK-126); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165877","Crops;heterogeneity;image fusion;spatiotemporal fusion;subfield monitoring","Agriculture;Monitoring;Spatiotemporal phenomena;Spatial resolution;Sensors;Reflectivity","agriculture;crops;image fusion;image matching;image registration;image resolution;time series","subfield level;satellite-based time-series crop monitoring;precision crop management;high-resolution spatiotemporal image fusion method;HISTIF;FCSM;filtering for cross-scale spatial matching;MMTC;multiplicative modulation of temporal change","","10","","58","CCBY","12 Aug 2020","","","IEEE","IEEE Journals"
"CNN-Based Fusion and Classification of Multi-Temporal Sentinel-1 & -2 Satellite Data","A. Shakya; M. Biswas; M. Pal","Computer Engineering Department, National Institute of Technology, Kurukshetra, India; Computer Engineering Department, National Institute of Technology, Kurukshetra, India; Civil Engineering Department, National Institute of Technology, Kurukshetra, India","2021 IEEE International India Geoscience and Remote Sensing Symposium (InGARSS)","13 Jun 2022","2021","","","57","60","SAR and optical data are widely used in image fusion to provide the complimentary information of each other and obtain the spatial and spectral features for improved classifications. This paper proposes to use multi-temporal data form Sentinel-1 (VV & VH polarization) and Sentinel-2 sensors for the fusion and classification over an agricultural area. Convolutional Neural Network (CNN)- based Pyramid method for fusion and Bayesian Optimized 2-D CNN for classification of fused multi-temporal data was used to extract spatial-spectral information. Results in terms of classification accuracy suggests slightly better performance by VV polarized fused images than the VH and also suggests an improved performance by multi-temporal data in comparison to the single date data over the study area.","","978-1-6654-4249-7","10.1109/InGARSS51564.2021.9791998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9791998","Fusion;Classification;Convolutional Neural Network (CNN);Bayesian Optimization","Satellites;Optical polarization;Neural networks;Sensor fusion;Optical imaging;Bayes methods;Convolutional neural networks","agriculture;convolutional neural nets;geophysical image processing;image classification;image fusion;remote sensing;synthetic aperture radar","multitemporal Sentinel-1 satellite data;multitemporal Sentinel-2 satellite data;Bayesian optimized 2D CNN;Sentinel-2 sensors;VV & VH polarization;spectral features;spatial features;complimentary information;image fusion;optical data;CNN-based fusion;single date data;VV polarized fused images;classification accuracy;spatial-spectral information;fused multitemporal data;convolutional neural network","","","","10","IEEE","13 Jun 2022","","","IEEE","IEEE Conferences"
"Multisource Data Fusion for the Detection of Settlements Without Electricity","Y. Ma; Y. Li; K. Feng; X. Geng; L. Jiao; F. Liu; Y. Yang","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","1839","1842","The international charity SolarAid aims to provide access to lights in areas without electricity, and it is a challenge to accurately and efficiently transmit the lights to the areas in need. Multisource, multitemporal, and multimodal remote sensing images can provide rich information about the target area, so using multisource remote sensing images for accurate detection of human settlements without electricity is a feasible solution. In this paper two separate detection tasks are formulated: building two attention SENet for settlements detection and light detection using the Sentinel-2 dataset and the Suomi Visible Infrared Imaging Radiometer Suite (VIIRS) night time dataset, respectively. In addition, we study a new outlier removal method based on the pixel distribution characteristics of the VIIRS dataset for data pre-processing, and propose a post-processing method based on region continuity for further correction of the results. Experiments show that our method can maximize the use of multisource data information and rank first in the detection of settlements without electricity challenge track (Track DSE) of the 2021 IEEE GRSS Data Fusion Contest.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553860","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553860","multisource data fusion;attention mechanism;outlier removal;Sentinel-2;VIIRS","Buildings;Data integration;Infrared imaging;Radiometry;Task analysis;Remote sensing","geophysical image processing;image classification;image fusion;infrared imaging;remote sensing;terrain mapping","IEEE GRSS Data Fusion Contest;electricity challenge track;multisource data information;post-processing method;data pre-processing;VIIRS dataset;outlier removal method;Suomi Visible Infrared Imaging Radiometer Suite night time dataset;Sentinel-2 dataset;light detection;settlements detection;detection tasks;human settlements;multisource remote sensing images;multimodal remote sensing images;international charity SolarAid;multisource data fusion","","2","","6","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"On the Characterization of Sen2Like Surface Reflectance Data Harmonization and Fusion Processes","S. Saunier; V. Debaecker; J. Louis; K. Garcia; C. Cuny; E. G. Cadau; V. Boccia; F. Gascon","Telespazio, France; Telespazio, France; Telespazio, France; Telespazio, France; Telespazio, France; Serco SpA c/o ESRIN, Italy; European Space Agency, ESRIN, Italy; European Space Agency, ESRIN, Italy","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4636","4639","There are growing and evolving expectations from the Earth Observation (EO) Community for merging an increasing number of input data streams recorded with the various space based Multi-Spectral (MS) High Resolution (HR) EO mission instruments. The various agencies and industries are now delivering Analysis Ready Data (ARD) aiming at stacking together multi source data. One critical objective of ARD is to enable the monitoring of rapid and subtle changes. This category of applications sets some strong requirements regarding the quality of the surface reflectance harmonization in the time series. In the context of Sentinel-2 / Landsat 8, this paper proposes a comprehensive analysis of the Sen2Like processing. It shows that it is possible to increase significantly the geometric coregistration accuracy and to ensure an appropriate cross calibration. However, some issues persist and are due to directional effects. These are partially corrected with the current approach but remain a major source of noise in time series.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553037","Analysis Ready Data;Sentinel-2;Landsat- 8;Sen2Like;processing;harmonization;validation","Reflectivity;Earth;Space missions;Time series analysis;Stacking;Software algorithms;Production","calibration;geophysical image processing;image fusion;image resolution;remote sensing;sensor fusion;time series","input data streams;space based MultiSpectral;Analysis Ready Data;multisource data;rapid changes;strong requirements;surface reflectance harmonization;time series;Sentinel-2;Sen2Like;surface reflectance data harmonization;fusion processes;Earth Observation Community;EO","","","","19","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A Deep Learning-Based Heterogeneous Spatio-Temporal-Spectral Fusion: SAR and Optical Images","M. Jiang; J. Li; H. Shen","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","1252","1255","Image fusion is a powerful means to integrate complementary spatio-temporal-spectral information among multi-source remote sensing images. The existing remote sensing image fusion is mostly limited to the fusion between optical images, and most of them are limited to the fusion between two sensors. Based on this, this paper proposes a heterogeneous spatio-temporal-spectral fusion method based on deep learning. Specifically, it combines the low-spatial-resolution (LR) cloudy image with the high-spatial-resolution (HR) SAR images and the HR cloud-free optical image to remove the clouds and improve the spatial resolution of the LR cloudy image. The SAR image is acquired at the same date as the LR cloudy image, while the HR cloud-free image is acquired at another date. Experiments are performed on the images of Landsat 8, Sentinel-1, and Sentinel-2. The experimental results show that the proposed method can effectively achieve the joint goal of spatial resolution improvement and cloud removal of the Landsat image.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554031","National Natural Science Foundation of China(grant numbers:62071341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554031","Heterogeneous;SAR;optical;cloud-removal;spatial resolution improvement","Earth;Artificial satellites;Clouds;Optical imaging;Radar polarimetry;Optical sensors;Spatial resolution","deep learning (artificial intelligence);geophysical image processing;geophysical techniques;image fusion;image resolution;optical images;remote sensing;synthetic aperture radar","Sentinel-1;Sentinel-2;Landsat image;spatial resolution improvement;HR cloud-free image;SAR image;LR cloudy image;HR cloud-free optical image;high-spatial-resolution SAR images;low-spatial-resolution cloudy image;heterogeneous spatio-temporal-spectral fusion method;remote sensing image fusion;multisource remote sensing images;complementary spatio-temporal-spectral information;optical images;deep learning-based heterogeneous spatio-temporal-spectral fusion","","3","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Model-Based Reduced-Rank Pansharpening","F. Palsson; M. O. Ulfarsson; J. R. Sveinsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Geoscience and Remote Sensing Letters","25 Mar 2020","2020","17","4","656","660","Observation of the Earth using satellites mounted with optical sensors is an important application of remote sensing. Owing to physical constraints, multispectral (MS) sensors acquire images of lower spatial resolution than a single-band panchromatic (PAN) sensor that acquires images of the same scene. Pansharpening fuses the MS and PAN images to obtain an MS image with the same spatial resolution as the PAN image. In this letter, we propose to expand a method, initially developed for Sentinel-2 single-sensor sharpening, for pansharpening. The expanded method is based on solving a non-convex MS acquisition model using optimization methods based on cyclic decent and manifold optimization. The tuning parameters of the method are chosen using Bayesian optimization with reduced-scale evaluation. The proposed method is compared with a number of established pansharpening methods and is validated using both synthetic and real data sets.","1558-0571","","10.1109/LGRS.2019.2926681","Icelandic Research Fund(grant numbers:174075-05); Research Fund of the University of Iceland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8778736","Cyclic descent;data fusion;image fusion;pansharpening","Optimization;Optical sensors;Spatial resolution;Image sensors;Sensor fusion","Bayes methods;geophysical image processing;image fusion;image resolution;optical sensors;optimisation;remote sensing","model-based reduced-rank pansharpening;optical sensors;remote sensing;physical constraints;multispectral sensors;lower spatial resolution;single-band panchromatic sensor;pansharpening fuses;MS image;PAN image;Sentinel-2 single-sensor sharpening;expanded method;nonconvex MS acquisition model;cyclic decent optimization;manifold optimization;Bayesian optimization;reduced-scale evaluation;pansharpening methods","","25","","14","IEEE","29 Jul 2019","","","IEEE","IEEE Journals"
"Blending Ultra Spectral Images of Multi-Source Remote Sensors","V. S. Chilkuri; D. Bharathi; R. Karthi","Department of Computer Science and Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Department of Computer Science and Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Department of Computer Science and Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India","2022 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)","30 Dec 2022","2022","","","1","5","Images and procedures from remote sensing are effective tools for studying the earth's surface. Data quality is essential for improving remote sensing applications and producing crisp, noise-free images. Due to the different gathering methods, obtaining a free set of data is quite challenging in most cases. So, picture or information fusion is crucial in far-flung sensing applications. Spatiotemporal fusion (STF) is a method for fusing pix with the proper temporal and spatial decision by integrating (temporally dense) coarse-resolution pictures with (temporally sparse) fine-resolution pictures. This paper makes a specialty of enforcing spatiotemporal fusion of multi-supply remote sensing pictures. In this paper, STF of multi-source remote sensing images, specifically Landsat and Sentinel sensors images is performed using the ESTARFM fusion method. In total 2 experiments were conducted with Landsat 7, Landsat 8, and Sentinel 2 data. In experiment 1 Landsat 7, and Sentinel 2 images are considered fine and coarse resolution images respectively, and in experiment 2 Landsat 8, and Sentinel 2 as fine and coarse resolution images. The metrics suggest that by applying the STF method, the similarity between the fused image and the original image at the prediction time of experiment 2 is more when compared to the corresponding image results of experiment 1.","","978-1-6654-7095-7","10.1109/ICECCME55909.2022.9988603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9988603","Spatiotemporal Fusion;Coarse resolution;Fine resolution;ESTARFM","Earth;Image sensors;Artificial satellites;Satellites;Sensor fusion;Prediction algorithms;Sensors","geophysical image processing;geophysical techniques;hydrological techniques;image fusion;image resolution;remote sensing","blending ultra spectral images;coarse resolution images;coarse-resolution pictures;corresponding image results;data quality;different gathering methods;earth;ESTARFM fusion method;experiment 1 Landsat 7;experiment 2 Landsat 8;far-flung sensing applications;fine resolution images;fine-resolution pictures;fused image;multisource remote sensing images;multisource remote sensors;multisupply remote sensing pictures;noise-free images;proper temporal decision;remote sensing applications;Sentinel 2 data;Sentinel 2 images;spatial decision;spatiotemporal fusion;STF method;time 2.0 as;total 2 experiments","","","","9","IEEE","30 Dec 2022","","","IEEE","IEEE Conferences"
"Spatiotemporal Fusion With Only Two Remote Sensing Images as Input","J. Wu; Q. Cheng; H. Li; S. Li; X. Guan; H. Shen","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences and the Collaborative Innovation Center for Geospatial Technology, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","22 Oct 2020","2020","13","","6206","6219","Spatiotemporal data fusion is an effective way of generating a dense time series with a high spatial resolution. Traditionally, the spatiotemporal fusion models, especially the popular ones such as the spatial and temporal adaptive reflectance fusion model, require at least three images as input, i.e., a coarse-resolution image on the target date and a pair of fine- and coarse-resolution images on the reference date. However, this cannot always be satisfied, as the high-quality coarse-resolution image on the reference date may be unavailable in some application scenarios. This led to efforts to achieve data fusion only using the other two images as input. In this article, we proposed an effective strategy that can be combined with any spatiotemporal fusion model to accomplish the fusion with simplified input. To confirm the validity of the method, we comprehensively compared the fusion performances under the two input modalities. In total, 38 tests were conducted with Moderate Resolution Imaging Spectroradiometer (MODIS), Landsat, and Sentinel-2 land surface reflectance products. Results suggest that by applying the proposed method, the fusion performance with only two input images is comparable or even superior to that with three input images. This article challenges the stereotype that spatiotemporal data fusion strictly needs at least three input images. The proposed method extends the application scenarios of spatiotemporal fusion, and creates opportunities to fuse sensors with barely overlapping temporal coverages, such as the Landsat 8 Operational Land Imager and the Sentinel-2 MultiSpectral Instrument.","2151-1535","","10.1109/JSTARS.2020.3028116","National Key R&D Program of China(grant numbers:2018YFB2100501); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9210773","Landsat 8;Moderate Resolution Imaging Spectroradiometer (MODIS);Sentinel-2;simplified input modality;spatiotemporal data fusion;two input images","Spatiotemporal phenomena;Spatial resolution;Remote sensing;Sensors;Artificial satellites;Earth;Data integration","geophysical image processing;image fusion;image resolution;remote sensing;sensor fusion;spatiotemporal phenomena;time series","input modalities;Sentinel-2 land surface reflectance products;spatiotemporal data fusion;Landsat 8 Operational Land Imager;remote sensing images;high spatial resolution;spatial reflectance fusion model;temporal adaptive reflectance fusion model;target date;reference date;high-quality coarse-resolution image;spatiotemporal fusion model","","6","","51","CCBY","1 Oct 2020","","","IEEE","IEEE Journals"
"Deep-Learning-Based Spatio-Temporal-Spectral Integrated Fusion of Heterogeneous Remote Sensing Images","M. Jiang; H. Shen; J. Li","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences and the Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2022","2022","60","","1","15","It is a challenging task to integrate the spatial, temporal, and spectral information of multisource remote sensing images, especially in the case of heterogeneous images. To this end, for the first time, this article proposes a heterogeneous integrated framework based on a novel deep residual cycle generative adversarial network (GAN). The proposed network consists of a forward fusion part and a backward degeneration feedback part. The forward part generates the desired fusion result from the various observations; the backward degeneration feedback part considers the imaging degradation process and regenerates the observations inversely from the fusion result. The heterogeneous integrated fusion framework supported by the proposed network can simultaneously merge the complementary spatial, temporal, and spectral information of multisource heterogeneous observations to achieve heterogeneous spatiospectral fusion, spatiotemporal fusion, and heterogeneous spatiotemporal–spectral fusion. Furthermore, the proposed heterogeneous integrated fusion framework can be leveraged to relieve the two bottlenecks of land-cover change and thick cloud cover. Thus, the inapparent and unobserved variation trends of surface features, which are caused by the low-resolution imaging and cloud contamination, can be detected and reconstructed well. Images from many different remote sensing satellites, i.e., Moderate Resolution Imaging Spectroradiometer (MODIS), Landsat 8, Sentinel-1, and Sentinel-2, were utilized in the experiments conducted in this study, and both the qualitative and quantitative evaluations confirmed the effectiveness of the proposed image fusion method.","1558-0644","","10.1109/TGRS.2022.3188998","National Natural Science Foundation of China(grant numbers:42130108,62071341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829539","Deep residual cycle generative adversarial network (GAN);heterogeneous integrated framework;land-cover change;thick cloud cover","Generators;Remote sensing;Spatial resolution;Generative adversarial networks;Feature extraction;Image fusion;Optical sensors","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);remote sensing;sensor fusion;spatiotemporal phenomena","imaging degradation process;heterogeneous integrated fusion framework;complementary spatial information;temporal, information;spectral information;multisource heterogeneous observations;heterogeneous spatiospectral fusion;spatiotemporal fusion;heterogeneous spatiotemporal-spectral fusion;low-resolution imaging;cloud contamination;different remote sensing satellites;Moderate Resolution Imaging Spectroradiometer;image fusion method;spatio-temporal-spectral integrated fusion;heterogeneous remote sensing images;multisource remote sensing images;heterogeneous images;heterogeneous integrated framework;deep residual cycle generative adversarial network;forward fusion part;backward degeneration feedback part;forward part;desired fusion result","","","","54","IEEE","14 Jul 2022","","","IEEE","IEEE Journals"
"Spectral–Temporal Fusion of Satellite Images via an End-to-End Two-Stream Attention With an Effective Reconstruction Network","T. Benzenati; Y. Kessentini; A. Kallel","LIMOSE Laboratory, University of Boumerdes, Boumerdes, Algeria; Laboratory of Signals, Systems, Artificial Intelligence and Networks Sfax, Digital Research Centre of Sfax, Sfax, Tunisia; Laboratory of Signals, Systems, Artificial Intelligence and Networks Sfax, Digital Research Centre of Sfax, Sfax, Tunisia","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","23 Jan 2023","2023","16","","1308","1320","Due to technical and budget constraints on current optical satellites, the acquisition of satellite images with the best resolutions is not practicable. In this article, aiming to produce products with high spectral (HS) and temporal resolutions, we introduced a two-stream spectral–temporal fusion technique based on attention mechanism called STA-Net. STA-Net aims to combine high spectral and low temporal (HSLT) resolution images with low spectral and high temporal (LSHT) resolution images to generate products with the best characteristics. The proposed technique involves two stages. In the first one, two fused images are generated by a two-stream architecture based on residual attention blocks. The temporal difference estimator stream estimates the temporal difference between HS images at desired and neighboring dates. The reflectance difference estimator is the second stream. It predicts the reflectance difference between the input images (HS–LS) to map LS images into HS products. In the second stage, a reconstruction network combines the latter two-stream outputs via an effective learnable weighted-sum strategy. The two-stage model is trained in an end-to-end fashion using an effective loss function to ensure the best fusion quality. To the best of our knowledge, this work represents the first attempt to address the spectral–temporal fusion using an end-to-end deep neural network model. Experimental results conducted on two actual datasets of Sentinel-2 (HSLT:10 spectral bands and long revisit period) and Planetscope (LSHT: four spectral bands and daily images) images, which proved the effectiveness of the proposed technique with respect to baseline technique.","2151-1535","","10.1109/JSTARS.2023.3234722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10008044","Attention mechanism;convolutional neural network (CNN);image fusion;multisensor image fusion;Planetscope;Sentinel-2;spectral–temporal fusion","Image resolution;Spatial resolution;Satellites;Monitoring;Earth;Remote sensing;Image reconstruction","","","","","","61","CCBY","6 Jan 2023","","","IEEE","IEEE Journals"
"Sar And Optical Data Fusion Based On Anisotropic Diffusion With Pca And Classification Using Patch-Based Svm With Lbp","A. Shakya; M. Biswas; M. Pal","Computer Engineering Department, National Institute of Technology, Kurukshetra, India; Computer Engineering Department, National Institute of Technology, Kurukshetra, India; Civil Engineering Department, National Institute of Technology, Kurukshetra, India","2020 IEEE India Geoscience and Remote Sensing Symposium (InGARSS)","23 Feb 2021","2020","","","25","28","SAR (VV and VH polarization) and optical data are widely used in image fusion to use the complimentary information of each other and to obtain the better-quality image (in terms of spatial and spectral features) for the improved classification results. The optical data acquisition depends on whether conditions while SAR data can acquire the data in presence of clouds. This paper uses anisotropic diffusion with PCA for the fusion of SAR (Sentinel 1 (S1)) and Optical (Sentinel 2 (S2)) data for patch-based SVM Classification with LBP (LBP-PSVM). Fusion results with VV polarization performed better than VH polarization using considered fusion method. Classification results suggests that the LBP-PSVM classifier is more effective in comparison to SVM and PSVM classifiers for considered data.","","978-1-7281-3114-6","10.1109/InGARSS48198.2020.9358949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9358949","Fusion;SVM;Anisotropic Diffusion;Principal Component Analysis;Local Binary Pattern","Support vector machines;Optical polarization;Optical imaging;Geometrical optics;Optical sensors;Synthetic aperture radar;Principal component analysis","feature extraction;geophysical image processing;image classification;image fusion;principal component analysis;radar imaging;remote sensing by radar;support vector machines;synthetic aperture radar","optical data fusion;anisotropic diffusion;VH polarization;image fusion;complimentary information;better-quality image;spatial features;spectral features;improved classification results;optical data acquisition;SAR data;PCA;VV polarization;fusion method;LBP-PSVM classifier;patch-based SVM classification","","1","","15","IEEE","23 Feb 2021","","","IEEE","IEEE Conferences"
"$M^3\text{Fusion}$: A Deep Learning Architecture for Multiscale Multimodal Multitemporal Satellite Data Fusion","P. Benedetti; D. Ienco; R. Gaetano; K. Ose; R. G. Pensa; S. Dupuy","UMR-TETIS Laboratory, IRSTEA, University of Montpellier, Montpellier, France; LIRMM Laboratory, Montpellier, France; UMR TETIS, University of Montpellier, AgroParisTech, CIRAD, CNRS, IRSTEA, Montpellier, France; UMR-TETIS Laboratory, IRSTEA, University of Montpellier, Montpellier, France; Department of Computer Science, University of Turin, Turin, Italy; UMR TETIS, University of Montpellier, AgroParisTech, CIRAD, CNRS, IRSTEA, Montpellier, France","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Jan 2019","2018","11","12","4939","4949","Modern Earth Observation systems provide remote sensing data at different temporal and spatial resolutions. Among all the available spatial mission, today the Sentinel-2 program supplies high temporal (every five days) and high spatial resolution (HSR) (10 m) images that can be useful to monitor land cover dynamics. On the other hand, very HSR (VHSR) imagery is still essential to figure out land cover mapping characterized by fine spatial patterns. Understanding how to jointly leverage these complementary sources in an efficient way when dealing with land cover mapping is a current challenge in remote sensing. With the aim of providing land cover mapping through the fusion of multitemporal HSR and VHSR satellite images, we propose a suitable end-to-end deep learning framework, namely M3Fusion, which is able to simultaneously leverage the temporal knowledge contained in time series data as well as the fine spatial information available in VHSR images. Experiments carried out on the Reunion Island study area confirm the quality of our proposal considering both quantitative and qualitative aspects.","2151-1535","","10.1109/JSTARS.2018.2876357","Agence Nationale de la Recherche; Investments for the Future Program(grant numbers:ANR-16-CONV-0004 (DigitAg)); GEOSUD project(grant numbers:ANR-10-EQPX-20); Programme National de Télédétection Spatiale; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516352","Data fusion;deep learning;land cover mapping;satellite image time series;sentinel-2;very high spatial resolution (VHSR)","Time series analysis;Spatial resolution;Remote sensing;Feature extraction;Satellites","geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing;terrain mapping;time series","Sentinel-2 program supplies high temporal;high spatial resolution images;land cover dynamics;HSR imagery;land cover mapping;fine spatial patterns;suitable end-to-end deep learning framework;temporal knowledge;time series data;fine spatial information;VHSR images;multiscale multimodal multitemporal satellite data Fusion;Modern Earth Observation systems;remote sensing data;different temporal resolutions;spatial resolutions;available spatial mission","","47","","33","IEEE","31 Oct 2018","","","IEEE","IEEE Journals"
"Generalized Linear Spectral Mixing Model for Spatial–Temporal–Spectral Fusion","J. Zhou; W. Sun; X. Meng; G. Yang; K. Ren; J. Peng","Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Hubei Key Laboratory of Applied Mathematics, Faculty of Mathematics and Statistics, Hubei University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","18 Jul 2022","2022","60","","1","16","Image fusion effectively solves the trade-off among spatial resolution, temporal resolution, and spectral resolution of remote sensing sensors. However, most of existing methods focus on the fusion of two of the spatial, temporal, and spectral metrics of remote sensing images. The few spatial–temporal–spectral fusion (STSF) methods available are mainly for fusing Moderate Resolution Imaging Spectroradiometer (MODIS) and Landsat images, which are not suitable for the characteristics of the spaceborne hyperspectral images (HSIs) with low temporal resolution, such as Hyperion, ZY-1 02D, and PRISMA. For this purpose, we proposed a novel generalized linear spectral mixing model for STSF (GLMM-STSF). In the method, the GLMM is introduced into the STSF problem, and the temporal variations of images at different times are transferred to the endmember and abundance matrix variations of images for estimation. To the best of our knowledge, for the first time, the STSF task of remote sensing images is handled from the perspective of spectral unmixing. Compared with the existing STSF fusion methods, our method targets the task of fusing spaceborne HSI with low temporal and spatial resolutions with multispectral image (MSI) featured by high temporal and spatial resolutions. Taking the STSF of ZY-1 02D hyperspectral and Sentinel-2 multispectral real datasets as an example, comparisons with related state-of-the-art methods demonstrate that our proposed method achieves superior fusion performance.","1558-0644","","10.1109/TGRS.2022.3188501","National Natural Science Foundation of China(grant numbers:42122009,41971296,42171326,41801252,41801256,61871177,42171351); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LR19D010001,LY22F010014,LQ18D010001); Natural Science Foundation of Hubei Province(grant numbers:2021CFA087); Public Science and Technology Plan Projects of Ningbo City(grant numbers:2021S089); Science and Technology Innovation 2025 Major Project of Ningbo City(grant numbers:2021Z107,2022Z032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815322","Hyperspectral;image fusion;multispectral;spatial resolution;spectral resolution;spectral unmixing;temporal resolution","Spatial resolution;Tensors;Task analysis;Sensors;Hyperspectral imaging;Sparse matrices;Earth","geophysical image processing;geophysical techniques;hyperspectral imaging;image fusion;image resolution;radiometry;remote sensing","generalized linear spectral mixing model;image fusion;trade-off among spatial resolution;spectral resolution;remote sensing sensors;spatial metrics;temporal, metrics;spectral metrics;remote sensing images;spatial-temporal-spectral fusion methods;spaceborne hyperspectral images;low temporal resolution;GLMM-STSF;STSF problem;endmember;STSF task;spectral unmixing;existing STSF fusion methods;low temporal resolutions;spatial resolutions;multispectral image;high temporal resolutions;ZY-1 02D hyperspectral;related state-of-the-art methods;superior fusion performance","","2","","70","IEEE","5 Jul 2022","","","IEEE","IEEE Journals"
"Automatic Area-Based Registration of Optical and SAR Images Through Generative Adversarial Networks and a Correlation-Type Metric","L. Maggiolo; D. Solarna; G. Moser; S. B. Serpico","University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2089","2092","The automatic registration of multisensor remote sensing images is a highly challenging task due to the inherently different physical, statistical, and textural properties of the input data. In the present paper, this problem is addressed in the case of optical-SAR images by proposing a novel method based on deep learning and area-based registration concepts. The method integrates a conditional generative adversarial network (cGAN), an area-based cross-correlation-type l2 similarity metric, and the COBYLA constrained maximization algorithm. Whereas correlation-type metrics are typically ineffective in the application to multisensor registration, the proposed approach allows exploiting the image translation capabilities of cGAN architectures to enable the use of an l2 similarity metric, which favors high computational efficiency. Experiments with Sentinel-1 and Sentinel-2 data suggest the effectiveness of this strategy and the capability of the proposed method to achieve accurate registration.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323235","European Space Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323235","Multisensor image registration;conditional generative adversarial network;$\ell^{2}$ similarity;COBYLA","Radar polarimetry;Optical imaging;Measurement;Optical sensors;Feature extraction;Training;Generative adversarial networks","geophysical image processing;geophysical signal processing;image classification;image fusion;image registration;radar imaging;remote sensing;remote sensing by radar;sensor fusion;synthetic aperture radar","automatic area-based registration;generative adversarial networks;correlation-type metric;automatic registration;multisensor remote sensing images;textural properties;optical-SAR images;deep learning;area-based registration concepts;conditional generative adversarial network;area-based cross-correlation-type l;image translation capabilities;Sentinel-2 data","","4","","14","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Stepwise Refinement Of Low Resolution Labels For Earth Observation Data: Part 1","D. Cerra; N. Merkle; C. Henry; K. Alonso; P. d’Angelo; S. Auer; R. Bahmanyar; X. Yuan; K. Bittner; M. Langheinrich; G. Zhang; M. Pato; J. Tian; P. Reinartz","German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen; German Aerospace Center (DLR), Remote Sensing Technology Institute, Oberpfaffenhofen","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","27 Sep 2021","2020","","","7054","7057","This paper describes the contribution of the DLR team ranking 3rd in Track 1 of the 2020 IEEE GRSS Data Fusion Contest, with results ranking 2nd in Track 2 of the same contest being reported in a companion paper. The classifications are based on refinements of low-resolution MODIS labeling using available higher resolution Sentinel-1 and Sentinel-2 data. Results are initialized with a handcrafted decision tree integrating output from a random forest classifier, and subsequently boosted by detectors for specific classes.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9547213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547213","","Earth;Data integration;Geoscience and remote sensing;Detectors;Labeling;Decision trees;Spatial resolution","geophysical image processing;image classification;image fusion;image resolution;terrain mapping","stepwise refinement;low resolution labels;Earth observation data;DLR team;2020 IEEE GRSS Data Fusion Contest;Sentinel-1 data;Sentinel-2 data;handcrafted decision tree","","2","","8","IEEE","27 Sep 2021","","","IEEE","IEEE Conferences"
"A Dynamic End-to-End Fusion Filter for Local Climate Zone Classification Using SAR and Multi-Spectrum Remote Sensing Data","P. Feng; Y. Lin; G. He; J. Guan; J. Wang; H. Shi","State Key Laboratory of Space-Ground Integrated Information Technology, CAST, Beijing, China; Group of Intelligent Signal Processing, Harbin Engineering University, Harbin, China; State Key Laboratory of Space-Ground Integrated Information Technology, CAST, Beijing, China; Group of Intelligent Signal Processing, Harbin Engineering University, Harbin, China; State Key Laboratory of Space-Ground Integrated Information Technology, CAST, Beijing, China; State Key Laboratory of Space-Ground Integrated Information Technology, CAST, Beijing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","4231","4234","Local Climate Zone (LCZ) classification is potentially popular because of its extensive applications. Recently, data from different remote sensors including synthetic aperture radar (SAR) and multi-spectrum are employed for LCZ classification. However, different bands in SAR and multi-spectrum are difficult to fuse because of their various physical properties. In this paper, an dynamic end-to-end fusion filter is proposed. Firstly, a convolutional neural network (CNN) based dynamic filter network (DFN) is introduced to integrate different bands in SAR and multi-spectrum data, which enhances the fusion accuracy by a flexible dynamic operation. Then the filter is used for feature extraction, hence improve the performance of the classifier. The proposed method is evaluated using Sentinel-1 and Sentinel-2 dataset and the improvement of accuracy shows the superiority of the proposed dynamic data fusion approach.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324427","National Natural Science Foundation of China(grant numbers:61806018,41801291); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324427","Dynamic filter network;Local Climate Zone Classification;Multi-spectrum;End-to-end;Fusion","Feature extraction;Synthetic aperture radar;Meteorology;Remote sensing;Fuses;Convolution;Vegetation","atmospheric techniques;atmospheric temperature;convolutional neural nets;feature extraction;geophysical image processing;image classification;image filtering;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar","SAR;LCZ classification;dynamic end-to-end fusion filter;dynamic filter network;flexible dynamic operation;dynamic data fusion approach;multispectrum remote sensing data;Local Climate Zone classification;remote sensors;feature extraction;Sentinel-1 and Sentinel-2 dataset","","1","","13","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Resolution Enhancement of Unsupervised Classification Maps Through Data Fusion of Spectral and Visible Images from Different Sensing Instruments","F. Kizel","Department of Mapping and Geoinformation Engineering, Technion-Israel Institute of Technology","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2887","2890","We propose a new methodology for enhancing the spatial resolution of unsupervised classification through a fusion of multispectral and visible images. The new method, DFuSIAL-C (Data Fusion through Spatial Information-Aided Learning for Classification), relies on automatically extracted invariant points (IPs), assumed to have the same land cover type in the two data sources. In contrast to typical methods, DFuSIAL-C does not require a full spatial, spectral, and temporal overlapping between the data sources and allows for the fusion of data from different sensors. An evaluation of the proposed method, compared to a state-of-the-art pansharpening fusion method, is carried out using Landsat-8 and Sentinel-2 images. Our experimental results show that the DFuSIAL-C obtains unsupervised classification maps with a significantly enhanced spatial resolution and an overall accuracy (OA) of 85%. Furthermore, we show that the proposed method is preferable when full overlapping is not available due to the acquisition by different instruments.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555098","Spectral Remote Sensing;Classification;Data Fusion;Spatial Information;Pansharpening;Machine Learning;Neural Networks","Earth;Artificial satellites;Instruments;Data integration;Pansharpening;Sensor fusion;Spatial databases","feature extraction;geophysical image processing;image classification;image fusion;image resolution;remote sensing","sensing instruments;resolution enhancement;enhanced spatial resolution;unsupervised classification maps;Sentinel-2 images;state-of-the-art pansharpening fusion method;data sources;land cover type;automatically extracted invariant points;Spatial Information-Aided;Data Fusion;DFuSIAL-C;visible images","","","","21","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Evaluation of four Spatiotemporal Gap-Filling Methods in Crop Phenology Monitoring","C. Wang; T. He","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","6061","6064","The high spatial resolution land surface phenology (LSP) monitoring is often limited by the temporal discontinuity of high spatial resolution observations. Many gap-filling methods have been proposed for LSP monitoring, however, a thorough intercomparison and evaluation is still lacking. Four widely-used methods, including the Spatial and Temporal Adaptive Reflectance Fusion Model (STARFM), the Flexible Spatiotemporal DAta Fusion (FSDAF), Multi-year based model, and Spatiotemporal Shape Matching Model (SSMM) were selected to extract green-up date (GUD) based on the Harmonized Landsat and Sentinel-2 (HLS) dataset. The results of the four methods show consistency with those of PhenoCam sites (R2>0.64) and there is a lag phenomenon. Compared within 3×3 VIIRS pixel window, the difference between mean VIIRS GUDs and aggregated 30m GUDs is small and the mean absolute difference is less than 7 days. Comprehensively, SSMM has high consistency (R2=0.75) and smaller bias (Bias=7.3days), which shows more potential in LSP monitoring.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884875","National Natural Science Foundation of China(grant numbers:42090012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884875","land surface phenology;green-up date;HLS;gap-filling","Adaptation models;Uncertainty;Shape;Land surface;Data models;Spatiotemporal phenomena;Data mining","crops;geophysical image processing;geophysical techniques;image fusion;image resolution;phenology;radiometry;remote sensing;sensor fusion;spatiotemporal phenomena;vegetation;vegetation mapping","Spatiotemporal gap-filling methods;crop phenology;high spatial resolution land surface phenology monitoring;temporal discontinuity;high spatial resolution observations;LSP monitoring;Flexible Spatiotemporal DAta Fusion;Multiyear based model;Spatiotemporal Shape Matching Model;Sentinel-2 dataset;3×3 VIIRS pixel window;high consistency;size 30.0 m;time 7.0 d;time 7.3 d","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Deep Self-Paced Residual Network for Multispectral Images Classification Based on Feature-Level Fusion","J. Zhang; D. Zhang; W. Ma; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center of Intelligent Perception and Computation, International Collaboration Joint Laboratory in Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center of Intelligent Perception and Computation, International Collaboration Joint Laboratory in Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center of Intelligent Perception and Computation, International Collaboration Joint Laboratory in Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center of Intelligent Perception and Computation, International Collaboration Joint Laboratory in Intelligent Perception and Computation, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","2 Nov 2018","2018","15","11","1740","1744","The classification methods based on fusion techniques of multisource multispectral (MS) images have been studied for a long time. However, it may be difficult to classify these data based on a feature level while avoiding the inconsistency of data caused by multisource and multiple regions or cities. In this letter, we propose a deep learning structure called 2-branch SPL-ResNet which combines the self-paced learning with deep residual network to classify multisource MS data based on the feature-level fusion. First, a 2-D discrete wavelet is used to obtain the multiscale features and sparse representation of MS data. Then, a 2-branch SPL-ResNet is established to extract respective characteristics of the two satellites. Finally, we implement the feature-level fusion by cascading the two feature vectors and then classify the integrated feature vector. We conduct the experiments on Landsat_8 and Sentinel_2 MS images. Compared with the commonly used classification methods such as support vector machine and convolutional neural networks, our proposed 2-branch SPL-ResNet framework has higher accuracy and more robustness.","1558-0571","","10.1109/LGRS.2018.2854847","Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438103,91438201); National Natural Science Foundation of China(grant numbers:61472306,61573267); Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project)(grant numbers:B07048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424470","Deep residual network (ResNet);image classification;image fusion;multisource multispectral (MS) data;self-paced learning (SPL)","Feature extraction;Satellites;Fuses;Data mining;Urban areas;Remote sensing;Support vector machines","convolution;discrete wavelet transforms;feature extraction;feedforward neural nets;geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;learning (artificial intelligence);support vector machines;vectors","2-branch SPL-ResNet framework;deep self-paced residual network;multispectral images classification;feature-level fusion;fusion techniques;multisource multispectral images;multiple regions;cities;deep learning structure;multisource MS data;2-D discrete wavelet;multiscale features;integrated feature vector;Sentinel_2 MS images;self-paced learning;sparse representation;satellites;Landsat_8;support vector machine;convolutional neural networks","","7","","15","IEEE","2 Aug 2018","","","IEEE","IEEE Journals"
"Sentinel-1-Based Water and Flood Mapping: Benchmarking Convolutional Neural Networks Against an Operational Rule-Based Processing Chain","M. Helleis; M. Wieland; C. Krullikowski; S. Martinis; S. Plank","German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","8 Mar 2022","2022","15","","2023","2036","In this study, the effectiveness of several convolutional neural network architectures (AlbuNet-34/FCN/DeepLabV3+/U-Net/U-Net++) for water and flood mapping using Sentinel-1 amplitude data is compared to an operational rule-based processor (S-1FS). This comparison is made using a globally distributed dataset of Sentinel-1 scenes and the corresponding ground truth water masks derived from Sentinel-2 data to evaluate the performance of the classifiers on a global scale in various environmental conditions. The impact of using single versus dual-polarized input data on the segmentation capabilities of AlbuNet-34 is evaluated. The weighted cross entropy loss is combined with the Lovász loss and various data augmentation methods are investigated. Furthermore, the concept of atrous spatial pyramid pooling used in DeepLabV3+ and the multiscale feature fusion inherent in U-Net++ are assessed. Finally, the generalization capacity of AlbuNet-34 is tested in a realistic flood mapping scenario by using additional data from two flood events and the Sen1Floods11 dataset. The model trained using dual polarized data outperforms the S-1FS significantly and increases the intersection over union (IoU) score by 5%. Using a weighted combination of the cross entropy and the Lovász loss increases the IoU score by another 2%. Geometric data augmentation degrades the performance while radiometric data augmentation leads to better testing results. FCN/DeepLabV3+/U-Net/U-Net++ perform not significantly different to AlbuNet-34. Models trained on data showing no distinct inundation perform very well in mapping the water extent during two flood events, reaching IoU scores of 0.96 and 0.94, respectively, and perform comparatively well on the Sen1Floods11 dataset.","2151-1535","","10.1109/JSTARS.2022.3152127","German Federal Ministry of Education and Research; Künstliche Intelligenz zur Analyse von Erdbeobachtungs- und Internetdaten zur Entscheidungsunterstützung im Katastrophenfall(grant numbers:13N15525); Helmholtz Artificial Intelligence Cooperation Unit; AI for Near Real Time Satellite-based Flood Response(grant numbers:ZT-IPF-5-39); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714780","Convolutional neural networks;data augmentation;semantic segmentation;Sen1Floods11;Sentinel-1;Sentinel-2;surface water monitoring","Floods;Data models;Synthetic aperture radar;Convolutional neural networks;Training;Sea surface;Thresholding (Imaging)","convolutional neural nets;emergency management;entropy;feature extraction;floods;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);neural nets;remote sensing","convolutional neural networks;operational rule-based processing chain;operational rule-based processor;S-1FS;globally distributed dataset;geometric data augmentation;Sentinel-1-based water mapping;flood mapping;AlbuNet-34;FCN;DeepLabV3+;U-Net;U-Net++;ground truth water masks;environmental conditions;dual-polarized input data;single-polarized input data;spatial pyramid pooling;multiscale feature fusion","","5","","72","CCBY","16 Feb 2022","","","IEEE","IEEE Journals"
"Enhanced Spatiotemporal Fusion via MODIS-Like Images","J. Li; Y. Li; R. Cai; L. He; J. Chen; A. Plaza","Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; State Key Laboratory of Earth Surface Processes and Resource Ecology, Faculty of Geographical Science, Institute of Remote Sensing Science and Engineering, Beijing Normal University, Beijing, China; Department of Technology of Computers and Communications, Escuela Politécnica, Hyperspectral Computing Laboratory, University of Extremadura, Cáceres, Spain","IEEE Transactions on Geoscience and Remote Sensing","28 Jan 2022","2022","60","","1","17","Spatiotemporal fusion (STF) aims at generating remote-sensing data with both high spatial and temporal resolution. In the literature, one of the most widely used strategies to accomplish this goal is to fuse high temporal resolution images collected by the Moderate Resolution Imaging Spectroradiometer (MODIS) with images with finer spatial resolution than those provided by MODIS (e.g., those collected by other satellite instruments such as Landsat or Sentinel-2). Current STF methods generally fuse an upsampled MODIS image with finer spatial resolution images. This leads to two main problems. First of all, the model uncertainty errors (resulting from the ill-posed upsampling problem) will be propagated into the fusion results, leading to spatial and spectral distortion. Furthermore, the spatial details of the upsampled MODIS image may be significantly different from those of the finer spatial resolution images, making the STF problem even more challenging. In order to tackle these issues, in this work, we develop a new linear regression-based STF strategy (LiSTF), which performs the reconstruction from a MODIS-like image (instead of from an upsampled MODIS image), thus reducing the model uncertainty errors and preserving better the spatial information. The MODIS-like images are built from the finer spatial resolution images via downsampling. Our experimental results, conducted using two publicly available datasets of Landsat–MODIS image pairs and one publicly available dataset of Sentinel–MODIS image pairs, reveal that our newly proposed LiSTF approach can significantly enhance the quantitative and qualitative performance of STF, particularly in terms of preserving the spatial information.","1558-0644","","10.1109/TGRS.2021.3106338","National Natural Science Foundation of China(grant numbers:42030111); National Key Research and Development Program of China(grant numbers:2017YFB0502900); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9526862","Landsat;Moderate Resolution Imaging Spectroradiometer (MODIS);MODIS-like images;Sentinel-2;spatiotemporal fusion (STF)","MODIS;Spatial resolution;Remote sensing;Earth;Artificial satellites;Superresolution;Uncertainty","geophysical image processing;image fusion;image resolution;image sampling;regression analysis;remote sensing;spatiotemporal phenomena","high temporal resolution images;Moderate Resolution Imaging Spectroradiometer;upsampled MODIS image;finer spatial resolution images;MODIS-like image;Landsat-MODIS image pairs;Sentinel-MODIS image pairs;enhanced spatiotemporal fusion","","2","","33","IEEE","1 Sep 2021","","","IEEE","IEEE Journals"
"A Fusion Approach for Water Area Classification Using Visible, Near Infrared and Synthetic Aperture Radar for South Asian Conditions","S. K. Ahmad; F. Hossain; H. Eldardiry; T. M. Pavelsky","Department of Civil and Environmental Engineering, University of Washington, Seattle, USA; Department of Civil and Environmental Engineering, University of Washington, Seattle, USA; Department of Civil and Environmental Engineering, University of Washington, Seattle, USA; Geological Sciences Department, University of North Carolina, Chapel Hill, USA","IEEE Transactions on Geoscience and Remote Sensing","25 Mar 2020","2020","58","4","2471","2480","Consistent estimation of water surface area from remote sensing remains challenging in regions such as South Asia with vegetation, mountainous topography, and persistent monsoonal cloud cover. High-resolution optical imagery, which is often used for global inundation mapping, is highly impacted by clouds, while synthetic aperture radar (SAR) imagery is not impacted by clouds and is affected by both topographic layover and vegetation. Here, we compare and contrast inundation extent measurements from visible (Landsat-8 and Sentinel-2) and SAR (Sentinel-1) imagery. Each data type (wavelength) has complementary strengths and weaknesses which were gauged separately over selected water bodies in Bangladesh. High-resolution cloud-free PlanetScope imagery at 3-m resolution was used as a reference to check the accuracy of each technique and data type. Next, the optical and radar images were fused for a rule-based water area classification algorithm to derive the optimal decision for the water mask. Results indicate that the fusion approach can improve the overall accuracy by up to 3.8%, 18.2%, and 8.3% during the wet season over using the individual products of Landsat8, Sentinel-1, and Sentinel-2, respectively, at three sites, while providing increased observational frequency. The fusion-derived products resulted in overall accuracy ranging from 85.8% to 98.7% and Kappa coefficient varying from 0.61 to 0.83. The proposed SAR-visible fusion technique has potential for improving satellite-based surface water monitoring and storage changes, especially for smaller water bodies in humid tropical climate of South Asia.","1558-0644","","10.1109/TGRS.2019.2950705","National Aeronautics and Space Administration(grant numbers:80NSSC18M0099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8906149","Area classification;remote sensing;synthetic aperture radar (SAR);visible imagery;water bodies","Synthetic aperture radar;Remote sensing;Optical surface waves;Surface topography;Vegetation mapping;Satellites;Earth","geophysical image processing;hydrological techniques;image classification;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar","satellite-based surface water monitoring;South Asia;water surface area;remote sensing;vegetation;mountainous topography;high-resolution optical imagery;global inundation mapping;synthetic aperture radar imagery;Sentinel-2;SAR imagery;Sentinel-1;high-resolution cloud-free PlanetScope imagery;optical radar images;rule-based water area classification algorithm;water mask;Landsat8;fusion-derived products;monsoonal cloud cover;SAR-visible fusion;water area classification;Bangladesh;PlanetScope imagery;humid tropical climate","","24","","39","IEEE","19 Nov 2019","","","IEEE","IEEE Journals"
"CoSpace: Common Subspace Learning From Hyperspectral-Multispectral Correspondences","D. Hong; N. Yokoya; J. Chanussot; X. X. Zhu","Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany; Geoinformatics Unit, RIKEN Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan; GIPSA-Lab, CNRS, Grenoble INP, Université Grenoble Alpes, Grenoble, France; Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany","IEEE Transactions on Geoscience and Remote Sensing","24 Jun 2019","2019","57","7","4349","4359","With a large amount of open satellite multispectral (MS) imagery (e.g., Sentinel-2 and Landsat-8), considerable attention has been paid to global MS land cover classification. However, its limited spectral information hinders further improving the classification performance. Hyperspectral imaging enables discrimination between spectrally similar classes but its swath width from space is narrow compared to MS ones. To achieve accurate land cover classification over a large coverage, we propose a cross-modality feature learning framework, called common subspace learning (CoSpace), by jointly considering subspace learning and supervised classification. By locally aligning the manifold structure of the two modalities, CoSpace linearly learns a shared latent subspace from hyperspectral-MS (HS-MS) correspondences. The MS out-of-samples can be then projected into the subspace, which are expected to take advantages of rich spectral information of the corresponding hyperspectral data used for learning, and thus leads to a better classification. Extensive experiments on two simulated HS-MS data sets (University of Houston and Chikusei), where HS-MS data sets have tradeoffs between coverage and spectral resolution, are performed to demonstrate the superiority and effectiveness of the proposed method in comparison with previous state-of-the-art methods.","1558-0644","","10.1109/TGRS.2018.2890705","H2020 European Research Council(grant numbers:ERC-2016-StG-714087); Helmholtz-Gemeinschaft(grant numbers:VH-NG-1018); German Research Foundation (DFG)(grant numbers:ZH 498/7-2); Japan Society for the Promotion of Science(grant numbers:15K20955); Alexander von Humboldt Fellowship for Post-Doctoral Researchers; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672122","Common subspace learning (CoSpace);cross-modality learning;hyperspectral;landcover classification;multispectral (MS);remote sensing","Hyperspectral imaging;Optimization;Satellites;Earth;Manifolds","geophysical image processing;hyperspectral imaging;image classification;image fusion;image resolution;learning (artificial intelligence);remote sensing","spectral information hinders;classification performance;hyperspectral imaging;spectrally similar classes;swath width;cross-modality feature learning;CoSpace;shared latent subspace;hyperspectral-MS correspondences;MS out-of-samples;simulated HS-MS data sets;spectral resolution;hyperspectral-multispectral correspondences;open satellite multispectral imagery;Sentinel-2;Landsat-8;global MS land cover classification;spectral information;University of Houston and Chikusei;common subspace learning","","154","","34","OAPA","20 Mar 2019","","","IEEE","IEEE Journals"
"Open Data for Global Multimodal Land Use Classification: Outcome of the 2017 IEEE GRSS Data Fusion Contest","N. Yokoya; P. Ghamisi; J. Xia; S. Sukhanov; R. Heremans; I. Tankoyeu; B. Bechtel; B. Le Saux; G. Moser; D. Tuia","RIKEN Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan; Department of Signal Processing in Earth Observation, Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Wessling, Germany; Department of Advanced Interdisciplinary Studies, The University of Tokyo, Tokyo, Japan; AGT International, Darmstadt, Germany; AGT International, Darmstadt, Germany; AGT International, Darmstadt, Germany; Center for Earth System Research and Sustainability, Universität Hamburg, Hamburg, Germany; DTIS, ONERA, Université Paris Saclay, Palaiseau, France; Department of Electrical, Electronic, Telecommunications Engineering and Naval Architecture, University of Genoa, Genoa, Italy; Laboratory of Geo-Information Science and Remote Sensing, Wageningen University & Research, Wageningen, The Netherlands","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Apr 2018","2018","11","5","1363","1377","In this paper, we present the scientific outcomes of the 2017 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The 2017 Contest was aimed at addressing the problem of local climate zones classification based on a multitemporal and multimodal dataset, including image (Landsat 8 and Sentinel-2) and vector data (from OpenStreetMap). The competition, based on separate geographical locations for the training and testing of the proposed solution, aimed at models that were accurate (assessed by accuracy metrics on an undisclosed reference for the test cities), general (assessed by spreading the test cities across the globe), and computationally feasible (assessed by having a test phase of limited time). The techniques proposed by the participants to the Contest spanned across a rather broad range of topics, and of mixed ideas and methodologies deriving from computer vision and machine learning but also deeply rooted in the specificities of remote sensing. In particular, rigorous atmospheric correction, the use of multidate images, and the use of ensemble methods fusing results obtained from different data sources/time instants made the difference.","2151-1535","","10.1109/JSTARS.2018.2799698","Cluster of Excellence “CliSAP”(grant numbers:EXC177); University of Hamburg; Swiss National Science Foundation(grant numbers:PP00P2-150593); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8338367","Convolutional neural networks (CNNs);crowdsourcing;deep learning (DL);ensemble learning;image analysis and data fusion (IADF);multimodal;multiresolution;multisource;OpenStreetMap (OSM);random fields","Urban areas;Remote sensing;Earth;Training;Artificial satellites;Data integration;Image resolution","geophysical image processing;image classification;image fusion;land use;learning (artificial intelligence);remote sensing;terrain mapping","remote sensing;local climate zones classification;multidate images;global multimodal land use classification;geographical locations;data fusion contest;ensemble methods;OpenStreetMap;Landsat 8;Sentinel-2;computer vision;machine learning;AD 2017","","84","","62","OAPA","16 Apr 2018","","","IEEE","IEEE Journals"
"A General Framework for Change Detection Using Multimodal Remote Sensing Data","S. Chirakkal; F. Bovolo; A. R. Misra; L. Bruzzone; A. Bhattacharya","Advanced Microwave and Hyperspectral Techniques Development Group, Space Applications Center, Indian Space Research Organisation, Ahmedabad, India; Center for Information and Communication Technology, Fondazione Bruno Kessler, Trento, Italy; Advanced Microwave and Hyperspectral Techniques Development Group, Space Applications Center, Indian Space Research Organisation, Ahmedabad, India; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Center of Studies in Resources Engineering, Indian Institute of Technology Bombay, Mumbai, India","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1 Nov 2021","2021","14","","10665","10680","A general framework for change detection is proposed to analyze multimodal remotely sensed data utilizing the Kronecker product between two data representations (vectors or matrices). The proposed method is sensor independent and provides comparable results to techniques that exist for specific sensors. The proposed fusion technique is a pixel-level approach that incorporates inputs from different modalities, rendering enriched multimodal data representation. Thus, the proposed hybridization procedure helps to assimilate multisensor information in a meaningful manner. A novel change index ($\zeta$) is defined for the general multimodal case. This index is then used to quantify the change in bitemporal remotely sensed data. This article explores the usability, consistency, and robustness of the proposed multimodal fusion framework, including the change index, with proper validation on two multimodal cases: 1) the dual-frequency ($C$- and $L$-band) fully polarimetric Danish EMISAR data and 2) the dual-polarimetric synthetic aperture radar and Sentinel-2 multispectral data. Detailed analysis and validation using extensive ground-truth data are presented to establish the proposed framework.","2151-1535","","10.1109/JSTARS.2021.3119358","India–Trento Program for Advanced Research; Department of Science and Technology of the Indian Government; Provincia Autonoma di Trento; Trentino Cultural Institute, now Bruno Kessler Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9568726","Change detection (CD);dual-frequency PolSAR;Kronecker product of matrices;multimodal data;synthetic aperture radar (SAR) optical fusion","Synthetic aperture radar;Remote sensing;Indexes;Optical sensors;Data integration;Optical imaging;Radar polarimetry","geophysical image processing;image fusion;radar imaging;radar polarimetry;remote sensing;sensor fusion;synthetic aperture radar","matrices;specific sensors;fusion technique;pixel-level approach;enriched multimodal data representation;novel change index;general multimodal case;bitemporal remotely sensed data;multimodal fusion framework;multimodal cases;Sentinel-2 multispectral data;extensive ground-truth data;change detection;multimodal remote sensing data;Kronecker product;vectors","","2","","55","CCBY","12 Oct 2021","","","IEEE","IEEE Journals"
"Data Fusion for Increasing Monitoring Capabilities of Sentinel Optical Data in Marine Environment","M. Kremezi; V. Karathanassi","Laboratory of Remote Sensing, School of Rural and Surveying Engineering, National Technical University of Athens, Athens, Greece; Laboratory of Remote Sensing, School of Rural and Surveying Engineering, National Technical University of Athens, Athens, Greece","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","31 Aug 2020","2020","13","","4809","4815","Marine monitoring constitutes one of the main thematic areas of the Sentinel mission. The Sentinel 3 OLCI (S3) sensor provides satellite data for services relevant to the ocean and land. While the spatial resolution of S3 images (300 m) is suitable for most marine applications, there are some applications such as floating debris detection, suspended mater estimation, etc., that require higher resolution. To fulfill this requirement this study applies an unmixing-based data fusion technique on S3 and BRDF-corrected Sentinel 2 (S2) images and evaluates the fused data by calculating the correlation coefficient and the spectral angle distance (SAD) indexes. Then, it explores the increased monitoring capabilities of the fused image by applying improved chlorophyll-a (Chl-a) and total suspended matter (TSM) algorithms, developed for satellite data. The fused image presents spectral similarity to S3 data and spatial similarity to S2 image. Consequently, the products provided by the fused image have much better resolution than those of S3 image, which enables detailed estimations of Chl-a and TSM concentrations. However, the dynamic nature of the marine environment that results in the formation of time-varying patterns at sea surface, in relation to the time lag between S2 and S3 image acquisitions may locally affect the accuracy of the products in the neighborhood of these patterns. This study exploits the effective elimination of directional reflectance effects in S2 ocean images, interprets the fused image and the generated ocean products, and points out the constraints regarding the synergy of Sentinel optical data for ocean areas.","2151-1535","","10.1109/JSTARS.2020.3018050","European Space Agency(grant numbers:4000131235/20/NL/GLC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9171412","Bidirectional reflectance distribution function (BRDF) correction;chlorophyll-a (Chl-a);data fusion;marine monitoring;sentinel;spectral unmixing;total suspended matter (TSM)","Spatial resolution;Oceans;Cloud computing;Data integration;Monitoring;Optical imaging","environmental monitoring (geophysics);geophysical image processing;image fusion;ocean composition;oceanographic techniques;remote sensing;underwater optics","S2 image acquisitions;TSM algorithms;total suspended matter;chlorophyll-a;correlation coefficient;unmixing-based data fusion;spectral angle distance indexes;BRDF-corrected Sentinel 2 images;Sentinel 3 OLCI sensor;Sentinel mission;marine monitoring;sentinel optical data;S2 ocean images;S3 image acquisitions;marine environment;fused image","","","","29","CCBY","19 Aug 2020","","","IEEE","IEEE Journals"
"Location Aware Super-Resolution for Satellite Data Fusion","O. Adigun; P. A. Olsen; R. Chandra","Dept. of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA; Microsoft Research Research For Industry, Redmond, WA; Microsoft Research Research For Industry, Redmond, WA","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","3758","3761","Satellite data fusion involves images with different spatial, temporal, and spectral resolution. These images are taken under different illumination conditions, with different sensors and atmospheric noise. We use classic super-resolution algorithms to synthesize commercial satellite images (Pléiades) from a public satellite source (Sentinel-2). Each super-resolution method is then further improved by adaptive sharpening to the location by use of matrix completion (regression with missing pixels). Finally, we consider ensemble systems and a residual channel attention dual network with stochastic dropout. The resulting systems are visibly less blurry with higher fidelity and yield improved performance.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884391","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884391","Super-resolution;matrix completion;cloud removal","Location awareness;Image sensors;Satellites;Superresolution;Data integration;Lighting;Geoscience and remote sensing","artificial satellites;geophysical image processing;image fusion;image resolution;image restoration;sensor fusion","satellite data fusion;different spatial;spectral resolution;different illumination conditions;atmospheric noise;super-resolution algorithms;commercial satellite images;public satellite source;super-resolution method;location aware super-resolution","","","","12","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Super-Resolved Multi-Temporal Segmentation with Deep Permutation-Invariant Networks","D. Valsesia; E. Magli",Politecnico di Torino; Politecnico di Torino,"IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","995","998","Multi-image super-resolution from multi-temporal satellite acquisitions of a scene has recently enjoyed great success thanks to new deep learning models. In this paper, we go beyond classic image reconstruction at a higher resolution by studying a super-resolved inference problem, namely semantic segmentation at a spatial resolution higher than the one of sensing platform. We expand upon recently proposed models exploiting temporal permutation invariance with a multi-resolution fusion module able to infer the rich semantic information needed by the segmentation task. The model presented in this paper has recently won the AI4EO challenge on Enhanced Sentinel 2 Agriculture.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884811","Super-resolution;image segmentation;deep neural networks","Deep learning;Uncertainty;Satellites;Superresolution;Neural networks;Semantics;Sensors","geophysical image processing;image fusion;image reconstruction;image resolution;image segmentation;learning (artificial intelligence)","super-resolved multitemporal segmentation;deep permutation-invariant networks;multiimage super-resolution;multitemporal satellite acquisitions;great success thanks;deep learning models;classic image reconstruction;super-resolved inference problem;semantic segmentation;spatial resolution;recently proposed models;temporal permutation invariance;multiresolution fusion module;rich semantic information;segmentation task","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Joint Classification of Hyperspectral and Multispectral Images for Mapping Coastal Wetlands","C. Liu; R. Tao; W. Li; M. Zhang; W. Sun; Q. Du","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Jan 2021","2021","14","","982","996","It is significant for restoration and protection of natural resources and ecological services in coastal wetlands to map different land cover types with satellite remote sensing data. Considering difficulties of wetland species classification, hyperspectral images (HSIs) with high spectral resolution and multispectral images (MSI) with high spatial resolution are considered to achieve complementary advantages of multisource data. An effective approach, named as multistream convolutional neural network, is proposed to achieve fine classification of coastal wetlands. First, regression processing is adopted to make chaotically scattered coastal wetland data more compact and different. Second, through appropriate feature extraction and feature fusion strategies, high-level information of multisource data in regression domain is fused to distinguish different land cover. Experiments on GF-5 HSIs and Sentinel-2 MSIs are carried out in order to validate the classification performance of the proposed approach in two coastal wetlands of research value in China, i.e., Yellow River Estuary and Yancheng coastal wetland. Experimental results demonstrate the effectiveness of the proposed method compared with the state-of-the-art methods in the field, especially when the number of sample size is extremely small.","2151-1535","","10.1109/JSTARS.2020.3040305","Beijing Natural Science Foundation(grant numbers:JQ20021); National Natural Science Foundation of China(grant numbers:61922013,61421001,U1833203); Beijing Natural Science Foundation(grant numbers:L191004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268458","Coastal wetlands;convolutional neural network (CNN);data fusion;hyperspectral imagery (HSI);least squares regression (LSR);multispectral imagery (MSI)","Wetlands;Sea measurements;Feature extraction;Vegetation mapping;Hyperspectral imaging;Spatial resolution;Earth","ecology;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;neural nets;regression analysis;remote sensing;rivers","multispectral images;mapping coastal;natural resources;ecological services;coastal wetlands;land cover types;satellite remote sensing data;considering difficulties;wetland species classification;hyperspectral images;high spectral resolution;high spatial resolution;multisource data;multistream convolutional neural network;fine classification;chaotically scattered coastal wetland data;high-level information;classification performance","","14","","73","CCBY","24 Nov 2020","","","IEEE","IEEE Journals"
"Temporal Shape-Based Fusion Method to Generate Continuous Vegetation Index at Fine Spatial Resolution","Y. Liu; X. Gu; T. Cheng; Y. Zhan; H. Zhang; J. Li; X. Wei; M. Gao; Q. Zhang; Y. Zhang","University of Chinese Academy of Sciences, Beijing, China; School of Remote Sensing and Information Engineering, North China Institute of Aerospace Engineering, Langfang, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; School of Geographic and Environmental Sciences, Tianjin Normal University, Tianjin, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","25 Oct 2022","2022","60","","1","14","In this study, a temporal shape-based fusion method (TSFM) using a spatially and temporally moving window is proposed to incorporate the time lag of fine- and coarse-resolution observations and to fully utilize target fine-resolution pixels and similar coarse-resolution pixels in the process. This method provides high-accuracy fused images with Pearson’s r of ~0.95, a root-mean-square error (RMSE) of ~0.04, and a bias of ~0.01 for commonly used fine spatial resolution satellites, including Landsat 7 and 8, Sentinel 2, and Gaofen 1, over different heterogeneous regions, such as urban, mountain, forest, and savanna regions. The fused fine-resolution enhanced vegetation index (EVI) time series using different fine spatial resolution satellite data as input are all highly correlated with the PhenoCam-monitored green chromatic coordinate (GCC), with no temporal lag. Compared with the commonly used data fusion method, this method provides equivalent and slightly higher accuracy because both neighboring similar pixels and the annual temporal variation are fully considered. This TSFM does not require each input fine-resolution image to be cloud-free; therefore, it can be used at a large spatial scale without further preprocessing and generates continuous datasets over a long-time range with only one input preparation process. The factors that could affect the method accuracy are the cloud detection accuracy of fine-resolution data and the temporal continuity of the coarse-resolution data. The method may also be used to produce spatially and temporally continuous surface reflectance and other surface reflectance-derived indices.","1558-0644","","10.1109/TGRS.2022.3211269","National Key Research and Development Program of China(grant numbers:2019YFE0127300,2020YFE0200700); National Natural Science Foundation of China(grant numbers:41901367); Major Special Project—the China High-Resolution Earth Observation System(grant numbers:30-Y30F06-9003-20/22); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9907037","Data fusion;spatially and temporally moving window;temporal shape;vegetation dynamic","Spatial resolution;Time series analysis;Vegetation mapping;Satellites;Data integration;Shape;Remote sensing","geophysical image processing;geophysical signal processing;image classification;image fusion;image resolution;remote sensing;sensor fusion;terrain mapping;time series;vegetation;vegetation mapping","root-mean-square error;different heterogeneous regions;fine-resolution enhanced vegetation index time series;different fine spatial resolution satellite data;temporal lag;commonly used data fusion method;neighboring similar pixels;annual temporal variation;input fine-resolution image;spatial scale;method accuracy;fine-resolution data;temporal continuity;coarse-resolution data;temporal shape-based fusion method;generate continuous vegetation index;time lag;coarse-resolution observations;target fine-resolution pixels;coarse-resolution pixels;high-accuracy fused images","","","","50","IEEE","3 Oct 2022","","","IEEE","IEEE Journals"
