AID,Year,Title,Abstract,Authors,Link,DOI
http://arxiv.org/abs/1907.10213v1,2019,"Image Super-Resolution Using a Wavelet-based Generative Adversarial
  Network","  In this paper, we consider the problem of super-resolution recons-truction.
This is a hot topic because super-resolution reconstruction has a wide range of
applications in the medical field, remote sensing monitoring, and criminal
investigation. Compared with traditional algorithms, the current
super-resolution reconstruction algorithm based on deep learning greatly
improves the clarity of reconstructed pictures. Existing work like
Super-Resolution Using a Generative Adversarial Network (SRGAN) can effectively
restore the texture details of the image. However, experimentally verified that
the texture details of the image recovered by the SRGAN are not robust. In
order to get super-resolution reconstructed images with richer high-frequency
details, we improve the network structure and propose a super-resolution
reconstruction algorithm combining wavelet transform and Generative Adversarial
Network. The proposed algorithm can efficiently reconstruct high-resolution
images with rich global information and local texture details. We have trained
our model by PyTorch framework and VOC2012 dataset, and tested it by Set5,
Set14, BSD100 and Urban100 test datasets.
",Qi Zhang; Huafeng Wang; Sichen Yang,http://arxiv.org/abs/1907.10213v1,10.48550/arXiv.1907.10213
http://arxiv.org/abs/2001.08073v2,2020,"ESRGAN+ : Further Improving Enhanced Super-Resolution Generative
  Adversarial Network","  Enhanced Super-Resolution Generative Adversarial Network (ESRGAN) is a
perceptual-driven approach for single image super resolution that is able to
produce photorealistic images. Despite the visual quality of these generated
images, there is still room for improvement. In this fashion, the model is
extended to further improve the perceptual quality of the images. We have
designed a novel block to replace the one used by the original ESRGAN.
Moreover, we introduce noise inputs to the generator network in order to
exploit stochastic variation. The resulting images present more realistic
textures. The code is available at https://github.com/ncarraz/ESRGANplus .
",Nathanaël Carraz Rakotonirina; Andry Rasoanaivo,http://arxiv.org/abs/2001.08073v2,10.1109/ICASSP40776.2020.9054071
http://arxiv.org/abs/1809.04783v2,2018,"Generative adversarial network-based image super-resolution using
  perceptual content losses","  In this paper, we propose a deep generative adversarial network for
super-resolution considering the trade-off between perception and distortion.
Based on good performance of a recently developed model for super-resolution,
i.e., deep residual network using enhanced upscale modules (EUSR), the proposed
model is trained to improve perceptual performance with only slight increase of
distortion. For this purpose, together with the conventional content loss,
i.e., reconstruction loss such as L1 or L2, we consider additional losses in
the training phase, which are the discrete cosine transform coefficients loss
and differential content loss. These consider perceptual part in the content
loss, i.e., consideration of proper high frequency components is helpful for
the trade-off problem in super-resolution. The experimental results show that
our proposed model has good performance for both perception and distortion, and
is effective in perceptual super-resolution applications.
",Manri Cheon; Jun-Hyuk Kim; Jun-Ho Choi; Jong-Seok Lee,http://arxiv.org/abs/1809.04783v2,10.48550/arXiv.1809.04783
http://arxiv.org/abs/2109.08770v2,2021,WiSoSuper: Benchmarking Super-Resolution Methods on Wind and Solar Data,"  The transition to green energy grids depends on detailed wind and solar
forecasts to optimize the siting and scheduling of renewable energy generation.
Operational forecasts from numerical weather prediction models, however, only
have a spatial resolution of 10 to 20-km, which leads to sub-optimal usage and
development of renewable energy farms. Weather scientists have been developing
super-resolution methods to increase the resolution, but often rely on simple
interpolation techniques or computationally expensive differential
equation-based models. Recently, machine learning-based models, specifically
the physics-informed resolution-enhancing generative adversarial network
(PhIREGAN), have outperformed traditional downscaling methods. We provide a
thorough and extensible benchmark of leading deep learning-based
super-resolution techniques, including the enhanced super-resolution generative
adversarial network (ESRGAN) and an enhanced deep super-resolution (EDSR)
network, on wind and solar data. We accompany the benchmark with a novel
public, processed, and machine learning-ready dataset for benchmarking
super-resolution methods on wind and solar data.
",Rupa Kurinchi-Vendhan; Björn Lütjens; Ritwik Gupta; Lucien Werner; Dava Newman,http://arxiv.org/abs/2109.08770v2,10.48550/arXiv.2109.08770
http://arxiv.org/abs/2004.13674v1,2020,"Residual Channel Attention Generative Adversarial Network for Image
  Super-Resolution and Noise Reduction","  Image super-resolution is one of the important computer vision techniques
aiming to reconstruct high-resolution images from corresponding low-resolution
ones. Most recently, deep learning-based approaches have been demonstrated for
image super-resolution. However, as the deep networks go deeper, they become
more difficult to train and more difficult to restore the finer texture
details, especially under real-world settings. In this paper, we propose a
Residual Channel Attention-Generative Adversarial Network(RCA-GAN) to solve
these problems. Specifically, a novel residual channel attention block is
proposed to form RCA-GAN, which consists of a set of residual blocks with
shortcut connections, and a channel attention mechanism to model the
interdependence and interaction of the feature representations among different
channels. Besides, a generative adversarial network (GAN) is employed to
further produce realistic and highly detailed results. Benefiting from these
improvements, the proposed RCA-GAN yields consistently better visual quality
with more detailed and natural textures than baseline models; and achieves
comparable or better performance compared with the state-of-the-art methods for
real-world image super-resolution.
",Jie Cai; Zibo Meng; Chiu Man Ho,http://arxiv.org/abs/2004.13674v1,10.48550/arXiv.2004.13674
http://arxiv.org/abs/1903.11821v1,2019,"SRDGAN: learning the noise prior for Super Resolution with Dual
  Generative Adversarial Networks","  Single Image Super Resolution (SISR) is the task of producing a high
resolution (HR) image from a given low-resolution (LR) image. It is a well
researched problem with extensive commercial applications such as digital
camera, video compression, medical imaging and so on. Most super resolution
works focus on the features learning architecture, which can recover the
texture details as close as possible. However, these works suffer from the
following challenges: (1) The low-resolution (LR) training images are
artificially synthesized using HR images with bicubic downsampling, which have
much richer-information than real demosaic-upscaled mobile images. The mismatch
between training and inference mobile data heavily blocks the improvement of
practical super resolution algorithms. (2) These methods cannot effectively
handle the blind distortions during super resolution in practical applications.
In this work, an end-to-end novel framework, including high-to-low network and
low-to-high network, is proposed to solve the above problems with dual
Generative Adversarial Networks (GAN). First, the above mismatch problems are
well explored with the high-to-low network, where clear high-resolution image
and the corresponding realistic low-resolution image pairs can be generated.
Moreover, a large-scale General Mobile Super Resolution Dataset, GMSR, is
proposed, which can be utilized for training or as a fair comparison benchmark
for super resolution methods. Second, an effective low-to-high network (super
resolution network) is proposed in the framework. Benefiting from the GMSR
dataset and novel training strategies, the super resolution model can
effectively handle detail recovery and denoising at the same time.
",Jingwei Guan; Cheng Pan; Songnan Li; Dahai Yu,http://arxiv.org/abs/1903.11821v1,10.48550/arXiv.1903.11821
http://arxiv.org/abs/1911.10773v2,2019,"Fine-grained Attention and Feature-sharing Generative Adversarial
  Networks for Single Image Super-Resolution","  The traditional super-resolution methods that aim to minimize the mean square
error usually produce the images with over-smoothed and blurry edges, due to
the lose of high-frequency details. In this paper, we propose two novel
techniques in the generative adversarial networks to produce photo-realistic
images for image super-resolution. Firstly, instead of producing a single score
to discriminate images between real and fake, we propose a variant, called
Fine-grained Attention Generative Adversarial Network for image
super-resolution (FASRGAN), to discriminate each pixel between real and fake.
FASRGAN adopts a Unet-like network as the discriminator with two outputs: an
image score and an image score map. The score map has the same spatial size as
the HR/SR images, serving as the fine-grained attention to represent the degree
of reconstruction difficulty for each pixel. Secondly, instead of using
different networks for the generator and the discriminator in the SR problem,
we use a feature-sharing network (Fs-SRGAN) for both the generator and the
discriminator. By network sharing, certain information is shared between the
generator and the discriminator, which in turn can improve the ability of
producing high-quality images. Quantitative and visual comparisons with the
state-of-the-art methods on the benchmark datasets demonstrate the superiority
of our methods. The application of super-resolution images to object
recognition further proves that the proposed methods endow the power to
reconstruction capabilities and the excellent super-resolution effects.
",Yitong Yan; Chuangchuang Liu; Changyou Chen; Xianfang Sun; Longcun Jin; Xiang Zhou,http://arxiv.org/abs/1911.10773v2,10.48550/arXiv.1911.10773
http://arxiv.org/abs/2211.00577v5,2022,"Fine-tuned Generative Adversarial Network-based Model for Medical Images
  Super-Resolution","  In medical image analysis, low-resolution images negatively affect the
performance of medical image interpretation and may cause misdiagnosis. Single
image super-resolution (SISR) methods can improve the resolution and quality of
medical images. Currently, Generative Adversarial Networks (GAN) based
super-resolution models have shown very good performance. Real-Enhanced
Super-Resolution Generative Adversarial Network (Real-ESRGAN) is one of the
practical GAN-based models which is widely used in the field of general image
super-resolution. One of the challenges in the field of medical image
super-resolution is that, unlike natural images, medical images do not have
high spatial resolution. To solve this problem, we can use transfer learning
technique and fine-tune the model that has been trained on external datasets
(often natural datasets). In our proposed approach, the pre-trained generator
and discriminator networks of the Real-ESRGAN model are fine-tuned using
medical image datasets. In this paper, we worked on chest X-ray and retinal
images and used the STARE dataset of retinal images and Tuberculosis Chest
X-rays (Shenzhen) dataset for fine-tuning. The proposed model produces more
accurate and natural textures, and its outputs have better details and
resolution compared to the original Real-ESRGAN outputs.
",Alireza Aghelan; Modjtaba Rouhani,http://arxiv.org/abs/2211.00577v5,10.48550/arXiv.2211.00577
http://arxiv.org/abs/2205.08659v1,2022,Semantically Accurate Super-Resolution Generative Adversarial Networks,"  This work addresses the problems of semantic segmentation and image
super-resolution by jointly considering the performance of both in training a
Generative Adversarial Network (GAN). We propose a novel architecture and
domain-specific feature loss, allowing super-resolution to operate as a
pre-processing step to increase the performance of downstream computer vision
tasks, specifically semantic segmentation. We demonstrate this approach using
Nearmap's aerial imagery dataset which covers hundreds of urban areas at 5-7 cm
per pixel resolution. We show the proposed approach improves perceived image
quality as well as quantitative segmentation accuracy across all prediction
classes, yielding an average accuracy improvement of 11.8% and 108% at 4x and
32x super-resolution, compared with state-of-the art single-network methods.
This work demonstrates that jointly considering image-based and task-specific
losses can improve the performance of both, and advances the state-of-the-art
in semantic-aware super-resolution of aerial imagery.
",Tristan Frizza; Donald G. Dansereau; Nagita Mehr Seresht; Michael Bewley,http://arxiv.org/abs/2205.08659v1,10.48550/arXiv.2205.08659
http://arxiv.org/abs/1707.00737v1,2017,"High-Quality Face Image SR Using Conditional Generative Adversarial
  Networks","  We propose a novel single face image super-resolution method, which named
Face Conditional Generative Adversarial Network(FCGAN), based on boundary
equilibrium generative adversarial networks. Without taking any facial prior
information, our method can generate a high-resolution face image from a
low-resolution one. Compared with existing studies, both our training and
testing phases are end-to-end pipeline with little pre/post-processing. To
enhance the convergence speed and strengthen feature propagation, skip-layer
connection is further employed in the generative and discriminative networks.
Extensive experiments demonstrate that our model achieves competitive
performance compared with state-of-the-art models.
",Huang Bin; Chen Weihai; Wu Xingming; Lin Chun-Liang,http://arxiv.org/abs/1707.00737v1,10.48550/arXiv.1707.00737
http://arxiv.org/abs/1908.06382v2,2019,"RankSRGAN: Generative Adversarial Networks with Ranker for Image
  Super-Resolution","  Generative Adversarial Networks (GAN) have demonstrated the potential to
recover realistic details for single image super-resolution (SISR). To further
improve the visual quality of super-resolved results, PIRM2018-SR Challenge
employed perceptual metrics to assess the perceptual quality, such as PI, NIQE,
and Ma. However, existing methods cannot directly optimize these
indifferentiable perceptual metrics, which are shown to be highly correlated
with human ratings. To address the problem, we propose Super-Resolution
Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator
in the direction of perceptual metrics. Specifically, we first train a Ranker
which can learn the behavior of perceptual metrics and then introduce a novel
rank-content loss to optimize the perceptual quality. The most appealing part
is that the proposed method can combine the strengths of different SR methods
to generate better results. Extensive experiments show that RankSRGAN achieves
visually pleasing results and reaches state-of-the-art performance in
perceptual metrics. Project page:
https://wenlongzhang0724.github.io/Projects/RankSRGAN
",Wenlong Zhang; Yihao Liu; Chao Dong; Yu Qiao,http://arxiv.org/abs/1908.06382v2,10.48550/arXiv.1908.06382
http://arxiv.org/abs/2107.09427v1,2021,"RankSRGAN: Super Resolution Generative Adversarial Networks with
  Learning to Rank","  Generative Adversarial Networks (GAN) have demonstrated the potential to
recover realistic details for single image super-resolution (SISR). To further
improve the visual quality of super-resolved results, PIRM2018-SR Challenge
employed perceptual metrics to assess the perceptual quality, such as PI, NIQE,
and Ma. However, existing methods cannot directly optimize these
indifferentiable perceptual metrics, which are shown to be highly correlated
with human ratings. To address the problem, we propose Super-Resolution
Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator
in the direction of different perceptual metrics. Specifically, we first train
a Ranker which can learn the behaviour of perceptual metrics and then introduce
a novel rank-content loss to optimize the perceptual quality. The most
appealing part is that the proposed method can combine the strengths of
different SR methods to generate better results. Furthermore, we extend our
method to multiple Rankers to provide multi-dimension constraints for the
generator. Extensive experiments show that RankSRGAN achieves visually pleasing
results and reaches state-of-the-art performance in perceptual metrics and
quality. Project page: https://wenlongzhang0517.github.io/Projects/RankSRGAN
",Wenlong Zhang; Yihao Liu; Chao Dong; Yu Qiao,http://arxiv.org/abs/2107.09427v1,10.48550/arXiv.2107.09427
http://arxiv.org/abs/2202.12337v1,2022,"Time Efficient Training of Progressive Generative Adversarial Network
  using Depthwise Separable Convolution and Super Resolution Generative
  Adversarial Network","  Generative Adversarial Networks have been employed successfully to generate
high-resolution augmented images of size 1024^2. Although the augmented images
generated are unprecedented, the training time of the model is exceptionally
high. Conventional GAN requires training of both Discriminator as well as the
Generator. In Progressive GAN, which is the current state-of-the-art GAN for
image augmentation, instead of training the GAN all at once, a new concept of
progressing growing of Discriminator and Generator simultaneously, was
proposed. Although the lower stages such as 4x4 and 8x8 train rather quickly,
the later stages consume a tremendous amount of time which could take days to
finish the model training. In our paper, we propose a novel pipeline that
combines Progressive GAN with slight modifications and Super Resolution GAN.
Super Resolution GAN up samples low-resolution images to high-resolution images
which can prove to be a useful resource to reduce the training time
exponentially.
",Atharva Karwande; Pranesh Kulkarni; Tejas Kolhe; Akshay Joshi; Soham Kamble,http://arxiv.org/abs/2202.12337v1,10.48550/arXiv.2202.12337
http://arxiv.org/abs/1907.03063v1,2019,MRI Super-Resolution with Ensemble Learning and Complementary Priors,"  Magnetic resonance imaging (MRI) is a widely used medical imaging modality.
However, due to the limitations in hardware, scan time, and throughput, it is
often clinically challenging to obtain high-quality MR images. The
super-resolution approach is potentially promising to improve MR image quality
without any hardware upgrade. In this paper, we propose an ensemble learning
and deep learning framework for MR image super-resolution. In our study, we
first enlarged low resolution images using 5 commonly used super-resolution
algorithms and obtained differentially enlarged image datasets with
complementary priors. Then, a generative adversarial network (GAN) is trained
with each dataset to generate super-resolution MR images. Finally, a
convolutional neural network is used for ensemble learning that synergizes the
outputs of GANs into the final MR super-resolution images. According to our
results, the ensemble learning results outcome any one of GAN outputs. Compared
with some state-of-the-art deep learning-based super-resolution methods, our
approach is advantageous in suppressing artifacts and keeping more image
details.
",Qing Lyu; Hongming Shan; Ge Wang,http://arxiv.org/abs/1907.03063v1,10.1109/TCI.2020.2964201
http://arxiv.org/abs/2003.03489v1,2020,"Super Resolution Using Segmentation-Prior Self-Attention Generative
  Adversarial Network","  Convolutional Neural Network (CNN) is intensively implemented to solve super
resolution (SR) tasks because of its superior performance. However, the problem
of super resolution is still challenging due to the lack of prior knowledge and
small receptive field of CNN. We propose the Segmentation-Piror Self-Attention
Generative Adversarial Network (SPSAGAN) to combine segmentation-priors and
feature attentions into a unified framework. This combination is led by a
carefully designed weighted addition to balance the influence of feature and
segmentation attentions, so that the network can emphasize textures in the same
segmentation category and meanwhile focus on the long-distance feature
relationship. We also propose a lightweight skip connection architecture called
Residual-in-Residual Sparse Block (RRSB) to further improve the
super-resolution performance and save computation. Extensive experiments show
that SPSAGAN can generate more realistic and visually pleasing textures
compared to state-of-the-art SFTGAN and ESRGAN on many SR datasets.
",Yuxin Zhang; Zuquan Zheng; Roland Hu,http://arxiv.org/abs/2003.03489v1,10.48550/arXiv.2003.03489
http://arxiv.org/abs/2212.13068v1,2022,"Transformer and GAN Based Super-Resolution Reconstruction Network for
  Medical Images","  Because of the necessity to obtain high-quality images with minimal radiation
doses, such as in low-field magnetic resonance imaging, super-resolution
reconstruction in medical imaging has become more popular (MRI). However, due
to the complexity and high aesthetic requirements of medical imaging, image
super-resolution reconstruction remains a difficult challenge. In this paper,
we offer a deep learning-based strategy for reconstructing medical images from
low resolutions utilizing Transformer and Generative Adversarial Networks
(T-GAN). The integrated system can extract more precise texture information and
focus more on important locations through global image matching after
successfully inserting Transformer into the generative adversarial network for
picture reconstruction. Furthermore, we weighted the combination of content
loss, adversarial loss, and adversarial feature loss as the final multi-task
loss function during the training of our proposed model T-GAN. In comparison to
established measures like PSNR and SSIM, our suggested T-GAN achieves optimal
performance and recovers more texture features in super-resolution
reconstruction of MRI scanned images of the knees and belly.
",Weizhi Du; Harvery Tian,http://arxiv.org/abs/2212.13068v1,10.48550/arXiv.2212.13068
http://arxiv.org/abs/1912.09497v1,2019,"Anisotropic Super Resolution in Prostate MRI using Super Resolution
  Generative Adversarial Networks","  Acquiring High Resolution (HR) Magnetic Resonance (MR) images requires the
patient to remain still for long periods of time, which causes patient
discomfort and increases the probability of motion induced image artifacts. A
possible solution is to acquire low resolution (LR) images and to process them
with the Super Resolution Generative Adversarial Network (SRGAN) to create a
super-resolved version. This work applies SRGAN to MR images of the prostate
and performs three experiments. The first experiment explores improving the
in-plane MR image resolution by factors of 4 and 8, and shows that, while the
PSNR and SSIM (Structural SIMilarity) metrics are lower than the isotropic
bicubic interpolation baseline, the SRGAN is able to create images that have
high edge fidelity. The second experiment explores anisotropic super-resolution
via synthetic images, in that the input images to the network are
anisotropically downsampled versions of HR images. This experiment demonstrates
the ability of the modified SRGAN to perform anisotropic super-resolution, with
quantitative image metrics that are comparable to those of the anisotropic
bicubic interpolation baseline. Finally, the third experiment applies a
modified version of the SRGAN to super-resolve anisotropic images obtained from
the through-plane slices of the volumetric MR data. The output super-resolved
images contain a significant amount of high frequency information that make
them visually close to their HR counterparts. Overall, the promising results
from each experiment show that super-resolution for MR images is a successful
technique and that producing isotropic MR image volumes from anisotropic slices
is an achievable goal.
",Rewa Sood; Mirabela Rusu,http://arxiv.org/abs/1912.09497v1,10.1109/ISBI.2019.8759237
http://arxiv.org/abs/2111.08685v1,2021,"A Latent Encoder Coupled Generative Adversarial Network (LE-GAN) for
  Efficient Hyperspectral Image Super-resolution","  Realistic hyperspectral image (HSI) super-resolution (SR) techniques aim to
generate a high-resolution (HR) HSI with higher spectral and spatial fidelity
from its low-resolution (LR) counterpart. The generative adversarial network
(GAN) has proven to be an effective deep learning framework for image
super-resolution. However, the optimisation process of existing GAN-based
models frequently suffers from the problem of mode collapse, leading to the
limited capacity of spectral-spatial invariant reconstruction. This may cause
the spectral-spatial distortion on the generated HSI, especially with a large
upscaling factor. To alleviate the problem of mode collapse, this work has
proposed a novel GAN model coupled with a latent encoder (LE-GAN), which can
map the generated spectral-spatial features from the image space to the latent
space and produce a coupling component to regularise the generated samples.
Essentially, we treat an HSI as a high-dimensional manifold embedded in a
latent space. Thus, the optimisation of GAN models is converted to the problem
of learning the distributions of high-resolution HSI samples in the latent
space, making the distributions of the generated super-resolution HSIs closer
to those of their original high-resolution counterparts. We have conducted
experimental evaluations on the model performance of super-resolution and its
capability in alleviating mode collapse. The proposed approach has been tested
and validated based on two real HSI datasets with different sensors (i.e.
AVIRIS and UHD-185) for various upscaling factors and added noise levels, and
compared with the state-of-the-art super-resolution models (i.e. HyCoNet, LTTR,
BAGAN, SR- GAN, WGAN).
",Yue Shi; Liangxiu Han; Lianghao Han; Sheng Chang; Tongle Hu; Darren Dancey,http://arxiv.org/abs/2111.08685v1,10.1109/TGRS.2022.3193441
http://arxiv.org/abs/2110.05047v1,2021,"Super-resolution reconstruction of turbulent flow at various Reynolds
  numbers based on generative adversarial networks","  This study presents a deep learning-based framework to reconstruct
high-resolution turbulent velocity fields from extremely low-resolution data at
various Reynolds numbers using the concept of generative adversarial networks
(GANs). A multiscale enhanced super-resolution generative adversarial network
(MS-ESRGAN) is applied as a model to reconstruct the high-resolution velocity
fields, and direct numerical simulation (DNS) data of turbulent channel flow
with large longitudinal ribs at various Reynolds numbers are used to evaluate
the performance of the model. The model is found to have the capacity to
accurately reproduce high-resolution velocity fields from data at two different
low-resolution levels in terms of the quantities of velocity fields and
turbulent statistics. The results further reveal that the model is able to
reconstruct velocity fields at Reynolds numbers that are not used in the
training process.
",Mustafa Z. Yousif; Linqi Yu; Hee-Chang Lim,http://arxiv.org/abs/2110.05047v1,10.1063/5.0074724
http://arxiv.org/abs/2111.14320v1,2021,"SwiftSRGAN -- Rethinking Super-Resolution for Efficient and Real-time
  Inference","  In recent years, there have been several advancements in the task of image
super-resolution using the state of the art Deep Learning-based architectures.
Many super-resolution-based techniques previously published, require high-end
and top-of-the-line Graphics Processing Unit (GPUs) to perform image
super-resolution. With the increasing advancements in Deep Learning approaches,
neural networks have become more and more compute hungry. We took a step back
and, focused on creating a real-time efficient solution. We present an
architecture that is faster and smaller in terms of its memory footprint. The
proposed architecture uses Depth-wise Separable Convolutions to extract
features and, it performs on-par with other super-resolution GANs (Generative
Adversarial Networks) while maintaining real-time inference and a low memory
footprint. A real-time super-resolution enables streaming high resolution media
content even under poor bandwidth conditions. While maintaining an efficient
trade-off between the accuracy and latency, we are able to produce a comparable
performance model which is one-eighth (1/8) the size of super-resolution GANs
and computes 74 times faster than super-resolution GANs.
",Koushik Sivarama Krishnan; Karthik Sivarama Krishnan,http://arxiv.org/abs/2111.14320v1,10.1109/ICICyTA53712.2021.9689188
http://arxiv.org/abs/1807.00270v1,2018,"Performance Comparison of Convolutional AutoEncoders, Generative
  Adversarial Networks and Super-Resolution for Image Compression","  Image compression has been investigated for many decades. Recently, deep
learning approaches have achieved a great success in many computer vision
tasks, and are gradually used in image compression. In this paper, we develop
three overall compression architectures based on convolutional autoencoders
(CAEs), generative adversarial networks (GANs) as well as super-resolution
(SR), and present a comprehensive performance comparison. According to
experimental results, CAEs achieve better coding efficiency than JPEG by
extracting compact features. GANs show potential advantages on large
compression ratio and high subjective quality reconstruction. Super-resolution
achieves the best rate-distortion (RD) performance among them, which is
comparable to BPG.
",Zhengxue Cheng; Heming Sun; Masaru Takeuchi; Jiro Katto,http://arxiv.org/abs/1807.00270v1,10.48550/arXiv.1807.00270
http://arxiv.org/abs/1809.00437v1,2018,"Unsupervised Image Super-Resolution using Cycle-in-Cycle Generative
  Adversarial Networks","  We consider the single image super-resolution problem in a more general case
that the low-/high-resolution pairs and the down-sampling process are
unavailable. Different from traditional super-resolution formulation, the
low-resolution input is further degraded by noises and blurring. This
complicated setting makes supervised learning and accurate kernel estimation
impossible. To solve this problem, we resort to unsupervised learning without
paired data, inspired by the recent successful image-to-image translation
applications. With generative adversarial networks (GAN) as the basic
component, we propose a Cycle-in-Cycle network structure to tackle the problem
within three steps. First, the noisy and blurry input is mapped to a noise-free
low-resolution space. Then the intermediate image is up-sampled with a
pre-trained deep model. Finally, we fine-tune the two modules in an end-to-end
manner to get the high-resolution output. Experiments on NTIRE2018 datasets
demonstrate that the proposed unsupervised method achieves comparable results
as the state-of-the-art supervised models.
",Yuan Yuan; Siyuan Liu; Jiawei Zhang; Yongbing Zhang; Chao Dong; Liang Lin,http://arxiv.org/abs/1809.00437v1,10.48550/arXiv.1809.00437
http://arxiv.org/abs/1902.02144v2,2019,"Progressive Generative Adversarial Networks for Medical Image Super
  resolution","  Anatomical landmark segmentation and pathology localization are important
steps in automated analysis of medical images. They are particularly
challenging when the anatomy or pathology is small, as in retinal images and
cardiac MRI, or when the image is of low quality due to device acquisition
parameters as in magnetic resonance (MR) scanners. We propose an image
super-resolution method using progressive generative adversarial networks
(P-GAN) that can take as input a low-resolution image and generate a high
resolution image of desired scaling factor. The super resolved images can be
used for more accurate detection of landmarks and pathology. Our primary
contribution is in proposing a multistage model where the output image quality
of one stage is progressively improved in the next stage by using a triplet
loss function. The triplet loss enables stepwise image quality improvement by
using the output of the previous stage as the baseline. This facilitates
generation of super resolved images of high scaling factor while maintaining
good image quality. Experimental results for image super-resolution show that
our proposed multistage P-GAN outperforms competing methods and baseline GAN.
",Dwarikanath Mahapatra; Behzad Bozorgtabar,http://arxiv.org/abs/1902.02144v2,10.48550/arXiv.1902.02144
http://arxiv.org/abs/2010.04506v1,2020,Phase-aware music super-resolution using generative adversarial networks,"  Audio super-resolution is a challenging task of recovering the missing
high-resolution features from a low-resolution signal. To address this,
generative adversarial networks (GAN) have been used to achieve promising
results by training the mappings between magnitudes of the low and
high-frequency components. However, phase information is not well-considered
for waveform reconstruction in conventional methods. In this paper, we tackle
the problem of music super-resolution and conduct a thorough investigation on
the importance of phase for this task. We use GAN to predict the magnitudes of
the high-frequency components. The corresponding phase information can be
extracted using either a GAN-based waveform synthesis system or a modified
Griffin-Lim algorithm. Experimental results show that phase information plays
an important role in the improvement of the reconstructed music quality.
Moreover, our proposed method significantly outperforms other state-of-the-art
methods in terms of objective evaluations.
",Shichao Hu; Bin Zhang; Beici Liang; Ethan Zhao; Simon Lui,http://arxiv.org/abs/2010.04506v1,10.48550/arXiv.2010.04506
http://arxiv.org/abs/2011.04145v1,2020,"Fine Perceptive GANs for Brain MR Image Super-Resolution in Wavelet
  Domain","  Magnetic resonance imaging plays an important role in computer-aided
diagnosis and brain exploration. However, limited by hardware, scanning time
and cost, it's challenging to acquire high-resolution (HR) magnetic resonance
(MR) image clinically. In this paper, fine perceptive generative adversarial
networks (FP-GANs) is proposed to produce HR MR images from low-resolution
counterparts. It can cope with the detail insensitive problem of the existing
super-resolution model in a divide-and-conquer manner. Specifically, FP-GANs
firstly divides an MR image into low-frequency global approximation and
high-frequency anatomical texture in wavelet domain. Then each sub-band
generative adversarial network (sub-band GAN) conquers the super-resolution
procedure of each single sub-band image. Meanwhile, sub-band attention is
deployed to tune focus between global and texture information. It can focus on
sub-band images instead of feature maps to further enhance the anatomical
reconstruction ability of FP-GANs. In addition, inverse discrete wavelet
transformation (IDWT) is integrated into model for taking the reconstruction of
whole image into account. Experiments on MultiRes_7T dataset demonstrate that
FP-GANs outperforms the competing methods quantitatively and qualitatively.
",Senrong You; Yong Liu; Baiying Lei; Shuqiang Wang,http://arxiv.org/abs/2011.04145v1,10.48550/arXiv.2011.04145
http://arxiv.org/abs/2206.09193v2,2022,"Multi-Modality Image Super-Resolution using Generative Adversarial
  Networks","  Over the past few years deep learning-based techniques such as Generative
Adversarial Networks (GANs) have significantly improved solutions to image
super-resolution and image-to-image translation problems. In this paper, we
propose a solution to the joint problem of image super-resolution and
multi-modality image-to-image translation. The problem can be stated as the
recovery of a high-resolution image in a modality, given a low-resolution
observation of the same image in an alternative modality. Our paper offers two
models to address this problem and will be evaluated on the recovery of
high-resolution day images given low-resolution night images of the same scene.
Promising qualitative and quantitative results will be presented for each
model.
",Aref Abedjooy; Mehran Ebrahimi,http://arxiv.org/abs/2206.09193v2,10.48550/arXiv.2206.09193
http://arxiv.org/abs/2207.08036v1,2022,Single MR Image Super-Resolution using Generative Adversarial Network,"  Spatial resolution of medical images can be improved using super-resolution
methods. Real Enhanced Super Resolution Generative Adversarial Network
(Real-ESRGAN) is one of the recent effective approaches utilized to produce
higher resolution images, given input images of lower resolution. In this
paper, we apply this method to enhance the spatial resolution of 2D MR images.
In our proposed approach, we slightly modify the structure of the Real-ESRGAN
to train 2D Magnetic Resonance images (MRI) taken from the Brain Tumor
Segmentation Challenge (BraTS) 2018 dataset. The obtained results are validated
qualitatively and quantitatively by computing SSIM (Structural Similarity Index
Measure), NRMSE (Normalized Root Mean Square Error), MAE (Mean Absolute Error),
and VIF (Visual Information Fidelity) values.
",Shawkh Ibne Rashid; Elham Shakibapour; Mehran Ebrahimi,http://arxiv.org/abs/2207.08036v1,10.48550/arXiv.2207.08036
http://arxiv.org/abs/2211.03550v3,2022,"Underwater Image Super-Resolution using Generative Adversarial
  Network-based Model","  Single image super-resolution (SISR) methods can enhance the resolution and
quality of underwater images. Enhancing the resolution of underwater images
leads to better performance of autonomous underwater vehicles (AUVs). In this
work, we fine-tune the Real-Enhanced Super-Resolution Generative Adversarial
Network (Real-ESRGAN) model to increase the resolution of underwater images. In
our proposed approach, the pre-trained generator and discriminator networks of
the Real-ESRGAN model are fine-tuned using underwater image datasets. We used
USR-248 and UFO-120 datasets to fine-tune the Real-ESRGAN model. Our fine-tuned
model produces images with better resolution and quality compared to the
original model.
",Alireza Aghelan,http://arxiv.org/abs/2211.03550v3,10.48550/arXiv.2211.03550
http://arxiv.org/abs/2104.10268v1,2021,"TWIST-GAN: Towards Wavelet Transform and Transferred GAN for
  Spatio-Temporal Single Image Super Resolution","  Single Image Super-resolution (SISR) produces high-resolution images with
fine spatial resolutions from aremotely sensed image with low spatial
resolution. Recently, deep learning and generative adversarial networks(GANs)
have made breakthroughs for the challenging task of single image
super-resolution (SISR). However, thegenerated image still suffers from
undesirable artifacts such as, the absence of texture-feature representationand
high-frequency information. We propose a frequency domain-based spatio-temporal
remote sensingsingle image super-resolution technique to reconstruct the HR
image combined with generative adversarialnetworks (GANs) on various frequency
bands (TWIST-GAN). We have introduced a new method incorporatingWavelet
Transform (WT) characteristics and transferred generative adversarial network.
The LR image hasbeen split into various frequency bands by using the WT,
whereas, the transfer generative adversarial networkpredicts high-frequency
components via a proposed architecture. Finally, the inverse transfer of
waveletsproduces a reconstructed image with super-resolution. The model is
first trained on an external DIV2 Kdataset and validated with the UC Merceed
Landsat remote sensing dataset and Set14 with each image sizeof 256x256.
Following that, transferred GANs are used to process spatio-temporal remote
sensing images inorder to minimize computation cost differences and improve
texture information. The findings are comparedqualitatively and qualitatively
with the current state-of-art approaches. In addition, we saved about 43% of
theGPU memory during training and accelerated the execution of our simplified
version by eliminating batchnormalization layers.
",Fayaz Ali Dharejo; Farah Deeba; Yuanchun Zhou; Bhagwan Das; Munsif Ali Jatoi; Muhammad Zawish; Yi Du; Xuezhi Wang,http://arxiv.org/abs/2104.10268v1,10.1145/3456726
http://arxiv.org/abs/2211.10563v2,2022,"Real-World Image Super Resolution via Unsupervised Bi-directional Cycle
  Domain Transfer Learning based Generative Adversarial Network","  Deep Convolutional Neural Networks (DCNNs) have exhibited impressive
performance on image super-resolution tasks. However, these deep learning-based
super-resolution methods perform poorly in real-world super-resolution tasks,
where the paired high-resolution and low-resolution images are unavailable and
the low-resolution images are degraded by complicated and unknown kernels. To
break these limitations, we propose the Unsupervised Bi-directional Cycle
Domain Transfer Learning-based Generative Adversarial Network (UBCDTL-GAN),
which consists of an Unsupervised Bi-directional Cycle Domain Transfer Network
(UBCDTN) and the Semantic Encoder guided Super Resolution Network (SESRN).
First, the UBCDTN is able to produce an approximated real-like LR image through
transferring the LR image from an artificially degraded domain to the
real-world LR image domain. Second, the SESRN has the ability to super-resolve
the approximated real-like LR image to a photo-realistic HR image. Extensive
experiments on unpaired real-world image benchmark datasets demonstrate that
the proposed method achieves superior performance compared to state-of-the-art
methods.
",Xiang Wang; Yimin Yang; Zhichang Guo; Zhili Zhou; Yu Liu; Qixiang Pang; Shan Du,http://arxiv.org/abs/2211.10563v2,10.48550/arXiv.2211.10563
http://arxiv.org/abs/2201.00601v1,2022,"Generative adversarial network for super-resolution imaging through a
  fiber","  A multimode fiber represents the ultimate limit in miniaturization of imaging
endoscopes. Here we propose a fiber imaging approach employing compressive
sensing with a data-driven machine learning framework. We implement a
generative adversarial network for image reconstruction without relying on a
sample sparsity constraint. The proposed method outperforms the conventional
compressive imaging algorithms in terms of image quality and noise robustness.
We experimentally demonstrate speckle-based imaging below the diffraction limit
at a sub-Nyquist speed through a multimode fiber.
",Wei Li; Ksenia Abrashitova; Gerwin Osnabrugge; Lyubov V. Amitonova,http://arxiv.org/abs/2201.00601v1,10.1103/PhysRevApplied.18.034075
http://arxiv.org/abs/2007.15324v1,2020,"Unsupervised deep learning for super-resolution reconstruction of
  turbulence","  Recent attempts to use deep learning for super-resolution reconstruction of
turbulent flows have used supervised learning, which requires paired data for
training. This limitation hinders more practical applications of
super-resolution reconstruction. Therefore, we present an unsupervised learning
model that adopts a cycle-consistent generative adversarial network that can be
trained with unpaired turbulence data for super-resolution reconstruction. Our
model is validated using three examples: (i) recovering the original flow field
from filtered data using direct numerical simulation (DNS) of homogeneous
isotropic turbulence; (ii) reconstructing full-resolution fields using
partially measured data from the DNS of turbulent channel flows; and (iii)
generating a DNS-resolution flow field from large eddy simulation (LES) data
for turbulent channel flows. In examples (i) and (ii), for which paired data
are available for supervised learning, our unsupervised model demonstrates
qualitatively and quantitatively similar performance as that of the best
supervised-learning model. More importantly, in example (iii), where supervised
learning is impossible, our model successfully reconstructs the high-resolution
flow field of statistical DNS quality from the LES data. This demonstrates that
unsupervised learning of turbulence data is indeed possible, opening a new door
for the wide application of super-resolution reconstruction of turbulent
fields.
",Hyojin Kim; Junhyuk Kim; Sungjin Won; Changghoon Lee,http://arxiv.org/abs/2007.15324v1,10.1017/jfm.2020.1028
http://arxiv.org/abs/2210.15887v1,2022,"Nonparallel High-Quality Audio Super Resolution with Domain Adaptation
  and Resampling CycleGANs","  Neural audio super-resolution models are typically trained on low- and
high-resolution audio signal pairs. Although these methods achieve highly
accurate super-resolution if the acoustic characteristics of the input data are
similar to those of the training data, challenges remain: the models suffer
from quality degradation for out-of-domain data, and paired data are required
for training. To address these problems, we propose Dual-CycleGAN, a
high-quality audio super-resolution method that can utilize unpaired data based
on two connected cycle consistent generative adversarial networks (CycleGAN).
Our method decomposes the super-resolution method into domain adaptation and
resampling processes to handle acoustic mismatch in the unpaired low- and
high-resolution signals. The two processes are then jointly optimized within
the CycleGAN framework. Experimental results verify that the proposed method
significantly outperforms conventional methods when paired data are not
available. Code and audio samples are available from
https://chomeyama.github.io/DualCycleGAN-Demo/.
",Reo Yoneyama; Ryuichi Yamamoto; Kentaro Tachibana,http://arxiv.org/abs/2210.15887v1,10.48550/arXiv.2210.15887
http://arxiv.org/abs/1810.05731v1,2018,Image Super-Resolution Using VDSR-ResNeXt and SRCGAN,"  Over the past decade, many Super Resolution techniques have been developed
using deep learning. Among those, generative adversarial networks (GAN) and
very deep convolutional networks (VDSR) have shown promising results in terms
of HR image quality and computational speed. In this paper, we propose two
approaches based on these two algorithms: VDSR-ResNeXt, which is a deep
multi-branch convolutional network inspired by VDSR and ResNeXt; and SRCGAN,
which is a conditional GAN that explicitly passes class labels as input to the
GAN. The two methods were implemented on common SR benchmark datasets for both
quantitative and qualitative assessment.
",Saifuddin Hitawala; Yao Li; Xian Wang; Dongyang Yang,http://arxiv.org/abs/1810.05731v1,10.48550/arXiv.1810.05731
http://arxiv.org/abs/1708.09105v3,2017,"Simultaneously Color-Depth Super-Resolution with Conditional Generative
  Adversarial Network","  Recently, Generative Adversarial Network (GAN) has been found wide
applications in style transfer, image-to-image translation and image
super-resolution. In this paper, a color-depth conditional GAN is proposed to
concurrently resolve the problems of depth super-resolution and color
super-resolution in 3D videos. Firstly, given the low-resolution depth image
and low-resolution color image, a generative network is proposed to leverage
mutual information of color image and depth image to enhance each other in
consideration of the geometry structural dependency of color-depth image in the
same scene. Secondly, three loss functions, including data loss, total
variation loss, and 8-connected gradient difference loss are introduced to
train this generative network in order to keep generated images close to the
real ones, in addition to the adversarial loss. Experimental results
demonstrate that the proposed approach produces high-quality color image and
depth image from low-quality image pair, and it is superior to several other
leading methods. Besides, we use the same neural network framework to resolve
the problem of image smoothing and edge detection at the same time.
",Lijun Zhao; Huihui Bai; Jie Liang; Bing Zeng; Anhong Wang; Yao Zhao,http://arxiv.org/abs/1708.09105v3,10.48550/arXiv.1708.09105
http://arxiv.org/abs/2009.03693v1,2020,"Deep Cyclic Generative Adversarial Residual Convolutional Networks for
  Real Image Super-Resolution","  Recent deep learning based single image super-resolution (SISR) methods
mostly train their models in a clean data domain where the low-resolution (LR)
and the high-resolution (HR) images come from noise-free settings (same domain)
due to the bicubic down-sampling assumption. However, such degradation process
is not available in real-world settings. We consider a deep cyclic network
structure to maintain the domain consistency between the LR and HR data
distributions, which is inspired by the recent success of CycleGAN in the
image-to-image translation applications. We propose the Super-Resolution
Residual Cyclic Generative Adversarial Network (SRResCycGAN) by training with a
generative adversarial network (GAN) framework for the LR to HR domain
translation in an end-to-end manner. We demonstrate our proposed approach in
the quantitative and qualitative experiments that generalize well to the real
image super-resolution and it is easy to deploy for the mobile/embedded
devices. In addition, our SR results on the AIM 2020 Real Image SR Challenge
datasets demonstrate that the proposed SR approach achieves comparable results
as the other state-of-art methods.
",Rao Muhammad Umer; Christian Micheloni,http://arxiv.org/abs/2009.03693v1,10.48550/arXiv.2009.03693
http://arxiv.org/abs/2010.14105v2,2020,"Micro-CT Synthesis and Inner Ear Super Resolution via Generative
  Adversarial Networks and Bayesian Inference","  Existing medical image super-resolution methods rely on pairs of low- and
high- resolution images to learn a mapping in a fully supervised manner.
However, such image pairs are often not available in clinical practice. In this
paper, we address super-resolution problem in a real-world scenario using
unpaired data and synthesize linearly \textbf{eight times} higher resolved
Micro-CT images of temporal bone structure, which is embedded in the inner ear.
We explore cycle-consistency generative adversarial networks for
super-resolution task and equip the translation approach with Bayesian
inference. We further introduce \emph{Hu Moment distance} the evaluation metric
to quantify the shape of the temporal bone. We evaluate our method on a public
inner ear CT dataset and have seen both visual and quantitative improvement
over state-of-the-art deep-learning-based methods. In addition, we perform a
multi-rater visual evaluation experiment and find that trained experts
consistently rate the proposed method the highest quality scores among all
methods. Furthermore, we are able to quantify uncertainty in the unpaired
translation task and the uncertainty map can provide structural information of
the temporal bone.
",Hongwei Li; Rameshwara G. N. Prasad; Anjany Sekuboyina; Chen Niu; Siwei Bai; Werner Hemmert; Bjoern Menze,http://arxiv.org/abs/2010.14105v2,10.48550/arXiv.2010.14105
http://arxiv.org/abs/2211.12180v1,2022,"SRTGAN: Triplet Loss based Generative Adversarial Network for Real-World
  Super-Resolution","  Many applications such as forensics, surveillance, satellite imaging, medical
imaging, etc., demand High-Resolution (HR) images. However, obtaining an HR
image is not always possible due to the limitations of optical sensors and
their costs. An alternative solution called Single Image Super-Resolution
(SISR) is a software-driven approach that aims to take a Low-Resolution (LR)
image and obtain the HR image. Most supervised SISR solutions use ground truth
HR image as a target and do not include the information provided in the LR
image, which could be valuable. In this work, we introduce Triplet Loss-based
Generative Adversarial Network hereafter referred as SRTGAN for Image
Super-Resolution problem on real-world degradation. We introduce a new
triplet-based adversarial loss function that exploits the information provided
in the LR image by using it as a negative sample. Allowing the patch-based
discriminator with access to both HR and LR images optimizes to better
differentiate between HR and LR images; hence, improving the adversary.
Further, we propose to fuse the adversarial loss, content loss, perceptual
loss, and quality loss to obtain Super-Resolution (SR) image with high
perceptual fidelity. We validate the superior performance of the proposed
method over the other existing methods on the RealSR dataset in terms of
quantitative and qualitative metrics.
",Dhruv Patel; Abhinav Jain; Simran Bawkar; Manav Khorasiya; Kalpesh Prajapati; Kishor Upla; Kiran Raja; Raghavendra Ramachandra; Christoph Busch,http://arxiv.org/abs/2211.12180v1,10.48550/arXiv.2211.12180
http://arxiv.org/abs/1807.11458v1,2018,"To learn image super-resolution, use a GAN to learn how to do image
  degradation first","  This paper is on image and face super-resolution. The vast majority of prior
work for this problem focus on how to increase the resolution of low-resolution
images which are artificially generated by simple bilinear down-sampling (or in
a few cases by blurring followed by down-sampling).We show that such methods
fail to produce good results when applied to real-world low-resolution, low
quality images. To circumvent this problem, we propose a two-stage process
which firstly trains a High-to-Low Generative Adversarial Network (GAN) to
learn how to degrade and downsample high-resolution images requiring, during
training, only unpaired high and low-resolution images. Once this is achieved,
the output of this network is used to train a Low-to-High GAN for image
super-resolution using this time paired low- and high-resolution images. Our
main result is that this network can be now used to efectively increase the
quality of real-world low-resolution images. We have applied the proposed
pipeline for the problem of face super-resolution where we report large
improvement over baselines and prior work although the proposed method is
potentially applicable to other object categories.
",Adrian Bulat; Jing Yang; Georgios Tzimiropoulos,http://arxiv.org/abs/1807.11458v1,10.48550/arXiv.1807.11458
http://arxiv.org/abs/1908.04297v1,2019,Super-resolution of Omnidirectional Images Using Adversarial Learning,"  An omnidirectional image (ODI) enables viewers to look in every direction
from a fixed point through a head-mounted display providing an immersive
experience compared to that of a standard image. Designing immersive virtual
reality systems with ODIs is challenging as they require high resolution
content. In this paper, we study super-resolution for ODIs and propose an
improved generative adversarial network based model which is optimized to
handle the artifacts obtained in the spherical observational space.
Specifically, we propose to use a fast PatchGAN discriminator, as it needs
fewer parameters and improves the super-resolution at a fine scale. We also
explore the generative models with adversarial learning by introducing a
spherical-content specific loss function, called 360-SS. To train and test the
performance of our proposed model we prepare a dataset of 4500 ODIs. Our
results demonstrate the efficacy of the proposed method and identify new
challenges in ODI super-resolution for future investigations.
",Cagri Ozcinar; Aakanksha Rana; Aljosa Smolic,http://arxiv.org/abs/1908.04297v1,10.48550/arXiv.1908.04297
http://arxiv.org/abs/1911.07934v3,2019,Training Set Effect on Super Resolution for Automated Target Recognition,"  Single Image Super Resolution (SISR) is the process of mapping a
low-resolution image to a high resolution image. This inherently has
applications in remote sensing as a way to increase the spatial resolution in
satellite imagery. This suggests a possible improvement to automated target
recognition in image classification and object detection. We explore the effect
that different training sets have on SISR with the network, Super Resolution
Generative Adversarial Network (SRGAN). We train 5 SRGANs on different land-use
classes (e.g. agriculture, cities, ports) and test them on the same unseen
dataset. We attempt to find the qualitative and quantitative differences in
SISR, binary classification, and object detection performance. We find that
curated training sets that contain objects in the test ontology perform better
on both computer vision tasks while having a complex distribution of images
allows object detection models to perform better. However, Super Resolution
(SR) might not be beneficial to certain problems and will see a diminishing
amount of returns for datasets that are closer to being solved.
",Matthew Ciolino; David Noever; Josh Kalin,http://arxiv.org/abs/1911.07934v3,10.48550/arXiv.1911.07934
http://arxiv.org/abs/2010.06499v1,2020,LASSR: Effective Super-Resolution Method for Plant Disease Diagnosis,"  The collection of high-resolution training data is crucial in building robust
plant disease diagnosis systems, since such data have a significant impact on
diagnostic performance. However, they are very difficult to obtain and are not
always available in practice. Deep learning-based techniques, and particularly
generative adversarial networks (GANs), can be applied to generate high-quality
super-resolution images, but these methods often produce unexpected artifacts
that can lower the diagnostic performance. In this paper, we propose a novel
artifact-suppression super-resolution method that is specifically designed for
diagnosing leaf disease, called Leaf Artifact-Suppression Super Resolution
(LASSR). Thanks to its own artifact removal module that detects and suppresses
artifacts to a considerable extent, LASSR can generate much more pleasing,
high-quality images compared to the state-of-the-art ESRGAN model. Experiments
based on a five-class cucumber disease (including healthy) discrimination model
show that training with data generated by LASSR significantly boosts the
performance on an unseen test dataset by nearly 22% compared with the baseline,
and that our approach is more than 2% better than a model trained with images
generated by ESRGAN.
",Quan Huu Cap; Hiroki Tani; Hiroyuki Uga; Satoshi Kagiwada; Hitoshi Iyatomi,http://arxiv.org/abs/2010.06499v1,10.48550/arXiv.2010.06499
http://arxiv.org/abs/2106.06011v1,2021,"A self-adapting super-resolution structures framework for automatic
  design of GAN","  With the development of deep learning, the single super-resolution image
reconstruction network models are becoming more and more complex. Small changes
in hyperparameters of the models have a greater impact on model performance. In
the existing works, experts have gradually explored a set of optimal model
parameters based on empirical values or performing brute-force search. In this
paper, we introduce a new super-resolution image reconstruction generative
adversarial network framework, and a Bayesian optimization method used to
optimizing the hyperparameters of the generator and discriminator. The
generator is made by self-calibrated convolution, and discriminator is made by
convolution lays. We have defined the hyperparameters such as the number of
network layers and the number of neurons. Our method adopts Bayesian
optimization as a optimization policy of GAN in our model. Not only can find
the optimal hyperparameter solution automatically, but also can construct a
super-resolution image reconstruction network, reducing the manual workload.
Experiments show that Bayesian optimization can search the optimal solution
earlier than the other two optimization algorithms.
",Yibo Guo; Haidi Wang; Yiming Fan; Shunyao Li; Mingliang Xu,http://arxiv.org/abs/2106.06011v1,10.48550/arXiv.2106.06011
http://arxiv.org/abs/2107.03145v1,2021,"A Deep Residual Star Generative Adversarial Network for multi-domain
  Image Super-Resolution","  Recently, most of state-of-the-art single image super-resolution (SISR)
methods have attained impressive performance by using deep convolutional neural
networks (DCNNs). The existing SR methods have limited performance due to a
fixed degradation settings, i.e. usually a bicubic downscaling of
low-resolution (LR) image. However, in real-world settings, the LR degradation
process is unknown which can be bicubic LR, bilinear LR, nearest-neighbor LR,
or real LR. Therefore, most SR methods are ineffective and inefficient in
handling more than one degradation settings within a single network. To handle
the multiple degradation, i.e. refers to multi-domain image super-resolution,
we propose a deep Super-Resolution Residual StarGAN (SR2*GAN), a novel and
scalable approach that super-resolves the LR images for the multiple LR domains
using only a single model. The proposed scheme is trained in a StarGAN like
network topology with a single generator and discriminator networks. We
demonstrate the effectiveness of our proposed approach in quantitative and
qualitative experiments compared to other state-of-the-art methods.
",Rao Muhammad Umer; Asad Munir; Christian Micheloni,http://arxiv.org/abs/2107.03145v1,10.48550/arXiv.2107.03145
http://arxiv.org/abs/1911.08711v1,2019,"Dual Reconstruction with Densely Connected Residual Network for Single
  Image Super-Resolution","  Deep learning-based single image super-resolution enables very fast and
high-visual-quality reconstruction. Recently, an enhanced super-resolution
based on generative adversarial network (ESRGAN) has achieved excellent
performance in terms of both qualitative and quantitative quality of the
reconstructed high-resolution image. In this paper, we propose to add one more
shortcut between two dense-blocks, as well as add shortcut between two
convolution layers inside a dense-block. With this simple strategy of adding
more shortcuts in the proposed network, it enables a faster learning process as
the gradient information can be back-propagated more easily. Based on the
improved ESRGAN, the dual reconstruction is proposed to learn different aspects
of the super-resolved image for judiciously enhancing the quality of the
reconstructed image. In practice, the super-resolution model is pre-trained
solely based on pixel distance, followed by fine-tuning the parameters in the
model based on adversarial loss and perceptual loss. Finally, we fuse two
different models by weighted-summing their parameters to obtain the final
super-resolution model. Experimental results demonstrated that the proposed
method achieves excellent performance in the real-world image super-resolution
challenge. We have also verified that the proposed dual reconstruction does
further improve the quality of the reconstructed image in terms of both PSNR
and SSIM.
",Chih-Chung Hsu; Chia-Hsiang Lin,http://arxiv.org/abs/1911.08711v1,10.48550/arXiv.1911.08711
http://arxiv.org/abs/2004.02086v2,2020,Arbitrary Scale Super-Resolution for Brain MRI Images,"  Recent attempts at Super-Resolution for medical images used deep learning
techniques such as Generative Adversarial Networks (GANs) to achieve
perceptually realistic single image Super-Resolution. Yet, they are constrained
by their inability to generalise to different scale factors. This involves high
storage and energy costs as every integer scale factor involves a separate
neural network. A recent paper has proposed a novel meta-learning technique
that uses a Weight Prediction Network to enable Super-Resolution on arbitrary
scale factors using only a single neural network. In this paper, we propose a
new network that combines that technique with SRGAN, a state-of-the-art
GAN-based architecture, to achieve arbitrary scale, high fidelity
Super-Resolution for medical images. By using this network to perform arbitrary
scale magnifications on images from the Multimodal Brain Tumor Segmentation
Challenge (BraTS) dataset, we demonstrate that it is able to outperform
traditional interpolation methods by up to 20$\%$ on SSIM scores whilst
retaining generalisability on brain MRI images. We show that performance across
scales is not compromised, and that it is able to achieve competitive results
with other state-of-the-art methods such as EDSR whilst being fifty times
smaller than them. Combining efficiency, performance, and generalisability,
this can hopefully become a new foundation for tackling Super-Resolution on
medical images.
  Check out the webapp here: https://metasrgan.herokuapp.com/ Check out the
github tutorial here: https://github.com/pancakewaffles/metasrgan-tutorial
",Chuan Tan; Jin Zhu; Pietro Lio',http://arxiv.org/abs/2004.02086v2,10.48550/arXiv.2004.02086
http://arxiv.org/abs/1710.04783v3,2017,"Retinal Vasculature Segmentation Using Local Saliency Maps and
  Generative Adversarial Networks For Image Super Resolution","  We propose an image super resolution(ISR) method using generative adversarial
networks (GANs) that takes a low resolution input fundus image and generates a
high resolution super resolved (SR) image upto scaling factor of $16$. This
facilitates more accurate automated image analysis, especially for small or
blurred landmarks and pathologies. Local saliency maps, which define each
pixel's importance, are used to define a novel saliency loss in the GAN cost
function. Experimental results show the resulting SR images have perceptual
quality very close to the original images and perform better than competing
methods that do not weigh pixels according to their importance. When used for
retinal vasculature segmentation, our SR images result in accuracy levels close
to those obtained when using the original images.
",Dwarikanath Mahapatra; Behzad Bozorgtabar,http://arxiv.org/abs/1710.04783v3,10.48550/arXiv.1710.04783
http://arxiv.org/abs/1903.09027v1,2019,Bandwidth Extension on Raw Audio via Generative Adversarial Networks,"  Neural network-based methods have recently demonstrated state-of-the-art
results on image synthesis and super-resolution tasks, in particular by using
variants of generative adversarial networks (GANs) with supervised feature
losses. Nevertheless, previous feature loss formulations rely on the
availability of large auxiliary classifier networks, and labeled datasets that
enable such classifiers to be trained. Furthermore, there has been
comparatively little work to explore the applicability of GAN-based methods to
domains other than images and video. In this work we explore a GAN-based method
for audio processing, and develop a convolutional neural network architecture
to perform audio super-resolution. In addition to several new architectural
building blocks for audio processing, a key component of our approach is the
use of an autoencoder-based loss that enables training in the GAN framework,
with feature losses derived from unlabeled data. We explore the impact of our
architectural choices, and demonstrate significant improvements over previous
works in terms of both objective and perceptual quality.
",Sung Kim; Visvesh Sathe,http://arxiv.org/abs/1903.09027v1,10.48550/arXiv.1903.09027
http://arxiv.org/abs/1907.04835v2,2019,"Enhanced generative adversarial network for 3D brain MRI
  super-resolution","  Single image super-resolution (SISR) reconstruction for magnetic resonance
imaging (MRI) has generated significant interest because of its potential to
not only speed up imaging but to improve quantitative processing and analysis
of available image data. Generative Adversarial Networks (GAN) have proven to
perform well in recovering image texture detail, and many variants have
therefore been proposed for SISR. In this work, we develop an enhancement to
tackle GAN-based 3D SISR by introducing a new residual-in-residual dense block
(RRDG) generator that is both memory efficient and achieves state-of-the-art
performance in terms of PSNR (Peak Signal to Noise Ratio), SSIM (Structural
Similarity) and NRMSE (Normalized Root Mean Squared Error) metrics. We also
introduce a patch GAN discriminator with improved convergence behavior to
better model brain image texture. We proposed a novel the anatomical fidelity
evaluation of the results using a pre-trained brain parcellation network.
Finally, these developments are combined through a simple and efficient method
to balance etween image and texture quality in the final output.
",Jiancong Wang; Yuhua Chen; Yifan Wu; Jianbo Shi; James Gee,http://arxiv.org/abs/1907.04835v2,10.48550/arXiv.1907.04835
http://arxiv.org/abs/2101.10165v1,2021,"Learning Structral coherence Via Generative Adversarial Network for
  Single Image Super-Resolution","  Among the major remaining challenges for single image super resolution (SISR)
is the capacity to recover coherent images with global shapes and local details
conforming to human vision system. Recent generative adversarial network (GAN)
based SISR methods have yielded overall realistic SR images, however, there are
always unpleasant textures accompanied with structural distortions in local
regions. To target these issues, we introduce the gradient branch into the
generator to preserve structural information by restoring high-resolution
gradient maps in SR process. In addition, we utilize a U-net based
discriminator to consider both the whole image and the detailed per-pixel
authenticity, which could encourage the generator to maintain overall coherence
of the reconstructed images. Moreover, we have studied objective functions and
LPIPS perceptual loss is added to generate more realistic and natural details.
Experimental results show that our proposed method outperforms state-of-the-art
perceptual-driven SR methods in perception index (PI), and obtains more
geometrically consistent and visually pleasing textures in natural image
restoration.
",Yuanzhuo Li; Yunan Zheng; Jie Chen; Zhenyu Xu; Yiguang Liu,http://arxiv.org/abs/2101.10165v1,10.48550/arXiv.2101.10165
http://arxiv.org/abs/2106.02619v1,2021,"Forward Super-Resolution: How Can GANs Learn Hierarchical Generative
  Models for Real-World Distributions","  Generative adversarial networks (GANs) are among the most successful models
for learning high-complexity, real-world distributions. However, in theory, due
to the highly non-convex, non-concave landscape of the minmax training
objective, GAN remains one of the least understood deep learning models. In
this work, we formally study how GANs can efficiently learn certain
hierarchically generated distributions that are close to the distribution of
images in practice. We prove that when a distribution has a structure that we
refer to as Forward Super-Resolution, then simply training generative
adversarial networks using gradient descent ascent (GDA) can indeed learn this
distribution efficiently, both in terms of sample and time complexities. We
also provide concrete empirical evidence that not only our assumption ""forward
super-resolution"" is very natural in practice, but also the underlying learning
mechanisms that we study in this paper (to allow us efficiently train GAN via
GDA in theory) simulates the actual learning process of GANs in practice on
real-world problems.
",Zeyuan Allen-Zhu; Yuanzhi Li,http://arxiv.org/abs/2106.02619v1,10.48550/arXiv.2106.02619
http://arxiv.org/abs/2204.13620v2,2022,Generative Adversarial Networks for Image Super-Resolution: A Survey,"  Single image super-resolution (SISR) has played an important role in the
field of image processing. Recent generative adversarial networks (GANs) can
achieve excellent results on low-resolution images with small samples. However,
there are little literatures summarizing different GANs in SISR. In this paper,
we conduct a comparative study of GANs from different perspectives. We first
take a look at developments of GANs. Second, we present popular architectures
for GANs in big and small samples for image applications. Then, we analyze
motivations, implementations and differences of GANs based optimization methods
and discriminative learning for image super-resolution in terms of supervised,
semi-supervised and unsupervised manners. Next, we compare performance of these
popular GANs on public datasets via quantitative and qualitative analysis in
SISR. Finally, we highlight challenges of GANs and potential research points
for SISR.
",Chunwei Tian; Xuanyu Zhang; Jerry Chun-Wei Lin; Wangmeng Zuo; Yanning Zhang; Chia-Wen Lin,http://arxiv.org/abs/2204.13620v2,10.48550/arXiv.2204.13620
http://arxiv.org/abs/2210.16248v1,2022,"Applying Physics-Informed Enhanced Super-Resolution Generative
  Adversarial Networks to Turbulent Non-Premixed Combustion on Non-Uniform
  Meshes and Demonstration of an Accelerated Simulation Workflow","  This paper extends the methodology to use physics-informed enhanced
super-resolution generative adversarial networks (PIESRGANs) for LES subfilter
modeling in turbulent flows with finite-rate chemistry and shows a successful
application to a non-premixed temporal jet case. This is an important topic
considering the need for more efficient and carbon-neutral energy devices to
fight the climate change. Multiple a priori and a posteriori results are
presented and discussed. As part of this, the impact of the underlying mesh on
the prediction quality is emphasized, and a multi-mesh approach is developed.
It is demonstrated how LES based on PIESRGAN can be employed to predict cases
at Reynolds numbers which were not used for training. Finally, the amount of
data needed for a successful prediction is elaborated.
",Mathis Bode,http://arxiv.org/abs/2210.16248v1,10.48550/arXiv.2210.16248
http://arxiv.org/abs/2111.10591v1,2021,"AGA-GAN: Attribute Guided Attention Generative Adversarial Network with
  U-Net for Face Hallucination","  The performance of facial super-resolution methods relies on their ability to
recover facial structures and salient features effectively. Even though the
convolutional neural network and generative adversarial network-based methods
deliver impressive performances on face hallucination tasks, the ability to use
attributes associated with the low-resolution images to improve performance is
unsatisfactory. In this paper, we propose an Attribute Guided Attention
Generative Adversarial Network which employs novel attribute guided attention
(AGA) modules to identify and focus the generation process on various facial
features in the image. Stacking multiple AGA modules enables the recovery of
both high and low-level facial structures. We design the discriminator to learn
discriminative features exploiting the relationship between the high-resolution
image and their corresponding facial attribute annotations. We then explore the
use of U-Net based architecture to refine existing predictions and synthesize
further facial details. Extensive experiments across several metrics show that
our AGA-GAN and AGA-GAN+U-Net framework outperforms several other cutting-edge
face hallucination state-of-the-art methods. We also demonstrate the viability
of our method when every attribute descriptor is not known and thus,
establishing its application in real-world scenarios.
",Abhishek Srivastava; Sukalpa Chanda; Umapada Pal,http://arxiv.org/abs/2111.10591v1,10.48550/arXiv.2111.10591
http://arxiv.org/abs/2109.04250v2,2021,"High-fidelity reconstruction of turbulent flow from spatially limited
  data using enhanced super-resolution generative adversarial network","  In this study, a deep learning-based approach is applied with the aim of
reconstructing high-resolution turbulent flow fields using minimal flow fields
data. A multi-scale enhanced super-resolution generative adversarial network
with a physics-based loss function is introduced as a model to reconstruct the
high-resolution flow fields. The model capability to reconstruct
high-resolution laminar flows is examined using data of laminar flow around a
square cylinder. The results reveal that the model can accurately reproduce the
high-resolution flow fields even when limited spatial information is provided.
The case of turbulent channel flow is used to assess the ability of the model
to reconstruct the high-resolution wall-bounded turbulent flow fields. The
instantaneous and statistical results obtained from the model agree well with
the ground truth data, indicating that the model can successfully learn to map
the coarse flow fields to the high-resolution once. Furthermore, the
computational cost of the proposed model, which is examined carefully, is found
to be effectively low. This demonstrates that using high-fidelity training data
with physics-guided generative adversarial network-based models can be
practically efficient in reconstructing high-resolution turbulent flow fields
from extremely coarse data.
",Mustafa Z. Yousif; Linqi Yu; HeeChang Lim,http://arxiv.org/abs/2109.04250v2,10.1063/5.0066077
http://arxiv.org/abs/1712.05927v2,2017,"SRPGAN: Perceptual Generative Adversarial Network for Single Image Super
  Resolution","  Single image super resolution (SISR) is to reconstruct a high resolution
image from a single low resolution image. The SISR task has been a very
attractive research topic over the last two decades. In recent years,
convolutional neural network (CNN) based models have achieved great performance
on SISR task. Despite the breakthroughs achieved by using CNN models, there are
still some problems remaining unsolved, such as how to recover high frequency
details of high resolution images. Previous CNN based models always use a pixel
wise loss, such as l2 loss. Although the high resolution images constructed by
these models have high peak signal-to-noise ratio (PSNR), they often tend to be
blurry and lack high-frequency details, especially at a large scaling factor.
In this paper, we build a super resolution perceptual generative adversarial
network (SRPGAN) framework for SISR tasks. In the framework, we propose a
robust perceptual loss based on the discriminator of the built SRPGAN model. We
use the Charbonnier loss function to build the content loss and combine it with
the proposed perceptual loss and the adversarial loss. Compared with other
state-of-the-art methods, our method has demonstrated great ability to
construct images with sharp edges and rich details. We also evaluate our method
on different benchmarks and compare it with previous CNN based methods. The
results show that our method can achieve much higher structural similarity
index (SSIM) scores on most of the benchmarks than the previous state-of-art
methods.
",Bingzhe Wu; Haodong Duan; Zhichao Liu; Guangyu Sun,http://arxiv.org/abs/1712.05927v2,10.48550/arXiv.1712.05927
http://arxiv.org/abs/1803.01417v3,2018,"Efficient and Accurate MRI Super-Resolution using a Generative
  Adversarial Network and 3D Multi-Level Densely Connected Network","  High-resolution (HR) magnetic resonance images (MRI) provide detailed
anatomical information important for clinical application and quantitative
image analysis. However, HR MRI conventionally comes at the cost of longer scan
time, smaller spatial coverage, and lower signal-to-noise ratio (SNR). Recent
studies have shown that single image super-resolution (SISR), a technique to
recover HR details from one single low-resolution (LR) input image, could
provide high-quality image details with the help of advanced deep convolutional
neural networks (CNN). However, deep neural networks consume memory heavily and
run slowly, especially in 3D settings. In this paper, we propose a novel 3D
neural network design, namely a multi-level densely connected super-resolution
network (mDCSRN) with generative adversarial network (GAN)-guided training. The
mDCSRN quickly trains and inferences and the GAN promotes realistic output
hardly distinguishable from original HR images. Our results from experiments on
a dataset with 1,113 subjects show that our new architecture beats other
popular deep learning methods in recovering 4x resolution-downgraded im-ages
and runs 6x faster.
",Yuhua Chen; Feng Shi; Anthony G. Christodoulou; Zhengwei Zhou; Yibin Xie; Debiao Li,http://arxiv.org/abs/1803.01417v3,10.48550/arXiv.1803.01417
http://arxiv.org/abs/1806.05764v2,2018,"Generative Adversarial Networks and Perceptual Losses for Video
  Super-Resolution","  Video super-resolution (VSR) has become one of the most critical problems in
video processing. In the deep learning literature, recent works have shown the
benefits of using adversarial-based and perceptual losses to improve the
performance on various image restoration tasks; however, these have yet to be
applied for video super-resolution. In this work, we propose a Generative
Adversarial Network(GAN)-based formulation for VSR. We introduce a new
generator network optimized for the VSR problem, named VSRResNet, along with a
new discriminator architecture to properly guide VSRResNet during the GAN
training. We further enhance our VSR GAN formulation with two regularizers, a
distance loss in feature-space and pixel-space, to obtain our final
VSRResFeatGAN model. We show that pre-training our generator with the
Mean-Squared-Error loss only quantitatively surpasses the current
state-of-the-art VSR models. Finally, we employ the PercepDist metric (Zhang et
al., 2018) to compare state-of-the-art VSR models. We show that this metric
more accurately evaluates the perceptual quality of SR solutions obtained from
neural networks, compared with the commonly used PSNR/SSIM metrics. Finally, we
show that our proposed model, the VSRResFeatGAN model, outperforms current
state-of-the-art SR models, both quantitatively and qualitatively.
",Alice Lucas; Santiago Lopez Tapia; Rafael Molina; Aggelos K. Katsaggelos,http://arxiv.org/abs/1806.05764v2,10.1109/TIP.2019.2895768
http://arxiv.org/abs/1809.00219v2,2018,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,"  The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work
that is capable of generating realistic textures during single image
super-resolution. However, the hallucinated details are often accompanied with
unpleasant artifacts. To further enhance the visual quality, we thoroughly
study three key components of SRGAN - network architecture, adversarial loss
and perceptual loss, and improve each of them to derive an Enhanced SRGAN
(ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block
(RRDB) without batch normalization as the basic network building unit.
Moreover, we borrow the idea from relativistic GAN to let the discriminator
predict relative realness instead of the absolute value. Finally, we improve
the perceptual loss by using the features before activation, which could
provide stronger supervision for brightness consistency and texture recovery.
Benefiting from these improvements, the proposed ESRGAN achieves consistently
better visual quality with more realistic and natural textures than SRGAN and
won the first place in the PIRM2018-SR Challenge. The code is available at
https://github.com/xinntao/ESRGAN .
",Xintao Wang; Ke Yu; Shixiang Wu; Jinjin Gu; Yihao Liu; Chao Dong; Chen Change Loy; Yu Qiao; Xiaoou Tang,http://arxiv.org/abs/1809.00219v2,10.48550/arXiv.1809.00219
http://arxiv.org/abs/1912.09507v1,2019,"An Application of Generative Adversarial Networks for Super Resolution
  Medical Imaging","  Acquiring High Resolution (HR) Magnetic Resonance (MR) images requires the
patient to remain still for long periods of time, which causes patient
discomfort and increases the probability of motion induced image artifacts. A
possible solution is to acquire low resolution (LR) images and to process them
with the Super Resolution Generative Adversarial Network (SRGAN) to create an
HR version. Acquiring LR images requires a lower scan time than acquiring HR
images, which allows for higher patient comfort and scanner throughput. This
work applies SRGAN to MR images of the prostate to improve the in-plane
resolution by factors of 4 and 8. The term 'super resolution' in the context of
this paper defines the post processing enhancement of medical images as opposed
to 'high resolution' which defines native image resolution acquired during the
MR acquisition phase. We also compare the SRGAN to three other models: SRCNN,
SRResNet, and Sparse Representation. While the SRGAN results do not have the
best Peak Signal to Noise Ratio (PSNR) or Structural Similarity (SSIM) metrics,
they are the visually most similar to the original HR images, as portrayed by
the Mean Opinion Score (MOS) results.
",Rewa Sood; Binit Topiwala; Karthik Choutagunta; Rohit Sood; Mirabela Rusu,http://arxiv.org/abs/1912.09507v1,10.1109/ICMLA.2018.00055
http://arxiv.org/abs/2001.08126v2,2020,"Optimizing Generative Adversarial Networks for Image Super Resolution
  via Latent Space Regularization","  Natural images can be regarded as residing in a manifold that is embedded in
a higher dimensional Euclidean space. Generative Adversarial Networks (GANs)
try to learn the distribution of the real images in the manifold to generate
samples that look real. But the results of existing methods still exhibit many
unpleasant artifacts and distortions even for the cases where the desired
ground truth target images are available for supervised learning such as in
single image super resolution (SISR). We probe for ways to alleviate these
problems for supervised GANs in this paper. We explicitly apply the Lipschitz
Continuity Condition (LCC) to regularize the GAN. An encoding network that maps
the image space to a new optimal latent space is derived from the LCC, and it
is used to augment the GAN as a coupling component. The LCC is also converted
to new regularization terms in the generator loss function to enforce local
invariance. The GAN is optimized together with the encoding network in an
attempt to make the generator converge to a more ideal and disentangled mapping
that can generate samples more faithful to the target images. When the proposed
models are applied to the single image super resolution problem, the results
outperform the state of the art.
",Sheng Zhong; Shifu Zhou,http://arxiv.org/abs/2001.08126v2,10.48550/arXiv.2001.08126
http://arxiv.org/abs/2008.03142v1,2020,"Generative Adversarial Network-Based Sinogram Super-Resolution for
  Computed Tomography Imaging","  Compared with the conventional 1*1 acquisition mode of projection in computed
tomography (CT) image reconstruction, the 2*2 acquisition mode improves the
collection efficiency of the projection and reduces the X-ray exposure time.
However, the collected projection based on the 2*2 acquisition mode has low
resolution (LR) and the reconstructed image quality is poor, thus limiting the
use of this mode in CT imaging systems. In this study, a novel
sinogram-super-resolution generative adversarial network (SSR-GAN) model is
proposed to obtain high-resolution (HR) sinograms from LR sinograms, thereby
improving the reconstruction image quality under the 2*2 acquisition mode. The
proposed generator is based on the residual network for LR sinogram feature
extraction and super-resolution (SR) sinogram generation. A relativistic
discriminator is designed to render the network capable of obtaining more
realistic SR sinograms. Moreover, we combine the cycle consistency loss,
sinogram domain loss, and reconstruction image domain loss in the total loss
function to supervise SR sinogram generation. Then, a trained model can be
obtained by inputting the paired LR/HR sinograms into the network. Finally, the
classic FBP reconstruction algorithm is used for CT image reconstruction based
on the generated SR sinogram. The qualitative and quantitative results of
evaluations on digital and real data illustrate that the proposed model not
only obtains clean SR sinograms from noisy LR sinograms but also outperforms
its counterparts.
",Chao Tang; Wenkun Zhang; Linyuan Wang; Ailong Cai; Ningning Liang; Lei Li; Bin Yan,http://arxiv.org/abs/2008.03142v1,10.48550/arXiv.2008.03142
http://arxiv.org/abs/2009.06129v1,2020,"Super Resolution of Arterial Spin Labeling MR Imaging Using Unsupervised
  Multi-Scale Generative Adversarial Network","  Arterial spin labeling (ASL) magnetic resonance imaging (MRI) is a powerful
imaging technology that can measure cerebral blood flow (CBF) quantitatively.
However, since only a small portion of blood is labeled compared to the whole
tissue volume, conventional ASL suffers from low signal-to-noise ratio (SNR),
poor spatial resolution, and long acquisition time. In this paper, we proposed
a super-resolution method based on a multi-scale generative adversarial network
(GAN) through unsupervised training. The network only needs the low-resolution
(LR) ASL image itself for training and the T1-weighted image as the anatomical
prior. No training pairs or pre-training are needed. A low-pass filter guided
item was added as an additional loss to suppress the noise interference from
the LR ASL image. After the network was trained, the super-resolution (SR)
image was generated by supplying the upsampled LR ASL image and corresponding
T1-weighted image to the generator of the last layer. Performance of the
proposed method was evaluated by comparing the peak signal-to-noise ratio
(PSNR) and structural similarity index (SSIM) using normal-resolution (NR) ASL
image (5.5 min acquisition) and high-resolution (HR) ASL image (44 min
acquisition) as the ground truth. Compared to the nearest, linear, and spline
interpolation methods, the proposed method recovers more detailed structure
information, reduces the image noise visually, and achieves the highest PSNR
and SSIM when using HR ASL image as the ground-truth.
",Jianan Cui; Kuang Gong; Paul Han; Huafeng Liu; Quanzheng Li,http://arxiv.org/abs/2009.06129v1,10.48550/arXiv.2009.06129
http://arxiv.org/abs/2108.03920v1,2021,"FA-GAN: Fused Attentive Generative Adversarial Networks for MRI Image
  Super-Resolution","  High-resolution magnetic resonance images can provide fine-grained anatomical
information, but acquiring such data requires a long scanning time. In this
paper, a framework called the Fused Attentive Generative Adversarial
Networks(FA-GAN) is proposed to generate the super-resolution MR image from
low-resolution magnetic resonance images, which can reduce the scanning time
effectively but with high resolution MR images. In the framework of the FA-GAN,
the local fusion feature block, consisting of different three-pass networks by
using different convolution kernels, is proposed to extract image features at
different scales. And the global feature fusion module, including the channel
attention module, the self-attention module, and the fusion operation, is
designed to enhance the important features of the MR image. Moreover, the
spectral normalization process is introduced to make the discriminator network
stable. 40 sets of 3D magnetic resonance images (each set of images contains
256 slices) are used to train the network, and 10 sets of images are used to
test the proposed method. The experimental results show that the PSNR and SSIM
values of the super-resolution magnetic resonance image generated by the
proposed FA-GAN method are higher than the state-of-the-art reconstruction
methods.
",Mingfeng Jiang; Minghao Zhi; Liying Wei; Xiaocheng Yang; Jucheng Zhang; Yongming Li; Pin Wang; Jiahao Huang; Guang Yang,http://arxiv.org/abs/2108.03920v1,10.48550/arXiv.2108.03920
http://arxiv.org/abs/1711.10703v1,2017,FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors,"  Face Super-Resolution (SR) is a domain-specific super-resolution problem. The
specific facial prior knowledge could be leveraged for better super-resolving
face images. We present a novel deep end-to-end trainable Face Super-Resolution
Network (FSRNet), which makes full use of the geometry prior, i.e., facial
landmark heatmaps and parsing maps, to super-resolve very low-resolution (LR)
face images without well-aligned requirement. Specifically, we first construct
a coarse SR network to recover a coarse high-resolution (HR) image. Then, the
coarse HR image is sent to two branches: a fine SR encoder and a prior
information estimation network, which extracts the image features, and
estimates landmark heatmaps/parsing maps respectively. Both image features and
prior information are sent to a fine SR decoder to recover the HR image. To
further generate realistic faces, we propose the Face Super-Resolution
Generative Adversarial Network (FSRGAN) to incorporate the adversarial loss
into FSRNet. Moreover, we introduce two related tasks, face alignment and
parsing, as the new evaluation metrics for face SR, which address the
inconsistency of classic metrics w.r.t. visual perception. Extensive benchmark
experiments show that FSRNet and FSRGAN significantly outperforms state of the
arts for very LR face SR, both quantitatively and qualitatively. Code will be
made available upon publication.
",Yu Chen; Ying Tai; Xiaoming Liu; Chunhua Shen; Jian Yang,http://arxiv.org/abs/1711.10703v1,10.48550/arXiv.1711.10703
http://arxiv.org/abs/2004.02234v1,2020,"Feature Super-Resolution Based Facial Expression Recognition for
  Multi-scale Low-Resolution Faces","  Facial Expressions Recognition(FER) on low-resolution images is necessary for
applications like group expression recognition in crowd scenarios(station,
classroom etc.). Classifying a small size facial image into the right
expression category is still a challenging task. The main cause of this problem
is the loss of discriminative feature due to reduced resolution.
Super-resolution method is often used to enhance low-resolution images, but the
performance on FER task is limited when on images of very low resolution. In
this work, inspired by feature super-resolution methods for object detection,
we proposed a novel generative adversary network-based feature level
super-resolution method for robust facial expression recognition(FSR-FER). In
particular, a pre-trained FER model was employed as feature extractor, and a
generator network G and a discriminator network D are trained with features
extracted from images of low resolution and original high resolution. Generator
network G tries to transform features of low-resolution images to more
discriminative ones by making them closer to the ones of corresponding
high-resolution images. For better classification performance, we also proposed
an effective classification-aware loss re-weighting strategy based on the
classification probability calculated by a fixed FER model to make our model
focus more on samples that are easily misclassified. Experiment results on
Real-World Affective Faces (RAF) Database demonstrate that our method achieves
satisfying results on various down-sample factors with a single model and has
better performance on low-resolution images compared with methods using image
super-resolution and expression recognition separately.
",Wei Jing; Feng Tian; Jizhong Zhang; Kuo-Ming Chao; Zhenxin Hong; Xu Liu,http://arxiv.org/abs/2004.02234v1,10.48550/arXiv.2004.02234
http://arxiv.org/abs/2201.06383v1,2022,Dual Perceptual Loss for Single Image Super-Resolution Using ESRGAN,"  The proposal of perceptual loss solves the problem that per-pixel difference
loss function causes the reconstructed image to be overly-smooth, which
acquires a significant progress in the field of single image super-resolution
reconstruction. Furthermore, the generative adversarial networks (GAN) is
applied to the super-resolution field, which effectively improves the visual
quality of the reconstructed image. However, under the condtion of high
upscaling factors, the excessive abnormal reasoning of the network produces
some distorted structures, so that there is a certain deviation between the
reconstructed image and the ground-truth image. In order to fundamentally
improve the quality of reconstructed images, this paper proposes a effective
method called Dual Perceptual Loss (DP Loss), which is used to replace the
original perceptual loss to solve the problem of single image super-resolution
reconstruction. Due to the complementary property between the VGG features and
the ResNet features, the proposed DP Loss considers the advantages of learning
two features simultaneously, which significantly improves the reconstruction
effect of images. The qualitative and quantitative analysis on benchmark
datasets demonstrates the superiority of our proposed method over
state-of-the-art super-resolution methods.
",Jie Song; Huawei Yi; Wenqian Xu; Xiaohui Li; Bo Li; Yuanyuan Liu,http://arxiv.org/abs/2201.06383v1,10.48550/arXiv.2201.06383
http://arxiv.org/abs/2005.10374v4,2020,"Stochastic Super-Resolution for Downscaling Time-Evolving Atmospheric
  Fields with a Generative Adversarial Network","  Generative adversarial networks (GANs) have been recently adopted for
super-resolution, an application closely related to what is referred to as
""downscaling"" in the atmospheric sciences: improving the spatial resolution of
low-resolution images. The ability of conditional GANs to generate an ensemble
of solutions for a given input lends itself naturally to stochastic
downscaling, but the stochastic nature of GANs is not usually considered in
super-resolution applications. Here, we introduce a recurrent, stochastic
super-resolution GAN that can generate ensembles of time-evolving
high-resolution atmospheric fields for an input consisting of a low-resolution
sequence of images of the same field. We test the GAN using two datasets, one
consisting of radar-measured precipitation from Switzerland, the other of cloud
optical thickness derived from the Geostationary Earth Observing Satellite 16
(GOES-16). We find that the GAN can generate realistic, temporally consistent
super-resolution sequences for both datasets. The statistical properties of the
generated ensemble are analyzed using rank statistics, a method adapted from
ensemble weather forecasting; these analyses indicate that the GAN produces
close to the correct amount of variability in its outputs. As the GAN generator
is fully convolutional, it can be applied after training to input images larger
than the images used to train it. It is also able to generate time series much
longer than the training sequences, as demonstrated by applying the generator
to a three-month dataset of the precipitation radar data. The source code to
our GAN is available at https://github.com/jleinonen/downscaling-rnn-gan.
",Jussi Leinonen; Daniele Nerini; Alexis Berne,http://arxiv.org/abs/2005.10374v4,10.1109/TGRS.2020.3032790
http://arxiv.org/abs/2006.11161v4,2020,"iSeeBetter: Spatio-temporal video super-resolution using recurrent
  generative back-projection networks","  Recently, learning-based models have enhanced the performance of single-image
super-resolution (SISR). However, applying SISR successively to each video
frame leads to a lack of temporal coherency. Convolutional neural networks
(CNNs) outperform traditional approaches in terms of image quality metrics such
as peak signal to noise ratio (PSNR) and structural similarity (SSIM). However,
generative adversarial networks (GANs) offer a competitive advantage by being
able to mitigate the issue of a lack of finer texture details, usually seen
with CNNs when super-resolving at large upscaling factors. We present
iSeeBetter, a novel GAN-based spatio-temporal approach to video
super-resolution (VSR) that renders temporally consistent super-resolution
videos. iSeeBetter extracts spatial and temporal information from the current
and neighboring frames using the concept of recurrent back-projection networks
as its generator. Furthermore, to improve the ""naturality"" of the
super-resolved image while eliminating artifacts seen with traditional
algorithms, we utilize the discriminator from super-resolution generative
adversarial network (SRGAN). Although mean squared error (MSE) as a primary
loss-minimization objective improves PSNR/SSIM, these metrics may not capture
fine details in the image resulting in misrepresentation of perceptual quality.
To address this, we use a four-fold (MSE, perceptual, adversarial, and
total-variation (TV)) loss function. Our results demonstrate that iSeeBetter
offers superior VSR fidelity and surpasses state-of-the-art performance.
",Aman Chadha; John Britto; M. Mani Roja,http://arxiv.org/abs/2006.11161v4,10.1007/s41095-020-0175-7
http://arxiv.org/abs/1804.06244v2,2018,cellSTORM - Cost-effective Super-Resolution on a Cellphone using dSTORM,"  Expensive scientific camera hardware is amongst the main cost factors in
modern, high-performance microscopes. Recent technological advantages have,
however, yielded consumer-grade camera devices that can provide surprisingly
good performance. The camera sensors of smartphones in particular have
benefited of this development. Combined with computing power and due to their
ubiquity, smartphones provide a fantastic opportunity for ""imaging on a
budget"". Here we show that a consumer cellphone is capable even of optical
super-resolution imaging by (direct) Stochastic Optical Reconstruction
Microscopy (dSTORM), achieving optical resolution better than 80 nm. In
addition to the use of standard reconstruction algorithms, we investigated an
approach by a trained image-to-image generative adversarial network (GAN). This
not only serves as a versatile technique to reconstruct video sequences under
conditions where traditional algorithms provide sub-optimal localization
performance, but also allows processing directly on the smartphone. We believe
that ""cellSTORM"" paves the way for affordable super-resolution microscopy
suitable for research and education, expanding access to cutting edge research
to a large community.
",Benedict Diederich; Patrick Then; Alexander Jügler; Ronny Förster; Rainer Heintzmann,http://arxiv.org/abs/1804.06244v2,10.1371/journal.pone.0209827
http://arxiv.org/abs/1810.06611v1,2018,Deep learning-based super-resolution in coherent imaging systems,"  We present a deep learning framework based on a generative adversarial
network (GAN) to perform super-resolution in coherent imaging systems. We
demonstrate that this framework can enhance the resolution of both pixel
size-limited and diffraction-limited coherent imaging systems. We
experimentally validated the capabilities of this deep learning-based coherent
imaging approach by super-resolving complex images acquired using a lensfree
on-chip holographic microscope, the resolution of which was pixel size-limited.
Using the same GAN-based approach, we also improved the resolution of a
lens-based holographic imaging system that was limited in resolution by the
numerical aperture of its objective lens. This deep learning-based
super-resolution framework can be broadly applied to enhance the
space-bandwidth product of coherent imaging systems using image data and
convolutional neural networks, and provides a rapid, non-iterative method for
solving inverse image reconstruction or enhancement problems in optics.
",Tairan Liu; Kevin de Haan; Yair Rivenson; Zhensong Wei; Xin Zeng; Yibo Zhang; Aydogan Ozcan,http://arxiv.org/abs/1810.06611v1,10.1038/s41598-019-40554-1
http://arxiv.org/abs/1812.04240v2,2018,Unsupervised Degradation Learning for Single Image Super-Resolution,"  Deep Convolution Neural Networks (CNN) have achieved significant performance
on single image super-resolution (SR) recently. However, existing CNN-based
methods use artificially synthetic low-resolution (LR) and high-resolution (HR)
image pairs to train networks, which cannot handle real-world cases since the
degradation from HR to LR is much more complex than manually designed. To solve
this problem, we propose a real-world LR images guided bi-cycle network for
single image super-resolution, in which the bidirectional structural
consistency is exploited to train both the degradation and SR reconstruction
networks in an unsupervised way. Specifically, we propose a degradation network
to model the real-world degradation process from HR to LR via generative
adversarial networks, and these generated realistic LR images paired with
real-world HR images are exploited for training the SR reconstruction network,
forming the first cycle. Then in the second reverse cycle, consistency of
real-world LR images are exploited to further stabilize the training of SR
reconstruction and degradation networks. Extensive experiments on both
synthetic and real-world images demonstrate that the proposed algorithm
performs favorably against state-of-the-art single image SR methods.
",Tianyu Zhao; Wenqi Ren; Changqing Zhang; Dongwei Ren; Qinghua Hu,http://arxiv.org/abs/1812.04240v2,10.48550/arXiv.1812.04240
http://arxiv.org/abs/1901.06199v1,2019,"Generative Adversarial Classifier for Handwriting Characters
  Super-Resolution","  Generative Adversarial Networks (GAN) receive great attentions recently due
to its excellent performance in image generation, transformation, and
super-resolution. However, GAN has rarely been studied and trained for
classification, leading that the generated images may not be appropriate for
classification. In this paper, we propose a novel Generative Adversarial
Classifier (GAC) particularly for low-resolution Handwriting Character
Recognition. Specifically, involving additionally a classifier in the training
process of normal GANs, GAC is calibrated for learning suitable structures and
restored characters images that benefits the classification. Experimental
results show that our proposed method can achieve remarkable performance in
handwriting characters 8x super-resolution, approximately 10% and 20% higher
than the present state-of-the-art methods respectively on benchmark data
CASIA-HWDB1.1 and MNIST.
",Zhuang Qian; Kaizhu Huang; Qiufeng Wang; Jimin Xiao; Rui Zhang,http://arxiv.org/abs/1901.06199v1,10.48550/arXiv.1901.06199
http://arxiv.org/abs/2010.04634v1,2020,Attaining Real-Time Super-Resolution for Microscopic Images Using GAN,"  In the last few years, several deep learning models, especially Generative
Adversarial Networks have received a lot of attention for the task of Single
Image Super-Resolution (SISR). These methods focus on building an end-to-end
framework, which produce a high resolution(SR) image from a given low
resolution(LR) image in a single step to achieve state-of-the-art performance.
This paper focuses on improving an existing deep-learning based method to
perform Super-Resolution Microscopy in real-time using a standard GPU. For
this, we first propose a tiling strategy, which takes advantage of parallelism
provided by a GPU to speed up the network training process. Further, we suggest
simple changes to the architecture of the generator and the discriminator of
SRGAN. Subsequently, We compare the quality and the running time for the
outputs produced by our model, opening its applications in different areas like
low-end benchtop and even mobile microscopy. Finally, we explore the
possibility of the trained network to produce High-Resolution HR outputs for
different domains.
",Vibhu Bhatia; Yatender Kumar,http://arxiv.org/abs/2010.04634v1,10.48550/arXiv.2010.04634
http://arxiv.org/abs/2012.01233v1,2020,"Investigating two super-resolution methods for downscaling
  precipitation: ESRGAN and CAR","  In an effort to provide optimal inputs to downstream modeling systems (e.g.,
a hydrodynamics model that simulates the water circulation of a lake), we
hereby strive to enhance resolution of precipitation fields from a weather
model by up to 9x. We test two super-resolution models: the enhanced
super-resolution generative adversarial networks (ESRGAN) proposed in 2017, and
the content adaptive resampler (CAR) proposed in 2020. Both models outperform
simple bicubic interpolation, with the ESRGAN exceeding expectations for
accuracy. We make several proposals for extending the work to ensure it can be
a useful tool for quantifying the impact of climate change on local ecosystems
while removing reliance on energy-intensive, high-resolution weather model
simulations.
",Campbell D. Watson; Chulin Wang; Timothy Lynar; Komminist Weldemariam,http://arxiv.org/abs/2012.01233v1,10.48550/arXiv.2012.01233
http://arxiv.org/abs/2103.15295v3,2021,Best-Buddy GANs for Highly Detailed Image Super-Resolution,"  We consider the single image super-resolution (SISR) problem, where a
high-resolution (HR) image is generated based on a low-resolution (LR) input.
Recently, generative adversarial networks (GANs) become popular to hallucinate
details. Most methods along this line rely on a predefined single-LR-single-HR
mapping, which is not flexible enough for the SISR task. Also, GAN-generated
fake details may often undermine the realism of the whole image. We address
these issues by proposing best-buddy GANs (Beby-GAN) for rich-detail SISR.
Relaxing the immutable one-to-one constraint, we allow the estimated patches to
dynamically seek the best supervision during training, which is beneficial to
producing more reasonable details. Besides, we propose a region-aware
adversarial learning strategy that directs our model to focus on generating
details for textured areas adaptively. Extensive experiments justify the
effectiveness of our method. An ultra-high-resolution 4K dataset is also
constructed to facilitate future super-resolution research.
",Wenbo Li; Kun Zhou; Lu Qi; Liying Lu; Nianjuan Jiang; Jiangbo Lu; Jiaya Jia,http://arxiv.org/abs/2103.15295v3,10.48550/arXiv.2103.15295
http://arxiv.org/abs/2106.05545v1,2021,"Super-Resolution Image Reconstruction Based on Self-Calibrated
  Convolutional GAN","  With the effective application of deep learning in computer vision,
breakthroughs have been made in the research of super-resolution images
reconstruction. However, many researches have pointed out that the
insufficiency of the neural network extraction on image features may bring the
deteriorating of newly reconstructed image. On the other hand, the generated
pictures are sometimes too artificial because of over-smoothing. In order to
solve the above problems, we propose a novel self-calibrated convolutional
generative adversarial networks. The generator consists of feature extraction
and image reconstruction. Feature extraction uses self-calibrated convolutions,
which contains four portions, and each portion has specific functions. It can
not only expand the range of receptive fields, but also obtain long-range
spatial and inter-channel dependencies. Then image reconstruction is performed,
and finally a super-resolution image is reconstructed. We have conducted
thorough experiments on different datasets including set5, set14 and BSD100
under the SSIM evaluation method. The experimental results prove the
effectiveness of the proposed network.
",Yibo Guo; Haidi Wang; Yiming Fan; Shunyao Li; Mingliang Xu,http://arxiv.org/abs/2106.05545v1,10.48550/arXiv.2106.05545
http://arxiv.org/abs/2106.08147v1,2021,Perceptually-inspired super-resolution of compressed videos,"  Spatial resolution adaptation is a technique which has often been employed in
video compression to enhance coding efficiency. This approach encodes a lower
resolution version of the input video and reconstructs the original resolution
during decoding. Instead of using conventional up-sampling filters, recent work
has employed advanced super-resolution methods based on convolutional neural
networks (CNNs) to further improve reconstruction quality. These approaches are
usually trained to minimise pixel-based losses such as Mean-Squared Error
(MSE), despite the fact that this type of loss metric does not correlate well
with subjective opinions. In this paper, a perceptually-inspired
super-resolution approach (M-SRGAN) is proposed for spatial up-sampling of
compressed video using a modified CNN model, which has been trained using a
generative adversarial network (GAN) on compressed content with perceptual loss
functions. The proposed method was integrated with HEVC HM 16.20, and has been
evaluated on the JVET Common Test Conditions (UHD test sequences) using the
Random Access configuration. The results show evident perceptual quality
improvement over the original HM 16.20, with an average bitrate saving of 35.6%
(Bj{\o}ntegaard Delta measurement) based on a perceptual quality metric, VMAF.
",Di Ma; Mariana Afonso; Fan Zhang; David R. Bull,http://arxiv.org/abs/2106.08147v1,10.1117/12.2530688
http://arxiv.org/abs/2109.14994v1,2021,"An investigation of pre-upsampling generative modelling and Generative
  Adversarial Networks in audio super resolution","  There have been several successful deep learning models that perform audio
super-resolution. Many of these approaches involve using preprocessed feature
extraction which requires a lot of domain-specific signal processing knowledge
to implement. Convolutional Neural Networks (CNNs) improved upon this framework
by automatically learning filters. An example of a convolutional approach is
AudioUNet, which takes inspiration from novel methods of upsampling images. Our
paper compares the pre-upsampling AudioUNet to a new generative model that
upsamples the signal before using deep learning to transform it into a more
believable signal. Based on the EDSR network for image super-resolution, the
newly proposed model outperforms UNet with a 20% increase in log spectral
distance and a mean opinion score of 4.06 compared to 3.82 for the two times
upsampling case. AudioEDSR also has 87% fewer parameters than AudioUNet. How
incorporating AudioUNet into a Wasserstein GAN (with gradient penalty)
(WGAN-GP) structure can affect training is also explored. Finally the effects
artifacting has on the current state of the art is analysed and solutions to
this problem are proposed. The methods used in this paper have broad
applications to telephony, audio recognition and audio generation tasks.
",James King; Ramon Viñas Torné; Alexander Campbell; Pietro Liò,http://arxiv.org/abs/2109.14994v1,10.48550/arXiv.2109.14994
http://arxiv.org/abs/2201.09314v1,2022,Perceptual cGAN for MRI Super-resolution,"  Capturing high-resolution magnetic resonance (MR) images is a time consuming
process, which makes it unsuitable for medical emergencies and pediatric
patients. Low-resolution MR imaging, by contrast, is faster than its
high-resolution counterpart, but it compromises on fine details necessary for a
more precise diagnosis. Super-resolution (SR), when applied to low-resolution
MR images, can help increase their utility by synthetically generating
high-resolution images with little additional time. In this paper, we present a
SR technique for MR images that is based on generative adversarial networks
(GANs), which have proven to be quite useful in generating sharp-looking
details in SR. We introduce a conditional GAN with perceptual loss, which is
conditioned upon the input low-resolution image, which improves the performance
for isotropic and anisotropic MRI super-resolution.
",Sahar Almahfouz Nasser; Saqib Shamsi; Valay Bundele; Bhavesh Garg; Amit Sethi,http://arxiv.org/abs/2201.09314v1,10.48550/arXiv.2201.09314
http://arxiv.org/abs/2203.09445v2,2022,Image Super-Resolution With Deep Variational Autoencoders,"  Image super-resolution (SR) techniques are used to generate a high-resolution
image from a low-resolution image. Until now, deep generative models such as
autoregressive models and Generative Adversarial Networks (GANs) have proven to
be effective at modelling high-resolution images. VAE-based models have often
been criticised for their feeble generative performance, but with new
advancements such as VDVAE, there is now strong evidence that deep VAEs have
the potential to outperform current state-of-the-art models for high-resolution
image generation. In this paper, we introduce VDVAE-SR, a new model that aims
to exploit the most recent deep VAE methodologies to improve upon the results
of similar models. VDVAE-SR tackles image super-resolution using transfer
learning on pretrained VDVAEs. The presented model is competitive with other
state-of-the-art models, having comparable results on image quality metrics.
",Darius Chira; Ilian Haralampiev; Ole Winther; Andrea Dittadi; Valentin Liévin,http://arxiv.org/abs/2203.09445v2,10.48550/arXiv.2203.09445
http://arxiv.org/abs/2206.13740v1,2022,"GAN-based Super-Resolution and Segmentation of Retinal Layers in Optical
  coherence tomography Scans","  In this paper, we design a Generative Adversarial Network (GAN)-based
solution for super-resolution and segmentation of optical coherence tomography
(OCT) scans of the retinal layers. OCT has been identified as a non-invasive
and inexpensive modality of imaging to discover potential biomarkers for the
diagnosis and progress determination of neurodegenerative diseases, such as
Alzheimer's Disease (AD). Current hypotheses presume the thickness of the
retinal layers, which are analyzable within OCT scans, can be effective
biomarkers. As a logical first step, this work concentrates on the challenging
task of retinal layer segmentation and also super-resolution for higher clarity
and accuracy. We propose a GAN-based segmentation model and evaluate
incorporating popular networks, namely, U-Net and ResNet, in the GAN
architecture with additional blocks of transposed convolution and sub-pixel
convolution for the task of upscaling OCT images from low to high resolution by
a factor of four. We also incorporate the Dice loss as an additional
reconstruction loss term to improve the performance of this joint optimization
task. Our best model configuration empirically achieved the Dice coefficient of
0.867 and mIOU of 0.765.
",Paria Jeihouni; Omid Dehzangi; Annahita Amireskandari; Ali Rezai; Nasser M. Nasrabadi,http://arxiv.org/abs/2206.13740v1,10.48550/arXiv.2206.13740
http://arxiv.org/abs/2208.03008v1,2022,Rethinking Degradation: Radiograph Super-Resolution via AID-SRGAN,"  In this paper, we present a medical AttentIon Denoising Super Resolution
Generative Adversarial Network (AID-SRGAN) for diographic image
super-resolution. First, we present a medical practical degradation model that
considers various degradation factors beyond downsampling. To the best of our
knowledge, this is the first composite degradation model proposed for
radiographic images. Furthermore, we propose AID-SRGAN, which can
simultaneously denoise and generate high-resolution (HR) radiographs. In this
model, we introduce an attention mechanism into the denoising module to make it
more robust to complicated degradation. Finally, the SR module reconstructs the
HR radiographs using the ""clean"" low-resolution (LR) radiographs. In addition,
we propose a separate-joint training approach to train the model, and extensive
experiments are conducted to show that the proposed method is superior to its
counterparts. e.g., our proposed method achieves $31.90$ of PSNR with a scale
factor of $4 \times$, which is $7.05 \%$ higher than that obtained by recent
work, SPSR [16]. Our dataset and code will be made available at:
https://github.com/yongsongH/AIDSRGAN-MICCAI2022.
",Yongsong Huang; Qingzhong Wang; Shinichiro Omachi,http://arxiv.org/abs/2208.03008v1,10.1007/978-3-031-21014-3_5
http://arxiv.org/abs/2209.03355v1,2022,"Generative Adversarial Super-Resolution at the Edge with Knowledge
  Distillation","  Single-Image Super-Resolution can support robotic tasks in environments where
a reliable visual stream is required to monitor the mission, handle
teleoperation or study relevant visual details. In this work, we propose an
efficient Generative Adversarial Network model for real-time Super-Resolution.
We adopt a tailored architecture of the original SRGAN and model quantization
to boost the execution on CPU and Edge TPU devices, achieving up to 200 fps
inference. We further optimize our model by distilling its knowledge to a
smaller version of the network and obtain remarkable improvements compared to
the standard training approach. Our experiments show that our fast and
lightweight model preserves considerably satisfying image quality compared to
heavier state-of-the-art models. Finally, we conduct experiments on image
transmission with bandwidth degradation to highlight the advantages of the
proposed system for mobile robotic applications.
",Simone Angarano; Francesco Salvetti; Mauro Martini; Marcello Chiaberge,http://arxiv.org/abs/2209.03355v1,10.48550/arXiv.2209.03355
http://arxiv.org/abs/2107.06536v1,2021,"Multi-Attention Generative Adversarial Network for Remote Sensing Image
  Super-Resolution","  Image super-resolution (SR) methods can generate remote sensing images with
high spatial resolution without increasing the cost, thereby providing a
feasible way to acquire high-resolution remote sensing images, which are
difficult to obtain due to the high cost of acquisition equipment and complex
weather. Clearly, image super-resolution is a severe ill-posed problem.
Fortunately, with the development of deep learning, the powerful fitting
ability of deep neural networks has solved this problem to some extent. In this
paper, we propose a network based on the generative adversarial network (GAN)
to generate high resolution remote sensing images, named the multi-attention
generative adversarial network (MA-GAN). We first designed a GAN-based
framework for the image SR task. The core to accomplishing the SR task is the
image generator with post-upsampling that we designed. The main body of the
generator contains two blocks; one is the pyramidal convolution in the
residual-dense block (PCRDB), and the other is the attention-based upsample
(AUP) block. The attentioned pyramidal convolution (AttPConv) in the PCRDB
block is a module that combines multi-scale convolution and channel attention
to automatically learn and adjust the scaling of the residuals for better
results. The AUP block is a module that combines pixel attention (PA) to
perform arbitrary multiples of upsampling. These two blocks work together to
help generate better quality images. For the loss function, we design a loss
function based on pixel loss and introduce both adversarial loss and feature
loss to guide the generator learning. We have compared our method with several
state-of-the-art methods on a remote sensing scene image dataset, and the
experimental results consistently demonstrate the effectiveness of the proposed
MA-GAN.
",Meng Xu; Zhihao Wang; Jiasong Zhu; Xiuping Jia; Sen Jia,http://arxiv.org/abs/2107.06536v1,10.48550/arXiv.2107.06536
http://arxiv.org/abs/1811.00344v2,2018,"Analyzing Perception-Distortion Tradeoff using Enhanced Perceptual
  Super-resolution Network","  Convolutional neural network (CNN) based methods have recently achieved great
success for image super-resolution (SR). However, most deep CNN based SR models
attempt to improve distortion measures (e.g. PSNR, SSIM, IFC, VIF) while
resulting in poor quantified perceptual quality (e.g. human opinion score,
no-reference quality measures such as NIQE). Few works have attempted to
improve the perceptual quality at the cost of performance reduction in
distortion measures. A very recent study has revealed that distortion and
perceptual quality are at odds with each other and there is always a trade-off
between the two. Often the restoration algorithms that are superior in terms of
perceptual quality, are inferior in terms of distortion measures. Our work
attempts to analyze the trade-off between distortion and perceptual quality for
the problem of single image SR. To this end, we use the well-known SR
architecture-enhanced deep super-resolution (EDSR) network and show that it can
be adapted to achieve better perceptual quality for a specific range of the
distortion measure. While the original network of EDSR was trained to minimize
the error defined based on per-pixel accuracy alone, we train our network using
a generative adversarial network framework with EDSR as the generator module.
Our proposed network, called enhanced perceptual super-resolution network
(EPSR), is trained with a combination of mean squared error loss, perceptual
loss, and adversarial loss. Our experiments reveal that EPSR achieves the
state-of-the-art trade-off between distortion and perceptual quality while the
existing methods perform well in either of these measures alone.
",Subeesh Vasu; Nimisha Thekke Madam; Rajagopalan A. N,http://arxiv.org/abs/1811.00344v2,10.48550/arXiv.1811.00344
http://arxiv.org/abs/2011.02427v2,2020,Super-Resolution of Real-World Faces,"  Real low-resolution (LR) face images contain degradations which are too
varied and complex to be captured by known downsampling kernels and
signal-independent noises. So, in order to successfully super-resolve real
faces, a method needs to be robust to a wide range of noise, blur, compression
artifacts etc. Some of the recent works attempt to model these degradations
from a dataset of real images using a Generative Adversarial Network (GAN).
They generate synthetically degraded LR images and use them with corresponding
real high-resolution(HR) image to train a super-resolution (SR) network using a
combination of a pixel-wise loss and an adversarial loss. In this paper, we
propose a two module super-resolution network where the feature extractor
module extracts robust features from the LR image, and the SR module generates
an HR estimate using only these robust features. We train a degradation GAN to
convert bicubically downsampled clean images to real degraded images, and
interpolate between the obtained degraded LR image and its clean LR
counterpart. This interpolated LR image is then used along with it's
corresponding HR counterpart to train the super-resolution network from end to
end. Entropy Regularized Wasserstein Divergence is used to force the encoded
features learnt from the clean and degraded images to closely resemble those
extracted from the interpolated image to ensure robustness.
",Saurabh Goswami;  Aakanksha; Rajagopalan A. N,http://arxiv.org/abs/2011.02427v2,10.48550/arXiv.2011.02427
http://arxiv.org/abs/2107.07975v1,2021,"Joint Semi-supervised 3D Super-Resolution and Segmentation with Mixed
  Adversarial Gaussian Domain Adaptation","  Optimising the analysis of cardiac structure and function requires accurate
3D representations of shape and motion. However, techniques such as cardiac
magnetic resonance imaging are conventionally limited to acquiring contiguous
cross-sectional slices with low through-plane resolution and potential
inter-slice spatial misalignment. Super-resolution in medical imaging aims to
increase the resolution of images but is conventionally trained on features
from low resolution datasets and does not super-resolve corresponding
segmentations. Here we propose a semi-supervised multi-task generative
adversarial network (Gemini-GAN) that performs joint super-resolution of the
images and their labels using a ground truth of high resolution 3D cines and
segmentations, while an unsupervised variational adversarial mixture
autoencoder (V-AMA) is used for continuous domain adaptation. Our proposed
approach is extensively evaluated on two transnational multi-ethnic populations
of 1,331 and 205 adults respectively, delivering an improvement on state of the
art methods in terms of Dice index, peak signal to noise ratio, and structural
similarity index measure. This framework also exceeds the performance of state
of the art generative domain adaptation models on external validation (Dice
index 0.81 vs 0.74 for the left ventricle). This demonstrates how joint
super-resolution and segmentation, trained on 3D ground-truth data with
cross-domain generalization, enables robust precision phenotyping in diverse
populations.
",Nicolo Savioli; Antonio de Marvao; Wenjia Bai; Shuo Wang; Stuart A. Cook; Calvin W. L. Chin; Daniel Rueckert; Declan P. O'Regan,http://arxiv.org/abs/2107.07975v1,10.48550/arXiv.2107.07975
http://arxiv.org/abs/1609.04802v5,2016,"Photo-Realistic Single Image Super-Resolution Using a Generative
  Adversarial Network","  Despite the breakthroughs in accuracy and speed of single image
super-resolution using faster and deeper convolutional neural networks, one
central problem remains largely unsolved: how do we recover the finer texture
details when we super-resolve at large upscaling factors? The behavior of
optimization-based super-resolution methods is principally driven by the choice
of the objective function. Recent work has largely focused on minimizing the
mean squared reconstruction error. The resulting estimates have high peak
signal-to-noise ratios, but they are often lacking high-frequency details and
are perceptually unsatisfying in the sense that they fail to match the fidelity
expected at the higher resolution. In this paper, we present SRGAN, a
generative adversarial network (GAN) for image super-resolution (SR). To our
knowledge, it is the first framework capable of inferring photo-realistic
natural images for 4x upscaling factors. To achieve this, we propose a
perceptual loss function which consists of an adversarial loss and a content
loss. The adversarial loss pushes our solution to the natural image manifold
using a discriminator network that is trained to differentiate between the
super-resolved images and original photo-realistic images. In addition, we use
a content loss motivated by perceptual similarity instead of similarity in
pixel space. Our deep residual network is able to recover photo-realistic
textures from heavily downsampled images on public benchmarks. An extensive
mean-opinion-score (MOS) test shows hugely significant gains in perceptual
quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of
the original high-resolution images than to those obtained with any
state-of-the-art method.
",Christian Ledig; Lucas Theis; Ferenc Huszar; Jose Caballero; Andrew Cunningham; Alejandro Acosta; Andrew Aitken; Alykhan Tejani; Johannes Totz; Zehan Wang; Wenzhe Shi,http://arxiv.org/abs/1609.04802v5,10.48550/arXiv.1609.04802
http://arxiv.org/abs/1910.00579v2,2019,Unsupervised Projection Networks for Generative Adversarial Networks,"  We propose the use of unsupervised learning to train projection networks that
project onto the latent space of an already trained generator. We apply our
method to a trained StyleGAN, and use our projection network to perform image
super-resolution and clustering of images into semantically identifiable
groups.
",Daiyaan Arfeen; Jesse Zhang,http://arxiv.org/abs/1910.00579v2,10.48550/arXiv.1910.00579
http://arxiv.org/abs/2001.11715v1,2020,A Generative Adversarial Network for AI-Aided Chair Design,"  We present a method for improving human design of chairs. The goal of the
method is generating enormous chair candidates in order to facilitate human
designer by creating sketches and 3d models accordingly based on the generated
chair design. It consists of an image synthesis module, which learns the
underlying distribution of training dataset, a super-resolution module, which
improve quality of generated image and human involvements. Finally, we manually
pick one of the generated candidates to create a real life chair for
illustration.
",Zhibo Liu; Feng Gao; Yizhou Wang,http://arxiv.org/abs/2001.11715v1,10.1109/MIPR.2019.00098
http://arxiv.org/abs/1901.02731v1,2019,"A Comprehensive guide to Bayesian Convolutional Neural Network with
  Variational Inference","  Artificial Neural Networks are connectionist systems that perform a given
task by learning on examples without having prior knowledge about the task.
This is done by finding an optimal point estimate for the weights in every
node. Generally, the network using point estimates as weights perform well with
large datasets, but they fail to express uncertainty in regions with little or
no data, leading to overconfident decisions.
  In this paper, Bayesian Convolutional Neural Network (BayesCNN) using
Variational Inference is proposed, that introduces probability distribution
over the weights. Furthermore, the proposed BayesCNN architecture is applied to
tasks like Image Classification, Image Super-Resolution and Generative
Adversarial Networks. The results are compared to point-estimates based
architectures on MNIST, CIFAR-10 and CIFAR-100 datasets for Image
CLassification task, on BSD300 dataset for Image Super Resolution task and on
CIFAR10 dataset again for Generative Adversarial Network task.
  BayesCNN is based on Bayes by Backprop which derives a variational
approximation to the true posterior. We, therefore, introduce the idea of
applying two convolutional operations, one for the mean and one for the
variance. Our proposed method not only achieves performances equivalent to
frequentist inference in identical architectures but also incorporate a
measurement for uncertainties and regularisation. It further eliminates the use
of dropout in the model. Moreover, we predict how certain the model prediction
is based on the epistemic and aleatoric uncertainties and empirically show how
the uncertainty can decrease, allowing the decisions made by the network to
become more deterministic as the training accuracy increases. Finally, we
propose ways to prune the Bayesian architecture and to make it more
computational and time effective.
",Kumar Shridhar; Felix Laumann; Marcus Liwicki,http://arxiv.org/abs/1901.02731v1,10.48550/arXiv.1901.02731
http://arxiv.org/abs/1912.06013v2,2019,"An Approach to Super-Resolution of Sentinel-2 Images Based on Generative
  Adversarial Networks","  This paper presents a generative adversarial network based super-resolution
(SR) approach (which is called as S2GAN) to enhance the spatial resolution of
Sentinel-2 spectral bands. The proposed approach consists of two main steps.
The first step aims to increase the spatial resolution of the bands with 20m
and 60m spatial resolutions by the scaling factors of 2 and 6, respectively. To
this end, we introduce a generator network that performs SR on the lower
resolution bands with the guidance of the bands associated to 10m spatial
resolution by utilizing the convolutional layers with residual connections and
a long skip-connection between inputs and outputs. The second step aims to
distinguish SR bands from their ground truth bands. This is achieved by the
proposed discriminator network, which alternately characterizes the high level
features of the two sets of bands and applying binary classification on the
extracted features. Then, we formulate the adversarial learning of the
generator and discriminator networks as a min-max game. In this learning
procedure, the generator aims to produce realistic SR bands as much as possible
so that the discriminator incorrectly classifies SR bands. Experimental results
obtained on different Sentinel-2 images show the effectiveness of the proposed
approach compared to both conventional and deep learning based SR approaches.
",Kexin Zhang; Gencer Sumbul; Begüm Demir,http://arxiv.org/abs/1912.06013v2,10.1109/M2GARSS47143.2020.9105165
http://arxiv.org/abs/2003.01907v2,2020,"Turbulence Enrichment using Physics-informed Generative Adversarial
  Networks","  Generative Adversarial Networks (GANs) have been widely used for generating
photo-realistic images. A variant of GANs called super-resolution GAN (SRGAN)
has already been used successfully for image super-resolution where low
resolution images can be upsampled to a $4\times$ larger image that is
perceptually more realistic. However, when such generative models are used for
data describing physical processes, there are additional known constraints that
models must satisfy including governing equations and boundary conditions. In
general, these constraints may not be obeyed by the generated data. In this
work, we develop physics-based methods for generative enrichment of turbulence.
We incorporate a physics-informed learning approach by a modification to the
loss function to minimize the residuals of the governing equations for the
generated data. We have analyzed two trained physics-informed models: a
supervised model based on convolutional neural networks (CNN) and a generative
model based on SRGAN: Turbulence Enrichment GAN (TEGAN), and show that they
both outperform simple bicubic interpolation in turbulence enrichment. We have
also shown that using the physics-informed learning can also significantly
improve the model's ability in generating data that satisfies the physical
governing equations. Finally, we compare the enriched data from TEGAN to show
that it is able to recover statistical metrics of the flow field including
energy metrics and well as inter-scale energy dynamics and flow morphology.
",Akshay Subramaniam; Man Long Wong; Raunak D Borker; Sravya Nimmagadda; Sanjiva K Lele,http://arxiv.org/abs/2003.01907v2,10.48550/arXiv.2003.01907
http://arxiv.org/abs/2004.04788v2,2020,D-SRGAN: DEM Super-Resolution with Generative Adversarial Networks,"  LIDAR (light detection and ranging) is an optical remote-sensing technique
that measures the distance between sensor and object, and the reflected energy
from the object. Over the years, LIDAR data has been used as the primary source
of Digital Elevation Models (DEMs). DEMs have been used in a variety of
applications like road extraction, hydrological modeling, flood mapping, and
surface analysis. A number of studies in flooding suggest the usage of
high-resolution DEMs as inputs in the applications improve the overall
reliability and accuracy. Despite the importance of high-resolution DEM, many
areas in the United States and the world do not have access to high-resolution
DEM due to technological limitations or the cost of the data collection. With
recent development in Graphical Processing Units (GPU) and novel algorithms,
deep learning techniques have become attractive to researchers for their
performance in learning features from high-resolution datasets. Numerous new
methods have been proposed such as Generative Adversarial Networks (GANs) to
create intelligent models that correct and augment large-scale datasets. In
this paper, a GAN based model is developed and evaluated, inspired by single
image super-resolution methods, to increase the spatial resolution of a given
DEM dataset up to 4 times without additional information related to data.
",Bekir Z Demiray; Muhammed Sit; Ibrahim Demir,http://arxiv.org/abs/2004.04788v2,10.48550/arXiv.2004.04788
http://arxiv.org/abs/2107.09989v1,2021,"High-Resolution Pelvic MRI Reconstruction Using a Generative Adversarial
  Network with Attention and Cyclic Loss","  Magnetic resonance imaging (MRI) is an important medical imaging modality,
but its acquisition speed is quite slow due to the physiological limitations.
Recently, super-resolution methods have shown excellent performance in
accelerating MRI. In some circumstances, it is difficult to obtain
high-resolution images even with prolonged scan time. Therefore, we proposed a
novel super-resolution method that uses a generative adversarial network (GAN)
with cyclic loss and attention mechanism to generate high-resolution MR images
from low-resolution MR images by a factor of 2. We implemented our model on
pelvic images from healthy subjects as training and validation data, while
those data from patients were used for testing. The MR dataset was obtained
using different imaging sequences, including T2, T2W SPAIR, and mDIXON-W. Four
methods, i.e., BICUBIC, SRCNN, SRGAN, and EDSR were used for comparison.
Structural similarity, peak signal to noise ratio, root mean square error, and
variance inflation factor were used as calculation indicators to evaluate the
performances of the proposed method. Various experimental results showed that
our method can better restore the details of the high-resolution MR image as
compared to the other methods. In addition, the reconstructed high-resolution
MR image can provide better lesion textures in the tumor patients, which is
promising to be used in clinical diagnosis.
",Guangyuan Li; Jun Lv; Xiangrong Tong; Chengyan Wang; Guang Yang,http://arxiv.org/abs/2107.09989v1,10.48550/arXiv.2107.09989
http://arxiv.org/abs/2205.03777v2,2022,"Semi-Cycled Generative Adversarial Networks for Real-World Face
  Super-Resolution","  Real-world face super-resolution (SR) is a highly ill-posed image restoration
task. The fully-cycled Cycle-GAN architecture is widely employed to achieve
promising performance on face SR, but prone to produce artifacts upon
challenging cases in real-world scenarios, since joint participation in the
same degradation branch will impact final performance due to huge domain gap
between real-world and synthetic LR ones obtained by generators. To better
exploit the powerful generative capability of GAN for real-world face SR, in
this paper, we establish two independent degradation branches in the forward
and backward cycle-consistent reconstruction processes, respectively, while the
two processes share the same restoration branch. Our Semi-Cycled Generative
Adversarial Networks (SCGAN) is able to alleviate the adverse effects of the
domain gap between the real-world LR face images and the synthetic LR ones, and
to achieve accurate and robust face SR performance by the shared restoration
branch regularized by both the forward and backward cycle-consistent learning
processes. Experiments on two synthetic and two real-world datasets demonstrate
that, our SCGAN outperforms the state-of-the-art methods on recovering the face
structures/details and quantitative metrics for real-world face SR. The code
will be publicly released at https://github.com/HaoHou-98/SCGAN.
",Hao Hou; Jun Xu; Yingkun Hou; Xiaotao Hu; Benzheng Wei; Dinggang Shen,http://arxiv.org/abs/2205.03777v2,10.48550/arXiv.2205.03777
http://arxiv.org/abs/1711.02666v1,2017,"Tensor-Generative Adversarial Network with Two-dimensional Sparse
  Coding: Application to Real-time Indoor Localization","  Localization technology is important for the development of indoor
location-based services (LBS). Global Positioning System (GPS) becomes invalid
in indoor environments due to the non-line-of-sight issue, so it is urgent to
develop a real-time high-accuracy localization approach for smartphones.
However, accurate localization is challenging due to issues such as real-time
response requirements, limited fingerprint samples and mobile device storage.
To address these problems, we propose a novel deep learning architecture:
Tensor-Generative Adversarial Network (TGAN).
  We first introduce a transform-based 3D tensor to model fingerprint samples.
Instead of those passive methods that construct a fingerprint database as a
prior, our model applies artificial neural network with deep learning to train
network classifiers and then gives out estimations. Then we propose a novel
tensor-based super-resolution scheme using the generative adversarial network
(GAN) that adopts sparse coding as the generator network and a residual
learning network as the discriminator. Further, we analyze the performance of
tensor-GAN and implement a trace-based localization experiment, which achieves
better performance. Compared to existing methods for smartphones indoor
positioning, that are energy-consuming and high demands on devices, TGAN can
give out an improved solution in localization accuracy, response time and
implementation complexity.
",Chenxiao Zhu; Lingqing Xu; Xiao-Yang Liu; Feng Qian,http://arxiv.org/abs/1711.02666v1,10.48550/arXiv.1711.02666
http://arxiv.org/abs/2209.12875v1,2022,Realistic Hair Synthesis with Generative Adversarial Networks,"  Recent successes in generative modeling have accelerated studies on this
subject and attracted the attention of researchers. One of the most important
methods used to achieve this success is Generative Adversarial Networks (GANs).
It has many application areas such as; virtual reality (VR), augmented reality
(AR), super resolution, image enhancement. Despite the recent advances in hair
synthesis and style transfer using deep learning and generative modelling, due
to the complex nature of hair still contains unsolved challenges. The methods
proposed in the literature to solve this problem generally focus on making
high-quality hair edits on images. In this thesis, a generative adversarial
network method is proposed to solve the hair synthesis problem. While
developing this method, it is aimed to achieve real-time hair synthesis while
achieving visual outputs that compete with the best methods in the literature.
The proposed method was trained with the FFHQ dataset and then its results in
hair style transfer and hair reconstruction tasks were evaluated. The results
obtained in these tasks and the operating time of the method were compared with
MichiGAN, one of the best methods in the literature. The comparison was made at
a resolution of 128x128. As a result of the comparison, it has been shown that
the proposed method achieves competitive results with MichiGAN in terms of
realistic hair synthesis, and performs better in terms of operating time.
",Muhammed Pektas; Aybars Ugur,http://arxiv.org/abs/2209.12875v1,10.48550/arXiv.2209.12875
http://arxiv.org/abs/1710.07035v1,2017,Generative Adversarial Networks: An Overview,"  Generative adversarial networks (GANs) provide a way to learn deep
representations without extensively annotated training data. They achieve this
through deriving backpropagation signals through a competitive process
involving a pair of networks. The representations that can be learned by GANs
may be used in a variety of applications, including image synthesis, semantic
image editing, style transfer, image super-resolution and classification. The
aim of this review paper is to provide an overview of GANs for the signal
processing community, drawing on familiar analogies and concepts where
possible. In addition to identifying different methods for training and
constructing GANs, we also point to remaining challenges in their theory and
application.
",Antonia Creswell; Tom White; Vincent Dumoulin; Kai Arulkumaran; Biswa Sengupta; Anil A Bharath,http://arxiv.org/abs/1710.07035v1,10.1109/MSP.2017.2765202
http://arxiv.org/abs/1909.12909v1,2019,"Deeply Matting-based Dual Generative Adversarial Network for Image and
  Document Label Supervision","  Although many methods have been proposed to deal with nature image
super-resolution (SR) and get impressive performance, the text images SR is not
good due to their ignorance of document images. In this paper, we propose a
matting-based dual generative adversarial network (mdGAN) for document image
SR. Firstly, the input image is decomposed into document text, foreground and
background layers using deep image matting. Then two parallel branches are
constructed to recover text boundary information and color information
respectively. Furthermore, in order to improve the restoration accuracy of
characters in output image, we use the input image's corresponding ground truth
text label as extra supervise information to refine the two-branch networks
during training. Experiments on real text images demonstrate that our method
outperforms several state-of-the-art methods quantitatively and qualitatively.
",Yubao Liu; Kai Lin,http://arxiv.org/abs/1909.12909v1,10.48550/arXiv.1909.12909
http://arxiv.org/abs/2102.04362v2,2021,"Protecting Intellectual Property of Generative Adversarial Networks from
  Ambiguity Attack","  Ever since Machine Learning as a Service (MLaaS) emerges as a viable business
that utilizes deep learning models to generate lucrative revenue, Intellectual
Property Right (IPR) has become a major concern because these deep learning
models can easily be replicated, shared, and re-distributed by any unauthorized
third parties. To the best of our knowledge, one of the prominent deep learning
models - Generative Adversarial Networks (GANs) which has been widely used to
create photorealistic image are totally unprotected despite the existence of
pioneering IPR protection methodology for Convolutional Neural Networks (CNNs).
This paper therefore presents a complete protection framework in both black-box
and white-box settings to enforce IPR protection on GANs. Empirically, we show
that the proposed method does not compromise the original GANs performance
(i.e. image generation, image super-resolution, style transfer), and at the
same time, it is able to withstand both removal and ambiguity attacks against
embedded watermarks.
",Ding Sheng Ong; Chee Seng Chan; Kam Woh Ng; Lixin Fan; Qiang Yang,http://arxiv.org/abs/2102.04362v2,10.48550/arXiv.2102.04362
http://arxiv.org/abs/2112.13191v1,2021,"DSRGAN: Detail Prior-Assisted Perceptual Single Image Super-Resolution
  via Generative Adversarial Networks","  The generative adversarial network (GAN) is successfully applied to study the
perceptual single image superresolution (SISR). However, the GAN often tends to
generate images with high frequency details being inconsistent with the real
ones. Inspired by conventional detail enhancement algorithms, we propose a
novel prior knowledge, the detail prior, to assist the GAN in alleviating this
problem and restoring more realistic details. The proposed method, named
DSRGAN, includes a well designed detail extraction algorithm to capture the
most important high frequency information from images. Then, two discriminators
are utilized for supervision on image-domain and detail-domain restorations,
respectively. The DSRGAN merges the restored detail into the final output via a
detail enhancement manner. The special design of DSRGAN takes advantages from
both the model-based conventional algorithm and the data-driven deep learning
network. Experimental results demonstrate that the DSRGAN outperforms the
state-of-the-art SISR methods on perceptual metrics and achieves comparable
results in terms of fidelity metrics simultaneously. Following the DSRGAN, it
is feasible to incorporate other conventional image processing algorithms into
a deep learning network to form a model-based deep SISR.
",Ziyang Liu; Zhengguo Li; Xingming Wu; Zhong Liu; Weihai Chen,http://arxiv.org/abs/2112.13191v1,10.48550/arXiv.2112.13191
http://arxiv.org/abs/1705.02438v1,2017,Face Super-Resolution Through Wasserstein GANs,"  Generative adversarial networks (GANs) have received a tremendous amount of
attention in the past few years, and have inspired applications addressing a
wide range of problems. Despite its great potential, GANs are difficult to
train. Recently, a series of papers (Arjovsky & Bottou, 2017a; Arjovsky et al.
2017b; and Gulrajani et al. 2017) proposed using Wasserstein distance as the
training objective and promised easy, stable GAN training across architectures
with minimal hyperparameter tuning. In this paper, we compare the performance
of Wasserstein distance with other training objectives on a variety of GAN
architectures in the context of single image super-resolution. Our results
agree that Wasserstein GAN with gradient penalty (WGAN-GP) provides stable and
converging GAN training and that Wasserstein distance is an effective metric to
gauge training progress.
",Zhimin Chen; Yuguang Tong,http://arxiv.org/abs/1705.02438v1,10.48550/arXiv.1705.02438
http://arxiv.org/abs/1711.10312v1,2017,"Super-Resolution for Overhead Imagery Using DenseNets and Adversarial
  Learning","  Recent advances in Generative Adversarial Learning allow for new modalities
of image super-resolution by learning low to high resolution mappings. In this
paper we present our work using Generative Adversarial Networks (GANs) with
applications to overhead and satellite imagery. We have experimented with
several state-of-the-art architectures. We propose a GAN-based architecture
using densely connected convolutional neural networks (DenseNets) to be able to
super-resolve overhead imagery with a factor of up to 8x. We have also
investigated resolution limits of these networks. We report results on several
publicly available datasets, including SpaceNet data and IARPA Multi-View
Stereo Challenge, and compare performance with other state-of-the-art
architectures.
",Marc Bosch; Christopher M. Gifford; Pedro A. Rodriguez,http://arxiv.org/abs/1711.10312v1,10.48550/arXiv.1711.10312
http://arxiv.org/abs/1804.02900v2,2018,A Fully Progressive Approach to Single-Image Super-Resolution,"  Recent deep learning approaches to single image super-resolution have
achieved impressive results in terms of traditional error measures and
perceptual quality. However, in each case it remains challenging to achieve
high quality results for large upsampling factors. To this end, we propose a
method (ProSR) that is progressive both in architecture and training: the
network upsamples an image in intermediate steps, while the learning process is
organized from easy to hard, as is done in curriculum learning. To obtain more
photorealistic results, we design a generative adversarial network (GAN), named
ProGanSR, that follows the same progressive multi-scale design principle. This
not only allows to scale well to high upsampling factors (e.g., 8x) but
constitutes a principled multi-scale approach that increases the reconstruction
quality for all upsampling factors simultaneously. In particular ProSR ranks
2nd in terms of SSIM and 4th in terms of PSNR in the NTIRE2018 SISR challenge
[34]. Compared to the top-ranking team, our model is marginally lower, but runs
5 times faster.
",Yifan Wang; Federico Perazzi; Brian McWilliams; Alexander Sorkine-Hornung; Olga Sorkine-Hornung; Christopher Schroers,http://arxiv.org/abs/1804.02900v2,10.48550/arXiv.1804.02900
http://arxiv.org/abs/1812.11440v1,2018,Brain MRI super-resolution using 3D generative adversarial networks,"  In this work we propose an adversarial learning approach to generate high
resolution MRI scans from low resolution images. The architecture, based on the
SRGAN model, adopts 3D convolutions to exploit volumetric information. For the
discriminator, the adversarial loss uses least squares in order to stabilize
the training. For the generator, the loss function is a combination of a least
squares adversarial loss and a content term based on mean square error and
image gradients in order to improve the quality of the generated images. We
explore different solutions for the upsampling phase. We present promising
results that improve classical interpolation, showing the potential of the
approach for 3D medical imaging super-resolution. Source code available at
https://github.com/imatge-upc/3D-GAN-superresolution
",Irina Sanchez; Veronica Vilaplana,http://arxiv.org/abs/1812.11440v1,10.48550/arXiv.1812.11440
http://arxiv.org/abs/1901.09953v2,2019,TGAN: Deep Tensor Generative Adversarial Nets for Large Image Generation,"  Deep generative models have been successfully applied to many applications.
However, existing works experience limitations when generating large images
(the literature usually generates small images, e.g. 32 * 32 or 128 * 128). In
this paper, we propose a novel scheme, called deep tensor adversarial
generative nets (TGAN), that generates large high-quality images by exploring
tensor structures. Essentially, the adversarial process of TGAN takes place in
a tensor space. First, we impose tensor structures for concise image
representation, which is superior in capturing the pixel proximity information
and the spatial patterns of elementary objects in images, over the
vectorization preprocess in existing works. Secondly, we propose TGAN that
integrates deep convolutional generative adversarial networks and tensor
super-resolution in a cascading manner, to generate high-quality images from
random distributions. More specifically, we design a tensor super-resolution
process that consists of tensor dictionary learning and tensor coefficients
learning. Finally, on three datasets, the proposed TGAN generates images with
more realistic textures, compared with state-of-the-art adversarial
autoencoders. The size of the generated images is increased by over 8.5 times,
namely 374 * 374 in PASCAL2.
",Zihan Ding; Xiao-Yang Liu; Miao Yin; Linghe Kong,http://arxiv.org/abs/1901.09953v2,10.48550/arXiv.1901.09953
http://arxiv.org/abs/1912.10427v1,2019,"Joint Face Super-Resolution and Deblurring Using a Generative
  Adversarial Network","  Facial image super-resolution (SR) is an important preprocessing for facial
image analysis, face recognition, and image-based 3D face reconstruction.
Recent convolutional neural network (CNN) based method has shown excellent
performance by learning mapping relation using pairs of low-resolution (LR) and
high-resolution (HR) facial images. However, since the HR facial image
reconstruction using CNN is conventionally aimed to increase the PSNR and SSIM
metrics, the reconstructed HR image might not be realistic even with high
scores. An adversarial framework is proposed in this study to reconstruct the
HR facial image by simultaneously generating an HR image with and without blur.
First, the spatial resolution of the LR facial image is increased by eight
times using a five-layer CNN. Then, the encoder extracts the features of the
up-scaled image. These features are finally sent to two branches (decoders) to
generate an HR facial image with and without blur. In addition, local and
global discriminators are combined to focus on the reconstruction of HR facial
structures. Experiment results show that the proposed algorithm generates a
realistic HR facial image. Furthermore, the proposed method can generate a
variety of different facial images.
",Jung Un Yun; In Kyu Park,http://arxiv.org/abs/1912.10427v1,10.48550/arXiv.1912.10427
http://arxiv.org/abs/2001.07766v1,2020,"Adaptive Loss Function for Super Resolution Neural Networks Using Convex
  Optimization Techniques","  Single Image Super-Resolution (SISR) task refers to learn a mapping from
low-resolution images to the corresponding high-resolution ones. This task is
known to be extremely difficult since it is an ill-posed problem. Recently,
Convolutional Neural Networks (CNNs) have achieved state of the art performance
on SISR. However, the images produced by CNNs do not contain fine details of
the images. Generative Adversarial Networks (GANs) aim to solve this issue and
recover sharp details. Nevertheless, GANs are notoriously difficult to train.
Besides that, they generate artifacts in the high-resolution images. In this
paper, we have proposed a method in which CNNs try to align images in different
spaces rather than only the pixel space. Such a space is designed using convex
optimization techniques. CNNs are encouraged to learn high-frequency components
of the images as well as low-frequency components. We have shown that the
proposed method can recover fine details of the images and it is stable in the
training process.
",Seyed Mehdi Ayyoubzadeh; Xiaolin Wu,http://arxiv.org/abs/2001.07766v1,10.48550/arXiv.2001.07766
http://arxiv.org/abs/2002.11397v1,2020,Unpaired Image Super-Resolution using Pseudo-Supervision,"  In most studies on learning-based image super-resolution (SR), the paired
training dataset is created by downscaling high-resolution (HR) images with a
predetermined operation (e.g., bicubic). However, these methods fail to
super-resolve real-world low-resolution (LR) images, for which the degradation
process is much more complicated and unknown. In this paper, we propose an
unpaired SR method using a generative adversarial network that does not require
a paired/aligned training dataset. Our network consists of an unpaired
kernel/noise correction network and a pseudo-paired SR network. The correction
network removes noise and adjusts the kernel of the inputted LR image; then,
the corrected clean LR image is upscaled by the SR network. In the training
phase, the correction network also produces a pseudo-clean LR image from the
inputted HR image, and then a mapping from the pseudo-clean LR image to the
inputted HR image is learned by the SR network in a paired manner. Because our
SR network is independent of the correction network, well-studied existing
network architectures and pixel-wise loss functions can be integrated with the
proposed framework. Experiments on diverse datasets show that the proposed
method is superior to existing solutions to the unpaired SR problem.
",Shunta Maeda,http://arxiv.org/abs/2002.11397v1,10.48550/arXiv.2002.11397
http://arxiv.org/abs/2005.00306v2,2020,"PCA-SRGAN: Incremental Orthogonal Projection Discrimination for Face
  Super-resolution","  Generative Adversarial Networks (GAN) have been employed for face super
resolution but they bring distorted facial details easily and still have
weakness on recovering realistic texture. To further improve the performance of
GAN based models on super-resolving face images, we propose PCA-SRGAN which
pays attention to the cumulative discrimination in the orthogonal projection
space spanned by PCA projection matrix of face data. By feeding the principal
component projections ranging from structure to details into the discriminator,
the discrimination difficulty will be greatly alleviated and the generator can
be enhanced to reconstruct clearer contour and finer texture, helpful to
achieve the high perception and low distortion eventually. This incremental
orthogonal projection discrimination has ensured a precise optimization
procedure from coarse to fine and avoids the dependence on the perceptual
regularization. We conduct experiments on CelebA and FFHQ face datasets. The
qualitative visual effect and quantitative evaluation have demonstrated the
overwhelming performance of our model over related works.
",Hao Dou; Chen Chen; Xiyuan Hu; Zuxing Xuan; Zhisen Hu; Silong Peng,http://arxiv.org/abs/2005.00306v2,10.48550/arXiv.2005.00306
http://arxiv.org/abs/2007.04356v1,2020,Journey Towards Tiny Perceptual Super-Resolution,"  Recent works in single-image perceptual super-resolution (SR) have
demonstrated unprecedented performance in generating realistic textures by
means of deep convolutional networks. However, these convolutional models are
excessively large and expensive, hindering their effective deployment to end
devices. In this work, we propose a neural architecture search (NAS) approach
that integrates NAS and generative adversarial networks (GANs) with recent
advances in perceptual SR and pushes the efficiency of small perceptual SR
models to facilitate on-device execution. Specifically, we search over the
architectures of both the generator and the discriminator sequentially,
highlighting the unique challenges and key observations of searching for an
SR-optimized discriminator and comparing them with existing discriminator
architectures in the literature. Our tiny perceptual SR (TPSR) models
outperform SRGAN and EnhanceNet on both full-reference perceptual metric
(LPIPS) and distortion metric (PSNR) while being up to 26.4$\times$ more memory
efficient and 33.6$\times$ more compute efficient respectively.
",Royson Lee; Łukasz Dudziak; Mohamed Abdelfattah; Stylianos I. Venieris; Hyeji Kim; Hongkai Wen; Nicholas D. Lane,http://arxiv.org/abs/2007.04356v1,10.48550/arXiv.2007.04356
http://arxiv.org/abs/2010.09594v1,2020,"Multi-Modal Super Resolution for Dense Microscopic Particle Size
  Estimation","  Particle Size Analysis (PSA) is an important process carried out in a number
of industries, which can significantly influence the properties of the final
product. A ubiquitous instrument for this purpose is the Optical Microscope
(OM). However, OMs are often prone to drawbacks like low resolution, small
focal depth, and edge features being masked due to diffraction. We propose a
powerful application of a combination of two Conditional Generative Adversarial
Networks (cGANs) that Super Resolve OM images to look like Scanning Electron
Microscope (SEM) images. We further demonstrate the use of a custom object
detection module that can perform efficient PSA of the super-resolved particles
on both, densely and sparsely packed images. The PSA results obtained from the
super-resolved images have been benchmarked against human annotators, and
results obtained from the corresponding SEM images. The proposed models show a
generalizable way of multi-modal image translation and super-resolution for
accurate particle size estimation.
",Sarvesh Patil; Chava Y P D Phani Rajanish; Naveen Margankunte,http://arxiv.org/abs/2010.09594v1,10.48550/arXiv.2010.09594
http://arxiv.org/abs/2011.04003v1,2020,"Real-time Surgical Environment Enhancement for Robot-Assisted Minimally
  Invasive Surgery Based on Super-Resolution","  In Robot-Assisted Minimally Invasive Surgery (RAMIS), a camera assistant is
normally required to control the position and zooming ratio of the laparoscope,
following the surgeon's instructions. However, moving the laparoscope
frequently may lead to unstable and suboptimal views, while the adjustment of
zooming ratio may interrupt the workflow of the surgical operation. To this
end, we propose a multi-scale Generative Adversarial Network (GAN)-based video
super-resolution method to construct a framework for automatic zooming ratio
adjustment. It can provide automatic real-time zooming for high-quality
visualization of the Region Of Interest (ROI) during the surgical operation. In
the pipeline of the framework, the Kernel Correlation Filter (KCF) tracker is
used for tracking the tips of the surgical tools, while the Semi-Global Block
Matching (SGBM) based depth estimation and Recurrent Neural Network (RNN)-based
context-awareness are developed to determine the upscaling ratio for zooming.
The framework is validated with the JIGSAW dataset and Hamlyn Centre
Laparoscopic/Endoscopic Video Datasets, with results demonstrating its
practicability.
",Ruoxi Wang; Dandan Zhang; Qingbiao Li; Xiao-Yun Zhou; Benny Lo,http://arxiv.org/abs/2011.04003v1,10.48550/arXiv.2011.04003
http://arxiv.org/abs/2012.00739v1,2020,GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution,"  We show that pre-trained Generative Adversarial Networks (GANs), e.g.,
StyleGAN, can be used as a latent bank to improve the restoration quality of
large-factor image super-resolution (SR). While most existing SR approaches
attempt to generate realistic textures through learning with adversarial loss,
our method, Generative LatEnt bANk (GLEAN), goes beyond existing practices by
directly leveraging rich and diverse priors encapsulated in a pre-trained GAN.
But unlike prevalent GAN inversion methods that require expensive
image-specific optimization at runtime, our approach only needs a single
forward pass to generate the upscaled image. GLEAN can be easily incorporated
in a simple encoder-bank-decoder architecture with multi-resolution skip
connections. Switching the bank allows the method to deal with images from
diverse categories, e.g., cat, building, human face, and car. Images upscaled
by GLEAN show clear improvements in terms of fidelity and texture faithfulness
in comparison to existing methods.
",Kelvin C. K. Chan; Xintao Wang; Xiangyu Xu; Jinwei Gu; Chen Change Loy,http://arxiv.org/abs/2012.00739v1,10.48550/arXiv.2012.00739
http://arxiv.org/abs/2105.01739v1,2021,Multigrid Solver With Super-Resolved Interpolation,"  The multigrid algorithm is an efficient numerical method for solving a
variety of elliptic partial differential equations (PDEs). The method damps
errors at progressively finer grid scales, resulting in faster convergence
compared to standard iterative methods such as Gauss-Seidel. The prolongation,
or coarse-to-fine interpolation operator within the multigrid algorithm lends
itself to a data-driven treatment with ML super resolution, commonly used to
increase the resolution of images. We (i) propose the novel integration of a
super resolution generative adversarial network (GAN) model with the multigrid
algorithm as the prolongation operator and (ii) show that the GAN-interpolation
improves the convergence properties of the multigrid in comparison to cubic
spline interpolation on a class of multiscale PDEs typically solved in physics
and engineering simulations.
",Francisco Holguin; GS Sidharth; Gavin Portwood,http://arxiv.org/abs/2105.01739v1,10.48550/arXiv.2105.01739
http://arxiv.org/abs/2105.07139v1,2021,"Image Super-Resolution Quality Assessment: Structural Fidelity Versus
  Statistical Naturalness","  Single image super-resolution (SISR) algorithms reconstruct high-resolution
(HR) images with their low-resolution (LR) counterparts. It is desirable to
develop image quality assessment (IQA) methods that can not only evaluate and
compare SISR algorithms, but also guide their future development. In this
paper, we assess the quality of SISR generated images in a two-dimensional (2D)
space of structural fidelity versus statistical naturalness. This allows us to
observe the behaviors of different SISR algorithms as a tradeoff in the 2D
space. Specifically, SISR methods are traditionally designed to achieve high
structural fidelity but often sacrifice statistical naturalness, while recent
generative adversarial network (GAN) based algorithms tend to create more
natural-looking results but lose significantly on structural fidelity.
Furthermore, such a 2D evaluation can be easily fused to a scalar quality
prediction. Interestingly, we find that a simple linear combination of a
straightforward local structural fidelity and a global statistical naturalness
measures produce surprisingly accurate predictions of SISR image quality when
tested using public subject-rated SISR image datasets. Code of the proposed
SFSN model is publicly available at \url{https://github.com/weizhou-geek/SFSN}.
",Wei Zhou; Zhou Wang; Zhibo Chen,http://arxiv.org/abs/2105.07139v1,10.48550/arXiv.2105.07139
http://arxiv.org/abs/2107.09523v2,2021,"ProfileSR-GAN: A GAN based Super-Resolution Method for Generating
  High-Resolution Load Profiles","  It is a common practice for utilities to down-sample smart meter measurements
from high resolution (e.g. 1-min or 1-sec) to low resolution (e.g. 15-, 30- or
60-min) to lower the data transmission and storage cost. However, down-sampling
can remove high-frequency components from time-series load profiles, making
them unsuitable for in-depth studies such as quasi-static power flow analysis
or non-intrusive load monitoring (NILM). Thus, in this paper, we propose
ProfileSR-GAN: a Generative Adversarial Network (GAN) based load profile
super-resolution (LPSR) framework for restoring high-frequency components lost
through the smoothing effect of the down-sampling process. The LPSR problem is
formulated as a Maximum-a-Prior problem. When training the ProfileSR-GAN
generator network, to make the generated profiles more realistic, we introduce
two new shape-related losses in addition to conventionally used content loss:
adversarial loss and feature-matching loss. Moreover, a new set of shape-based
evaluation metrics are proposed to evaluate the realisticness of the generated
profiles. Simulation results show that ProfileSR-GAN outperforms Mean-Square
Loss based methods in all shape-based metrics. The successful application in
NILM further demonstrates that ProfileSR-GAN is effective in recovering
high-resolution realistic waveforms.
",Lidong Song; Yiyan Li; Ning Lu,http://arxiv.org/abs/2107.09523v2,10.48550/arXiv.2107.09523
http://arxiv.org/abs/2107.12679v1,2021,"MFAGAN: A Compression Framework for Memory-Efficient On-Device
  Super-Resolution GAN","  Generative adversarial networks (GANs) have promoted remarkable advances in
single-image super-resolution (SR) by recovering photo-realistic images.
However, high memory consumption of GAN-based SR (usually generators) causes
performance degradation and more energy consumption, hindering the deployment
of GAN-based SR into resource-constricted mobile devices. In this paper, we
propose a novel compression framework \textbf{M}ulti-scale \textbf{F}eature
\textbf{A}ggregation Net based \textbf{GAN} (MFAGAN) for reducing the memory
access cost of the generator. First, to overcome the memory explosion of dense
connections, we utilize a memory-efficient multi-scale feature aggregation net
as the generator. Second, for faster and more stable training, our method
introduces the PatchGAN discriminator. Third, to balance the student
discriminator and the compressed generator, we distill both the generator and
the discriminator. Finally, we perform a hardware-aware neural architecture
search (NAS) to find a specialized SubGenerator for the target mobile phone.
Benefiting from these improvements, the proposed MFAGAN achieves up to
\textbf{8.3}$\times$ memory saving and \textbf{42.9}$\times$ computation
reduction, with only minor visual quality degradation, compared with ESRGAN.
Empirical studies also show $\sim$\textbf{70} milliseconds latency on Qualcomm
Snapdragon 865 chipset.
",Wenlong Cheng; Mingbo Zhao; Zhiling Ye; Shuhang Gu,http://arxiv.org/abs/2107.12679v1,10.48550/arXiv.2107.12679
http://arxiv.org/abs/2202.01116v2,2022,An Optimal Transport Perspective on Unpaired Image Super-Resolution,"  Real-world image super-resolution (SR) tasks often do not have paired
datasets, which limits the application of supervised techniques. As a result,
the tasks are usually approached by unpaired techniques based on Generative
Adversarial Networks (GANs), which yield complex training losses with several
regularization terms, e.g., content or identity losses. We theoretically
investigate optimization problems which arise in such models and find two
surprizing observations. First, the learned SR map is always an optimal
transport (OT) map. Second, we theoretically prove and empirically show that
the learned map is biased, i.e., it does not actually transform the
distribution of low-resolution images to high-resolution ones. Inspired by
these findings, we propose an algorithm for unpaired SR which learns an
unbiased OT map for the perceptual transport cost. Unlike the existing
GAN-based alternatives, our algorithm has a simple optimization objective
reducing the need for complex hyperparameter selection and an application of
additional regularizations. At the same time, it provides a nearly
state-of-the-art performance on the large-scale unpaired AIM19 dataset.
",Milena Gazdieva; Litu Rout; Alexander Korotin; Andrey Kravchenko; Alexander Filippov; Evgeny Burnaev,http://arxiv.org/abs/2202.01116v2,10.48550/arXiv.2202.01116
http://arxiv.org/abs/2202.13799v2,2022,OUR-GAN: One-shot Ultra-high-Resolution Generative Adversarial Networks,"  We propose OUR-GAN, the first one-shot ultra-high-resolution (UHR) image
synthesis framework that generates non-repetitive images with 4K or higher
resolution from a single training image. OUR-GAN generates a visually coherent
image at low resolution and then gradually increases the resolution by
super-resolution. Since OUR-GAN learns from a real UHR image, it can synthesize
large-scale shapes with fine details while maintaining long-range coherence,
which is difficult with conventional generative models that generate large
images based on the patch distribution learned from relatively small images.
OUR-GAN applies seamless subregion-wise super-resolution that synthesizes 4k or
higher UHR images with limited memory, preventing discontinuity at the
boundary. Additionally, OUR-GAN improves visual coherence maintaining diversity
by adding vertical positional embeddings to the feature maps. In experiments on
the ST4K and RAISE datasets, OUR-GAN exhibited improved fidelity, visual
coherency, and diversity compared with existing methods. The synthesized images
are presented at https://anonymous-62348.github.io.
",Donghwee Yoon; Junseok Oh; Hayeong Choi; Minjae Yi; Injung Kim,http://arxiv.org/abs/2202.13799v2,10.48550/arXiv.2202.13799
http://arxiv.org/abs/2203.09195v1,2022,"Details or Artifacts: A Locally Discriminative Learning Approach to
  Realistic Image Super-Resolution","  Single image super-resolution (SISR) with generative adversarial networks
(GAN) has recently attracted increasing attention due to its potentials to
generate rich details. However, the training of GAN is unstable, and it often
introduces many perceptually unpleasant artifacts along with the generated
details. In this paper, we demonstrate that it is possible to train a GAN-based
SISR model which can stably generate perceptually realistic details while
inhibiting visual artifacts. Based on the observation that the local statistics
(e.g., residual variance) of artifact areas are often different from the areas
of perceptually friendly details, we develop a framework to discriminate
between GAN-generated artifacts and realistic details, and consequently
generate an artifact map to regularize and stabilize the model training
process. Our proposed locally discriminative learning (LDL) method is simple
yet effective, which can be easily plugged in off-the-shelf SISR methods and
boost their performance. Experiments demonstrate that LDL outperforms the
state-of-the-art GAN based SISR methods, achieving not only higher
reconstruction accuracy but also superior perceptual quality on both synthetic
and real-world datasets. Codes and models are available at
https://github.com/csjliang/LDL.
",Jie Liang; Hui Zeng; Lei Zhang,http://arxiv.org/abs/2203.09195v1,10.48550/arXiv.2203.09195
http://arxiv.org/abs/2207.08689v1,2022,"Quality Assessment of Image Super-Resolution: Balancing Deterministic
  and Statistical Fidelity","  There has been a growing interest in developing image super-resolution (SR)
algorithms that convert low-resolution (LR) to higher resolution images, but
automatically evaluating the visual quality of super-resolved images remains a
challenging problem. Here we look at the problem of SR image quality assessment
(SR IQA) in a two-dimensional (2D) space of deterministic fidelity (DF) versus
statistical fidelity (SF). This allows us to better understand the advantages
and disadvantages of existing SR algorithms, which produce images at different
clusters in the 2D space of (DF, SF). Specifically, we observe an interesting
trend from more traditional SR algorithms that are typically inclined to
optimize for DF while losing SF, to more recent generative adversarial network
(GAN) based approaches that by contrast exhibit strong advantages in achieving
high SF but sometimes appear weak at maintaining DF. Furthermore, we propose an
uncertainty weighting scheme based on content-dependent sharpness and texture
assessment that merges the two fidelity measures into an overall quality
prediction named the Super Resolution Image Fidelity (SRIF) index, which
demonstrates superior performance against state-of-the-art IQA models when
tested on subject-rated datasets.
",Wei Zhou; Zhou Wang,http://arxiv.org/abs/2207.08689v1,10.48550/arXiv.2207.08689
http://arxiv.org/abs/2208.02861v1,2022,"Latent Multi-Relation Reasoning for GAN-Prior based Image
  Super-Resolution","  Recently, single image super-resolution (SR) under large scaling factors has
witnessed impressive progress by introducing pre-trained generative adversarial
networks (GANs) as priors. However, most GAN-Priors based SR methods are
constrained by an attribute disentanglement problem in inverted latent codes
which directly leads to mismatches of visual attributes in the generator layers
and further degraded reconstruction. In addition, stochastic noises fed to the
generator are employed for unconditional detail generation, which tends to
produce unfaithful details that compromise the fidelity of the generated SR
image. We design LAREN, a LAtent multi-Relation rEasoNing technique that
achieves superb large-factor SR through graph-based multi-relation reasoning in
latent space. LAREN consists of two innovative designs. The first is
graph-based disentanglement that constructs a superior disentangled latent
space via hierarchical multi-relation reasoning. The second is graph-based code
generation that produces image-specific codes progressively via recursive
relation reasoning which enables prior GANs to generate desirable image
details. Extensive experiments show that LAREN achieves superior large-factor
image SR and outperforms the state-of-the-art consistently across multiple
benchmarks.
",Jiahui Zhang; Fangneng Zhan; Yingchen Yu; Rongliang Wu; Xiaoqin Zhang; Shijian Lu,http://arxiv.org/abs/2208.02861v1,10.48550/arXiv.2208.02861
http://arxiv.org/abs/2209.12433v1,2022,"Deep generative model super-resolves spatially correlated multiregional
  climate data","  Super-resolving the coarse outputs of global climate simulations, termed
downscaling, is crucial in making political and social decisions on systems
requiring long-term climate change projections. Existing fast super-resolution
techniques, however, have yet to preserve the spatially correlated nature of
climatological data, which is particularly important when we address systems
with spatial expanse, such as the development of transportation infrastructure.
Herein, we show an adversarial network-based machine learning enables us to
correctly reconstruct the inter-regional spatial correlations in downscaling
with high magnification up to fifty, while maintaining the pixel-wise
statistical consistency. Direct comparison with the measured meteorological
data of temperature and precipitation distributions reveals that integrating
climatologically important physical information is essential for the accurate
downscaling, which prompts us to call our approach $\pi$SRGAN (Physics Informed
Super-Resolution Generative Adversarial Network). The present method has a
potential application to the inter-regionally consistent assessment of the
climate change impact.
",Norihiro Oyama; Noriko N. Ishizaki; Satoshi Koide; Hiroaki Yoshida,http://arxiv.org/abs/2209.12433v1,10.48550/arXiv.2209.12433
http://arxiv.org/abs/2211.07555v1,2022,"Contrastive Learning for Climate Model Bias Correction and
  Super-Resolution","  Climate models often require post-processing in order to make accurate
estimates of local climate risk. The most common post-processing applied is
bias-correction and spatial resolution enhancement. However, the statistical
methods typically used for this not only are incapable of capturing
multivariate spatial correlation information but are also reliant on rich
observational data often not available outside of developed countries, limiting
their potential. Here we propose an alternative approach to this challenge
based on a combination of image super resolution (SR) and contrastive learning
generative adversarial networks (GANs). We benchmark performance against NASA's
flagship post-processed CMIP6 climate model product, NEX-GDDP. We find that our
model successfully reaches a spatial resolution double that of NASA's product
while also achieving comparable or improved levels of bias correction in both
daily precipitation and temperature. The resulting higher fidelity simulations
of present and forward-looking climate can enable more local, accurate models
of hazards like flooding, drought, and heatwaves.
",Tristan Ballard; Gopal Erinjippurath,http://arxiv.org/abs/2211.07555v1,10.48550/arXiv.2211.07555
http://arxiv.org/abs/2301.00504v1,2023,"Spectral Bandwidth Recovery of Optical Coherence Tomography Images using
  Deep Learning","  Optical coherence tomography (OCT) captures cross-sectional data and is used
for the screening, monitoring, and treatment planning of retinal diseases.
Technological developments to increase the speed of acquisition often results
in systems with a narrower spectral bandwidth, and hence a lower axial
resolution. Traditionally, image-processing-based techniques have been utilized
to reconstruct subsampled OCT data and more recently, deep-learning-based
methods have been explored. In this study, we simulate reduced axial scan
(A-scan) resolution by Gaussian windowing in the spectral domain and
investigate the use of a learning-based approach for image feature
reconstruction. In anticipation of the reduced resolution that accompanies
wide-field OCT systems, we build upon super-resolution techniques to explore
methods to better aid clinicians in their decision-making to improve patient
outcomes, by reconstructing lost features using a pixel-to-pixel approach with
an altered super-resolution generative adversarial network (SRGAN)
architecture.
",Timothy T. Yu; Da Ma; Jayden Cole; Myeong Jin Ju; Mirza F. Beg; Marinko V. Sarunic,http://arxiv.org/abs/2301.00504v1,10.48550/arXiv.2301.00504
http://arxiv.org/abs/2106.02599v1,2021,SOUP-GAN: Super-Resolution MRI Using Generative Adversarial Networks,"  There is a growing demand for high-resolution (HR) medical images in both the
clinical and research applications. Image quality is inevitably traded off with
the acquisition time for better patient comfort, lower examination costs, dose,
and fewer motion-induced artifacts. For many image-based tasks, increasing the
apparent resolution in the perpendicular plane to produce multi-planar
reformats or 3D images is commonly used. Single image super-resolution (SR) is
a promising technique to provide HR images based on unsupervised learning to
increase resolution of a 2D image, but there are few reports on 3D SR. Further,
perceptual loss is proposed in the literature to better capture the textual
details and edges than using pixel-wise loss functions, by comparing the
semantic distances in the high-dimensional feature space of a pre-trained 2D
network (e.g., VGG). However, it is not clear how one should generalize it to
3D medical images, and the attendant implications are still unclear. In this
paper, we propose a framework called SOUP-GAN: Super-resolution Optimized Using
Perceptual-tuned Generative Adversarial Network (GAN), in order to produce
thinner slice (e.g., high resolution in the 'Z' plane) medical images with
anti-aliasing and deblurring. The proposed method outperforms other
conventional resolution-enhancement methods and previous SR work on medical
images upon both qualitative and quantitative comparisons. Specifically, we
examine the model in terms of its generalization for various SR ratios and
imaging modalities. By addressing those limitations, our model shows promise as
a novel 3D SR interpolation technique, providing potential applications in both
clinical and research settings.
",Kuan Zhang; Haoji Hu; Kenneth Philbrick; Gian Marco Conte; Joseph D. Sobek; Pouria Rouzrokh; Bradley J. Erickson,http://arxiv.org/abs/2106.02599v1,10.48550/arXiv.2106.02599
http://arxiv.org/abs/2111.03260v1,2021,"Remote Sensing Image Super-resolution and Object Detection: Benchmark
  and State of the Art","  For the past two decades, there have been significant efforts to develop
methods for object detection in Remote Sensing (RS) images. In most cases, the
datasets for small object detection in remote sensing images are inadequate.
Many researchers used scene classification datasets for object detection, which
has its limitations; for example, the large-sized objects outnumber the small
objects in object categories. Thus, they lack diversity; this further affects
the detection performance of small object detectors in RS images. This paper
reviews current datasets and object detection methods (deep learning-based) for
remote sensing images. We also propose a large-scale, publicly available
benchmark Remote Sensing Super-resolution Object Detection (RSSOD) dataset. The
RSSOD dataset consists of 1,759 hand-annotated images with 22,091 instances of
very high resolution (VHR) images with a spatial resolution of ~0.05 m. There
are five classes with varying frequencies of labels per class. The image
patches are extracted from satellite images, including real image distortions
such as tangential scale distortion and skew distortion. We also propose a
novel Multi-class Cyclic super-resolution Generative adversarial network with
Residual feature aggregation (MCGR) and auxiliary YOLOv5 detector to benchmark
image super-resolution-based object detection and compare with the existing
state-of-the-art methods based on image super-resolution (SR). The proposed
MCGR achieved state-of-the-art performance for image SR with an improvement of
1.2dB PSNR compared to the current state-of-the-art NLSN method. MCGR achieved
best object detection mAPs of 0.758, 0.881, 0.841, and 0.983, respectively, for
five-class, four-class, two-class, and single classes, respectively surpassing
the performance of the state-of-the-art object detectors YOLOv5, EfficientDet,
Faster RCNN, SSD, and RetinaNet.
",Yi Wang; Syed Muhammad Arsalan Bashir; Mahrukh Khan; Qudrat Ullah; Rui Wang; Yilin Song; Zhe Guo; Yilong Niu,http://arxiv.org/abs/2111.03260v1,10.1016/j.eswa.2022.116793
http://arxiv.org/abs/1511.05666v4,2015,Super-Resolution with Deep Convolutional Sufficient Statistics,"  Inverse problems in image and audio, and super-resolution in particular, can
be seen as high-dimensional structured prediction problems, where the goal is
to characterize the conditional distribution of a high-resolution output given
its low-resolution corrupted observation. When the scaling ratio is small,
point estimates achieve impressive performance, but soon they suffer from the
regression-to-the-mean problem, result of their inability to capture the
multi-modality of this conditional distribution. Modeling high-dimensional
image and audio distributions is a hard task, requiring both the ability to
model complex geometrical structures and textured regions. In this paper, we
propose to use as conditional model a Gibbs distribution, where its sufficient
statistics are given by deep convolutional neural networks. The features
computed by the network are stable to local deformation, and have reduced
variance when the input is a stationary texture. These properties imply that
the resulting sufficient statistics minimize the uncertainty of the target
signals given the degraded observations, while being highly informative. The
filters of the CNN are initialized by multiscale complex wavelets, and then we
propose an algorithm to fine-tune them by estimating the gradient of the
conditional log-likelihood, which bears some similarities with Generative
Adversarial Networks. We evaluate experimentally the proposed approach in the
image super-resolution task, but the approach is general and could be used in
other challenging ill-posed problems such as audio bandwidth extension.
",Joan Bruna; Pablo Sprechmann; Yann LeCun,http://arxiv.org/abs/1511.05666v4,10.48550/arXiv.1511.05666
http://arxiv.org/abs/1811.00367v1,2018,Bi-GANs-ST for Perceptual Image Super-resolution,"  Image quality measurement is a critical problem for image super-resolution
(SR) algorithms. Usually, they are evaluated by some well-known objective
metrics, e.g., PSNR and SSIM, but these indices cannot provide suitable results
in accordance with the perception of human being. Recently, a more reasonable
perception measurement has been proposed in [1], which is also adopted by the
PIRM-SR 2018 challenge. In this paper, motivated by [1], we aim to generate a
high-quality SR result which balances between the two indices, i.e., the
perception index and root-mean-square error (RMSE). To do so, we design a new
deep SR framework, dubbed Bi-GANs-ST, by integrating two complementary
generative adversarial networks (GAN) branches. One is memory residual SRGAN
(MR-SRGAN), which emphasizes on improving the objective performance, such as
reducing the RMSE. The other is weight perception SRGAN (WP-SRGAN), which
obtains the result that favors better subjective perception via a two-stage
adversarial training mechanism. Then, to produce final result with excellent
perception scores and RMSE, we use soft-thresholding method to merge the
results generated by the two GANs. Our method performs well on the perceptual
image super-resolution task of the PIRM 2018 challenge. Experimental results on
five benchmarks show that our proposal achieves highly competent performance
compared with other state-of-the-art methods.
",Xiaotong Luo; Rong Chen; Yuan Xie; Yanyun Qu; Cuihua Li,http://arxiv.org/abs/1811.00367v1,10.48550/arXiv.1811.00367
http://arxiv.org/abs/1812.04821v4,2018,Efficient Super Resolution For Large-Scale Images Using Attentional GAN,"  Single Image Super Resolution (SISR) is a well-researched problem with broad
commercial relevance. However, most of the SISR literature focuses on
small-size images under 500px, whereas business needs can mandate the
generation of very high resolution images. At Expedia Group, we were tasked
with generating images of at least 2000px for display on the website, four
times greater than the sizes typically reported in the literature. This
requirement poses a challenge that state-of-the-art models, validated on small
images, have not been proven to handle. In this paper, we investigate solutions
to the problem of generating high-quality images for large-scale super
resolution in a commercial setting. We find that training a generative
adversarial network (GAN) with attention from scratch using a large-scale
lodging image data set generates images with high PSNR and SSIM scores. We
describe a novel attentional SISR model for large-scale images, A-SRGAN, that
uses a Flexible Self Attention layer to enable processing of large-scale
images. We also describe a distributed algorithm which speeds up training by
around a factor of five.
",Harsh Nilesh Pathak; Xinxin Li; Shervin Minaee; Brooke Cowan,http://arxiv.org/abs/1812.04821v4,10.48550/arXiv.1812.04821
http://arxiv.org/abs/1907.10399v2,2019,"Progressive Perception-Oriented Network for Single Image
  Super-Resolution","  Recently, it has been demonstrated that deep neural networks can
significantly improve the performance of single image super-resolution (SISR).
Numerous studies have concentrated on raising the quantitative quality of
super-resolved (SR) images. However, these methods that target PSNR
maximization usually produce blurred images at large upscaling factor. The
introduction of generative adversarial networks (GANs) can mitigate this issue
and show impressive results with synthetic high-frequency textures.
Nevertheless, these GAN-based approaches always have a tendency to add fake
textures and even artifacts to make the SR image of visually higher-resolution.
In this paper, we propose a novel perceptual image super-resolution method that
progressively generates visually high-quality results by constructing a
stage-wise network. Specifically, the first phase concentrates on minimizing
pixel-wise error, and the second stage utilizes the features extracted by the
previous stage to pursue results with better structural retention. The final
stage employs fine structure features distilled by the second phase to produce
more realistic results. In this way, we can maintain the pixel, and structural
level information in the perceptual image as much as possible. It is useful to
note that the proposed method can build three types of images in a feed-forward
process. Also, we explore a new generator that adopts multi-scale hierarchical
features fusion. Extensive experiments on benchmark datasets show that our
approach is superior to the state-of-the-art methods. Code is available at
https://github.com/Zheng222/PPON.
",Zheng Hui; Jie Li; Xinbo Gao; Xiumei Wang,http://arxiv.org/abs/1907.10399v2,10.48550/arXiv.1907.10399
http://arxiv.org/abs/1911.01045v1,2019,"FCSR-GAN: Joint Face Completion and Super-resolution via Multi-task
  Learning","  Combined variations containing low-resolution and occlusion often present in
face images in the wild, e.g., under the scenario of video surveillance. While
most of the existing face image recovery approaches can handle only one type of
variation per model, in this work, we propose a deep generative adversarial
network (FCSR-GAN) for performing joint face completion and face
super-resolution via multi-task learning. The generator of FCSR-GAN aims to
recover a high-resolution face image without occlusion given an input
low-resolution face image with occlusion. The discriminator of FCSR-GAN uses a
set of carefully designed losses (an adversarial loss, a perceptual loss, a
pixel loss, a smooth loss, a style loss, and a face prior loss) to assure the
high quality of the recovered high-resolution face images without occlusion.
The whole network of FCSR-GAN can be trained end-to-end using our two-stage
training strategy. Experimental results on the public-domain CelebA and Helen
databases show that the proposed approach outperforms the state-of-the-art
methods in jointly performing face super-resolution (up to 8 $\times$) and face
completion, and shows good generalization ability in cross-database testing.
Our FCSR-GAN is also useful for improving face identification performance when
there are low-resolution and occlusion in face images.
",Jiancheng Cai; Hu Han; Shiguang Shan; Xilin Chen,http://arxiv.org/abs/1911.01045v1,10.48550/arXiv.1911.01045
http://arxiv.org/abs/1911.03558v3,2019,"Joint Demosaicing and Super-Resolution (JDSR): Network Design and
  Perceptual Optimization","  Image demosaicing and super-resolution are two important tasks in color
imaging pipeline. So far they have been mostly independently studied in the
open literature of deep learning; little is known about the potential benefit
of formulating a joint demosaicing and super-resolution (JDSR) problem. In this
paper, we propose an end-to-end optimization solution to the JDSR problem and
demonstrate its practical significance in computational imaging. Our technical
contributions are mainly two-fold. On network design, we have developed a
Residual-Dense Squeeze-and-Excitation Networks (RDSEN) supported by a
pre-demosaicing network (PDNet) as the pre-processing step. We address the
issue of spatio-spectral attention for color-filter-array (CFA) data and
discuss how to achieve better information flow by concatenating Residue-Dense
Squeeze-and-Excitation Blocks (RDSEBs) for JDSR. Experimental results have
shown that significant PSNR/SSIM gain can be achieved by RDSEN over previous
network architectures including state-of-the-art RCAN. On perceptual
optimization, we propose to leverage the latest ideas including relativistic
discriminator and pre-excitation perceptual loss function to further improve
the visual quality of textured regions in reconstructed images. Our extensive
experiment results have shown that Texture-enhanced Relativistic average
Generative Adversarial Network (TRaGAN) can produce both subjectively more
pleasant images and objectively lower perceptual distortion scores than
standard GAN for JDSR. Finally, we have verified the benefit of JDSR to
high-quality image reconstruction from real-world Bayer pattern data collected
by NASA Mars Curiosity.
",Xuan Xu; Yanfang Ye; Xin Li,http://arxiv.org/abs/1911.03558v3,10.48550/arXiv.1911.03558
http://arxiv.org/abs/2003.01217v2,2020,"MRI Super-Resolution with GAN and 3D Multi-Level DenseNet: Smaller,
  Faster, and Better","  High-resolution (HR) magnetic resonance imaging (MRI) provides detailed
anatomical information that is critical for diagnosis in the clinical
application. However, HR MRI typically comes at the cost of long scan time,
small spatial coverage, and low signal-to-noise ratio (SNR). Recent studies
showed that with a deep convolutional neural network (CNN), HR generic images
could be recovered from low-resolution (LR) inputs via single image
super-resolution (SISR) approaches. Additionally, previous works have shown
that a deep 3D CNN can generate high-quality SR MRIs by using learned image
priors. However, 3D CNN with deep structures, have a large number of parameters
and are computationally expensive. In this paper, we propose a novel 3D CNN
architecture, namely a multi-level densely connected super-resolution network
(mDCSRN), which is light-weight, fast and accurate. We also show that with the
generative adversarial network (GAN)-guided training, the mDCSRN-GAN provides
appealing sharp SR images with rich texture details that are highly comparable
with the referenced HR images. Our results from experiments on a large public
dataset with 1,113 subjects showed that this new architecture outperformed
other popular deep learning methods in recovering 4x resolution-downgraded
images in both quality and speed.
",Yuhua Chen; Anthony G. Christodoulou; Zhengwei Zhou; Feng Shi; Yibin Xie; Debiao Li,http://arxiv.org/abs/2003.01217v2,10.48550/arXiv.2003.01217
http://arxiv.org/abs/2003.13081v1,2020,Structure-Preserving Super Resolution with Gradient Guidance,"  Structures matter in single image super resolution (SISR). Recent studies
benefiting from generative adversarial network (GAN) have promoted the
development of SISR by recovering photo-realistic images. However, there are
always undesired structural distortions in the recovered images. In this paper,
we propose a structure-preserving super resolution method to alleviate the
above issue while maintaining the merits of GAN-based methods to generate
perceptual-pleasant details. Specifically, we exploit gradient maps of images
to guide the recovery in two aspects. On the one hand, we restore
high-resolution gradient maps by a gradient branch to provide additional
structure priors for the SR process. On the other hand, we propose a gradient
loss which imposes a second-order restriction on the super-resolved images.
Along with the previous image-space loss functions, the gradient-space
objectives help generative networks concentrate more on geometric structures.
Moreover, our method is model-agnostic, which can be potentially used for
off-the-shelf SR networks. Experimental results show that we achieve the best
PI and LPIPS performance and meanwhile comparable PSNR and SSIM compared with
state-of-the-art perceptual-driven SR methods. Visual results demonstrate our
superiority in restoring structures while generating natural SR images.
",Cheng Ma; Yongming Rao; Yean Cheng; Ce Chen; Jiwen Lu; Jie Zhou,http://arxiv.org/abs/2003.13081v1,10.48550/arXiv.2003.13081
http://arxiv.org/abs/2005.00953v1,2020,"Deep Generative Adversarial Residual Convolutional Networks for
  Real-World Super-Resolution","  Most current deep learning based single image super-resolution (SISR) methods
focus on designing deeper / wider models to learn the non-linear mapping
between low-resolution (LR) inputs and the high-resolution (HR) outputs from a
large number of paired (LR/HR) training data. They usually take as assumption
that the LR image is a bicubic down-sampled version of the HR image. However,
such degradation process is not available in real-world settings i.e. inherent
sensor noise, stochastic noise, compression artifacts, possible mismatch
between image degradation process and camera device. It reduces significantly
the performance of current SISR methods due to real-world image corruptions. To
address these problems, we propose a deep Super-Resolution Residual
Convolutional Generative Adversarial Network (SRResCGAN) to follow the
real-world degradation settings by adversarial training the model with
pixel-wise supervision in the HR domain from its generated LR counterpart. The
proposed network exploits the residual learning by minimizing the energy-based
objective function with powerful image regularization and convex optimization
techniques. We demonstrate our proposed approach in quantitative and
qualitative experiments that generalize robustly to real input and it is easy
to deploy for other down-scaling operators and mobile/embedded devices.
",Rao Muhammad Umer; Gian Luca Foresti; Christian Micheloni,http://arxiv.org/abs/2005.00953v1,10.48550/arXiv.2005.00953
http://arxiv.org/abs/2012.05959v1,2020,Super-resolution Guided Pore Detection for Fingerprint Recognition,"  Performance of fingerprint recognition algorithms substantially rely on fine
features extracted from fingerprints. Apart from minutiae and ridge patterns,
pore features have proven to be usable for fingerprint recognition. Although
features from minutiae and ridge patterns are quite attainable from
low-resolution images, using pore features is practical only if the fingerprint
image is of high resolution which necessitates a model that enhances the image
quality of the conventional 500 ppi legacy fingerprints preserving the fine
details. To find a solution for recovering pore information from low-resolution
fingerprints, we adopt a joint learning-based approach that combines both
super-resolution and pore detection networks. Our modified single image
Super-Resolution Generative Adversarial Network (SRGAN) framework helps to
reliably reconstruct high-resolution fingerprint samples from low-resolution
ones assisting the pore detection network to identify pores with a high
accuracy. The network jointly learns a distinctive feature representation from
a real low-resolution fingerprint sample and successfully synthesizes a
high-resolution sample from it. To add discriminative information and
uniqueness for all the subjects, we have integrated features extracted from a
deep fingerprint verifier with the SRGAN quality discriminator. We also add
ridge reconstruction loss, utilizing ridge patterns to make the best use of
extracted features. Our proposed method solves the recognition problem by
improving the quality of fingerprint images. High recognition accuracy of the
synthesized samples that is close to the accuracy achieved using the original
high-resolution images validate the effectiveness of our proposed model.
",Syeda Nyma Ferdous; Ali Dabouei; Jeremy Dawson; Nasser M Nasrabadi,http://arxiv.org/abs/2012.05959v1,10.48550/arXiv.2012.05959
http://arxiv.org/abs/2012.14142v1,2020,"Perception Consistency Ultrasound Image Super-resolution via
  Self-supervised CycleGAN","  Due to the limitations of sensors, the transmission medium and the intrinsic
properties of ultrasound, the quality of ultrasound imaging is always not
ideal, especially its low spatial resolution. To remedy this situation, deep
learning networks have been recently developed for ultrasound image
super-resolution (SR) because of the powerful approximation capability.
However, most current supervised SR methods are not suitable for ultrasound
medical images because the medical image samples are always rare, and usually,
there are no low-resolution (LR) and high-resolution (HR) training pairs in
reality. In this work, based on self-supervision and cycle generative
adversarial network (CycleGAN), we propose a new perception consistency
ultrasound image super-resolution (SR) method, which only requires the LR
ultrasound data and can ensure the re-degenerated image of the generated SR one
to be consistent with the original LR image, and vice versa. We first generate
the HR fathers and the LR sons of the test ultrasound LR image through image
enhancement, and then make full use of the cycle loss of LR-SR-LR and HR-LR-SR
and the adversarial characteristics of the discriminator to promote the
generator to produce better perceptually consistent SR results. The evaluation
of PSNR/IFC/SSIM, inference efficiency and visual effects under the benchmark
CCA-US and CCA-US datasets illustrate our proposed approach is effective and
superior to other state-of-the-art methods.
",Heng Liu; Jianyong Liu; Tao Tao; Shudong Hou; Jungong Han,http://arxiv.org/abs/2012.14142v1,10.48550/arXiv.2012.14142
http://arxiv.org/abs/2105.10738v1,2021,MIASSR: An Approach for Medical Image Arbitrary Scale Super-Resolution,"  Single image super-resolution (SISR) aims to obtain a high-resolution output
from one low-resolution image. Currently, deep learning-based SISR approaches
have been widely discussed in medical image processing, because of their
potential to achieve high-quality, high spatial resolution images without the
cost of additional scans. However, most existing methods are designed for
scale-specific SR tasks and are unable to generalise over magnification scales.
In this paper, we propose an approach for medical image arbitrary-scale
super-resolution (MIASSR), in which we couple meta-learning with generative
adversarial networks (GANs) to super-resolve medical images at any scale of
magnification in (1, 4]. Compared to state-of-the-art SISR algorithms on
single-modal magnetic resonance (MR) brain images (OASIS-brains) and
multi-modal MR brain images (BraTS), MIASSR achieves comparable fidelity
performance and the best perceptual quality with the smallest model size. We
also employ transfer learning to enable MIASSR to tackle SR tasks of new
medical modalities, such as cardiac MR images (ACDC) and chest computed
tomography images (COVID-CT). The source code of our work is also public. Thus,
MIASSR has the potential to become a new foundational pre-/post-processing step
in clinical image analysis tasks such as reconstruction, image quality
enhancement, and segmentation.
",Jin Zhu; Chuan Tan; Junwei Yang; Guang Yang; Pietro Lio',http://arxiv.org/abs/2105.10738v1,10.48550/arXiv.2105.10738
http://arxiv.org/abs/2109.12530v1,2021,Structure-Preserving Image Super-Resolution,"  Structures matter in single image super-resolution (SISR). Benefiting from
generative adversarial networks (GANs), recent studies have promoted the
development of SISR by recovering photo-realistic images. However, there are
still undesired structural distortions in the recovered images. In this paper,
we propose a structure-preserving super-resolution (SPSR) method to alleviate
the above issue while maintaining the merits of GAN-based methods to generate
perceptual-pleasant details. Firstly, we propose SPSR with gradient guidance
(SPSR-G) by exploiting gradient maps of images to guide the recovery in two
aspects. On the one hand, we restore high-resolution gradient maps by a
gradient branch to provide additional structure priors for the SR process. On
the other hand, we propose a gradient loss to impose a second-order restriction
on the super-resolved images, which helps generative networks concentrate more
on geometric structures. Secondly, since the gradient maps are handcrafted and
may only be able to capture limited aspects of structural information, we
further extend SPSR-G by introducing a learnable neural structure extractor
(NSE) to unearth richer local structures and provide stronger supervision for
SR. We propose two self-supervised structure learning methods, contrastive
prediction and solving jigsaw puzzles, to train the NSEs. Our methods are
model-agnostic, which can be potentially used for off-the-shelf SR networks.
Experimental results on five benchmark datasets show that the proposed methods
outperform state-of-the-art perceptual-driven SR methods under LPIPS, PSNR, and
SSIM metrics. Visual results demonstrate the superiority of our methods in
restoring structures while generating natural SR images. Code is available at
https://github.com/Maclory/SPSR.
",Cheng Ma; Yongming Rao; Jiwen Lu; Jie Zhou,http://arxiv.org/abs/2109.12530v1,10.1109/TPAMI.2021.3114428
http://arxiv.org/abs/2202.11701v2,2022,Super-resolution GANs of randomly-seeded fields,"  Reconstruction of field quantities from sparse measurements is a problem
arising in a broad spectrum of applications. This task is particularly
challenging when the mapping between sparse measurements and field quantities
is performed in an unsupervised manner. Further complexity is added for moving
sensors and/or random on-off status. Under such conditions, the most
straightforward solution is to interpolate the scattered data onto a regular
grid. However, the spatial resolution achieved with this approach is ultimately
limited by the mean spacing between the sparse measurements. In this work, we
propose a super-resolution generative adversarial network (GAN) framework to
estimate field quantities from random sparse sensors without needing any
full-field high-resolution training. The algorithm exploits random sampling to
provide incomplete views of the {high-resolution} underlying distributions. It
is hereby referred to as RAndomly-SEEDed super-resolution GAN (RaSeedGAN). The
proposed technique is tested on synthetic databases of fluid flow simulations,
ocean surface temperature distributions measurements, and particle image
velocimetry data of a zero-pressure-gradient turbulent boundary layer. The
results show excellent performance even in cases with high sparsity or with
levels of noise. To our knowledge, this is the first GAN algorithm for
full-field high-resolution estimation from randomly-seeded fields with no need
of full-field high-resolution representations.
",Alejandro Güemes; Carlos Sanmiguel Vila; Stefano Discetti,http://arxiv.org/abs/2202.11701v2,10.48550/arXiv.2202.11701
http://arxiv.org/abs/2207.10181v1,2022,"Flow-based Visual Quality Enhancer for Super-resolution Magnetic
  Resonance Spectroscopic Imaging","  Magnetic Resonance Spectroscopic Imaging (MRSI) is an essential tool for
quantifying metabolites in the body, but the low spatial resolution limits its
clinical applications. Deep learning-based super-resolution methods provided
promising results for improving the spatial resolution of MRSI, but the
super-resolved images are often blurry compared to the experimentally-acquired
high-resolution images. Attempts have been made with the generative adversarial
networks to improve the image visual quality. In this work, we consider another
type of generative model, the flow-based model, of which the training is more
stable and interpretable compared to the adversarial networks. Specifically, we
propose a flow-based enhancer network to improve the visual quality of
super-resolution MRSI. Different from previous flow-based models, our enhancer
network incorporates anatomical information from additional image modalities
(MRI) and uses a learnable base distribution. In addition, we impose a guide
loss and a data-consistency loss to encourage the network to generate images
with high visual quality while maintaining high fidelity. Experiments on a
1H-MRSI dataset acquired from 25 high-grade glioma patients indicate that our
enhancer network outperforms the adversarial networks and the baseline
flow-based methods. Our method also allows visual quality adjustment and
uncertainty estimation.
",Siyuan Dong; Gilbert Hangel; Eric Z. Chen; Shanhui Sun; Wolfgang Bogner; Georg Widhalm; Chenyu You; John A. Onofrey; Robin de Graaf; James S. Duncan,http://arxiv.org/abs/2207.10181v1,10.48550/arXiv.2207.10181
http://arxiv.org/abs/2209.01325v1,2022,Quasi-supervised Learning for Super-resolution PET,"  Low resolution of positron emission tomography (PET) limits its diagnostic
performance. Deep learning has been successfully applied to achieve
super-resolution PET. However, commonly used supervised learning methods in
this context require many pairs of low- and high-resolution (LR and HR) PET
images. Although unsupervised learning utilizes unpaired images, the results
are not as good as that obtained with supervised deep learning. In this paper,
we propose a quasi-supervised learning method, which is a new type of
weakly-supervised learning methods, to recover HR PET images from LR
counterparts by leveraging similarity between unpaired LR and HR image patches.
Specifically, LR image patches are taken from a patient as inputs, while the
most similar HR patches from other patients are found as labels. The similarity
between the matched HR and LR patches serves as a prior for network
construction. Our proposed method can be implemented by designing a new network
or modifying an existing network. As an example in this study, we have modified
the cycle-consistent generative adversarial network (CycleGAN) for
super-resolution PET. Our numerical and experimental results qualitatively and
quantitatively show the merits of our method relative to the state-ofthe-art
methods. The code is publicly available at
https://github.com/PigYang-ops/CycleGAN-QSDL.
",Guangtong Yang; Chen Li; Yudong Yao; Ge Wang; Yueyang Teng,http://arxiv.org/abs/2209.01325v1,10.48550/arXiv.2209.01325
http://arxiv.org/abs/1703.10155v2,2017,CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training,"  We present variational generative adversarial networks, a general learning
framework that combines a variational auto-encoder with a generative
adversarial network, for synthesizing images in fine-grained categories, such
as faces of a specific person or objects in a category. Our approach models an
image as a composition of label and latent attributes in a probabilistic model.
By varying the fine-grained category label fed into the resulting generative
model, we can generate images in a specific category with randomly drawn values
on a latent attribute vector. Our approach has two novel aspects. First, we
adopt a cross entropy loss for the discriminative and classifier network, but a
mean discrepancy objective for the generative network. This kind of asymmetric
loss function makes the GAN training more stable. Second, we adopt an encoder
network to learn the relationship between the latent space and the real image
space, and use pairwise feature matching to keep the structure of generated
images. We experiment with natural images of faces, flowers, and birds, and
demonstrate that the proposed models are capable of generating realistic and
diverse samples with fine-grained category labels. We further show that our
models can be applied to other tasks, such as image inpainting,
super-resolution, and data augmentation for training better face recognition
models.
",Jianmin Bao; Dong Chen; Fang Wen; Houqiang Li; Gang Hua,http://arxiv.org/abs/1703.10155v2,10.48550/arXiv.1703.10155
http://arxiv.org/abs/1811.09134v1,2018,"IEGAN: Multi-purpose Perceptual Quality Image Enhancement Using
  Generative Adversarial Network","  Despite the breakthroughs in quality of image enhancement, an end-to-end
solution for simultaneous recovery of the finer texture details and sharpness
for degraded images with low resolution is still unsolved. Some existing
approaches focus on minimizing the pixel-wise reconstruction error which
results in a high peak signal-to-noise ratio. The enhanced images fail to
provide high-frequency details and are perceptually unsatisfying, i.e., they
fail to match the quality expected in a photo-realistic image. In this paper,
we present Image Enhancement Generative Adversarial Network (IEGAN), a
versatile framework capable of inferring photo-realistic natural images for
both artifact removal and super-resolution simultaneously. Moreover, we propose
a new loss function consisting of a combination of reconstruction loss, feature
loss and an edge loss counterpart. The feature loss helps to push the output
image to the natural image manifold and the edge loss preserves the sharpness
of the output image. The reconstruction loss provides low-level semantic
information to the generator regarding the quality of the generated images
compared to the original. Our approach has been experimentally proven to
recover photo-realistic textures from heavily compressed low-resolution images
on public benchmarks and our proposed high-resolution World100 dataset.
",Soumya Shubhra Ghosh; Yang Hua; Sankha Subhra Mukherjee; Neil Robertson,http://arxiv.org/abs/1811.09134v1,10.1109/WACV.2019.00070
http://arxiv.org/abs/1903.02225v1,2019,"DepthwiseGANs: Fast Training Generative Adversarial Networks for
  Realistic Image Synthesis","  Recent work has shown significant progress in the direction of synthetic data
generation using Generative Adversarial Networks (GANs). GANs have been applied
in many fields of computer vision including text-to-image conversion, domain
transfer, super-resolution, and image-to-video applications. In computer
vision, traditional GANs are based on deep convolutional neural networks.
However, deep convolutional neural networks can require extensive computational
resources because they are based on multiple operations performed by
convolutional layers, which can consist of millions of trainable parameters.
Training a GAN model can be difficult and it takes a significant amount of time
to reach an equilibrium point. In this paper, we investigate the use of
depthwise separable convolutions to reduce training time while maintaining data
generation performance. Our results show that a DepthwiseGAN architecture can
generate realistic images in shorter training periods when compared to a
StarGan architecture, but that model capacity still plays a significant role in
generative modelling. In addition, we show that depthwise separable
convolutions perform best when only applied to the generator. For quality
evaluation of generated images, we use the Fr\'echet Inception Distance (FID),
which compares the similarity between the generated image distribution and that
of the training dataset.
",Mkhuseli Ngxande; Jules-Raymond Tapamo; Michael Burke,http://arxiv.org/abs/1903.02225v1,10.48550/arXiv.1903.02225
http://arxiv.org/abs/1905.00307v2,2019,"3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and
  Translation","  Over the past few years, Generative Adversarial Networks (GANs) have garnered
increased interest among researchers in Computer Vision, with applications
including, but not limited to, image generation, translation, imputation, and
super-resolution. Nevertheless, no GAN-based method has been proposed in the
literature that can successfully represent, generate or translate 3D facial
shapes (meshes). This can be primarily attributed to two facts, namely that (a)
publicly available 3D face databases are scarce as well as limited in terms of
sample size and variability (e.g., few subjects, little diversity in race and
gender), and (b) mesh convolutions for deep networks present several challenges
that are not entirely tackled in the literature, leading to operator
approximations and model instability, often failing to preserve high-frequency
components of the distribution. As a result, linear methods such as Principal
Component Analysis (PCA) have been mainly utilized towards 3D shape analysis,
despite being unable to capture non-linearities and high frequency details of
the 3D face - such as eyelid and lip variations. In this work, we present
3DFaceGAN, the first GAN tailored towards modeling the distribution of 3D
facial surfaces, while retaining the high frequency details of 3D face shapes.
We conduct an extensive series of both qualitative and quantitative
experiments, where the merits of 3DFaceGAN are clearly demonstrated against
other, state-of-the-art methods in tasks such as 3D shape representation,
generation, and translation.
",Stylianos Moschoglou; Stylianos Ploumpis; Mihalis Nicolaou; Athanasios Papaioannou; Stefanos Zafeiriou,http://arxiv.org/abs/1905.00307v2,10.48550/arXiv.1905.00307
http://arxiv.org/abs/1909.10833v1,2019,Enhancing Traffic Scene Predictions with Generative Adversarial Networks,"  We present a new two-stage pipeline for predicting frames of traffic scenes
where relevant objects can still reliably be detected. Using a recent video
prediction network, we first generate a sequence of future frames based on past
frames. A second network then enhances these frames in order to make them
appear more realistic. This ensures the quality of the predicted frames to be
sufficient to enable accurate detection of objects, which is especially
important for autonomously driving cars. To verify this two-stage approach, we
conducted experiments on the Cityscapes dataset. For enhancing, we trained two
image-to-image translation methods based on generative adversarial networks,
one for blind motion deblurring and one for image super-resolution. All
resulting predictions were quantitatively evaluated using both traditional
metrics and a state-of-the-art object detection network showing that the
enhanced frames appear qualitatively improved. While the traditional image
comparison metrics, i.e., MSE, PSNR, and SSIM, failed to confirm this visual
impression, the object detection evaluation resembles it well. The best
performing prediction-enhancement pipeline is able to increase the average
precision values for detecting cars by about 9% for each prediction step,
compared to the non-enhanced predictions.
",Peter König; Sandra Aigner; Marco Körner,http://arxiv.org/abs/1909.10833v1,10.48550/arXiv.1909.10833
http://arxiv.org/abs/1909.12116v4,2019,"Optimal Transport driven CycleGAN for Unsupervised Learning in Inverse
  Problems","  To improve the performance of classical generative adversarial network (GAN),
Wasserstein generative adversarial networks (W-GAN) was developed as a
Kantorovich dual formulation of the optimal transport (OT) problem using
Wasserstein-1 distance. However, it was not clear how cycleGAN-type generative
models can be derived from the optimal transport theory. Here we show that a
novel cycleGAN architecture can be derived as a Kantorovich dual OT formulation
if a penalized least square (PLS) cost with deep learning-based inverse path
penalty is used as a transportation cost. One of the most important advantages
of this formulation is that depending on the knowledge of the forward problem,
distinct variations of cycleGAN architecture can be derived: for example, one
with two pairs of generators and discriminators, and the other with only a
single pair of generator and discriminator. Even for the two generator cases,
we show that the structural knowledge of the forward operator can lead to a
simpler generator architecture which significantly simplifies the neural
network training. The new cycleGAN formulation, what we call the OT-cycleGAN,
have been applied for various biomedical imaging problems, such as accelerated
magnetic resonance imaging (MRI), super-resolution microscopy, and low-dose
x-ray computed tomography (CT). Experimental results confirm the efficacy and
flexibility of the theory.
",Byeongsu Sim; Gyutaek Oh; Jeongsol Kim; Chanyong Jung; Jong Chul Ye,http://arxiv.org/abs/1909.12116v4,10.48550/arXiv.1909.12116
http://arxiv.org/abs/1911.11380v1,2019,"Using Physics-Informed Super-Resolution Generative Adversarial Networks
  for Subgrid Modeling in Turbulent Reactive Flows","  Turbulence is still one of the main challenges for accurately predicting
reactive flows. Therefore, the development of new turbulence closures which can
be applied to combustion problems is essential. Data-driven modeling has become
very popular in many fields over the last years as large, often extensively
labeled, datasets became available and training of large neural networks became
possible on GPUs speeding up the learning process tremendously. However, the
successful application of deep neural networks in fluid dynamics, for example
for subgrid modeling in the context of large-eddy simulations (LESs), is still
challenging. Reasons for this are the large amount of degrees of freedom in
realistic flows, the high requirements with respect to accuracy and error
robustness, as well as open questions, such as the generalization capability of
trained neural networks in such high-dimensional, physics-constrained
scenarios. This work presents a novel subgrid modeling approach based on a
generative adversarial network (GAN), which is trained with unsupervised deep
learning (DL) using adversarial and physics-informed losses. A two-step
training method is used to improve the generalization capability, especially
extrapolation, of the network. The novel approach gives good results in a
priori as well as a posteriori tests with decaying turbulence including
turbulent mixing. The applicability of the network in complex combustion
scenarios is furthermore discussed by employing it to a reactive LES of the
Spray A case defined by the Engine Combustion Network (ECN).
",Mathis Bode; Michael Gauding; Zeyu Lian; Dominik Denker; Marco Davidovic; Konstantin Kleinheinz; Jenia Jitsev; Heinz Pitsch,http://arxiv.org/abs/1911.11380v1,10.48550/arXiv.1911.11380
http://arxiv.org/abs/2004.03150v1,2020,"Deep Attentive Generative Adversarial Network for Photo-Realistic Image
  De-Quantization","  Most of current display devices are with eight or higher bit-depth. However,
the quality of most multimedia tools cannot achieve this bit-depth standard for
the generating images. De-quantization can improve the visual quality of low
bit-depth image to display on high bit-depth screen. This paper proposes DAGAN
algorithm to perform super-resolution on image intensity resolution, which is
orthogonal to the spatial resolution, realizing photo-realistic de-quantization
via an end-to-end learning pattern. Until now, this is the first attempt to
apply Generative Adversarial Network (GAN) framework for image de-quantization.
Specifically, we propose the Dense Residual Self-attention (DenseResAtt)
module, which is consisted of dense residual blocks armed with self-attention
mechanism, to pay more attention on high-frequency information. Moreover, the
series connection of sequential DenseResAtt modules forms deep attentive
network with superior discriminative learning ability in image de-quantization,
modeling representative feature maps to recover as much useful information as
possible. In addition, due to the adversarial learning framework can reliably
produce high quality natural images, the specified content loss as well as the
adversarial loss are back-propagated to optimize the training of model. Above
all, DAGAN is able to generate the photo-realistic high bit-depth image without
banding artifacts. Experiment results on several public benchmarks prove that
the DAGAN algorithm possesses ability to achieve excellent visual effect and
satisfied quantitative performance.
",Yang Zhang; Changhui Hu; Xiaobo Lu,http://arxiv.org/abs/2004.03150v1,10.48550/arXiv.2004.03150
http://arxiv.org/abs/2006.08198v2,2020,AutoGAN-Distiller: Searching to Compress Generative Adversarial Networks,"  The compression of Generative Adversarial Networks (GANs) has lately drawn
attention, due to the increasing demand for deploying GANs into mobile devices
for numerous applications such as image translation, enhancement and editing.
However, compared to the substantial efforts to compressing other deep models,
the research on compressing GANs (usually the generators) remains at its
infancy stage. Existing GAN compression algorithms are limited to handling
specific GAN architectures and losses. Inspired by the recent success of AutoML
in deep compression, we introduce AutoML to GAN compression and develop an
AutoGAN-Distiller (AGD) framework. Starting with a specifically designed
efficient search space, AGD performs an end-to-end discovery for new efficient
generators, given the target computational resource constraints. The search is
guided by the original GAN model via knowledge distillation, therefore
fulfilling the compression. AGD is fully automatic, standalone (i.e., needing
no trained discriminators), and generically applicable to various GAN models.
We evaluate AGD in two representative GAN tasks: image translation and super
resolution. Without bells and whistles, AGD yields remarkably lightweight yet
more competitive compressed models, that largely outperform existing
alternatives. Our codes and pretrained models are available at
https://github.com/TAMU-VITA/AGD.
",Yonggan Fu; Wuyang Chen; Haotao Wang; Haoran Li; Yingyan Lin; Zhangyang Wang,http://arxiv.org/abs/2006.08198v2,10.48550/arXiv.2006.08198
http://arxiv.org/abs/2010.01473v3,2020,Spatial Frequency Bias in Convolutional Generative Adversarial Networks,"  As the success of Generative Adversarial Networks (GANs) on natural images
quickly propels them into various real-life applications across different
domains, it becomes more and more important to clearly understand their
limitations. Specifically, understanding GANs' capability across the full
spectrum of spatial frequencies, i.e. beyond the low-frequency dominant
spectrum of natural images, is critical for assessing the reliability of GAN
generated data in any detail-sensitive application (e.g. denoising, filling and
super-resolution in medical and satellite images). In this paper, we show that
the ability of convolutional GANs to learn a distribution is significantly
affected by the spatial frequency of the underlying carrier signal, that is,
GANs have a bias against learning high spatial frequencies. Crucially, we show
that this bias is not merely a result of the scarcity of high frequencies in
natural images, rather, it is a systemic bias hindering the learning of high
frequencies regardless of their prominence in a dataset. Furthermore, we
explain why large-scale GANs' ability to generate fine details on natural
images does not exclude them from the adverse effects of this bias. Finally, we
propose a method for manipulating this bias with minimal computational
overhead. This method can be used to explicitly direct computational resources
towards any specific spatial frequency of interest in a dataset, extending the
flexibility of GANs.
",Mahyar Khayatkhoei; Ahmed Elgammal,http://arxiv.org/abs/2010.01473v3,10.48550/arXiv.2010.01473
http://arxiv.org/abs/2111.01105v1,2021,"FREGAN : an application of generative adversarial networks in enhancing
  the frame rate of videos","  A digital video is a collection of individual frames, while streaming the
video the scene utilized the time slice for each frame. High refresh rate and
high frame rate is the demand of all high technology applications. The action
tracking in videos becomes easier and motion becomes smoother in gaming
applications due to the high refresh rate. It provides a faster response
because of less time in between each frame that is displayed on the screen.
FREGAN (Frame Rate Enhancement Generative Adversarial Network) model has been
proposed, which predicts future frames of a video sequence based on a sequence
of past frames. In this paper, we investigated the GAN model and proposed
FREGAN for the enhancement of frame rate in videos. We have utilized Huber loss
as a loss function in the proposed FREGAN. It provided excellent results in
super-resolution and we have tried to reciprocate that performance in the
application of frame rate enhancement. We have validated the effectiveness of
the proposed model on the standard datasets (UCF101 and RFree500). The
experimental outcomes illustrate that the proposed model has a Peak
signal-to-noise ratio (PSNR) of 34.94 and a Structural Similarity Index (SSIM)
of 0.95.
",Rishik Mishra; Neeraj Gupta; Nitya Shukla,http://arxiv.org/abs/2111.01105v1,10.48550/arXiv.2111.01105
http://arxiv.org/abs/2112.13865v1,2021,"Astronomical Image Colorization and upscaling with Generative
  Adversarial Networks","  Automatic colorization of images without human intervention has been a
subject of interest in the machine learning community for a brief period of
time. Assigning color to an image is a highly ill-posed problem because of its
innate nature of possessing very high degrees of freedom; given an image, there
is often no single color-combination that is correct. Besides colorization,
another problem in reconstruction of images is Single Image Super Resolution,
which aims at transforming low resolution images to a higher resolution. This
research aims to provide an automated approach for the problem by focusing on a
very specific domain of images, namely astronomical images, and process them
using Generative Adversarial Networks (GANs). We explore the usage of various
models in two different color spaces, RGB and L*a*b. We use transferred
learning owing to a small data set, using pre-trained ResNet-18 as a backbone,
i.e. encoder for the U-net and fine-tune it further. The model produces
visually appealing images which hallucinate high resolution, colorized data in
these results which does not exist in the original image. We present our
results by evaluating the GANs quantitatively using distance metrics such as L1
distance and L2 distance in each of the color spaces across all channels to
provide a comparative analysis. We use Frechet inception distance (FID) to
compare the distribution of the generated images with the distribution of the
real image to assess the model's performance.
",Shreyas Kalvankar; Hrushikesh Pandit; Pranav Parwate; Atharva Patil; Snehal Kamalapur,http://arxiv.org/abs/2112.13865v1,10.48550/arXiv.2112.13865
http://arxiv.org/abs/2206.01618v2,2022,"A transformer-based synthetic-inflow generator for spatially-developing
  turbulent boundary layers","  This study proposes a newly-developed deep-learning-based method to generate
turbulent inflow conditions for spatially-developing turbulent boundary layer
(TBL) simulations. A combination of a transformer and a multiscale-enhanced
super-resolution generative adversarial network is utilized to predict velocity
fields of a spatially-developing TBL at various planes normal to the streamwise
direction. Datasets of direct numerical simulation (DNS) of flat plate flow
spanning a momentum thickness-based Reynolds number, Re_theta = 661.5 - 1502.0,
are used to train and test the model. The model shows a remarkable ability to
predict the instantaneous velocity fields with detailed fluctuations and
reproduce the turbulence statistics as well as spatial and temporal spectra
with commendable accuracy as compared with the DNS results. The proposed model
also exhibits a reasonable accuracy for predicting velocity fields at Reynolds
numbers that are not used in the training process. With the aid of transfer
learning, the computational cost of the proposed model is considered to be
effectively low. The results demonstrate, for the first time that
transformer-based models can be efficient in predicting the dynamics of
turbulent flows. It also shows that combining these models with generative
adversarial networks-based models can be useful in tackling various
turbulence-related problems, including the development of efficient
synthetic-turbulent inflow generators.
",Mustafa Z. Yousif; Meng Zhang; Linqi Yu; Ricardo Vinuesa; HeeChang Lim,http://arxiv.org/abs/2206.01618v2,10.48550/arXiv.2206.01618
http://arxiv.org/abs/2210.12524v1,2022,Efficient Hair Style Transfer with Generative Adversarial Networks,"  Despite the recent success of image generation and style transfer with
Generative Adversarial Networks (GANs), hair synthesis and style transfer
remain challenging due to the shape and style variability of human hair in
in-the-wild conditions. The current state-of-the-art hair synthesis approaches
struggle to maintain global composition of the target style and cannot be used
in real-time applications due to their high running costs on high-resolution
portrait images. Therefore, We propose a novel hairstyle transfer method,
called EHGAN, which reduces computational costs to enable real-time processing
while improving the transfer of hairstyle with better global structure compared
to the other state-of-the-art hair synthesis methods. To achieve this goal, we
train an encoder and a low-resolution generator to transfer hairstyle and then,
increase the resolution of results with a pre-trained super-resolution model.
We utilize Adaptive Instance Normalization (AdaIN) and design our novel Hair
Blending Block (HBB) to obtain the best performance of the generator. EHGAN
needs around 2.7 times and over 10,000 times less time consumption than the
state-of-the-art MichiGAN and LOHO methods respectively while obtaining better
photorealism and structural similarity to the desired style than its
competitors.
",Muhammed Pektas; Baris Gecer; Aybars Ugur,http://arxiv.org/abs/2210.12524v1,10.48550/arXiv.2210.12524
http://arxiv.org/abs/1907.07131v3,2019,"Boosting Resolution and Recovering Texture of micro-CT Images with Deep
  Learning","  Digital Rock Imaging is constrained by detector hardware, and a trade-off
between the image field of view (FOV) and the image resolution must be made.
This can be compensated for with super resolution (SR) techniques that take a
wide FOV, low resolution (LR) image, and super resolve a high resolution (HR),
high FOV image. The Enhanced Deep Super Resolution Generative Adversarial
Network (EDSRGAN) is trained on the Deep Learning Digital Rock Super Resolution
Dataset, a diverse compilation 12000 of raw and processed uCT images. The
network shows comparable performance of 50% to 70% reduction in relative error
over bicubic interpolation. GAN performance in recovering texture shows
superior visual similarity compared to SRCNN and other methods. Difference maps
indicate that the SRCNN section of the SRGAN network recovers large scale edge
(grain boundaries) features while the GAN network regenerates perceptually
indistinguishable high frequency texture. Network performance is generalised
with augmentation, showing high adaptability to noise and blur. HR images are
fed into the network, generating HR-SR images to extrapolate network
performance to sub-resolution features present in the HR images themselves.
Results show that under-resolution features such as dissolved minerals and thin
fractures are regenerated despite the network operating outside of trained
specifications. Comparison with Scanning Electron Microscope images shows
details are consistent with the underlying geometry of the sample. Recovery of
textures benefits the characterisation of digital rocks with a high proportion
of under-resolution micro-porous features, such as carbonate and coal samples.
Images that are normally constrained by the mineralogy of the rock (coal), by
fast transient imaging (waterflooding), or by the energy of the source
(microporosity), can be super resolved accurately for further analysis
downstream.
",Ying Da Wang; Ryan T. Armstrong; Peyman Mostaghimi,http://arxiv.org/abs/1907.07131v3,10.1029/2019WR026052
http://arxiv.org/abs/2105.07200v2,2021,"Multi-scale super-resolution generation of low-resolution scanned
  pathological images","  Background. Digital pathology has aroused widespread interest in modern
pathology. The key of digitalization is to scan the whole slide image (WSI) at
high magnification. The lager the magnification is, the richer details WSI will
provide, but the scanning time is longer and the file size of obtained is
larger. Methods. We design a strategy to scan slides with low resolution (5X)
and a super-resolution method is proposed to restore the image details when in
diagnosis. The method is based on a multi-scale generative adversarial network,
which sequentially generates three high-resolution images such as 10X, 20X and
40X. Results. The peak-signal-to-noise-ratio of 10X to 40X generated images are
24.16, 22.27 and 20.44, and the structural-similarity-index are 0.845, 0.680
and 0.512, which are better than other super-resolution networks. Visual
scoring average and standard deviation from three pathologists is 3.63
plus-minus 0.52, 3.70 plus-minus 0.57 and 3.74 plus-minus 0.56 and the p value
of analysis of variance is 0.37, indicating that generated images include
sufficient information for diagnosis. The average value of Kappa test is 0.99,
meaning the diagnosis of generated images is highly consistent with that of the
real images. Conclusion. This proposed method can generate high-quality 10X,
20X, 40X images from 5X images at the same time, in which the time and storage
costs of digitalization can be effectively reduced up to 1/64 of the previous
costs. The proposed method provides a better alternative for low-cost storage,
faster image share of digital pathology. Keywords. Digital pathology;
Super-resolution; Low resolution scanning; Low cost
",Kai Sun; Yanhua Gao; Ting Xie; Xun Wang; Qingqing Yang; Le Chen; Kuansong Wang; Gang Yu,http://arxiv.org/abs/2105.07200v2,10.48550/arXiv.2105.07200
http://arxiv.org/abs/2107.02338v1,2021,"Impact of deep learning-based image super-resolution on binary signal
  detection","  Deep learning-based image super-resolution (DL-SR) has shown great promise in
medical imaging applications. To date, most of the proposed methods for DL-SR
have only been assessed by use of traditional measures of image quality (IQ)
that are commonly employed in the field of computer vision. However, the impact
of these methods on objective measures of image quality that are relevant to
medical imaging tasks remains largely unexplored. In this study, we investigate
the impact of DL-SR methods on binary signal detection performance. Two popular
DL-SR methods, the super-resolution convolutional neural network (SRCNN) and
the super-resolution generative adversarial network (SRGAN), were trained by
use of simulated medical image data. Binary signal-known-exactly with
background-known-statistically (SKE/BKS) and signal-known-statistically with
background-known-statistically (SKS/BKS) detection tasks were formulated.
Numerical observers, which included a neural network-approximated ideal
observer and common linear numerical observers, were employed to assess the
impact of DL-SR on task performance. The impact of the complexity of the DL-SR
network architectures on task-performance was quantified. In addition, the
utility of DL-SR for improving the task-performance of sub-optimal observers
was investigated. Our numerical experiments confirmed that, as expected, DL-SR
could improve traditional measures of IQ. However, for many of the study
designs considered, the DL-SR methods provided little or no improvement in task
performance and could even degrade it. It was observed that DL-SR could improve
the task-performance of sub-optimal observers under certain conditions. The
presented study highlights the urgent need for the objective assessment of
DL-SR methods and suggests avenues for improving their efficacy in medical
imaging applications.
",Xiaohui Zhang; Varun A. Kelkar; Jason Granstedt; Hua Li; Mark A. Anastasio,http://arxiv.org/abs/2107.02338v1,10.48550/arXiv.2107.02338
http://arxiv.org/abs/1801.07330v2,2018,"High-throughput, high-resolution registration-free generated adversarial
  network microscopy","  We combine generative adversarial network (GAN) with light microscopy to
achieve deep learning super-resolution under a large field of view (FOV). By
appropriately adopting prior microscopy data in an adversarial training, the
neural network can recover a high-resolution, accurate image of new specimen
from its single low-resolution measurement. Its capacity has been broadly
demonstrated via imaging various types of samples, such as USAF resolution
target, human pathological slides, fluorescence-labelled fibroblast cells, and
deep tissues in transgenic mouse brain, by both wide-field and light-sheet
microscopes. The gigapixel, multi-color reconstruction of these samples
verifies a successful GAN-based single image super-resolution procedure. We
also propose an image degrading model to generate low resolution images for
training, making our approach free from the complex image registration during
training dataset preparation. After a welltrained network being created, this
deep learning-based imaging approach is capable of recovering a large FOV (~95
mm2), high-resolution (~1.7 {\mu}m) image at high speed (within 1 second),
while not necessarily introducing any changes to the setup of existing
microscopes.
",Hao Zhang; Xinlin Xie; Chunyu Fang; Yicong Yang; Di Jin; Peng Fei,http://arxiv.org/abs/1801.07330v2,10.48550/arXiv.1801.07330
http://arxiv.org/abs/1911.03464v3,2019,"Perception-oriented Single Image Super-Resolution via Dual Relativistic
  Average Generative Adversarial Networks","  The presence of residual and dense neural networks which greatly promotes the
development of image Super-Resolution(SR) have witnessed a lot of impressive
results. Depending on our observation, although more layers and connections
could always improve performance, the increase of model parameters is not
conducive to launch application of SR algorithms. Furthermore, algorithms
supervised by L1/L2 loss can achieve considerable performance on traditional
metrics such as PSNR and SSIM, yet resulting in blurry and over-smoothed
outputs without sufficient high-frequency details, namely low perceptual
index(PI). Regarding the issues, this paper develops a perception-oriented
single image SR algorithm via dual relativistic average generative adversarial
networks. In the generator part, a novel residual channel attention block is
proposed to recalibrate significance of specific channels, further increasing
feature expression capabilities. Parameters of convolutional layers within each
block are shared to expand receptive fields while maintain the amount of
tunable parameters unchanged. The feature maps are subsampled using sub-pixel
convolution to obtain reconstructed high-resolution images. The discriminator
part consists of two relativistic average discriminators that work in pixel
domain and feature domain, respectively, fully exploiting the prior that half
of data in a mini-batch are fake. Different weighted combinations of perceptual
loss and adversarial loss are utilized to supervise the generator to
equilibrate perceptual quality and objective results. Experimental results and
ablation studies show that our proposed algorithm can rival state-of-the-art SR
algorithms, both perceptually(PI-minimization) and
objectively(PSNR-maximization) with fewer parameters.
",Yuan Ma; Kewen Liu; Hongxia Xiong; Panpan Fang; Xiaojun Li; Yalei Chen; Chaoyang Liu,http://arxiv.org/abs/1911.03464v3,10.48550/arXiv.1911.03464
http://arxiv.org/abs/2104.12623v1,2021,"Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against
  Image Translation Generative Adversarial Networks","  Machine learning models are typically made available to potential client
users via inference APIs. Model extraction attacks occur when a malicious
client uses information gleaned from queries to the inference API of a victim
model $F_V$ to build a surrogate model $F_A$ that has comparable functionality.
Recent research has shown successful model extraction attacks against image
classification, and NLP models. In this paper, we show the first model
extraction attack against real-world generative adversarial network (GAN) image
translation models. We present a framework for conducting model extraction
attacks against image translation models, and show that the adversary can
successfully extract functional surrogate models. The adversary is not required
to know $F_V$'s architecture or any other information about it beyond its
intended image translation task, and queries $F_V$'s inference interface using
data drawn from the same domain as the training data for $F_V$. We evaluate the
effectiveness of our attacks using three different instances of two popular
categories of image translation: (1) Selfie-to-Anime and (2) Monet-to-Photo
(image style transfer), and (3) Super-Resolution (super resolution). Using
standard performance metrics for GANs, we show that our attacks are effective
in each of the three cases -- the differences between $F_V$ and $F_A$, compared
to the target are in the following ranges: Selfie-to-Anime: FID $13.36-68.66$,
Monet-to-Photo: FID $3.57-4.40$, and Super-Resolution: SSIM: $0.06-0.08$ and
PSNR: $1.43-4.46$. Furthermore, we conducted a large scale (125 participants)
user study on Selfie-to-Anime and Monet-to-Photo to show that human perception
of the images produced by the victim and surrogate models can be considered
equivalent, within an equivalence bound of Cohen's $d=0.3$.
",Sebastian Szyller; Vasisht Duddu; Tommi Gröndahl; N. Asokan,http://arxiv.org/abs/2104.12623v1,10.48550/arXiv.2104.12623
http://arxiv.org/abs/1801.09710v2,2018,"tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution
  Fluid Flow","  We propose a temporally coherent generative model addressing the
super-resolution problem for fluid flows. Our work represents a first approach
to synthesize four-dimensional physics fields with neural networks. Based on a
conditional generative adversarial network that is designed for the inference
of three-dimensional volumetric data, our model generates consistent and
detailed results by using a novel temporal discriminator, in addition to the
commonly used spatial one. Our experiments show that the generator is able to
infer more realistic high-resolution details by using additional physical
quantities, such as low-resolution velocities or vorticities. Besides
improvements in the training process and in the generated outputs, these inputs
offer means for artistic control as well. We additionally employ a
physics-aware data augmentation step, which is crucial to avoid overfitting and
to reduce memory requirements. In this way, our network learns to generate
advected quantities with highly detailed, realistic, and temporally coherent
features. Our method works instantaneously, using only a single time-step of
low-resolution fluid data. We demonstrate the abilities of our method using a
variety of complex inputs and applications in two and three dimensions.
",You Xie; Erik Franz; Mengyu Chu; Nils Thuerey,http://arxiv.org/abs/1801.09710v2,10.1145/3072959.3073643
http://arxiv.org/abs/1810.06693v1,2018,Lesion Focused Super-Resolution,"  Super-resolution (SR) for image enhancement has great importance in medical
image applications. Broadly speaking, there are two types of SR, one requires
multiple low resolution (LR) images from different views of the same object to
be reconstructed to the high resolution (HR) output, and the other one relies
on the learning from a large amount of training datasets, i.e., LR-HR pairs. In
real clinical environment, acquiring images from multi-views is expensive and
sometimes infeasible. In this paper, we present a novel Generative Adversarial
Networks (GAN) based learning framework to achieve SR from its LR version. By
performing simulation based studies on the Multimodal Brain Tumor Segmentation
Challenge (BraTS) datasets, we demonstrate the efficacy of our method in
application of brain tumor MRI enhancement. Compared to bilinear interpolation
and other state-of-the-art SR methods, our model is lesion focused, which is
not only resulted in better perceptual image quality without blurring, but also
more efficient and directly benefit for the following clinical tasks, e.g.,
lesion detection and abnormality enhancement. Therefore, we can envisage the
application of our SR method to boost image spatial resolution while
maintaining crucial diagnostic information for further clinical tasks.
",Jin Zhu; Guang Yang; Pietro Lio,http://arxiv.org/abs/1810.06693v1,10.48550/arXiv.1810.06693
http://arxiv.org/abs/1901.03419v1,2019,"How Can We Make GAN Perform Better in Single Medical Image
  Super-Resolution? A Lesion Focused Multi-Scale Approach","  Single image super-resolution (SISR) is of great importance as a low-level
computer vision task. The fast development of Generative Adversarial Network
(GAN) based deep learning architectures realises an efficient and effective
SISR to boost the spatial resolution of natural images captured by digital
cameras. However, the SISR for medical images is still a very challenging
problem. This is due to (1) compared to natural images, in general, medical
images have lower signal to noise ratios, (2) GAN based models pre-trained on
natural images may synthesise unrealistic patterns in medical images which
could affect the clinical interpretation and diagnosis, and (3) the vanilla GAN
architecture may suffer from unstable training and collapse mode that can also
affect the SISR results. In this paper, we propose a novel lesion focused SR
(LFSR) method, which incorporates GAN to achieve perceptually realistic SISR
results for brain tumour MRI images. More importantly, we test and make
comparison using recently developed GAN variations, e.g., Wasserstein GAN
(WGAN) and WGAN with Gradient Penalty (WGAN-GP), and propose a novel
multi-scale GAN (MS-GAN), to achieve a more stabilised and efficient training
and improved perceptual quality of the super-resolved results. Based on both
quantitative evaluations and our designed mean opinion score, the proposed LFSR
coupled with MS-GAN has performed better in terms of both perceptual quality
and efficiency.
",Jin Zhu; Guang Yang; Pietro Lio,http://arxiv.org/abs/1901.03419v1,10.48550/arXiv.1901.03419
http://arxiv.org/abs/1910.04074v1,2019,"Wavelet Domain Style Transfer for an Effective Perception-distortion
  Tradeoff in Single Image Super-Resolution","  In single image super-resolution (SISR), given a low-resolution (LR) image,
one wishes to find a high-resolution (HR) version of it which is both accurate
and photo-realistic. Recently, it has been shown that there exists a
fundamental tradeoff between low distortion and high perceptual quality, and
the generative adversarial network (GAN) is demonstrated to approach the
perception-distortion (PD) bound effectively. In this paper, we propose a novel
method based on wavelet domain style transfer (WDST), which achieves a better
PD tradeoff than the GAN based methods. Specifically, we propose to use 2D
stationary wavelet transform (SWT) to decompose one image into low-frequency
and high-frequency sub-bands. For the low-frequency sub-band, we improve its
objective quality through an enhancement network. For the high-frequency
sub-band, we propose to use WDST to effectively improve its perceptual quality.
By feat of the perfect reconstruction property of wavelets, these sub-bands can
be re-combined to obtain an image which has simultaneously high objective and
perceptual quality. The numerical results on various datasets show that our
method achieves the best trade-off between the distortion and perceptual
quality among the existing state-of-the-art SISR methods.
",Xin Deng; Ren Yang; Mai Xu; Pier Luigi Dragotti,http://arxiv.org/abs/1910.04074v1,10.48550/arXiv.1910.04074
http://arxiv.org/abs/1912.12879v3,2019,"Self-supervised Fine-tuning for Correcting Super-Resolution
  Convolutional Neural Networks","  While Convolutional Neural Networks (CNNs) trained for image and video
super-resolution (SR) regularly achieve new state-of-the-art performance, they
also suffer from significant drawbacks. One of their limitations is their lack
of robustness to unseen image formation models during training. Other
limitations include the generation of artifacts and hallucinated content when
training Generative Adversarial Networks (GANs) for SR. While the Deep Learning
literature focuses on presenting new training schemes and settings to resolve
these various issues, we show that one can avoid training and correct for SR
results with a fully self-supervised fine-tuning approach. More specifically,
at test time, given an image and its known image formation model, we fine-tune
the parameters of the trained network and iteratively update them using a data
fidelity loss. We apply our fine-tuning algorithm on multiple image and video
SR CNNs and show that it can successfully correct for a sub-optimal SR solution
by entirely relying on internal learning at test time. We apply our method on
the problem of fine-tuning for unseen image formation models and on removal of
artifacts introduced by GANs.
",Alice Lucas; Santiago Lopez-Tapia; Rafael Molina; Aggelos K. Katsaggelos,http://arxiv.org/abs/1912.12879v3,10.48550/arXiv.1912.12879
http://arxiv.org/abs/2003.00255v2,2020,"Joint Face Completion and Super-resolution using Multi-scale Feature
  Relation Learning","  Previous research on face restoration often focused on repairing a specific
type of low-quality facial images such as low-resolution (LR) or occluded
facial images. However, in the real world, both the above-mentioned forms of
image degradation often coexist. Therefore, it is important to design a model
that can repair LR occluded images simultaneously. This paper proposes a
multi-scale feature graph generative adversarial network (MFG-GAN) to implement
the face restoration of images in which both degradation modes coexist, and
also to repair images with a single type of degradation. Based on the GAN, the
MFG-GAN integrates the graph convolution and feature pyramid network to restore
occluded low-resolution face images to non-occluded high-resolution face
images. The MFG-GAN uses a set of customized losses to ensure that high-quality
images are generated. In addition, we designed the network in an end-to-end
format. Experimental results on the public-domain CelebA and Helen databases
show that the proposed approach outperforms state-of-the-art methods in
performing face super-resolution (up to 4x or 8x) and face completion
simultaneously. Cross-database testing also revealed that the proposed approach
has good generalizability.
",Zhilei Liu; Yunpeng Wu; Le Li; Cuicui Zhang; Baoyuan Wu,http://arxiv.org/abs/2003.00255v2,10.48550/arXiv.2003.00255
http://arxiv.org/abs/2003.09085v5,2020,"Small-Object Detection in Remote Sensing Images with End-to-End
  Edge-Enhanced GAN and Object Detector Network","  The detection performance of small objects in remote sensing images is not
satisfactory compared to large objects, especially in low-resolution and noisy
images. A generative adversarial network (GAN)-based model called enhanced
super-resolution GAN (ESRGAN) shows remarkable image enhancement performance,
but reconstructed images miss high-frequency edge information. Therefore,
object detection performance degrades for small objects on recovered noisy and
low-resolution remote sensing images. Inspired by the success of edge enhanced
GAN (EEGAN) and ESRGAN, we apply a new edge-enhanced super-resolution GAN
(EESRGAN) to improve the image quality of remote sensing images and use
different detector networks in an end-to-end manner where detector loss is
backpropagated into the EESRGAN to improve the detection performance. We
propose an architecture with three components: ESRGAN, Edge Enhancement Network
(EEN), and Detection network. We use residual-in-residual dense blocks (RRDB)
for both the ESRGAN and EEN, and for the detector network, we use the faster
region-based convolutional network (FRCNN) (two-stage detector) and single-shot
multi-box detector (SSD) (one stage detector). Extensive experiments on a
public (car overhead with context) and a self-assembled (oil and gas storage
tank) satellite dataset show superior performance of our method compared to the
standalone state-of-the-art object detectors.
",Jakaria Rabbi; Nilanjan Ray; Matthias Schubert; Subir Chowdhury; Dennis Chao,http://arxiv.org/abs/2003.09085v5,10.48550/arXiv.2003.09085
http://arxiv.org/abs/2102.03113v1,2021,Real-World Super-Resolution of Face-Images from Surveillance Cameras,"  Most existing face image Super-Resolution (SR) methods assume that the
Low-Resolution (LR) images were artificially downsampled from High-Resolution
(HR) images with bicubic interpolation. This operation changes the natural
image characteristics and reduces noise. Hence, SR methods trained on such data
most often fail to produce good results when applied to real LR images. To
solve this problem, we propose a novel framework for generation of realistic
LR/HR training pairs. Our framework estimates realistic blur kernels, noise
distributions, and JPEG compression artifacts to generate LR images with
similar image characteristics as the ones in the source domain. This allows us
to train a SR model using high quality face images as Ground-Truth (GT). For
better perceptual quality we use a Generative Adversarial Network (GAN) based
SR model where we have exchanged the commonly used VGG-loss [24] with
LPIPS-loss [52]. Experimental results on both real and artificially corrupted
face images show that our method results in more detailed reconstructions with
less noise compared to existing State-of-the-Art (SoTA) methods. In addition,
we show that the traditional non-reference Image Quality Assessment (IQA)
methods fail to capture this improvement and demonstrate that the more recent
NIMA metric [16] correlates better with human perception via Mean Opinion Rank
(MOR).
",Andreas Aakerberg; Kamal Nasrollahi; Thomas B. Moeslund,http://arxiv.org/abs/2102.03113v1,10.48550/arXiv.2102.03113
http://arxiv.org/abs/2104.00100v1,2021,"MR Slice Profile Estimation by Learning to Match Internal Patch
  Distributions","  To super-resolve the through-plane direction of a multi-slice 2D magnetic
resonance (MR) image, its slice selection profile can be used as the
degeneration model from high resolution (HR) to low resolution (LR) to create
paired data when training a supervised algorithm. Existing super-resolution
algorithms make assumptions about the slice selection profile since it is not
readily known for a given image. In this work, we estimate a slice selection
profile given a specific image by learning to match its internal patch
distributions. Specifically, we assume that after applying the correct slice
selection profile, the image patch distribution along HR in-plane directions
should match the distribution along the LR through-plane direction. Therefore,
we incorporate the estimation of a slice selection profile as part of learning
a generator in a generative adversarial network (GAN). In this way, the slice
selection profile can be learned without any external data. Our algorithm was
tested using simulations from isotropic MR images, incorporated in a
through-plane super-resolution algorithm to demonstrate its benefits, and also
used as a tool to measure image resolution. Our code is at
https://github.com/shuohan/espreso2.
",Shuo Han; Samuel Remedios; Aaron Carass; Michael Schär; Jerry L. Prince,http://arxiv.org/abs/2104.00100v1,10.48550/arXiv.2104.00100
http://arxiv.org/abs/2104.09435v2,2021,"Deep learning enables reference-free isotropic super-resolution for
  volumetric fluorescence microscopy","  Volumetric imaging by fluorescence microscopy is often limited by anisotropic
spatial resolution from inferior axial resolution compared to the lateral
resolution. To address this problem, here we present a deep-learning-enabled
unsupervised super-resolution technique that enhances anisotropic images in
volumetric fluorescence microscopy. In contrast to the existing deep learning
approaches that require matched high-resolution target volume images, our
method greatly reduces the effort to put into practice as the training of a
network requires as little as a single 3D image stack, without a priori
knowledge of the image formation process, registration of training data, or
separate acquisition of target data. This is achieved based on the optimal
transport driven cycle-consistent generative adversarial network that learns
from an unpaired matching between high-resolution 2D images in lateral image
plane and low-resolution 2D images in the other planes. Using fluorescence
confocal microscopy and light-sheet microscopy, we demonstrate that the trained
network not only enhances axial resolution, but also restores suppressed visual
details between the imaging planes and removes imaging artifacts.
",Hyoungjun Park; Myeongsu Na; Bumju Kim; Soohyun Park; Ki Hean Kim; Sunghoe Chang; Jong Chul Ye,http://arxiv.org/abs/2104.09435v2,10.1038/s41467-022-30949-6
http://arxiv.org/abs/2105.03579v2,2021,Unsupervised Remote Sensing Super-Resolution via Migration Image Prior,"  Recently, satellites with high temporal resolution have fostered wide
attention in various practical applications. Due to limitations of bandwidth
and hardware cost, however, the spatial resolution of such satellites is
considerably low, largely limiting their potentials in scenarios that require
spatially explicit information. To improve image resolution, numerous
approaches based on training low-high resolution pairs have been proposed to
address the super-resolution (SR) task. Despite their success, however,
low/high spatial resolution pairs are usually difficult to obtain in satellites
with a high temporal resolution, making such approaches in SR impractical to
use. In this paper, we proposed a new unsupervised learning framework, called
""MIP"", which achieves SR tasks without low/high resolution image pairs. First,
random noise maps are fed into a designed generative adversarial network (GAN)
for reconstruction. Then, the proposed method converts the reference image to
latent space as the migration image prior. Finally, we update the input noise
via an implicit method, and further transfer the texture and structured
information from the reference image. Extensive experimental results on the
Draper dataset show that MIP achieves significant improvements over
state-of-the-art methods both quantitatively and qualitatively. The proposed
MIP is open-sourced at http://github.com/jiaming-wang/MIP.
",Jiaming Wang; Zhenfeng Shao; Tao Lu; Xiao Huang; Ruiqian Zhang; Yu Wang,http://arxiv.org/abs/2105.03579v2,10.48550/arXiv.2105.03579
http://arxiv.org/abs/2105.09466v1,2021,"Real-time super-resolution mapping of locally anisotropic grain
  orientations for ultrasonic non-destructive evaluation of crystalline
  material","  Estimating the spatially varying microstructures of heterogeneous and locally
anisotropic media non-destructively is necessary for the accurate detection of
flaws and reliable monitoring of manufacturing processes. Conventional
algorithms used for solving this inverse problem come with significant
computational cost, particularly in the case of high dimensional non-linear
tomographic problems. In this paper, we propose a framework which uses deep
neural networks (DNNs) with full aperture, pitch-catch and pulse-echo
transducer configurations to reconstruct material maps of crystallographic
orientation. We also present the first ever application of generative
adversarial networks (GANs) to achieve super resolution of ultrasonic
tomographic images, providing a factor-four increase in image resolution and up
to a 50% increase in structural similarity. The importance of including
appropriate prior knowledge in the GAN training dataset to increase inversion
accuracy is highlighted; known information about the material's structure
should be present in the training data. We show that after a computationally
expensive training process, the DNNs and GANs can be used in less that one
second (0.9 seconds on a standard desktop computer) to provide a high
resolution map of the material's grain orientations.
",Jonathan Singh; Katherine M. M. Tant; Andrew Curtis; Anthony J. Mulholland,http://arxiv.org/abs/2105.09466v1,10.48550/arXiv.2105.09466
http://arxiv.org/abs/2109.09071v1,2021,"Simple and Efficient Unpaired Real-world Super-Resolution using Image
  Statistics","  Learning super-resolution (SR) network without the paired low resolution (LR)
and high resolution (HR) image is difficult because direct supervision through
the corresponding HR counterpart is unavailable. Recently, many real-world SR
researches take advantage of the unpaired image-to-image translation technique.
That is, they used two or more generative adversarial networks (GANs), each of
which translates images from one domain to another domain, \eg, translates
images from the HR domain to the LR domain. However, it is not easy to stably
learn such a translation with GANs using unpaired data. In this study, we
present a simple and efficient method of training of real-world SR network. To
stably train the network, we use statistics of an image patch, such as means
and variances. Our real-world SR framework consists of two GANs, one for
translating HR images to LR images (degradation task) and the other for
translating LR to HR (SR task). We argue that the unpaired image translation
using GANs can be learned efficiently with our proposed data sampling strategy,
namely, variance matching. We test our method on the NTIRE 2020 real-world SR
dataset. Our method outperforms the current state-of-the-art method in terms of
the SSIM metric as well as produces comparable results on the LPIPS metric.
",Kwangjin Yoon,http://arxiv.org/abs/2109.09071v1,10.48550/arXiv.2109.09071
http://arxiv.org/abs/2112.10046v1,2021,"A-ESRGAN: Training Real-World Blind Super-Resolution with Attention
  U-Net Discriminators","  Blind image super-resolution(SR) is a long-standing task in CV that aims to
restore low-resolution images suffering from unknown and complex distortions.
Recent work has largely focused on adopting more complicated degradation models
to emulate real-world degradations. The resulting models have made
breakthroughs in perceptual loss and yield perceptually convincing results.
However, the limitation brought by current generative adversarial network
structures is still significant: treating pixels equally leads to the ignorance
of the image's structural features, and results in performance drawbacks such
as twisted lines and background over-sharpening or blurring. In this paper, we
present A-ESRGAN, a GAN model for blind SR tasks featuring an attention U-Net
based, multi-scale discriminator that can be seamlessly integrated with other
generators. To our knowledge, this is the first work to introduce attention
U-Net structure as the discriminator of GAN to solve blind SR problems. And the
paper also gives an interpretation for the mechanism behind multi-scale
attention U-Net that brings performance breakthrough to the model. Through
comparison experiments with prior works, our model presents state-of-the-art
level performance on the non-reference natural image quality evaluator metric.
And our ablation studies have shown that with our discriminator, the RRDB based
generator can leverage the structural features of an image in multiple scales,
and consequently yields more perceptually realistic high-resolution images
compared to prior works.
",Zihao Wei; Yidong Huang; Yuang Chen; Chenhao Zheng; Jinnan Gao,http://arxiv.org/abs/2112.10046v1,10.48550/arXiv.2112.10046
http://arxiv.org/abs/2207.14812v1,2022,GLEAN: Generative Latent Bank for Image Super-Resolution and Beyond,"  We show that pre-trained Generative Adversarial Networks (GANs) such as
StyleGAN and BigGAN can be used as a latent bank to improve the performance of
image super-resolution. While most existing perceptual-oriented approaches
attempt to generate realistic outputs through learning with adversarial loss,
our method, Generative LatEnt bANk (GLEAN), goes beyond existing practices by
directly leveraging rich and diverse priors encapsulated in a pre-trained GAN.
But unlike prevalent GAN inversion methods that require expensive
image-specific optimization at runtime, our approach only needs a single
forward pass for restoration. GLEAN can be easily incorporated in a simple
encoder-bank-decoder architecture with multi-resolution skip connections.
Employing priors from different generative models allows GLEAN to be applied to
diverse categories (\eg~human faces, cats, buildings, and cars). We further
present a lightweight version of GLEAN, named LightGLEAN, which retains only
the critical components in GLEAN. Notably, LightGLEAN consists of only 21% of
parameters and 35% of FLOPs while achieving comparable image quality. We extend
our method to different tasks including image colorization and blind image
restoration, and extensive experiments show that our proposed models perform
favorably in comparison to existing methods. Codes and models are available at
https://github.com/open-mmlab/mmediting.
",Kelvin C. K. Chan; Xiangyu Xu; Xintao Wang; Jinwei Gu; Chen Change Loy,http://arxiv.org/abs/2207.14812v1,10.48550/arXiv.2207.14812
http://arxiv.org/abs/2210.07751v1,2022,"Blind Super-Resolution for Remote Sensing Images via Conditional
  Stochastic Normalizing Flows","  Remote sensing images (RSIs) in real scenes may be disturbed by multiple
factors such as optical blur, undersampling, and additional noise, resulting in
complex and diverse degradation models. At present, the mainstream SR
algorithms only consider a single and fixed degradation (such as bicubic
interpolation) and cannot flexibly handle complex degradations in real scenes.
Therefore, designing a super-resolution (SR) model that can cope with various
degradations is gradually attracting the attention of researchers. Some studies
first estimate the degradation kernels and then perform degradation-adaptive SR
but face the problems of estimation error amplification and insufficient
high-frequency details in the results. Although blind SR algorithms based on
generative adversarial networks (GAN) have greatly improved visual quality,
they still suffer from pseudo-texture, mode collapse, and poor training
stability. In this article, we propose a novel blind SR framework based on the
stochastic normalizing flow (BlindSRSNF) to address the above problems.
BlindSRSNF learns the conditional probability distribution over the
high-resolution image space given a low-resolution (LR) image by explicitly
optimizing the variational bound on the likelihood. BlindSRSNF is easy to train
and can generate photo-realistic SR results that outperform GAN-based models.
Besides, we introduce a degradation representation strategy based on
contrastive learning to avoid the error amplification problem caused by the
explicit degradation estimation. Comprehensive experiments show that the
proposed algorithm can obtain SR results with excellent visual perception
quality on both simulated LR and real-world RSIs.
",Hanlin Wu; Ning Ni; Shan Wang; Libao Zhang,http://arxiv.org/abs/2210.07751v1,10.48550/arXiv.2210.07751
http://arxiv.org/abs/2302.08047v1,2023,"TcGAN: Semantic-Aware and Structure-Preserved GANs with Individual
  Vision Transformer for Fast Arbitrary One-Shot Image Generation","  One-shot image generation (OSG) with generative adversarial networks that
learn from the internal patches of a given image has attracted world wide
attention. In recent studies, scholars have primarily focused on extracting
features of images from probabilistically distributed inputs with pure
convolutional neural networks (CNNs). However, it is quite difficult for CNNs
with limited receptive domain to extract and maintain the global structural
information. Therefore, in this paper, we propose a novel structure-preserved
method TcGAN with individual vision transformer to overcome the shortcomings of
the existing one-shot image generation methods. Specifically, TcGAN preserves
global structure of an image during training to be compatible with local
details while maintaining the integrity of semantic-aware information by
exploiting the powerful long-range dependencies modeling capability of the
transformer. We also propose a new scaling formula having scale-invariance
during the calculation period, which effectively improves the generated image
quality of the OSG model on image super-resolution tasks. We present the design
of the TcGAN converter framework, comprehensive experimental as well as
ablation studies demonstrating the ability of TcGAN to achieve arbitrary image
generation with the fastest running time. Lastly, TcGAN achieves the most
excellent performance in terms of applying it to other image processing tasks,
e.g., super-resolution as well as image harmonization, the results further
prove its superiority.
",Yunliang Jiang; Lili Yan; Xiongtao Zhang; Yong Liu; Danfeng Sun,http://arxiv.org/abs/2302.08047v1,10.48550/arXiv.2302.08047
http://arxiv.org/abs/1901.11094v1,2019,"Resolution enhancement in scanning electron microscopy using deep
  learning","  We report resolution enhancement in scanning electron microscopy (SEM) images
using a generative adversarial network. We demonstrate the veracity of this
deep learning-based super-resolution technique by inferring unresolved features
in low-resolution SEM images and comparing them with the accurately
co-registered high-resolution SEM images of the same samples. Through spatial
frequency analysis, we also report that our method generates images with
frequency spectra matching higher resolution SEM images of the same
fields-of-view. By using this technique, higher resolution SEM images can be
taken faster, while also reducing both electron charging and damage to the
samples.
",Kevin de Haan; Zachary S. Ballard; Yair Rivenson; Yichen Wu; Aydogan Ozcan,http://arxiv.org/abs/1901.11094v1,10.1038/s41598-019-48444-2
http://arxiv.org/abs/1902.11203v1,2019,Two-phase Hair Image Synthesis by Self-Enhancing Generative Model,"  Generating plausible hair image given limited guidance, such as sparse
sketches or low-resolution image, has been made possible with the rise of
Generative Adversarial Networks (GANs). Traditional image-to-image translation
networks can generate recognizable results, but finer textures are usually lost
and blur artifacts commonly exist. In this paper, we propose a two-phase
generative model for high-quality hair image synthesis. The two-phase pipeline
first generates a coarse image by an existing image translation model, then
applies a re-generating network with self-enhancing capability to the coarse
image. The self-enhancing capability is achieved by a proposed structure
extraction layer, which extracts the texture and orientation map from a hair
image. Extensive experiments on two tasks, Sketch2Hair and Hair
Super-Resolution, demonstrate that our approach is able to synthesize plausible
hair image with finer details, and outperforms the state-of-the-art.
",Haonan Qiu; Chuan Wang; Hang Zhu; Xiangyu Zhu; Jinjin Gu; Xiaoguang Han,http://arxiv.org/abs/1902.11203v1,10.1111/cgf.13847
http://arxiv.org/abs/1903.00142v1,2019,A Unified Neural Architecture for Instrumental Audio Tasks,"  Within Music Information Retrieval (MIR), prominent tasks -- including
pitch-tracking, source-separation, super-resolution, and synthesis -- typically
call for specialised methods, despite their similarities. Conditional
Generative Adversarial Networks (cGANs) have been shown to be highly versatile
in learning general image-to-image translations, but have not yet been adapted
across MIR. In this work, we present an end-to-end supervisable architecture to
perform all aforementioned audio tasks, consisting of a WaveNet synthesiser
conditioned on the output of a jointly-trained cGAN spectrogram translator. In
doing so, we demonstrate the potential of such flexible techniques to unify MIR
tasks, promote efficient transfer learning, and converge research to the
improvement of powerful, general methods. Finally, to the best of our
knowledge, we present the first application of GANs to guided instrument
synthesis.
",Steven Spratley; Daniel Beck; Trevor Cohn,http://arxiv.org/abs/1903.00142v1,10.48550/arXiv.1903.00142
http://arxiv.org/abs/1903.01392v2,2019,An Adversarial Super-Resolution Remedy for Radar Design Trade-offs,"  Radar is of vital importance in many fields, such as autonomous driving,
safety and surveillance applications. However, it suffers from stringent
constraints on its design parametrization leading to multiple trade-offs. For
example, the bandwidth in FMCW radars is inversely proportional with both the
maximum unambiguous range and range resolution. In this work, we introduce a
new method for circumventing radar design trade-offs. We propose the use of
recent advances in computer vision, more specifically generative adversarial
networks (GANs), to enhance low-resolution radar acquisitions into higher
resolution counterparts while maintaining the advantages of the low-resolution
parametrization. The capability of the proposed method was evaluated on the
velocity resolution and range-azimuth trade-offs in micro-Doppler signatures
and FMCW uniform linear array (ULA) radars, respectively.
",Karim Armanious; Sherif Abdulatif; Fady Aziz; Urs Schneider; Bin Yang,http://arxiv.org/abs/1903.01392v2,10.23919/EUSIPCO.2019.8902510
http://arxiv.org/abs/1906.04681v2,2019,"A Hybrid Approach Between Adversarial Generative Networks and
  Actor-Critic Policy Gradient for Low Rate High-Resolution Image Compression","  Image compression is an essential approach for decreasing the size in bytes
of the image without deteriorating the quality of it. Typically, classic
algorithms are used but recently deep-learning has been successfully applied.
In this work, is presented a deep super-resolution work-flow for image
compression that maps low-resolution JPEG image to the high-resolution. The
pipeline consists of two components: first, an encoder-decoder neural network
learns how to transform the downsampling JPEG images to high resolution.
Second, a combination between Generative Adversarial Networks (GANs) and
reinforcement learning Actor-Critic (A3C) loss pushes the encoder-decoder to
indirectly maximize High Peak Signal-to-Noise Ratio (PSNR). Although PSNR is a
fully differentiable metric, this work opens the doors to new solutions for
maximizing non-differential metrics through an end-to-end approach between
encoder-decoder networks and reinforcement learning policy gradient methods.
",Nicoló Savioli,http://arxiv.org/abs/1906.04681v2,10.48550/arXiv.1906.04681
http://arxiv.org/abs/1906.05284v2,2019,Image-Adaptive GAN based Reconstruction,"  In the recent years, there has been a significant improvement in the quality
of samples produced by (deep) generative models such as variational
auto-encoders and generative adversarial networks. However, the representation
capabilities of these methods still do not capture the full distribution for
complex classes of images, such as human faces. This deficiency has been
clearly observed in previous works that use pre-trained generative models to
solve imaging inverse problems. In this paper, we suggest to mitigate the
limited representation capabilities of generators by making them image-adaptive
and enforcing compliance of the restoration with the observations via
back-projections. We empirically demonstrate the advantages of our proposed
approach for image super-resolution and compressed sensing.
",Shady Abu Hussein; Tom Tirer; Raja Giryes,http://arxiv.org/abs/1906.05284v2,10.48550/arXiv.1906.05284
http://arxiv.org/abs/1906.06575v4,2019,"Single Image Super-resolution via Dense Blended Attention Generative
  Adversarial Network for Clinical Diagnosis","  During training phase, more connections (e.g. channel concatenation in last
layer of DenseNet) means more occupied GPU memory and lower GPU utilization,
requiring more training time. The increase of training time is also not
conducive to launch application of SR algorithms. This's why we abandoned
DenseNet as basic network. Futhermore, we abandoned this paper due to its
limitation only applied on medical images. Please view our lastest work applied
on general images at arXiv:1911.03464.
",Kewen Liu; Yuan Ma; Hongxia Xiong; Zejun Yan; Zhijun Zhou; Chaoyang Liu; Panpan Fang; Xiaojun Li; Yalei Chen,http://arxiv.org/abs/1906.06575v4,10.48550/arXiv.1906.06575
http://arxiv.org/abs/1908.00274v1,2019,"Content and Colour Distillation for Learning Image Translations with the
  Spatial Profile Loss","  Generative adversarial networks has emerged as a defacto standard for image
translation problems. To successfully drive such models, one has to rely on
additional networks e.g., discriminators and/or perceptual networks. Training
these networks with pixel based losses alone are generally not sufficient to
learn the target distribution. In this paper, we propose a novel method of
computing the loss directly between the source and target images that enable
proper distillation of shape/content and colour/style. We show that this is
useful in typical image-to-image translations allowing us to successfully drive
the generator without relying on additional networks. We demonstrate this on
many difficult image translation problems such as image-to-image domain
mapping, single image super-resolution and photo realistic makeup transfer. Our
extensive evaluation shows the effectiveness of the proposed formulation and
its ability to synthesize realistic images. [Code release:
https://github.com/ssarfraz/SPL]
",M. Saquib Sarfraz; Constantin Seibold; Haroon Khalid; Rainer Stiefelhagen,http://arxiv.org/abs/1908.00274v1,10.48550/arXiv.1908.00274
http://arxiv.org/abs/1910.10344v1,2019,"Facial Expression Restoration Based on Improved Graph Convolutional
  Networks","  Facial expression analysis in the wild is challenging when the facial image
is with low resolution or partial occlusion. Considering the correlations among
different facial local regions under different facial expressions, this paper
proposes a novel facial expression restoration method based on generative
adversarial network by integrating an improved graph convolutional network
(IGCN) and region relation modeling block (RRMB). Unlike conventional graph
convolutional networks taking vectors as input features, IGCN can use tensors
of face patches as inputs. It is better to retain the structure information of
face patches. The proposed RRMB is designed to address facial generative tasks
including inpainting and super-resolution with facial action units detection,
which aims to restore facial expression as the ground-truth. Extensive
experiments conducted on BP4D and DISFA benchmarks demonstrate the
effectiveness of our proposed method through quantitative and qualitative
evaluations.
",Zhilei Liu; Le Li; Yunpeng Wu; Cuicui Zhang,http://arxiv.org/abs/1910.10344v1,10.1007/978-3-030-37734-2_43
http://arxiv.org/abs/1911.04410v2,2019,"A deep learning framework for morphologic detail beyond the diffraction
  limit in infrared spectroscopic imaging","  Infrared (IR) microscopes measure spectral information that quantifies
molecular content to assign the identity of biomedical cells but lack the
spatial quality of optical microscopy to appreciate morphologic features. Here,
we propose a method to utilize the semantic information of cellular identity
from IR imaging with the morphologic detail of pathology images in a deep
learning-based approach to image super-resolution. Using Generative Adversarial
Networks (GANs), we enhance the spatial detail in IR imaging beyond the
diffraction limit while retaining their spectral contrast. This technique can
be rapidly integrated with modern IR microscopes to provide a framework useful
for routine pathology.
",Kianoush Falahkheirkhah; Kevin Yeh; Shachi Mittal; Luke Pfister; Rohit Bhargava,http://arxiv.org/abs/1911.04410v2,10.48550/arXiv.1911.04410
http://arxiv.org/abs/1912.07116v2,2019,Image Processing Using Multi-Code GAN Prior,"  Despite the success of Generative Adversarial Networks (GANs) in image
synthesis, applying trained GAN models to real image processing remains
challenging. Previous methods typically invert a target image back to the
latent space either by back-propagation or by learning an additional encoder.
However, the reconstructions from both of the methods are far from ideal. In
this work, we propose a novel approach, called mGANprior, to incorporate the
well-trained GANs as effective prior to a variety of image processing tasks. In
particular, we employ multiple latent codes to generate multiple feature maps
at some intermediate layer of the generator, then compose them with adaptive
channel importance to recover the input image. Such an over-parameterization of
the latent space significantly improves the image reconstruction quality,
outperforming existing competitors. The resulting high-fidelity image
reconstruction enables the trained GAN models as prior to many real-world
applications, such as image colorization, super-resolution, image inpainting,
and semantic manipulation. We further analyze the properties of the layer-wise
representation learned by GAN models and shed light on what knowledge each
layer is capable of representing.
",Jinjin Gu; Yujun Shen; Bolei Zhou,http://arxiv.org/abs/1912.07116v2,10.48550/arXiv.1912.07116
http://arxiv.org/abs/2002.08900v1,2020,SynFi: Automatic Synthetic Fingerprint Generation,"  Authentication and identification methods based on human fingerprints are
ubiquitous in several systems ranging from government organizations to consumer
products. The performance and reliability of such systems directly rely on the
volume of data on which they have been verified. Unfortunately, a large volume
of fingerprint databases is not publicly available due to many privacy and
security concerns.
  In this paper, we introduce a new approach to automatically generate
high-fidelity synthetic fingerprints at scale. Our approach relies on (i)
Generative Adversarial Networks to estimate the probability distribution of
human fingerprints and (ii) Super-Resolution methods to synthesize fine-grained
textures. We rigorously test our system and show that our methodology is the
first to generate fingerprints that are computationally indistinguishable from
real ones, a task that prior art could not accomplish.
",M. Sadegh Riazi; Seyed M. Chavoshian; Farinaz Koushanfar,http://arxiv.org/abs/2002.08900v1,10.48550/arXiv.2002.08900
http://arxiv.org/abs/2004.13060v3,2020,GIMP-ML: Python Plugins for using Computer Vision Models in GIMP,"  This paper introduces GIMP-ML v1.1, a set of Python plugins for the widely
popular GNU Image Manipulation Program (GIMP). It enables the use of recent
advances in computer vision to the conventional image editing pipeline.
Applications from deep learning such as monocular depth estimation, semantic
segmentation, mask generative adversarial networks, image super-resolution,
de-noising, de-hazing, matting, enlightening and coloring have been
incorporated with GIMP through Python-based plugins. Additionally, operations
on images such as k-means based color clustering have also been added. GIMP-ML
relies on standard Python packages such as numpy, pytorch, open-cv, scipy.
Apart from these, several image manipulation techniques using these plugins
have been compiled and demonstrated in the YouTube channel
(https://youtube.com/user/kritiksoman) with the objective of demonstrating the
use-cases for machine learning based image modification. In addition, GIMP-ML
also aims to bring the benefits of using deep learning networks used for
computer vision tasks to routine image processing workflows. The code and
installation procedure for configuring these plugins is available at
https://github.com/kritiksoman/GIMP-ML.
",Kritik Soman,http://arxiv.org/abs/2004.13060v3,10.48550/arXiv.2004.13060
http://arxiv.org/abs/2005.11875v2,2020,Bayesian Conditional GAN for MRI Brain Image Synthesis,"  As a powerful technique in medical imaging, image synthesis is widely used in
applications such as denoising, super resolution and modality transformation
etc. Recently, the revival of deep neural networks made immense progress in the
field of medical imaging. Although many deep leaning based models have been
proposed to improve the image synthesis accuracy, the evaluation of the model
uncertainty, which is highly important for medical applications, has been a
missing part. In this work, we propose to use Bayesian conditional generative
adversarial network (GAN) with concrete dropout to improve image synthesis
accuracy. Meanwhile, an uncertainty calibration approach is involved in the
whole pipeline to make the uncertainty generated by Bayesian network
interpretable. The method is validated with the T1w to T2w MR image translation
with a brain tumor dataset of 102 subjects. Compared with the conventional
Bayesian neural network with Monte Carlo dropout, results of the proposed
method reach a significant lower RMSE with a p-value of 0.0186. Improvement of
the calibration of the generated uncertainty by the uncertainty recalibration
method is also illustrated.
",Gengyan Zhao; Mary E. Meyerand; Rasmus M. Birn,http://arxiv.org/abs/2005.11875v2,10.48550/arXiv.2005.11875
http://arxiv.org/abs/2006.10250v1,2020,Progressively Unfreezing Perceptual GAN,"  Generative adversarial networks (GANs) are widely used in image generation
tasks, yet the generated images are usually lack of texture details. In this
paper, we propose a general framework, called Progressively Unfreezing
Perceptual GAN (PUPGAN), which can generate images with fine texture details.
Particularly, we propose an adaptive perceptual discriminator with a
pre-trained perceptual feature extractor, which can efficiently measure the
discrepancy between multi-level features of the generated and real images. In
addition, we propose a progressively unfreezing scheme for the adaptive
perceptual discriminator, which ensures a smooth transfer process from a large
scale classification task to a specified image generation task. The qualitative
and quantitative experiments with comparison to the classical baselines on
three image generation tasks, i.e. single image super-resolution, paired
image-to-image translation and unpaired image-to-image translation demonstrate
the superiority of PUPGAN over the compared approaches.
",Jinxuan Sun; Yang Chen; Junyu Dong; Guoqiang Zhong,http://arxiv.org/abs/2006.10250v1,10.48550/arXiv.2006.10250
http://arxiv.org/abs/2006.15228v1,2020,"HypervolGAN: An efficient approach for GAN with multi-objective training
  function","  Since the advent of generative adversarial networks (GANs), various loss
functions have been developed and combined to constitute the overall training
objective function, in order to improve model performance or for specific
learning tasks. For instance, in image enhancement or restoration, there are
often several criteria to consider such as signal-noise ratio, smoothness,
structures and details. However, when the optimization goal has more than one
adversarial loss, balancing multiple losses in the overall function becomes a
challenging, critical and time-consuming problem. In this paper, we propose to
tackle the problem by means of efficient multi-objective optimization. The
proposed HypervolGAN adopts an adapted version of hypervolume maximization
method to effectively define the multi-objective training function for GAN. We
tested our proposed method on solving single image super-resolution problem.
Experiments show that the proposed HypervolGAN is efficient in saving
computational time and efforts for fine-tuning weights of various losses, and
can generate enhanced samples that have better quality than results given by
baseline GANs. The work explores the integration of adversarial learning and
optimization techniques, which can benefit not only image processing but also a
wide range of applications.
",Jingwen Su; Hujun Yin,http://arxiv.org/abs/2006.15228v1,10.48550/arXiv.2006.15228
http://arxiv.org/abs/2008.12463v2,2020,Accelerated WGAN update strategy with loss change rate balancing,"  Optimizing the discriminator in Generative Adversarial Networks (GANs) to
completion in the inner training loop is computationally prohibitive, and on
finite datasets would result in overfitting. To address this, a common update
strategy is to alternate between k optimization steps for the discriminator D
and one optimization step for the generator G. This strategy is repeated in
various GAN algorithms where k is selected empirically. In this paper, we show
that this update strategy is not optimal in terms of accuracy and convergence
speed, and propose a new update strategy for Wasserstein GANs (WGAN) and other
GANs using the WGAN loss(e.g. WGAN-GP, Deblur GAN, and Super-resolution GAN).
The proposed update strategy is based on a loss change ratio comparison of G
and D. We demonstrate that the proposed strategy improves both convergence
speed and accuracy.
",Xu Ouyang; Gady Agam,http://arxiv.org/abs/2008.12463v2,10.48550/arXiv.2008.12463
http://arxiv.org/abs/2009.09574v1,2020,"Reconstruct high-resolution multi-focal plane images from a single 2D
  wide field image","  High-resolution 3D medical images are important for analysis and diagnosis,
but axial scanning to acquire them is very time-consuming. In this paper, we
propose a fast end-to-end multi-focal plane imaging network (MFPINet) to
reconstruct high-resolution multi-focal plane images from a single 2D
low-resolution wild filed image without relying on scanning. To acquire
realistic MFP images fast, the proposed MFPINet adopts generative adversarial
network framework and the strategies of post-sampling and refocusing all focal
planes at one time. We conduct a series experiments on cytology microscopy
images and demonstrate that MFPINet performs well on both axial refocusing and
horizontal super resolution. Furthermore, MFPINet is approximately 24 times
faster than current refocusing methods for reconstructing the same volume
images. The proposed method has the potential to greatly increase the speed of
high-resolution 3D imaging and expand the application of low-resolution
wide-field images.
",Jiabo Ma; Sibo Liu; Shenghua Cheng; Xiuli Liu; Li Cheng; Shaoqun Zeng,http://arxiv.org/abs/2009.09574v1,10.48550/arXiv.2009.09574
http://arxiv.org/abs/2012.04842v2,2020,Improving the Fairness of Deep Generative Models without Retraining,"  Generative Adversarial Networks (GANs) advance face synthesis through
learning the underlying distribution of observed data. Despite the high-quality
generated faces, some minority groups can be rarely generated from the trained
models due to a biased image generation process. To study the issue, we first
conduct an empirical study on a pre-trained face synthesis model. We observe
that after training the GAN model not only carries the biases in the training
data but also amplifies them to some degree in the image generation process. To
further improve the fairness of image generation, we propose an interpretable
baseline method to balance the output facial attributes without retraining. The
proposed method shifts the interpretable semantic distribution in the latent
space for a more balanced image generation while preserving the sample
diversity. Besides producing more balanced data regarding a particular
attribute (e.g., race, gender, etc.), our method is generalizable to handle
more than one attribute at a time and synthesize samples of fine-grained
subgroups. We further show the positive applicability of the balanced data
sampled from GANs to quantify the biases in other face recognition systems,
like commercial face attribute classifiers and face super-resolution
algorithms.
",Shuhan Tan; Yujun Shen; Bolei Zhou,http://arxiv.org/abs/2012.04842v2,10.48550/arXiv.2012.04842
http://arxiv.org/abs/2101.02384v1,2021,VHS to HDTV Video Translation using Multi-task Adversarial Learning,"  There are large amount of valuable video archives in Video Home System (VHS)
format. However, due to the analog nature, their quality is often poor.
Compared to High-definition television (HDTV), VHS video not only has a dull
color appearance but also has a lower resolution and often appears blurry. In
this paper, we focus on the problem of translating VHS video to HDTV video and
have developed a solution based on a novel unsupervised multi-task adversarial
learning model. Inspired by the success of generative adversarial network (GAN)
and CycleGAN, we employ cycle consistency loss, adversarial loss and perceptual
loss together to learn a translation model. An important innovation of our work
is the incorporation of super-resolution model and color transfer model that
can solve unsupervised multi-task problem. To our knowledge, this is the first
work that dedicated to the study of the relation between VHS and HDTV and the
first computational solution to translate VHS to HDTV. We present experimental
results to demonstrate the effectiveness of our solution qualitatively and
quantitatively.
",Hongming Luo; Guangsen Liao; Xianxu Hou; Bozhi Liu; Fei Zhou; Guoping Qiu,http://arxiv.org/abs/2101.02384v1,10.48550/arXiv.2101.02384
http://arxiv.org/abs/2105.10189v3,2021,Combining Transformer Generators with Convolutional Discriminators,"  Transformer models have recently attracted much interest from computer vision
researchers and have since been successfully employed for several problems
traditionally addressed with convolutional neural networks. At the same time,
image synthesis using generative adversarial networks (GANs) has drastically
improved over the last few years. The recently proposed TransGAN is the first
GAN using only transformer-based architectures and achieves competitive results
when compared to convolutional GANs. However, since transformers are
data-hungry architectures, TransGAN requires data augmentation, an auxiliary
super-resolution task during training, and a masking prior to guide the
self-attention mechanism. In this paper, we study the combination of a
transformer-based generator and convolutional discriminator and successfully
remove the need of the aforementioned required design choices. We evaluate our
approach by conducting a benchmark of well-known CNN discriminators, ablate the
size of the transformer-based generator, and show that combining both
architectural elements into a hybrid model leads to better results.
Furthermore, we investigate the frequency spectrum properties of generated
images and observe that our model retains the benefits of an attention based
generator.
",Ricard Durall; Stanislav Frolov; Jörn Hees; Federico Raue; Franz-Josef Pfreundt; Andreas Dengel; Janis Keupe,http://arxiv.org/abs/2105.10189v3,10.48550/arXiv.2105.10189
http://arxiv.org/abs/2106.15575v1,2021,"A Mixed-Supervision Multilevel GAN Framework for Image Quality
  Enhancement","  Deep neural networks for image quality enhancement typically need large
quantities of highly-curated training data comprising pairs of low-quality
images and their corresponding high-quality images. While high-quality image
acquisition is typically expensive and time-consuming, medium-quality images
are faster to acquire, at lower equipment costs, and available in larger
quantities. Thus, we propose a novel generative adversarial network (GAN) that
can leverage training data at multiple levels of quality (e.g., high and medium
quality) to improve performance while limiting costs of data curation. We apply
our mixed-supervision GAN to (i) super-resolve histopathology images and (ii)
enhance laparoscopy images by combining super-resolution and surgical smoke
removal. Results on large clinical and pre-clinical datasets show the benefits
of our mixed-supervision GAN over the state of the art.
",Uddeshya Upadhyay; Suyash Awate,http://arxiv.org/abs/2106.15575v1,10.48550/arXiv.2106.15575
http://arxiv.org/abs/2109.10679v1,2021,"Application of Video-to-Video Translation Networks to Computational
  Fluid Dynamics","  In recent years, the evolution of artificial intelligence, especially deep
learning, has been remarkable, and its application to various fields has been
growing rapidly. In this paper, I report the results of the application of
generative adversarial networks (GANs), specifically video-to-video translation
networks, to computational fluid dynamics (CFD) simulations. The purpose of
this research is to reduce the computational cost of CFD simulations with GANs.
The architecture of GANs in this research is a combination of the
image-to-image translation networks (the so-called ""pix2pix"") and Long
Short-Term Memory (LSTM). It is shown that the results of high-cost and
high-accuracy simulations (with high-resolution computational grids) can be
estimated from those of low-cost and low-accuracy simulations (with
low-resolution grids). In particular, the time evolution of density
distributions in the cases of a high-resolution grid is reproduced from that in
the cases of a low-resolution grid through GANs, and the density inhomogeneity
estimated from the image generated by GANs recovers the ground truth with good
accuracy. Qualitative and quantitative comparisons of the results of the
proposed method with those of several super-resolution algorithms are also
presented.
",Hiromitsu Kigure,http://arxiv.org/abs/2109.10679v1,10.3389/frai.2021.670208
http://arxiv.org/abs/2110.03814v1,2021,StyleGAN-induced data-driven regularization for inverse problems,"  Recent advances in generative adversarial networks (GANs) have opened up the
possibility of generating high-resolution photo-realistic images that were
impossible to produce previously. The ability of GANs to sample from
high-dimensional distributions has naturally motivated researchers to leverage
their power for modeling the image prior in inverse problems. We extend this
line of research by developing a Bayesian image reconstruction framework that
utilizes the full potential of a pre-trained StyleGAN2 generator, which is the
currently dominant GAN architecture, for constructing the prior distribution on
the underlying image. Our proposed approach, which we refer to as learned
Bayesian reconstruction with generative models (L-BRGM), entails joint
optimization over the style-code and the input latent code, and enhances the
expressive power of a pre-trained StyleGAN2 generator by allowing the
style-codes to be different for different generator layers. Considering the
inverse problems of image inpainting and super-resolution, we demonstrate that
the proposed approach is competitive with, and sometimes superior to,
state-of-the-art GAN-based image reconstruction methods.
",Arthur Conmy; Subhadip Mukherjee; Carola-Bibiane Schönlieb,http://arxiv.org/abs/2110.03814v1,10.48550/arXiv.2110.03814
http://arxiv.org/abs/2203.04382v1,2022,"Regularized Training of Intermediate Layers for Generative Models for
  Inverse Problems","  Generative Adversarial Networks (GANs) have been shown to be powerful and
flexible priors when solving inverse problems. One challenge of using them is
overcoming representation error, the fundamental limitation of the network in
representing any particular signal. Recently, multiple proposed inversion
algorithms reduce representation error by optimizing over intermediate layer
representations. These methods are typically applied to generative models that
were trained agnostic of the downstream inversion algorithm. In our work, we
introduce a principle that if a generative model is intended for inversion
using an algorithm based on optimization of intermediate layers, it should be
trained in a way that regularizes those intermediate layers. We instantiate
this principle for two notable recent inversion algorithms: Intermediate Layer
Optimization and the Multi-Code GAN prior. For both of these inversion
algorithms, we introduce a new regularized GAN training algorithm and
demonstrate that the learned generative model results in lower reconstruction
errors across a wide range of under sampling ratios when solving compressed
sensing, inpainting, and super-resolution problems.
",Sean Gunn; Jorio Cocola; Paul Hand,http://arxiv.org/abs/2203.04382v1,10.48550/arXiv.2203.04382
http://arxiv.org/abs/2207.00460v1,2022,"Exploring the solution space of linear inverse problems with GAN latent
  geometry","  Inverse problems consist in reconstructing signals from incomplete sets of
measurements and their performance is highly dependent on the quality of the
prior knowledge encoded via regularization. While traditional approaches focus
on obtaining a unique solution, an emerging trend considers exploring multiple
feasibile solutions. In this paper, we propose a method to generate multiple
reconstructions that fit both the measurements and a data-driven prior learned
by a generative adversarial network. In particular, we show that, starting from
an initial solution, it is possible to find directions in the latent space of
the generative model that are null to the forward operator, and thus keep
consistency with the measurements, while inducing significant perceptual
change. Our exploration approach allows to generate multiple solutions to the
inverse problem an order of magnitude faster than existing approaches; we show
results on image super-resolution and inpainting problems.
",Antonio Montanaro; Diego Valsesia; Enrico Magli,http://arxiv.org/abs/2207.00460v1,10.48550/arXiv.2207.00460
http://arxiv.org/abs/2210.16206v1,2022,"Applying Physics-Informed Enhanced Super-Resolution Generative
  Adversarial Networks to Turbulent Premixed Combustion and Engine-like Flame
  Kernel Direct Numerical Simulation Data","  Models for finite-rate-chemistry in underresolved flows still pose one of the
main challenges for predictive simulations of complex configurations. The
problem gets even more challenging if turbulence is involved. This work
advances the recently developed PIESRGAN modeling approach to turbulent
premixed combustion. For that, the physical information processed by the
network and considered in the loss function are adjusted, the training process
is smoothed, and especially effects from density changes are considered. The
resulting model provides good results for a priori and a posteriori tests on
direct numerical simulation data of a fully turbulent premixed flame kernel.
The limits of the modeling approach are discussed. Finally, the model is
employed to compute further realizations of the premixed flame kernel, which
are analyzed with a scale-sensitive framework regarding their cycle-to-cycle
variations. The work shows that the data-driven PIESRGAN subfilter model can
very accurately reproduce direct numerical simulation data on much coarser
meshes, which is hardly possible with classical subfilter models, and enables
studying statistical processes more efficiently due to the smaller computing
cost.
",Mathis Bode; Michael Gauding; Dominik Goeb; Tobias Falkenstein; Heinz Pitsch,http://arxiv.org/abs/2210.16206v1,10.48550/arXiv.2210.16206
http://arxiv.org/abs/2212.02586v1,2022,AtomVision: A machine vision library for atomistic images,"  Computer vision techniques have immense potential for materials design
applications. In this work, we introduce an integrated and general-purpose
AtomVision library that can be used to generate, curate scanning tunneling
microscopy (STM) and scanning transmission electron microscopy (STEM) datasets
and apply machine learning techniques. To demonstrate the applicability of this
library, we 1) generate and curate an atomistic image dataset of about 10000
materials, 2) develop and compare convolutional and graph neural network models
to classify the Bravais lattices, 3) develop fully convolutional neural network
using U-Net architecture to pixelwise classify atom vs background, 4) use
generative adversarial network for super-resolution, 5) curate a natural
language processing based image dataset using open-access arXiv dataset, and 6)
integrate the computational framework with experimental microscopy tools.
AtomVision library is available at https://github.com/usnistgov/atomvision.
",Kamal Choudhary; Ramya Gurunathan; Brian DeCost; Adam Biacchi,http://arxiv.org/abs/2212.02586v1,10.48550/arXiv.2212.02586
http://arxiv.org/abs/2110.11281v3,2021,"Fusion of complementary 2D and 3D mesostructural datasets using
  generative adversarial networks","  Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
",Amir Dahari; Steve Kench; Isaac Squires; Samuel J. Cooper,http://arxiv.org/abs/2110.11281v3,10.48550/arXiv.2110.11281
http://arxiv.org/abs/2209.09696v1,2022,"Synthesis of realistic fetal MRI with conditional Generative Adversarial
  Networks","  Fetal brain magnetic resonance imaging serves as an emerging modality for
prenatal counseling and diagnosis in disorders affecting the brain. Machine
learning based segmentation plays an important role in the quantification of
brain development. However, a limiting factor is the lack of sufficiently
large, labeled training data. Our study explored the application of SPADE, a
conditional general adversarial network (cGAN), which learns the mapping from
the label to the image space. The input to the network was super-resolution
T2-weighted cerebral MRI data of 120 fetuses (gestational age range: 20-35
weeks, normal and pathological), which were annotated for 7 different tissue
categories. SPADE networks were trained on 256*256 2D slices of the
reconstructed volumes (image and label pairs) in each orthogonal orientation.
To combine the generated volumes from each orientation into one image, a simple
mean of the outputs of the three networks was taken. Based on the label maps
only, we synthesized highly realistic images. However, some finer details, like
small vessels were not synthesized. A structural similarity index (SSIM) of
0.972+-0.016 and correlation coefficient of 0.974+-0.008 were achieved. To
demonstrate the capacity of the cGAN to create new anatomical variants, we
artificially dilated the ventricles in the segmentation map and created
synthetic MRI of different degrees of fetal hydrocephalus. cGANs, such as the
SPADE algorithm, allow the generation of hypothetically unseen scenarios and
anatomical configurations in the label space, which data in turn can be
utilized for training various machine learning algorithms. In the future, this
algorithm would be used for generating large, synthetic datasets representing
fetal brain development. These datasets would potentially improve the
performance of currently available segmentation networks.
",Marina Fernandez Garcia; Rodrigo Gonzalez Laiz; Hui Ji; Kelly Payette; Andras Jakab,http://arxiv.org/abs/2209.09696v1,10.48550/arXiv.2209.09696
http://arxiv.org/abs/2211.04488v1,2022,Deblending Galaxies with Generative Adversarial Networks,"  Deep generative models including generative adversarial networks (GANs) are
powerful unsupervised tools in learning the distributions of data sets.
Building a simple GAN architecture in PyTorch and training on the CANDELS data
set, we generate galaxy images with the Hubble Space Telescope resolution
starting from a noise vector. We proceed by modifying the GAN architecture to
improve the Subaru Hyper Suprime-Cam ground-based images by increasing their
resolution to the HST resolution. We use the super resolution GAN on a large
sample of blended galaxies which we create using CANDELS cutouts. In our
simulated blend sample, $\sim 20 \%$ would unrecognizably be blended even in
the HST resolution cutouts. In the HSC-like cutouts this fraction rises to
$\sim 90\%$. With our modified GAN we can lower this value to $\sim 50\%$. We
quantify the blending fraction in the high, low and GAN resolutions over the
whole manifold of angular separation, flux ratios, sizes and redshift
difference between the two blended objects. The two peaks found by the GAN
deblender result in ten times improvement in the photometry measurement of the
blended objects. Modifying the architecture of the GAN, we also train a
Multi-wavelength GAN with seven band optical+NIR HST cutouts. This
multi-wavelength GAN improves the fraction of detected blends by another $\sim
10\%$ compared to the single-band GAN. This is most beneficial to the current
and future precision cosmology experiments (e.g., LSST, SPHEREx, Euclid,
Roman), specifically those relying on weak gravitational lensing, where
blending is a major source of systematic error.
",Shoubaneh Hemmati; Eric Huff; Hooshang Nayyeri; Agnès Ferté; Peter Melchior; Bahram Mobasher; Jason Rhodes; Abtin Shahidi; Harry Teplitz,http://arxiv.org/abs/2211.04488v1,10.3847/1538-4357/aca1b8
http://arxiv.org/abs/2302.08720v1,2023,"Algorithmic Hallucinations of Near-Surface Winds: Statistical
  Downscaling with Generative Adversarial Networks to Convection-Permitting
  Scales","  Providing small-scale information about weather and climate is challenging,
especially for variables strongly controlled by processes that are unresolved
by low-resolution (LR) models. This paper explores emerging machine learning
methods from the fields of image super-resolution (SR) and deep learning for
statistical downscaling of near-surface winds to convection-permitting scales.
Specifically, Generative Adversarial Networks (GANs) are conditioned on LR
inputs from a global reanalysis to generate high-resolution (HR) surface winds
that emulate those simulated over North America by the Weather Research and
Forecasting (WRF) model. Unlike traditional SR models, where LR inputs are
idealized coarsened versions of the HR images, WRF emulation involves
non-idealized LR inputs from a coarse-resolution reanalysis. In addition to
matching the statistical properties of WRF simulations, GANs quickly generate
HR fields with impressive realism. However, objectively assessing the realism
of the SR models requires careful selection of evaluation metrics. In
particular, performance measures based on spatial power spectra reveal the way
that GAN configurations change spatial structures in the generated fields,
where biases in spatial variability originate, and how models depend on
different LR covariates. Inspired by recent computer vision research, a novel
methodology that separates spatial frequencies in HR fields is used in an
attempt to optimize the SR GANs further. This method, called frequency
separation, resulted in deterioration in realism of the generated HR fields.
However, frequency separation did show how spatial structures are influenced by
the metrics used to optimize the SR models, which led to the development of a
more effective partial frequency separation approach.
",Nicolaas J. Annau; Alex J. Cannon; Adam H. Monahan,http://arxiv.org/abs/2302.08720v1,10.48550/arXiv.2302.08720
http://arxiv.org/abs/1610.04490v3,2016,Amortised MAP Inference for Image Super-resolution,"  Image super-resolution (SR) is an underdetermined inverse problem, where a
large number of plausible high-resolution images can explain the same
downsampled image. Most current single image SR methods use empirical risk
minimisation, often with a pixel-wise mean squared error (MSE) loss. However,
the outputs from such methods tend to be blurry, over-smoothed and generally
appear implausible. A more desirable approach would employ Maximum a Posteriori
(MAP) inference, preferring solutions that always have a high probability under
the image prior, and thus appear more plausible. Direct MAP estimation for SR
is non-trivial, as it requires us to build a model for the image prior from
samples. Furthermore, MAP inference is often performed via optimisation-based
iterative algorithms which don't compare well with the efficiency of
neural-network-based alternatives. Here we introduce new methods for amortised
MAP inference whereby we calculate the MAP estimate directly using a
convolutional neural network. We first introduce a novel neural network
architecture that performs a projection to the affine subspace of valid SR
solutions ensuring that the high resolution output of the network is always
consistent with the low resolution input. We show that, using this
architecture, the amortised MAP inference problem reduces to minimising the
cross-entropy between two distributions, similar to training generative models.
We propose three methods to solve this optimisation problem: (1) Generative
Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates
gradient-estimates from denoising to train the network, and (3) a baseline
method using a maximum-likelihood-trained image prior. Our experiments show
that the GAN based approach performs best on real image data. Lastly, we
establish a connection between GANs and amortised variational inference as in
e.g. variational autoencoders.
",Casper Kaae Sønderby; Jose Caballero; Lucas Theis; Wenzhe Shi; Ferenc Huszár,http://arxiv.org/abs/1610.04490v3,10.48550/arXiv.1610.04490
http://arxiv.org/abs/1808.04256v3,2018,"CT Super-resolution GAN Constrained by the Identical, Residual, and
  Cycle Learning Ensemble(GAN-CIRCLE)","  Computed tomography (CT) is widely used in screening, diagnosis, and
image-guided therapy for both clinical and research purposes. Since CT involves
ionizing radiation, an overarching thrust of related technical research is
development of novel methods enabling ultrahigh quality imaging with fine
structural details while reducing the X-ray radiation. In this paper, we
present a semi-supervised deep learning approach to accurately recover
high-resolution (HR) CT images from low-resolution (LR) counterparts.
Specifically, with the generative adversarial network (GAN) as the building
block, we enforce the cycle-consistency in terms of the Wasserstein distance to
establish a nonlinear end-to-end mapping from noisy LR input images to denoised
and deblurred HR outputs. We also include the joint constraints in the loss
function to facilitate structural preservation. In this deep imaging process,
we incorporate deep convolutional neural network (CNN), residual learning, and
network in network techniques for feature extraction and restoration. In
contrast to the current trend of increasing network depth and complexity to
boost the CT imaging performance, which limit its real-world applications by
imposing considerable computational and memory overheads, we apply a parallel
$1\times1$ CNN to compress the output of the hidden layer and optimize the
number of layers and the number of filters for each convolutional layer.
Quantitative and qualitative evaluations demonstrate that our proposed model is
accurate, efficient and robust for super-resolution (SR) image restoration from
noisy LR input images. In particular, we validate our composite SR networks on
three large-scale CT datasets, and obtain promising results as compared to the
other state-of-the-art methods.
",Chenyu You; Guang Li; Yi Zhang; Xiaoliu Zhang; Hongming Shan; Shenghong Ju; Zhen Zhao; Zhuiyang Zhang; Wenxiang Cong; Michael W. Vannier; Punam K. Saha; Ge Wang,http://arxiv.org/abs/1808.04256v3,10.1109/TMI.2019.2922960
http://arxiv.org/abs/2009.04420v2,2020,"Cephalogram Synthesis and Landmark Detection in Dental Cone-Beam CT
  Systems","  Due to the lack of standardized 3D cephalometric analytic methodology, 2D
cephalograms synthesized from 3D cone-beam computed tomography (CBCT) volumes
are widely used for cephalometric analysis in dental CBCT systems. However,
compared with conventional X-ray film based cephalograms, such synthetic
cephalograms lack image contrast and resolution. In addition, the radiation
dose during the scan for 3D reconstruction causes potential health risks. In
this work, we propose a sigmoid-based intensity transform that uses the
nonlinear optical property of X-ray films to increase image contrast of
synthetic cephalograms. To improve image resolution, super resolution deep
learning techniques are investigated. For low dose purpose, the pixel-to-pixel
generative adversarial network (pix2pixGAN) is proposed for 2D cephalogram
synthesis directly from two CBCT projections. For landmark detection in the
synthetic cephalograms, an efficient automatic landmark detection method using
the combination of LeNet-5 and ResNet50 is proposed. Our experiments
demonstrate the efficacy of pix2pixGAN in 2D cephalogram synthesis, achieving
an average peak signal-to-noise ratio (PSNR) value of 33.8 with reference to
the cephalograms synthesized from 3D CBCT volumes. Pix2pixGAN also achieves the
best performance in super resolution, achieving an average PSNR value of 32.5
without the introduction of checkerboard or jagging artifacts. Our proposed
automatic landmark detection method achieves 86.7% successful detection rate in
the 2 mm clinical acceptable range on the ISBI Test1 data, which is comparable
to the state-of-the-art methods. The method trained on conventional
cephalograms can be directly applied to landmark detection in the synthetic
cephalograms, achieving 93.0% and 80.7% successful detection rate in 4 mm
precision range for synthetic cephalograms from 3D volumes and 2D projections
respectively.
",Yixing Huang; Fuxin Fan; Christopher Syben; Philipp Roser; Leonid Mill; Andreas Maier,http://arxiv.org/abs/2009.04420v2,10.1016/j.media.2021.102028
http://arxiv.org/abs/2110.11684v2,2021,"Multimodal-Boost: Multimodal Medical Image Super-Resolution using
  Multi-Attention Network with Wavelet Transform","  Deep learning based single image super resolution (SISR) algorithms has
revolutionized the overall diagnosis framework by continually improving the
architectural components and training strategies associated with convolutional
neural networks (CNN) on low-resolution images. However, existing work lacks in
two ways: i) the SR output produced exhibits poor texture details, and often
produce blurred edges, ii) most of the models have been developed for a single
modality, hence, require modification to adapt to a new one. This work
addresses (i) by proposing generative adversarial network (GAN) with deep
multi-attention modules to learn high-frequency information from low-frequency
data. Existing approaches based on the GAN have yielded good SR results;
however, the texture details of their SR output have been experimentally
confirmed to be deficient for medical images particularly. The integration of
wavelet transform (WT) and GANs in our proposed SR model addresses the
aforementioned limitation concerning textons. While the WT divides the LR image
into multiple frequency bands, the transferred GAN uses multi-attention and
upsample blocks to predict high-frequency components. Additionally, we present
a learning method for training domain-specific classifiers as perceptual loss
functions. Using a combination of multi-attention GAN loss and a perceptual
loss function results in an efficient and reliable performance. Applying the
same model for medical images from diverse modalities is challenging, our work
addresses (ii) by training and performing on several modalities via transfer
learning. Using two medical datasets, we validate our proposed SR network
against existing state-of-the-art approaches and achieve promising results in
terms of SSIM and PSNR.
",Fayaz Ali Dharejo; Muhammad Zawish; Farah Deeba Yuanchun Zhou; Kapal Dev; Sunder Ali Khowaja; Nawab Muhammad Faseeh Qureshi,http://arxiv.org/abs/2110.11684v2,10.48550/arXiv.2110.11684
http://arxiv.org/abs/2112.08644v1,2021,"A comparative study of paired versus unpaired deep learning methods for
  physically enhancing digital rock image resolution","  X-ray micro-computed tomography (micro-CT) has been widely leveraged to
characterise pore-scale geometry in subsurface porous rock. Recent developments
in super resolution (SR) methods using deep learning allow the digital
enhancement of low resolution (LR) images over large spatial scales, creating
SR images comparable to the high resolution (HR) ground truth. This circumvents
traditional resolution and field-of-view trade-offs. An outstanding issue is
the use of paired (registered) LR and HR data, which is often required in the
training step of such methods but is difficult to obtain. In this work, we
rigorously compare two different state-of-the-art SR deep learning techniques,
using both paired and unpaired data, with like-for-like ground truth data. The
first approach requires paired images to train a convolutional neural network
(CNN) while the second approach uses unpaired images to train a generative
adversarial network (GAN). The two approaches are compared using a micro-CT
carbonate rock sample with complicated micro-porous textures. We implemented
various image based and numerical verifications and experimental validation to
quantitatively evaluate the physical accuracy and sensitivities of the two
methods. Our quantitative results show that unpaired GAN approach can
reconstruct super-resolution images as precise as paired CNN method, with
comparable training times and dataset requirement. This unlocks new
applications for micro-CT image enhancement using unpaired deep learning
methods; image registration is no longer needed during the data processing
stage. Decoupled images from data storage platforms can be exploited more
efficiently to train networks for SR digital rock applications. This opens up a
new pathway for various applications of multi-scale flow simulation in
heterogeneous porous media.
",Yufu Niu; Samuel J. Jackson; Naif Alqahtani; Peyman Mostaghimi; Ryan T. Armstrong,http://arxiv.org/abs/2112.08644v1,10.48550/arXiv.2112.08644
http://arxiv.org/abs/2202.13142v2,2022,"Real-World Blind Super-Resolution via Feature Matching with Implicit
  High-Resolution Priors","  A key challenge of real-world image super-resolution (SR) is to recover the
missing details in low-resolution (LR) images with complex unknown degradations
(e.g., downsampling, noise and compression). Most previous works restore such
missing details in the image space. To cope with the high diversity of natural
images, they either rely on the unstable GANs that are difficult to train and
prone to artifacts, or resort to explicit references from high-resolution (HR)
images that are usually unavailable. In this work, we propose Feature Matching
SR (FeMaSR), which restores realistic HR images in a much more compact feature
space. Unlike image-space methods, our FeMaSR restores HR images by matching
distorted LR image {\it features} to their distortion-free HR counterparts in
our pretrained HR priors, and decoding the matched features to obtain realistic
HR images. Specifically, our HR priors contain a discrete feature codebook and
its associated decoder, which are pretrained on HR images with a Vector
Quantized Generative Adversarial Network (VQGAN). Notably, we incorporate a
novel semantic regularization in VQGAN to improve the quality of reconstructed
images. For the feature matching, we first extract LR features with an LR
encoder consisting of several Swin Transformer blocks and then follow a simple
nearest neighbour strategy to match them with the pretrained codebook. In
particular, we equip the LR encoder with residual shortcut connections to the
decoder, which is critical to the optimization of feature matching loss and
also helps to complement the possible feature matching errors. Experimental
results show that our approach produces more realistic HR images than previous
methods. Codes are released at \url{https://github.com/chaofengc/FeMaSR}.
",Chaofeng Chen; Xinyu Shi; Yipeng Qin; Xiaoming Li; Xiaoguang Han; Tao Yang; Shihui Guo,http://arxiv.org/abs/2202.13142v2,10.48550/arXiv.2202.13142
http://arxiv.org/abs/2205.07236v1,2022,"Combating COVID-19 using Generative Adversarial Networks and Artificial
  Intelligence for Medical Images: A Scoping Review","  This review presents a comprehensive study on the role of GANs in addressing
the challenges related to COVID-19 data scarcity and diagnosis. It is the first
review that summarizes the different GANs methods and the lungs images datasets
for COVID-19. It attempts to answer the questions related to applications of
GANs, popular GAN architectures, frequently used image modalities, and the
availability of source code. This review included 57 full-text studies that
reported the use of GANs for different applications in COVID-19 lungs images
data. Most of the studies (n=42) used GANs for data augmentation to enhance the
performance of AI techniques for COVID-19 diagnosis. Other popular applications
of GANs were segmentation of lungs and super-resolution of the lungs images.
The cycleGAN and the conditional GAN were the most commonly used architectures
used in nine studies each. 29 studies used chest X-Ray images while 21 studies
used CT images for the training of GANs. For majority of the studies (n=47),
the experiments were done and results were reported using publicly available
data. A secondary evaluation of the results by radiologists/clinicians was
reported by only two studies. Conclusion: Studies have shown that GANs have
great potential to address the data scarcity challenge for lungs images of
COVID-19. Data synthesized with GANs have been helpful to improve the training
of the Convolutional Neural Network (CNN) models trained for the diagnosis of
COVID-19. Besides, GANs have also contributed to enhancing the CNNs performance
through the super-resolution of the images and segmentation. This review also
identified key limitations of the potential transformation of GANs based
methods in clinical applications.
",Hazrat Ali; Zubair Shah,http://arxiv.org/abs/2205.07236v1,10.2196/37365
http://arxiv.org/abs/1705.02583v1,2017,"A Design Methodology for Efficient Implementation of Deconvolutional
  Neural Networks on an FPGA","  In recent years deep learning algorithms have shown extremely high
performance on machine learning tasks such as image classification and speech
recognition. In support of such applications, various FPGA accelerator
architectures have been proposed for convolutional neural networks (CNNs) that
enable high performance for classification tasks at lower power than CPU and
GPU processors. However, to date, there has been little research on the use of
FPGA implementations of deconvolutional neural networks (DCNNs). DCNNs, also
known as generative CNNs, encode high-dimensional probability distributions and
have been widely used for computer vision applications such as scene
completion, scene segmentation, image creation, image denoising, and
super-resolution imaging. We propose an FPGA architecture for deconvolutional
networks built around an accelerator which effectively handles the complex
memory access patterns needed to perform strided deconvolutions, and that
supports convolution as well. We also develop a three-step design optimization
method that systematically exploits statistical analysis, design space
exploration and VLSI optimization. To verify our FPGA deconvolutional
accelerator design methodology we train DCNNs offline on two representative
datasets using the generative adversarial network method (GAN) run on
Tensorflow, and then map these DCNNs to an FPGA DCNN-plus-accelerator
implementation to perform generative inference on a Xilinx Zynq-7000 FPGA. Our
DCNN implementation achieves a peak performance density of 0.012 GOPs/DSP.
",Xinyu Zhang; Srinjoy Das; Ojash Neopane; Ken Kreutz-Delgado,http://arxiv.org/abs/1705.02583v1,10.48550/arXiv.1705.02583
http://arxiv.org/abs/1809.06557v1,2018,"Image Super-Resolution via Deterministic-Stochastic Synthesis and Local
  Statistical Rectification","  Single image superresolution has been a popular research topic in the last
two decades and has recently received a new wave of interest due to deep neural
networks. In this paper, we approach this problem from a different perspective.
With respect to a downsampled low resolution image, we model a high resolution
image as a combination of two components, a deterministic component and a
stochastic component. The deterministic component can be recovered from the
low-frequency signals in the downsampled image. The stochastic component, on
the other hand, contains the signals that have little correlation with the low
resolution image. We adopt two complementary methods for generating these two
components. While generative adversarial networks are used for the stochastic
component, deterministic component reconstruction is formulated as a regression
problem solved using deep neural networks. Since the deterministic component
exhibits clearer local orientations, we design novel loss functions tailored
for such properties for training the deep regression network. These two methods
are first applied to the entire input image to produce two distinct
high-resolution images. Afterwards, these two images are fused together using
another deep neural network that also performs local statistical rectification,
which tries to make the local statistics of the fused image match the same
local statistics of the groundtruth image. Quantitative results and a user
study indicate that the proposed method outperforms existing state-of-the-art
algorithms with a clear margin.
",Weifeng Ge; Bingchen Gong; Yizhou Yu,http://arxiv.org/abs/1809.06557v1,10.1145/3272127.3275060
http://arxiv.org/abs/1902.00159v1,2019,Compressing GANs using Knowledge Distillation,"  Generative Adversarial Networks (GANs) have been used in several machine
learning tasks such as domain transfer, super resolution, and synthetic data
generation. State-of-the-art GANs often use tens of millions of parameters,
making them expensive to deploy for applications in low SWAP (size, weight, and
power) hardware, such as mobile devices, and for applications with real time
capabilities. There has been no work found to reduce the number of parameters
used in GANs. Therefore, we propose a method to compress GANs using knowledge
distillation techniques, in which a smaller ""student"" GAN learns to mimic a
larger ""teacher"" GAN. We show that the distillation methods used on MNIST,
CIFAR-10, and Celeb-A datasets can compress teacher GANs at ratios of 1669:1,
58:1, and 87:1, respectively, while retaining the quality of the generated
image. From our experiments, we observe a qualitative limit for GAN's
compression. Moreover, we observe that, with a fixed parameter budget,
compressed GANs outperform GANs trained using standard training methods. We
conjecture that this is partially owing to the optimization landscape of
over-parameterized GANs which allows efficient training using alternating
gradient descent. Thus, training an over-parameterized GAN followed by our
proposed compression scheme provides a high quality generative model with a
small number of parameters.
",Angeline Aguinaldo; Ping-Yeh Chiang; Alex Gain; Ameya Patil; Kolten Pearson; Soheil Feizi,http://arxiv.org/abs/1902.00159v1,10.48550/arXiv.1902.00159
http://arxiv.org/abs/1902.00369v3,2019,Medical Image Super-Resolution Using a Generative Adversarial Network,"  During the growing popularity of electronic medical records, electronic
medical record (EMR) data has exploded increasingly. It is very meaningful to
retrieve high quality EMR in mass data. In this paper, an EMR value network
with retrieval function is constructed by taking stroke disease as the research
object. It mainly includes: 1) It establishes the electronic medical record
database and corresponding stroke knowledge graph. 2) The strategy of
similarity measurement is included three parts(patients' chief complaint,
pathology results and medical images). Patients' chief complaints are text
data, mainly describing patients' symptoms and expressed in words or phrases,
and patients' chief complaints are input in independent tick of various
symptoms. The data of the pathology results is a structured and digitized
expression, so the input method is the same as the patient's chief complaint;
Image similarity adopts content-based image retrieval(CBIR) technology. 3) The
analytic hierarchy process (AHP) is used to establish the weights of the three
types of data and then synthesize them into an indicator. The accuracy rate of
similarity in top 5 was more than 85\% based on EMR database with more 200
stroke records using leave-one-out method. It will be the good tool for
assistant diagnosis and doctor training, as good quality records are colleted
into the databases, like Doctor Watson, in the future.
",Yongpei Zhu; Xuesheng Zhang; Kehong Yuan,http://arxiv.org/abs/1902.00369v3,10.48550/arXiv.1902.00369
http://arxiv.org/abs/1902.09698v2,2019,"GAN-based Projector for Faster Recovery with Convergence Guarantees in
  Linear Inverse Problems","  A Generative Adversarial Network (GAN) with generator $G$ trained to model
the prior of images has been shown to perform better than sparsity-based
regularizers in ill-posed inverse problems. Here, we propose a new method of
deploying a GAN-based prior to solve linear inverse problems using projected
gradient descent (PGD). Our method learns a network-based projector for use in
the PGD algorithm, eliminating expensive computation of the Jacobian of $G$.
Experiments show that our approach provides a speed-up of $60\text{-}80\times$
over earlier GAN-based recovery methods along with better accuracy. Our main
theoretical result is that if the measurement matrix is moderately conditioned
on the manifold range($G$) and the projector is $\delta$-approximate, then the
algorithm is guaranteed to reach $O(\delta)$ reconstruction error in
$O(log(1/\delta))$ steps in the low noise regime. Additionally, we propose a
fast method to design such measurement matrices for a given $G$. Extensive
experiments demonstrate the efficacy of this method by requiring
$5\text{-}10\times$ fewer measurements than random Gaussian measurement
matrices for comparable recovery performance. Because the learning of the GAN
and projector is decoupled from the measurement operator, our GAN-based
projector and recovery algorithm are applicable without retraining to all
linear inverse problems, as confirmed by experiments on compressed sensing,
super-resolution, and inpainting.
",Ankit Raj; Yuqi Li; Yoram Bresler,http://arxiv.org/abs/1902.09698v2,10.48550/arXiv.1902.09698
http://arxiv.org/abs/1904.10654v2,2019,"Super-Resolved Image Perceptual Quality Improvement via Multi-Feature
  Discriminators","  Generative adversarial network (GAN) for image super-resolution (SR) has
attracted enormous interests in recent years. However, the GAN-based SR methods
only use image discriminator to distinguish SR images and high-resolution (HR)
images. Image discriminator fails to discriminate images accurately since image
features cannot be fully expressed. In this paper, we design a new GAN-based SR
framework GAN-IMC which includes generator, image discriminator, morphological
component discriminator and color discriminator. The combination of multiple
feature discriminators improves the accuracy of image discrimination.
Adversarial training between the generator and multi-feature discriminators
forces SR images to converge with HR images in terms of data and features
distribution. Moreover, in some cases, feature enhancement of salient regions
is also worth considering. GAN-IMC is further optimized by weighted content
loss (GAN-IMCW), which effectively restores and enhances salient regions in SR
images. The effectiveness and robustness of our method are confirmed by
extensive experiments on public datasets. Compared with state-of-the-art
methods, the proposed method not only achieves competitive Perceptual Index
(PI) and Natural Image Quality Evaluator (NIQE) values but also obtains
pleasant visual perception in image edge, texture, color and salient regions.
",Xuan Zhu; Yue Cheng; Jinye Peng; Rongzhi Wang; Mingnan Le; Xin Liu,http://arxiv.org/abs/1904.10654v2,10.1117/1.JEI.29.1.013017
http://arxiv.org/abs/1905.13300v1,2019,Generative Imaging and Image Processing via Generative Encoder,"  This paper introduces a novel generative encoder (GE) model for generative
imaging and image processing with applications in compressed sensing and
imaging, image compression, denoising, inpainting, deblurring, and
super-resolution. The GE model consists of a pre-training phase and a solving
phase. In the pre-training phase, we separately train two deep neural networks:
a generative adversarial network (GAN) with a generator $\G$ that captures the
data distribution of a given image set, and an auto-encoder (AE) network with
an encoder $\EN$ that compresses images following the estimated distribution by
GAN. In the solving phase, given a noisy image $x=\mathcal{P}(x^*)$, where
$x^*$ is the target unknown image, $\mathcal{P}$ is an operator adding an
addictive, or multiplicative, or convolutional noise, or equivalently given
such an image $x$ in the compressed domain, i.e., given $m=\EN(x)$, we solve
the optimization problem
  \[
  z^*=\underset{z}{\mathrm{argmin}} \|\EN(\G(z))-m\|_2^2+\lambda\|z\|_2^2
  \] to recover the image $x^*$ in a generative way via
$\hat{x}:=\G(z^*)\approx x^*$, where $\lambda>0$ is a hyperparameter. The GE
model unifies the generative capacity of GANs and the stability of AEs in an
optimization framework above instead of stacking GANs and AEs into a single
network or combining their loss functions into one as in existing literature.
Numerical experiments show that the proposed model outperforms several
state-of-the-art algorithms.
",Lin Chen; Haizhao Yang,http://arxiv.org/abs/1905.13300v1,10.48550/arXiv.1905.13300
http://arxiv.org/abs/1906.01689v1,2019,A Multi-Pass GAN for Fluid Flow Super-Resolution,"  We propose a novel method to up-sample volumetric functions with generative
neural networks using several orthogonal passes. Our method decomposes
generative problems on Cartesian field functions into multiple smaller
sub-problems that can be learned more efficiently. Specifically, we utilize two
separate generative adversarial networks: the first one up-scales slices which
are parallel to the XY-plane, whereas the second one refines the whole volume
along the Z-axis working on slices in the YZ-plane. In this way, we obtain full
coverage for the 3D target function and can leverage spatio-temporal
supervision with a set of discriminators. Additionally, we demonstrate that our
method can be combined with curriculum learning and progressive growing
approaches. We arrive at a first method that can up-sample volumes by a factor
of eight along each dimension, i.e., increasing the number of degrees of
freedom by 512. Large volumetric up-scaling factors such as this one have
previously not been attainable as the required number of weights in the neural
networks renders adversarial training runs prohibitively difficult. We
demonstrate the generality of our trained networks with a series of comparisons
to previous work, a variety of complex 3D results, and an analysis of the
resulting performance.
",Maximilian Werhahn; You Xie; Mengyu Chu; Nils Thuerey,http://arxiv.org/abs/1906.01689v1,10.1145/3340251
http://arxiv.org/abs/1908.11506v2,2019,"Virtual Thin Slice: 3D Conditional GAN-based Super-resolution for CT
  Slice Interval","  Many CT slice images are stored with large slice intervals to reduce storage
size in clinical practice. This leads to low resolution perpendicular to the
slice images (i.e., z-axis), which is insufficient for 3D visualization or
image analysis. In this paper, we present a novel architecture based on
conditional Generative Adversarial Networks (cGANs) with the goal of generating
high resolution images of main body parts including head, chest, abdomen and
legs. However, GANs are known to have a difficulty with generating a diversity
of patterns due to a phenomena known as mode collapse. To overcome the lack of
generated pattern variety, we propose to condition the discriminator on the
different body parts. Furthermore, our generator networks are extended to be
three dimensional fully convolutional neural networks, allowing for the
generation of high resolution images from arbitrary fields of view. In our
verification tests, we show that the proposed method obtains the best scores by
PSNR/SSIM metrics and Visual Turing Test, allowing for accurate reproduction of
the principle anatomy in high resolution. We expect that the proposed method
contribute to effective utilization of the existing vast amounts of thick CT
images stored in hospitals.
",Akira Kudo; Yoshiro Kitamura; Yuanzhong Li; Satoshi Iizuka; Edgar Simo-Serra,http://arxiv.org/abs/1908.11506v2,10.48550/arXiv.1908.11506
http://arxiv.org/abs/1910.00928v1,2019,Deep learning at scale for subgrid modeling in turbulent flows,"  Modeling of turbulent flows is still challenging. One way to deal with the
large scale separation due to turbulence is to simulate only the large scales
and model the unresolved contributions as done in large-eddy simulation (LES).
This paper focuses on two deep learning (DL) strategies, regression and
reconstruction, which are data-driven and promising alternatives to classical
modeling concepts. Using three-dimensional (3-D) forced turbulence direct
numerical simulation (DNS) data, subgrid models are evaluated, which predict
the unresolved part of quantities based on the resolved solution. For
regression, it is shown that feedforward artificial neural networks (ANNs) are
able to predict the fully-resolved scalar dissipation rate using filtered input
data. It was found that a combination of a large-scale quantity, such as the
filtered passive scalar itself, and a small-scale quantity, such as the
filtered energy dissipation rate, gives the best agreement with the actual DNS
data. Furthermore, a DL network motivated by enhanced super-resolution
generative adversarial networks (ESRGANs) was used to reconstruct
fully-resolved 3-D velocity fields from filtered velocity fields. The energy
spectrum shows very good agreement. As size of scientific data is often in the
order of terabytes or more, DL needs to be combined with high performance
computing (HPC). Necessary code improvements for HPC-DL are discussed with
respect to the supercomputer JURECA. After optimizing the training code, 396.2
TFLOPS were achieved.
",Mathis Bode; Michael Gauding; Konstantin Kleinheinz; Heinz Pitsch,http://arxiv.org/abs/1910.00928v1,10.48550/arXiv.1910.00928
http://arxiv.org/abs/1911.12850v2,2019,Quality analysis of DCGAN-generated mammography lesions,"  Medical image synthesis has gained a great focus recently, especially after
the introduction of Generative Adversarial Networks (GANs). GANs have been used
widely to provide anatomically-plausible and diverse samples for augmentation
and other applications, including segmentation and super resolution. In our
previous work, Deep Convolutional GANs were used to generate synthetic
mammogram lesions, masses mainly, that could enhance the classification
performance in imbalanced datasets. In this new work, a deeper investigation
was carried out to explore other aspects of the generated images evaluation,
i.e., realism, feature space distribution, and observers studies. t-Stochastic
Neighbor Embedding (t-SNE) was used to reduce the dimensionality of real and
fake images to enable 2D visualisations. Additionally, two expert radiologists
performed a realism-evaluation study. Visualisations showed that the generated
images have a similar feature distribution of the real ones, avoiding outliers.
Moreover, Receiver Operating Characteristic (ROC) curve showed that the
radiologists could not, in many cases, distinguish between synthetic and real
lesions, giving 48% and 61% accuracies in a balanced sample set.
",Basel Alyafi; Oliver Diaz; Joan C Vilanova; Javier del Riego; Robert Marti,http://arxiv.org/abs/1911.12850v2,10.48550/arXiv.1911.12850
http://arxiv.org/abs/2001.05447v1,2020,"Segmentation and Generation of Magnetic Resonance Images by Deep Neural
  Networks","  Magnetic Resonance Images (MRIs) are extremely used in the medical field to
detect and better understand diseases. In order to fasten automatic processing
of scans and enhance medical research, this project focuses on automatically
segmenting targeted parts of MRIs and generating new MRI datasets from random
noise. More specifically, a Deep Neural Network architecture called U-net is
used to segment bones and cartilages of Knee MRIs, and several Generative
Adversarial Networks (GANs) are compared and tuned to create new realistic and
high quality brain MRIs that can be used as training set for more advanced
models. Three main architectures are described: Deep Convolution GAN (DCGAN),
Super Resolution Residual GAN (SRResGAN) and Progressive GAN (ProGAN), and five
loss functions are tested: the Original loss, LSGAN, WGAN, WGAN_GP and DRAGAN.
Moreover, a quantitative benchmark is carried out thanks to evaluation measures
using Principal Component Analysis. The results show that U-net can achieve
state-of-the-art performance in segmenting bones and cartilages in Knee MRIs
(Accuracy of more than 99.5%). Moreover, the three GAN architectures can
successfully generate realistic brain MRIs even if some models have
difficulties to converge. The main insights to stabilize the networks are using
one-sided smoothing labels, regularization with gradient penalty in the loss
function (like in WGAN_GP or DRAGAN), adding a minibatch similarity layer in
the Discriminator and a long training time.
",Antoine Delplace,http://arxiv.org/abs/2001.05447v1,10.48550/arXiv.2001.05447
http://arxiv.org/abs/2002.06682v3,2020,Generator From Edges: Reconstruction of Facial Images,"  Applications that involve supervised training require paired images.
Researchers of single image super-resolution (SISR) create such images by
artificially generating blurry input images from the corresponding ground
truth. Similarly we can create paired images with the canny edge. We propose
Generator From Edges (GFE) [Figure 2]. Our aim is to determine the best
architecture for GFE, along with reviews of perceptual loss [1, 2]. To this
end, we conducted three experiments. First, we explored the effects of the
adversarial loss often used in SISR. In particular, we uncovered that it is not
an essential component to form a perceptual loss. Eliminating adversarial loss
will lead to a more effective architecture from the perspective of hardware
resource. It also means that considerations for the problems pertaining to
generative adversarial network (GAN) [3], such as mode collapse, are not
necessary. Second, we reexamined VGG loss and found that the mid-layers yield
the best results. By extracting the full potential of VGG loss, the overall
performance of perceptual loss improves significantly. Third, based on the
findings of the first two experiments, we reevaluated the dense network to
construct GFE. Using GFE as an intermediate process, reconstructing a facial
image from a pencil sketch can become an easy task.
",Nao Takano; Gita Alaghband,http://arxiv.org/abs/2002.06682v3,10.48550/arXiv.2002.06682
http://arxiv.org/abs/2006.16644v1,2020,"Rethinking CNN-Based Pansharpening: Guided Colorization of Panchromatic
  Images via GANs","  Convolutional Neural Networks (CNN)-based approaches have shown promising
results in pansharpening of satellite images in recent years. However, they
still exhibit limitations in producing high-quality pansharpening outputs. To
that end, we propose a new self-supervised learning framework, where we treat
pansharpening as a colorization problem, which brings an entirely novel
perspective and solution to the problem compared to existing methods that base
their solution solely on producing a super-resolution version of the
multispectral image. Whereas CNN-based methods provide a reduced resolution
panchromatic image as input to their model along with reduced resolution
multispectral images, hence learn to increase their resolution together, we
instead provide the grayscale transformed multispectral image as input, and
train our model to learn the colorization of the grayscale input. We further
address the fixed downscale ratio assumption during training, which does not
generalize well to the full-resolution scenario. We introduce a noise injection
into the training by randomly varying the downsampling ratios. Those two
critical changes, along with the addition of adversarial training in the
proposed PanColorization Generative Adversarial Networks (PanColorGAN)
framework, help overcome the spatial detail loss and blur problems that are
observed in CNN-based pansharpening. The proposed approach outperforms the
previous CNN-based and traditional methods as demonstrated in our experiments.
",Furkan Ozcelik; Ugur Alganci; Elif Sertel; Gozde Unal,http://arxiv.org/abs/2006.16644v1,10.1109/TGRS.2020.3010441
http://arxiv.org/abs/2007.12142v2,2020,"PIPAL: a Large-Scale Image Quality Assessment Dataset for Perceptual
  Image Restoration","  Image quality assessment (IQA) is the key factor for the fast development of
image restoration (IR) algorithms. The most recent IR methods based on
Generative Adversarial Networks (GANs) have achieved significant improvement in
visual performance, but also presented great challenges for quantitative
evaluation. Notably, we observe an increasing inconsistency between perceptual
quality and the evaluation results. Then we raise two questions: (1) Can
existing IQA methods objectively evaluate recent IR algorithms? (2) When focus
on beating current benchmarks, are we getting better IR algorithms? To answer
these questions and promote the development of IQA methods, we contribute a
large-scale IQA dataset, called Perceptual Image Processing Algorithms (PIPAL)
dataset. Especially, this dataset includes the results of GAN-based methods,
which are missing in previous datasets. We collect more than 1.13 million human
judgments to assign subjective scores for PIPAL images using the more reliable
""Elo system"". Based on PIPAL, we present new benchmarks for both IQA and
super-resolution methods. Our results indicate that existing IQA methods cannot
fairly evaluate GAN-based IR algorithms. While using appropriate evaluation
methods is important, IQA methods should also be updated along with the
development of IR algorithms. At last, we improve the performance of IQA
networks on GAN-based distortions by introducing anti-aliasing pooling.
Experiments show the effectiveness of the proposed method.
",Jinjin Gu; Haoming Cai; Haoyu Chen; Xiaoxing Ye; Jimmy Ren; Chao Dong,http://arxiv.org/abs/2007.12142v2,10.48550/arXiv.2007.12142
http://arxiv.org/abs/2012.04111v1,2020,"SuperFront: From Low-resolution to High-resolution Frontal Face
  Synthesis","  Advances in face rotation, along with other face-based generative tasks, are
more frequent as we advance further in topics of deep learning. Even as
impressive milestones are achieved in synthesizing faces, the importance of
preserving identity is needed in practice and should not be overlooked. Also,
the difficulty should not be more for data with obscured faces, heavier poses,
and lower quality. Existing methods tend to focus on samples with variation in
pose, but with the assumption data is high in quality. We propose a generative
adversarial network (GAN) -based model to generate high-quality, identity
preserving frontal faces from one or multiple low-resolution (LR) faces with
extreme poses. Specifically, we propose SuperFront-GAN (SF-GAN) to synthesize a
high-resolution (HR), frontal face from one-to-many LR faces with various poses
and with the identity-preserved. We integrate a super-resolution (SR) side-view
module into SF-GAN to preserve identity information and fine details of the
side-views in HR space, which helps model reconstruct high-frequency
information of faces (i.e., periocular, nose, and mouth regions). Moreover,
SF-GAN accepts multiple LR faces as input, and improves each added sample. We
squeeze additional gain in performance with an orthogonal constraint in the
generator to penalize redundant latent representations and, hence, diversify
the learned features space. Quantitative and qualitative results demonstrate
the superiority of SF-GAN over others.
",Yu Yin; Joseph P. Robinson; Songyao Jiang; Yue Bai; Can Qin; Yun Fu,http://arxiv.org/abs/2012.04111v1,10.48550/arXiv.2012.04111
http://arxiv.org/abs/2103.07387v2,2021,"From coarse wall measurements to turbulent velocity fields through deep
  learning","  This work evaluates the applicability of super-resolution generative
adversarial networks (SRGANs) as a methodology for the reconstruction of
turbulent-flow quantities from coarse wall measurements. The method is applied
both for the resolution enhancement of wall fields and the estimation of
wall-parallel velocity fields from coarse wall measurements of shear stress and
pressure. The analysis has been carried out with a database of a turbulent
open-channel flow with friction Reynolds number $Re_{\tau}=180$ generated
through direct numerical simulation. Coarse wall measurements have been
generated with three different downsampling factors $f_d=[4,8,16]$ from the
high-resolution fields, and wall-parallel velocity fields have been
reconstructed at four inner-scaled wall-normal distances $y^+=[15,30,50,100]$.
We first show that SRGAN can be used to enhance the resolution of coarse wall
measurements. If compared with direct reconstruction from the sole coarse wall
measurements, SRGAN provides better instantaneous reconstructions, both in
terms of mean-squared error and spectral-fractional error. Even though lower
resolutions in the input wall data make it more challenging to achieve highly
accurate predictions, the proposed SRGAN-based network yields very good
reconstruction results. Furthermore, it is shown that even for the most
challenging cases the SRGAN is capable of capturing the large-scale structures
that populate the flow. The proposed novel methodology has great potential for
closed-loop control applications relying on non-intrusive sensing.
",Alejandro Güemes; Stefano Discetti; Andrea Ianiro; Beril Sirmacek; Hossein Azizpour; Ricardo Vinuesa,http://arxiv.org/abs/2103.07387v2,10.1063/5.0058346
http://arxiv.org/abs/2107.07397v1,2021,"Level generation and style enhancement -- deep learning for game
  development overview","  We present practical approaches of using deep learning to create and enhance
level maps and textures for video games -- desktop, mobile, and web. We aim to
present new possibilities for game developers and level artists. The task of
designing levels and filling them with details is challenging. It is both
time-consuming and takes effort to make levels rich, complex, and with a
feeling of being natural. Fortunately, recent progress in deep learning
provides new tools to accompany level designers and visual artists. Moreover,
they offer a way to generate infinite worlds for game replayability and adjust
educational games to players' needs. We present seven approaches to create
level maps, each using statistical methods, machine learning, or deep learning.
In particular, we include:
  - Generative Adversarial Networks for creating new images from existing
examples (e.g. ProGAN).
  - Super-resolution techniques for upscaling images while preserving crisp
detail (e.g. ESRGAN).
  - Neural style transfer for changing visual themes.
  - Image translation - turning semantic maps into images (e.g. GauGAN).
  - Semantic segmentation for turning images into semantic masks (e.g. U-Net).
  - Unsupervised semantic segmentation for extracting semantic features (e.g.
Tile2Vec).
  - Texture synthesis - creating large patterns based on a smaller sample (e.g.
InGAN).
",Piotr Migdał; Bartłomiej Olechno; Błażej Podgórski,http://arxiv.org/abs/2107.07397v1,10.48550/arXiv.2107.07397
http://arxiv.org/abs/2108.11100v1,2021,Multi-Attributed and Structured Text-to-Face Synthesis,"  Generative Adversarial Networks (GANs) have revolutionized image synthesis
through many applications like face generation, photograph editing, and image
super-resolution. Image synthesis using GANs has predominantly been uni-modal,
with few approaches that can synthesize images from text or other data modes.
Text-to-image synthesis, especially text-to-face synthesis, has promising use
cases of robust face-generation from eye witness accounts and augmentation of
the reading experience with visual cues. However, only a couple of datasets
provide consolidated face data and textual descriptions for text-to-face
synthesis. Moreover, these textual annotations are less extensive and
descriptive, which reduces the diversity of faces generated from it. This paper
empirically proves that increasing the number of facial attributes in each
textual description helps GANs generate more diverse and real-looking faces. To
prove this, we propose a new methodology that focuses on using structured
textual descriptions. We also consolidate a Multi-Attributed and Structured
Text-to-face (MAST) dataset consisting of high-quality images with structured
textual annotations and make it available to researchers to experiment and
build upon. Lastly, we report benchmark Frechet's Inception Distance (FID),
Facial Semantic Similarity (FSS), and Facial Semantic Distance (FSD) scores for
the MAST dataset.
",Rohan Wadhawan; Tanuj Drall; Shubham Singh; Shampa Chakraverty,http://arxiv.org/abs/2108.11100v1,10.1109/TEMSMET51618.2020.9557583
http://arxiv.org/abs/2201.11998v1,2022,Image Superresolution using Scale-Recurrent Dense Network,"  Recent advances in the design of convolutional neural network (CNN) have
yielded significant improvements in the performance of image super-resolution
(SR). The boost in performance can be attributed to the presence of residual or
dense connections within the intermediate layers of these networks. The
efficient combination of such connections can reduce the number of parameters
drastically while maintaining the restoration quality. In this paper, we
propose a scale recurrent SR architecture built upon units containing series of
dense connections within a residual block (Residual Dense Blocks (RDBs)) that
allow extraction of abundant local features from the image. Our scale recurrent
design delivers competitive performance for higher scale factors while being
parametrically more efficient as compared to current state-of-the-art
approaches. To further improve the performance of our network, we employ
multiple residual connections in intermediate layers (referred to as
Multi-Residual Dense Blocks), which improves gradient propagation in existing
layers. Recent works have discovered that conventional loss functions can guide
a network to produce results which have high PSNRs but are perceptually
inferior. We mitigate this issue by utilizing a Generative Adversarial Network
(GAN) based framework and deep feature (VGG) losses to train our network. We
experimentally demonstrate that different weighted combinations of the VGG loss
and the adversarial loss enable our network outputs to traverse along the
perception-distortion curve. The proposed networks perform favorably against
existing methods, both perceptually and objectively (PSNR-based) with fewer
parameters.
",Kuldeep Purohit; Srimanta Mandal; A. N. Rajagopalan,http://arxiv.org/abs/2201.11998v1,10.48550/arXiv.2201.11998
http://arxiv.org/abs/2202.03046v1,2022,A new face swap method for image and video domains: a technical report,"  Deep fake technology became a hot field of research in the last few years.
Researchers investigate sophisticated Generative Adversarial Networks (GAN),
autoencoders, and other approaches to establish precise and robust algorithms
for face swapping. Achieved results show that the deep fake unsupervised
synthesis task has problems in terms of the visual quality of generated data.
These problems usually lead to high fake detection accuracy when an expert
analyzes them. The first problem is that existing image-to-image approaches do
not consider video domain specificity and frame-by-frame processing leads to
face jittering and other clearly visible distortions. Another problem is the
generated data resolution, which is low for many existing methods due to high
computational complexity. The third problem appears when the source face has
larger proportions (like bigger cheeks), and after replacement it becomes
visible on the face border. Our main goal was to develop such an approach that
could solve these problems and outperform existing solutions on a number of
clue metrics. We introduce a new face swap pipeline that is based on
FaceShifter architecture and fixes the problems stated above. With a new eye
loss function, super-resolution block, and Gaussian-based face mask generation
leads to improvements in quality which is confirmed during evaluation.
",Daniil Chesakov; Anastasia Maltseva; Alexander Groshev; Andrey Kuznetsov; Denis Dimitrov,http://arxiv.org/abs/2202.03046v1,10.48550/arXiv.2202.03046
http://arxiv.org/abs/2202.05311v2,2022,"Mining the manifolds of deep generative models for multiple
  data-consistent solutions of ill-posed tomographic imaging problems","  Tomographic imaging is in general an ill-posed inverse problem. Typically, a
single regularized image estimate of the sought-after object is obtained from
tomographic measurements. However, there may be multiple objects that are all
consistent with the same measurement data. The ability to generate such
alternate solutions is important because it may enable new assessments of
imaging systems. In principle, this can be achieved by means of posterior
sampling methods. In recent years, deep neural networks have been employed for
posterior sampling with promising results. However, such methods are not yet
for use with large-scale tomographic imaging applications. On the other hand,
empirical sampling methods may be computationally feasible for large-scale
imaging systems and enable uncertainty quantification for practical
applications. Empirical sampling involves solving a regularized inverse problem
within a stochastic optimization framework to obtain alternate data-consistent
solutions. In this work, a new empirical sampling method is proposed that
computes multiple solutions of a tomographic inverse problem that are
consistent with the same acquired measurement data. The method operates by
repeatedly solving an optimization problem in the latent space of a style-based
generative adversarial network (StyleGAN), and was inspired by the Photo
Upsampling via Latent Space Exploration (PULSE) method that was developed for
super-resolution tasks. The proposed method is demonstrated and analyzed via
numerical studies that involve two stylized tomographic imaging modalities.
These studies establish the ability of the method to perform efficient
empirical sampling and uncertainty quantification.
",Sayantan Bhadra; Umberto Villa; Mark A. Anastasio,http://arxiv.org/abs/2202.05311v2,10.48550/arXiv.2202.05311
http://arxiv.org/abs/2203.12297v1,2022,"Increasing the accuracy and resolution of precipitation forecasts using
  deep generative models","  Accurately forecasting extreme rainfall is notoriously difficult, but is also
ever more crucial for society as climate change increases the frequency of such
extremes. Global numerical weather prediction models often fail to capture
extremes, and are produced at too low a resolution to be actionable, while
regional, high-resolution models are hugely expensive both in computation and
labour. In this paper we explore the use of deep generative models to
simultaneously correct and downscale (super-resolve) global ensemble forecasts
over the Continental US. Specifically, using fine-grained radar observations as
our ground truth, we train a conditional Generative Adversarial Network --
coined CorrectorGAN -- via a custom training procedure and augmented loss
function, to produce ensembles of high-resolution, bias-corrected forecasts
based on coarse, global precipitation forecasts in addition to other relevant
meteorological fields. Our model outperforms an interpolation baseline, as well
as super-resolution-only and CNN-based univariate methods, and approaches the
performance of an operational regional high-resolution model across an array of
established probabilistic metrics. Crucially, CorrectorGAN, once trained,
produces predictions in seconds on a single machine. These results raise
exciting questions about the necessity of regional models, and whether
data-driven downscaling and correction methods can be transferred to data-poor
regions that so far have had no access to high-resolution forecasts.
",Ilan Price; Stephan Rasp,http://arxiv.org/abs/2203.12297v1,10.48550/arXiv.2203.12297
http://arxiv.org/abs/2204.02028v2,2022,"A Generative Deep Learning Approach to Stochastic Downscaling of
  Precipitation Forecasts","  Despite continuous improvements, precipitation forecasts are still not as
accurate and reliable as those of other meteorological variables. A major
contributing factor to this is that several key processes affecting
precipitation distribution and intensity occur below the resolved scale of
global weather models. Generative adversarial networks (GANs) have been
demonstrated by the computer vision community to be successful at
super-resolution problems, i.e., learning to add fine-scale structure to coarse
images. Leinonen et al. (2020) previously applied a GAN to produce ensembles of
reconstructed high-resolution atmospheric fields, given coarsened input data.
In this paper, we demonstrate this approach can be extended to the more
challenging problem of increasing the accuracy and resolution of comparatively
low-resolution input from a weather forecasting model, using high-resolution
radar measurements as a ""ground truth"". The neural network must learn to add
resolution and structure whilst accounting for non-negligible forecast error.
We show that GANs and VAE-GANs can match the statistical properties of
state-of-the-art pointwise post-processing methods whilst creating
high-resolution, spatially coherent precipitation maps. Our model compares
favourably to the best existing downscaling methods in both pixel-wise and
pooled CRPS scores, power spectrum information and rank histograms (used to
assess calibration). We test our models and show that they perform in a range
of scenarios, including heavy rainfall.
",Lucy Harris; Andrew T. T. McRae; Matthew Chantry; Peter D. Dueben; Tim N. Palmer,http://arxiv.org/abs/2204.02028v2,10.1029/2022MS003120
http://arxiv.org/abs/2205.14377v2,2022,"Enhancing Quality of Pose-varied Face Restoration with Local Weak
  Feature Sensing and GAN Prior","  Facial semantic guidance (including facial landmarks, facial heatmaps, and
facial parsing maps) and facial generative adversarial networks (GAN) prior
have been widely used in blind face restoration (BFR) in recent years. Although
existing BFR methods have achieved good performance in ordinary cases, these
solutions have limited resilience when applied to face images with serious
degradation and pose-varied (e.g., looking right, looking left, laughing, etc.)
in real-world scenarios. In this work, we propose a well-designed blind face
restoration network with generative facial prior. The proposed network is
mainly comprised of an asymmetric codec and a StyleGAN2 prior network. In the
asymmetric codec, we adopt a mixed multi-path residual block (MMRB) to
gradually extract weak texture features of input images, which can better
preserve the original facial features and avoid excessive fantasy. The MMRB can
also be plug-and-play in other networks. Furthermore, thanks to the affluent
and diverse facial priors of the StyleGAN2 model, we adopt a fine-tuned
approach to flexibly restore natural and realistic facial details. Besides, a
novel self-supervised training strategy is specially designed for face
restoration tasks to fit the distribution closer to the target and maintain
training stability. Extensive experiments on both synthetic and real-world
datasets demonstrate that our model achieves superior performance to the prior
art for face restoration and face super-resolution tasks.
",Kai Hu; Yu Liu; Renhe Liu; Wei Lu; Gang Yu; Bin Fu,http://arxiv.org/abs/2205.14377v2,10.48550/arXiv.2205.14377
http://arxiv.org/abs/2206.07388v1,2022,"Subsurface Depths Structure Maps Reconstruction with Generative
  Adversarial Networks","  This paper described a method for reconstruction of detailed-resolution depth
structure maps, usually obtained after the 3D seismic surveys, using the data
from 2D seismic depth maps. The method uses two algorithms based on the
generative-adversarial neural network architecture. The first algorithm
StyleGAN2-ADA accumulates in the hidden space of the neural network the
semantic images of mountainous terrain forms first, and then with help of
transfer learning, in the ideal case - the structure geometry of stratigraphic
horizons. The second algorithm, the Pixel2Style2Pixel encoder, using the
semantic level of generalization of the first algorithm, learns to reconstruct
the original high-resolution images from their degraded copies
(super-resolution technology). There was demonstrated a methodological approach
to transferring knowledge on the structural forms of stratigraphic horizon
boundaries from the well-studied areas to the underexplored ones. Using the
multimodal synthesis of Pixel2Style2Pixel encoder, it is proposed to create a
probabilistic depth space, where each point of the project area is represented
by the density of probabilistic depth distribution of equally probable
reconstructed geological forms of structural images. Assessment of the
reconstruction quality was carried out for two blocks. Using this method,
credible detailed depth reconstructions comparable with the quality of 3D
seismic maps have been obtained from 2D seismic maps.
",Dmitry Ivlev,http://arxiv.org/abs/2206.07388v1,10.48550/arXiv.2206.07388
http://arxiv.org/abs/2207.00808v1,2022,On the modern deep learning approaches for precipitation downscaling,"  Deep Learning (DL) based downscaling has become a popular tool in earth
sciences recently. Increasingly, different DL approaches are being adopted to
downscale coarser precipitation data and generate more accurate and reliable
estimates at local (~few km or even smaller) scales. Despite several studies
adopting dynamical or statistical downscaling of precipitation, the accuracy is
limited by the availability of ground truth. A key challenge to gauge the
accuracy of such methods is to compare the downscaled data to point-scale
observations which are often unavailable at such small scales. In this work, we
carry out the DL-based downscaling to estimate the local precipitation data
from the India Meteorological Department (IMD), which was created by
approximating the value from station location to a grid point. To test the
efficacy of different DL approaches, we apply four different methods of
downscaling and evaluate their performance. The considered approaches are (i)
Deep Statistical Downscaling (DeepSD), augmented Convolutional Long Short Term
Memory (ConvLSTM), fully convolutional network (U-NET), and Super-Resolution
Generative Adversarial Network (SR-GAN). A custom VGG network, used in the
SR-GAN, is developed in this work using precipitation data. The results
indicate that SR-GAN is the best method for precipitation data downscaling. The
downscaled data is validated with precipitation values at IMD station. This DL
method offers a promising alternative to statistical downscaling.
",Bipin Kumar; Kaustubh Atey; Bhupendra Bahadur Singh; Rajib Chattopadhyay; Nachiket Acharya; Manmeet Singh; Ravi S. Nanjundiah; Suryachandra A. Rao,http://arxiv.org/abs/2207.00808v1,10.48550/arXiv.2207.00808
http://arxiv.org/abs/2209.01702v1,2022,"Time-domain speech super-resolution with GAN based modeling for
  telephony speaker verification","  Automatic Speaker Verification (ASV) technology has become commonplace in
virtual assistants. However, its performance suffers when there is a mismatch
between the train and test domains. Mixed bandwidth training, i.e., pooling
training data from both domains, is a preferred choice for developing a
universal model that works for both narrowband and wideband domains. We propose
complementing this technique by performing neural upsampling of narrowband
signals, also known as bandwidth extension. Our main goal is to discover and
analyze high-performing time-domain Generative Adversarial Network (GAN) based
models to improve our downstream state-of-the-art ASV system. We choose GANs
since they (1) are powerful for learning conditional distribution and (2) allow
flexible plug-in usage as a pre-processor during the training of downstream
task (ASV) with data augmentation. Prior works mainly focus on feature-domain
bandwidth extension and limited experimental setups. We address these
limitations by 1) using time-domain extension models, 2) reporting results on
three real test sets, 2) extending training data, and 3) devising new test-time
schemes. We compare supervised (conditional GAN) and unsupervised GANs
(CycleGAN) and demonstrate average relative improvement in Equal Error Rate of
8.6% and 7.7%, respectively. For further analysis, we study changes in
spectrogram visual quality, audio perceptual quality, t-SNE embeddings, and ASV
score distributions. We show that our bandwidth extension leads to phenomena
such as a shift of telephone (test) embeddings towards wideband (train)
signals, a negative correlation of perceptual quality with downstream
performance, and condition-independent score calibration.
",Saurabh Kataria; Jesús Villalba; Laureano Moro-Velázquez; Piotr Żelasko; Najim Dehak,http://arxiv.org/abs/2209.01702v1,10.48550/arXiv.2209.01702
http://arxiv.org/abs/2209.11112v2,2022,CMGAN: Conformer-Based Metric-GAN for Monaural Speech Enhancement,"  Convolution-augmented transformers (Conformers) are recently proposed in
various speech-domain applications, such as automatic speech recognition (ASR)
and speech separation, as they can capture both local and global dependencies.
In this paper, we propose a conformer-based metric generative adversarial
network (CMGAN) for speech enhancement (SE) in the time-frequency (TF) domain.
The generator encodes the magnitude and complex spectrogram information using
two-stage conformer blocks to model both time and frequency dependencies. The
decoder then decouples the estimation into a magnitude mask decoder branch to
filter out unwanted distortions and a complex refinement branch to further
improve the magnitude estimation and implicitly enhance the phase information.
Additionally, we include a metric discriminator to alleviate metric mismatch by
optimizing the generator with respect to a corresponding evaluation score.
Objective and subjective evaluations illustrate that CMGAN is able to show
superior performance compared to state-of-the-art methods in three speech
enhancement tasks (denoising, dereverberation and super-resolution). For
instance, quantitative denoising analysis on Voice Bank+DEMAND dataset
indicates that CMGAN outperforms various previous models with a margin, i.e.,
PESQ of 3.41 and SSNR of 11.10 dB.
",Sherif Abdulatif; Ruizhe Cao; Bin Yang,http://arxiv.org/abs/2209.11112v2,10.48550/arXiv.2209.11112
http://arxiv.org/abs/2210.16219v1,2022,"Applying Physics-Informed Enhanced Super-Resolution Generative
  Adversarial Networks to Finite-Rate-Chemistry Flows and Predicting Lean
  Premixed Gas Turbine Combustors","  The accurate prediction of small scales in underresolved flows is still one
of the main challenges in predictive simulations of complex configurations.
Over the last few years, data-driven modeling has become popular in many fields
as large, often extensively labeled datasets are now available and training of
large neural networks has become possible on graphics processing units (GPUs)
that speed up the learning process tremendously. In fact, the successful
application of deep neural networks in fluid dynamics, such as for
underresolved reactive flows, is still challenging. This work advances the
recently introduced PIESRGAN to reactive finite-rate-chemistry flows. However,
since combustion chemistry typically acts on the smallest scales, the original
approach needs to be extended. Therefore, the modeling approach of PIESRGAN is
modified to accurately account for the challenges in the context of laminar
finite-rate-chemistry flows. The modified PIESRGAN-based model gives good
agreement in a priori and a posteriori tests in a laminar lean premixed
combustion setup. Furthermore, a reduced PIESRGAN-based model is presented that
solves only the major species on a reconstructed field and employs PIERSGAN
lookup for the remaining species, utilizing staggering in time. The advantages
of the discriminator-supported training are shown, and the usability of the new
model demonstrated in the context of a model gas turbine combustor.
",Mathis Bode,http://arxiv.org/abs/2210.16219v1,10.48550/arXiv.2210.16219
http://arxiv.org/abs/2211.16791v1,2022,"Adaptive adversarial training method for improving multi-scale GAN based
  on generalization bound theory","  In recent years, multi-scale generative adversarial networks (GANs) have been
proposed to build generalized image processing models based on single sample.
Constraining on the sample size, multi-scale GANs have much difficulty
converging to the global optimum, which ultimately leads to limitations in
their capabilities. In this paper, we pioneered the introduction of PAC-Bayes
generalized bound theory into the training analysis of specific models under
different adversarial training methods, which can obtain a non-vacuous upper
bound on the generalization error for the specified multi-scale GAN structure.
Based on the drastic changes we found of the generalization error bound under
different adversarial attacks and different training states, we proposed an
adaptive training method which can greatly improve the image manipulation
ability of multi-scale GANs. The final experimental results show that our
adaptive training method in this paper has greatly contributed to the
improvement of the quality of the images generated by multi-scale GANs on
several image manipulation tasks. In particular, for the image super-resolution
restoration task, the multi-scale GAN model trained by the proposed method
achieves a 100% reduction in natural image quality evaluator (NIQE) and a 60%
reduction in root mean squared error (RMSE), which is better than many models
trained on large-scale datasets.
",Jing Tang; Bo Tao; Zeyu Gong; Zhouping Yin,http://arxiv.org/abs/2211.16791v1,10.48550/arXiv.2211.16791
http://arxiv.org/abs/1801.08614v1,2018,"Accurate Weakly Supervised Deep Lesion Segmentation on CT Scans:
  Self-Paced 3D Mask Generation from RECIST","  Volumetric lesion segmentation via medical imaging is a powerful means to
precisely assess multiple time-point lesion/tumor changes. Because manual 3D
segmentation is prohibitively time consuming and requires radiological
experience, current practices rely on an imprecise surrogate called response
evaluation criteria in solid tumors (RECIST). Despite their coarseness, RECIST
marks are commonly found in current hospital picture and archiving systems
(PACS), meaning they can provide a potentially powerful, yet extraordinarily
challenging, source of weak supervision for full 3D segmentation. Toward this
end, we introduce a convolutional neural network based weakly supervised
self-paced segmentation (WSSS) method to 1) generate the initial lesion
segmentation on the axial RECIST-slice; 2) learn the data distribution on
RECIST-slices; 3) adapt to segment the whole volume slice by slice to finally
obtain a volumetric segmentation. In addition, we explore how super-resolution
images (2~5 times beyond the physical CT imaging), generated from a proposed
stacked generative adversarial network, can aid the WSSS performance. We employ
the DeepLesion dataset, a comprehensive CT-image lesion dataset of 32,735
PACS-bookmarked findings, which include lesions, tumors, and lymph nodes of
varying sizes, categories, body regions and surrounding contexts. These are
drawn from 10,594 studies of 4,459 patients. We also validate on a lymph-node
dataset, where 3D ground truth masks are available for all images. For the
DeepLesion dataset, we report mean Dice coefficients of 93% on RECIST-slices
and 76% in 3D lesion volumes. We further validate using a subjective user
study, where an experienced radiologist accepted our WSSS-generated lesion
segmentation results with a high probability of 92.4%.
",Jinzheng Cai; Youbao Tang; Le Lu; Adam P. Harrison; Ke Yan; Jing Xiao; Lin Yang; Ronald M. Summers,http://arxiv.org/abs/1801.08614v1,10.48550/arXiv.1801.08614
http://arxiv.org/abs/2101.06813v1,2021,"Fast and accurate learned multiresolution dynamical downscaling for
  precipitation","  This study develops a neural network-based approach for emulating
high-resolution modeled precipitation data with comparable statistical
properties but at greatly reduced computational cost. The key idea is to use
combination of low- and high- resolution simulations to train a neural network
to map from the former to the latter. Specifically, we define two types of
CNNs, one that stacks variables directly and one that encodes each variable
before stacking, and we train each CNN type both with a conventional loss
function, such as mean square error (MSE), and with a conditional generative
adversarial network (CGAN), for a total of four CNN variants. We compare the
four new CNN-derived high-resolution precipitation results with precipitation
generated from original high resolution simulations, a bilinear interpolater
and the state-of-the-art CNN-based super-resolution (SR) technique. Results
show that the SR technique produces results similar to those of the bilinear
interpolator with smoother spatial and temporal distributions and smaller data
variabilities and extremes than the original high resolution simulations. While
the new CNNs trained by MSE generate better results over some regions than the
interpolator and SR technique do, their predictions are still not as close as
the original high resolution simulations. The CNNs trained by CGAN generate
more realistic and physically reasonable results, better capturing not only
data variability in time and space but also extremes such as intense and
long-lasting storms. The new proposed CNN-based downscaling approach can
downscale precipitation from 50~km to 12~km in 14~min for 30~years once the
network is trained (training takes 4~hours using 1~GPU), while the conventional
dynamical downscaling would take 1~month using 600 CPU cores to generate
simulations at the resolution of 12~km over contiguous United States.
",Jiali Wang; Zhengchun Liu; Ian Foster; Won Chang; Rajkumar Kettimuthu; Rao Kotamarthi,http://arxiv.org/abs/2101.06813v1,10.48550/arXiv.2101.06813
http://arxiv.org/abs/2105.11857v1,2021,"Estimates of maize plant density from UAV RGB images using Faster-RCNN
  detection model: impact of the spatial resolution","  Early-stage plant density is an essential trait that determines the fate of a
genotype under given environmental conditions and management practices. The use
of RGB images taken from UAVs may replace traditional visual counting in fields
with improved throughput, accuracy and access to plant localization. However,
high-resolution (HR) images are required to detect small plants present at
early stages. This study explores the impact of image ground sampling distance
(GSD) on the performances of maize plant detection at 3-5 leaves stage using
Faster-RCNN. Data collected at HR (GSD=0.3cm) over 6 contrasted sites were used
for model training. Two additional sites with images acquired both at high and
low (GSD=0.6cm) resolution were used for model evaluation. Results show that
Faster-RCNN achieved very good plant detection and counting (rRMSE=0.08)
performances when native HR images are used both for training and validation.
Similarly, good performances were observed (rRMSE=0.11) when the model is
trained over synthetic low-resolution (LR) images obtained by down-sampling the
native training HR images, and applied to the synthetic LR validation images.
Conversely, poor performances are obtained when the model is trained on a given
spatial resolution and applied to another spatial resolution. Training on a mix
of HR and LR images allows to get very good performances on the native HR
(rRMSE=0.06) and synthetic LR (rRMSE=0.10) images. However, very low
performances are still observed over the native LR images (rRMSE=0.48), mainly
due to the poor quality of the native LR images. Finally, an advanced
super-resolution method based on GAN (generative adversarial network) that
introduces additional textural information derived from the native HR images
was applied to the native LR validation images. Results show some significant
improvement (rRMSE=0.22) compared to bicubic up-sampling approach.
",Kaaviya Velumani; Raul Lopez-Lozano; Simon Madec; Wei Guo; Joss Gillet; Alexis Comar; Frederic Baret,http://arxiv.org/abs/2105.11857v1,10.34133/2021/9824843
http://arxiv.org/abs/2110.04279v1,2021,"StairwayGraphNet for Inter- and Intra-modality Multi-resolution Brain
  Graph Alignment and Synthesis","  Synthesizing multimodality medical data provides complementary knowledge and
helps doctors make precise clinical decisions. Although promising, existing
multimodal brain graph synthesis frameworks have several limitations. First,
they mainly tackle only one problem (intra- or inter-modality), limiting their
generalizability to synthesizing inter- and intra-modality simultaneously.
Second, while few techniques work on super-resolving low-resolution brain
graphs within a single modality (i.e., intra), inter-modality graph
super-resolution remains unexplored though this would avoid the need for costly
data collection and processing. More importantly, both target and source
domains might have different distributions, which causes a domain fracture
between them. To fill these gaps, we propose a multi-resolution
StairwayGraphNet (SG-Net) framework to jointly infer a target graph modality
based on a given modality and super-resolve brain graphs in both inter and
intra domains. Our SG-Net is grounded in three main contributions: (i)
predicting a target graph from a source one based on a novel graph generative
adversarial network in both inter (e.g., morphological-functional) and intra
(e.g., functional-functional) domains, (ii) generating high-resolution brain
graphs without resorting to the time consuming and expensive MRI processing
steps, and (iii) enforcing the source distribution to match that of the ground
truth graphs using an inter-modality aligner to relax the loss function to
optimize. Moreover, we design a new Ground Truth-Preserving loss function to
guide both generators in learning the topological structure of ground truth
brain graphs more accurately. Our comprehensive experiments on predicting
target brain graphs from source graphs using a multi-resolution stairway showed
the outperformance of our method in comparison with its variants and
state-of-the-art method.
",Islem Mhiri; Mohamed Ali Mahjoub; Islem Rekik,http://arxiv.org/abs/2110.04279v1,10.48550/arXiv.2110.04279
